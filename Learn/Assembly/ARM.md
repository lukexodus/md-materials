# Syllabus

## Module 1: Foundations

- Computer architecture fundamentals
- Number systems and binary representation
- Data types and sizes
- Memory organization and addressing
- ARM architecture overview and history
- ARM processor families and variants
- Registers and register conventions
- Instruction set architecture concepts

## Module 2: Development Environment

- Toolchain installation and setup
- Assemblers (GNU AS, ARM assembler)
- Linkers and linking process
- Debuggers (GDB, LLDB)
- Emulators and simulators (QEMU)
- Cross-compilation basics
- Build systems and makefiles
- Integrated development environments

## Module 3: Basic Instructions

- Data movement instructions (MOV, LDR, STR)
- Immediate values and operands
- Arithmetic instructions (ADD, SUB, MUL, DIV)
- Logical instructions (AND, OR, XOR, NOT)
- Shift and rotate operations
- Comparison instructions (CMP, TST)
- Instruction encoding and formats
- Instruction cycle and execution

## Module 4: Memory and Addressing

- Load and store architecture
- Addressing modes (immediate, register, indexed)
- Pre-indexed and post-indexed addressing
- Offset addressing
- Stack operations
- Heap and static memory
- Memory alignment requirements
- Endianness (little-endian vs big-endian)

## Module 5: Control Flow

- Conditional execution
- Condition flags (N, Z, C, V)
- Branch instructions (B, BL, BX)
- Conditional branches
- Loops (for, while, do-while patterns)
- Switch/case implementations
- Function calls and returns
- Jump tables

## Module 6: Functions and Procedures

- ARM Procedure Call Standard (AAPCS)
- Stack frames
- Parameter passing conventions
- Return values
- Caller-saved vs callee-saved registers
- Leaf and non-leaf functions
- Nested function calls
- Recursion implementation

## Module 7: Advanced Data Operations

- Multi-register operations (LDM, STM)
- Block data transfers
- Byte and halfword operations
- Sign extension and zero extension
- Bit field manipulation
- Saturation arithmetic
- SIMD instructions basics
- Packed data operations

## Module 8: Optimization Techniques

- Instruction scheduling
- Loop unrolling
- Software pipelining
- Register allocation strategies
- Strength reduction
- Common subexpression elimination
- Branch prediction considerations
- Cache-aware programming

## Module 9: ARM Thumb and Thumb-2

- Thumb instruction set overview
- 16-bit instruction encoding
- Thumb-2 mixed 16/32-bit instructions
- Mode switching (ARM/Thumb interworking)
- Code density benefits
- Performance tradeoffs
- IT blocks (If-Then)
- Thumb-2 exclusive instructions

## Module 10: Floating Point (VFP/NEON)

- VFP architecture and registers
- Single and double precision operations
- Floating point arithmetic
- Conversions between integer and float
- NEON SIMD architecture
- Vector registers
- Parallel data processing
- NEON intrinsics vs assembly

## Module 11: System Programming

- Processor modes (User, FIQ, IRQ, Supervisor, etc.)
- Mode switching
- Exception handling
- Interrupt processing
- System calls
- Memory management unit (MMU) basics
- Cache control
- Coprocessor instructions

## Module 12: ARM64 (AArch64)

- 64-bit architecture differences
- Extended register set (X0-X30)
- New instruction encodings
- AArch64 calling conventions
- Addressing mode changes
- Removed features from AArch32
- Performance improvements
- Migration considerations

## Module 13: Interfacing with High-Level Languages

- Calling C from assembly
- Calling assembly from C
- Inline assembly
- Mixed-language linking
- Name mangling
- ABI compatibility
- Volatile registers and clobbering
- Structure passing

## Module 14: Hardware Interfacing

- Memory-mapped I/O
- GPIO programming
- Peripheral access
- Device registers
- Bit-banging protocols
- Timer programming
- UART communication
- Interrupt controllers

## Module 15: Embedded Systems

- Bare-metal programming
- Boot process and startup code
- Linker scripts
- Vector tables
- Real-time constraints
- Power management
- Watchdog timers
- DMA operations

## Module 16: Security Features

- ARM TrustZone basics
- Secure and non-secure states
- Cryptography instructions
- Memory protection
- Privilege levels
- Secure boot concepts
- Side-channel considerations
- Pointer authentication (ARMv8.3+)

## Module 17: Debugging and Profiling

- Debug symbols and DWARF
- Breakpoints and watchpoints
- Single-stepping
- Register and memory inspection
- Core dumps
- Performance counters
- Profiling techniques
- Trace analysis

## Module 18: Real-World Projects

- Bootloader development
- Operating system kernel modules
- Device drivers
- DSP algorithms
- Graphics primitives
- Cryptographic implementations
- Network packet processing
- Game engine components

---

# Foundations

## Computer Architecture Fundamentals

Computer architecture defines the structural and functional organization of computer systems. ARM architecture follows a specific design philosophy that influences how programs execute at the hardware level.

### Processor Components

The processor contains several critical components that work together to execute instructions. The **Arithmetic Logic Unit (ALU)** performs mathematical and logical operations on data. The **Control Unit** decodes instructions and generates control signals to coordinate other components. **Registers** provide fast storage locations directly accessible by the processor, holding operands, addresses, and intermediate results.

### ARM Architecture Philosophy

ARM processors use a **Reduced Instruction Set Computer (RISC)** design philosophy. This approach emphasizes a smaller number of simple instructions that execute in a single cycle, contrasting with **Complex Instruction Set Computer (CISC)** designs that offer more complex instructions requiring multiple cycles.

The ARM architecture features a **load-store architecture**, meaning arithmetic operations only work on register operands. Data must be explicitly loaded from memory into registers before processing, and results must be stored back to memory through dedicated instructions.

### Instruction Execution Cycle

The processor executes instructions through a repeating cycle. During **fetch**, the processor retrieves the instruction from memory at the address stored in the Program Counter (PC). In the **decode** phase, the control unit interprets the instruction and identifies required operands. The **execute** phase performs the actual operation using the ALU or other functional units. Finally, **writeback** stores the result in the destination register or memory location.

Modern ARM processors implement **pipelining**, where multiple instructions occupy different stages simultaneously, increasing throughput. [Inference] This allows the processor to begin fetching the next instruction while still executing the current one.

### Register-Based Computation

ARM processors expose a set of general-purpose registers directly to the programmer. In ARMv7 (32-bit), there are 16 general-purpose registers (R0-R15), with R13 serving as the Stack Pointer (SP), R14 as the Link Register (LR), and R15 as the Program Counter (PC). In ARMv8 (64-bit), there are 31 general-purpose registers (X0-X30) plus a zero register and stack pointer.

Operations occur primarily in registers because register access is significantly faster than memory access. The limited number of registers makes efficient register allocation critical for performance.

### Memory Hierarchy

Computer systems organize memory in a hierarchy based on speed and capacity tradeoffs. Registers sit at the top, offering the fastest access but smallest capacity. **Cache memory** (L1, L2, L3) provides faster access than main memory by storing frequently accessed data. **Main memory (RAM)** offers larger capacity with slower access times. **Secondary storage** (disks, SSDs) provides persistent storage with the highest capacity and slowest access.

The processor automatically manages cache, but understanding memory hierarchy helps optimize code by improving **locality of reference**—accessing nearby memory locations in time (temporal locality) or space (spatial locality).

### Endianness

ARM processors support both **little-endian** and **big-endian** byte ordering, though little-endian is more common. In little-endian format, the least significant byte occupies the lowest memory address. For a 32-bit value 0x12345678 at address 0x1000:

Little-endian stores: 0x1000: 0x78, 0x1001: 0x56, 0x1002: 0x34, 0x1003: 0x12

Big-endian stores: 0x1000: 0x12, 0x1001: 0x34, 0x1002: 0x56, 0x1003: 0x78

## Number Systems and Binary Representation

Assembly programming requires understanding how numbers are represented in binary at the hardware level.

### Positional Number Systems

Decimal (base-10) uses digits 0-9 with each position representing a power of 10. Binary (base-2) uses digits 0-1 with each position representing a power of 2. Hexadecimal (base-16) uses digits 0-9 and A-F, with each position representing a power of 16.

The decimal number 157 converts to binary as: 157 = 128 + 16 + 8 + 4 + 1 = 2^7 + 2^4 + 2^3 + 2^2 + 2^0 = 10011101₂

In hexadecimal: 157 = 9×16 + 13 = 0x9D

Hexadecimal provides a compact representation where each hex digit corresponds to exactly 4 binary bits, making it convenient for expressing binary patterns.

### Binary Arithmetic

Binary addition follows the same principles as decimal addition with carry propagation:

```
  1011 (11)
+ 0110 (6)
------
 10001 (17)
```

Binary subtraction can be performed directly or through addition of the negative representation. Multiplication and division follow standard algorithms but operate in base-2.

### Unsigned Integer Representation

Unsigned integers represent only non-negative values using straightforward binary encoding. An n-bit unsigned integer can represent values from 0 to 2^n - 1.

For 8-bit unsigned:

- Minimum: 00000000₂ = 0
- Maximum: 11111111₂ = 255

For 32-bit unsigned:

- Range: 0 to 4,294,967,295

### Signed Integer Representation

Signed integers must represent both positive and negative values. ARM processors use **two's complement** representation, which has several mathematical advantages.

In two's complement, the most significant bit (MSB) serves as the sign bit. Positive numbers have MSB=0 and use standard binary representation. Negative numbers have MSB=1 and are formed by inverting all bits of the absolute value and adding 1.

For 8-bit two's complement:

- Range: -128 to +127
- +5: 00000101
- -5: 11111011 (invert 00000101 → 11111010, add 1 → 11111011)

For 32-bit two's complement:

- Range: -2,147,483,648 to +2,147,483,647

Two's complement allows the same hardware adder to perform both addition and subtraction. Adding a negative number (in two's complement) produces the correct result without special circuitry.

### Sign Extension

When converting a smaller signed value to a larger representation, **sign extension** replicates the sign bit into the additional high-order bits, preserving the value.

Converting 8-bit -5 (11111011) to 16-bit: 11111011 → 11111111 11111011

Converting 8-bit +5 (00000101) to 16-bit: 00000101 → 00000000 00000101

ARM provides specific instructions (SXTB, SXTH for signed; UXTB, UXTH for unsigned) for these operations.

### Floating-Point Representation

ARM processors implement IEEE 754 floating-point standard for representing real numbers. A floating-point number consists of three components: sign bit, exponent, and mantissa (fraction).

Single-precision (32-bit) format:

- 1 sign bit
- 8 exponent bits (biased by 127)
- 23 mantissa bits (with implicit leading 1)

Double-precision (64-bit) format:

- 1 sign bit
- 11 exponent bits (biased by 1023)
- 52 mantissa bits (with implicit leading 1)

The value represented is: (-1)^sign × 1.mantissa × 2^(exponent - bias)

[Inference] This representation allows for a wide range of magnitudes but with variable precision—larger numbers have less precision in the least significant digits.

### Bitwise Operations

Binary representations enable efficient bitwise operations that manipulate individual bits. **AND** produces 1 only when both bits are 1. **OR** produces 1 when at least one bit is 1. **XOR** produces 1 when bits differ. **NOT** inverts all bits.

These operations are fundamental for:

- Masking specific bits: `value & 0x0F` extracts lower 4 bits
- Setting bits: `value | 0x80` sets bit 7
- Clearing bits: `value & ~0x80` clears bit 7
- Toggling bits: `value ^ 0x80` toggles bit 7

### Bit Shifting

**Logical shift left (LSL)** shifts bits leftward, filling vacated positions with zeros. Each left shift by one position multiplies the value by 2.

**Logical shift right (LSR)** shifts bits rightward, filling vacated positions with zeros. Each right shift by one position divides unsigned values by 2.

**Arithmetic shift right (ASR)** shifts bits rightward but preserves the sign bit by replicating it, correctly dividing signed values by 2.

**Rotate right (ROR)** shifts bits rightward with bits shifted out re-entering from the left.

## Data Types and Sizes

ARM assembly operates on data at various granularities, each with specific size and alignment requirements.

### Fundamental Data Sizes

**Byte**: 8 bits, representing values 0-255 (unsigned) or -128 to +127 (signed). ARM uses 'B' suffix in instructions (LDRB, STRB).

**Halfword**: 16 bits (2 bytes), representing values 0-65535 (unsigned) or -32768 to +32767 (signed). ARM uses 'H' suffix in instructions (LDRH, STRH).

**Word**: 32 bits (4 bytes) in ARMv7, representing values 0 to 2^32-1 (unsigned) or -2^31 to 2^31-1 (signed). Default size for most ARM 32-bit operations (LDR, STR).

**Doubleword**: 64 bits (8 bytes), used in ARMv8 64-bit mode and for double-precision floating-point. ARM uses 'D' suffix or 64-bit registers.

### ARMv7 (32-bit) Data Types

In 32-bit ARM architecture, registers are 32 bits wide. Operations default to word size unless specified otherwise with instruction suffixes.

Integer types:

- `char`: typically 8 bits
- `short`: typically 16 bits
- `int`: typically 32 bits
- `long`: typically 32 bits (platform-dependent)
- `long long`: typically 64 bits

Pointer size: 32 bits (can address 4GB of memory)

### ARMv8 (64-bit) Data Types

In 64-bit ARM architecture (AArch64), registers can operate as 32-bit (W registers) or 64-bit (X registers).

Integer types:

- `char`: typically 8 bits
- `short`: typically 16 bits
- `int`: typically 32 bits
- `long`: typically 64 bits (platform-dependent)
- `long long`: typically 64 bits

Pointer size: 64 bits (theoretical 16 exabyte address space, though practical implementations use fewer bits)

### Floating-Point Types

ARM processors with floating-point units support IEEE 754 formats:

**Single-precision (float)**: 32 bits, provides approximately 7 decimal digits of precision. Range approximately ±10^±38. Uses S registers in ARM.

**Double-precision (double)**: 64 bits, provides approximately 16 decimal digits of precision. Range approximately ±10^±308. Uses D registers in ARM.

The VFP (Vector Floating Point) and NEON extensions provide dedicated registers and instructions for floating-point operations.

### SIMD Data Types

ARM NEON extension supports **Single Instruction Multiple Data (SIMD)** operations, processing multiple data elements simultaneously. NEON registers (64-bit or 128-bit) can be interpreted as vectors of smaller elements.

A 128-bit NEON register can hold:

- 16 × 8-bit integers
- 8 × 16-bit integers
- 4 × 32-bit integers or floats
- 2 × 64-bit integers or doubles

This allows parallel processing of multiple values with a single instruction, beneficial for multimedia and signal processing applications.

### Alignment Requirements

ARM processors impose alignment requirements for optimal performance and, in some modes, correctness.

**Natural alignment**: Data should be aligned to addresses that are multiples of their size:

- Bytes: any address (1-byte aligned)
- Halfwords: even addresses (2-byte aligned)
- Words: addresses divisible by 4 (4-byte aligned)
- Doublewords: addresses divisible by 8 (8-byte aligned)

[Unverified] Unaligned access may cause performance penalties or, in some ARM configurations, trigger alignment faults that crash the program. ARMv7 and later generally support unaligned access in specific modes, but aligned access is always faster.

### Structure and Array Layout

Composite data types have specific memory layouts. Structures place members sequentially in memory, potentially with padding bytes to maintain alignment requirements.

**Example** structure:

```c
struct Example {
    char a;      // 1 byte at offset 0
    // 3 bytes padding
    int b;       // 4 bytes at offset 4
    short c;     // 2 bytes at offset 8
    // 2 bytes padding
};  // total size: 12 bytes
```

Arrays store elements contiguously without padding between elements. A word array at address 0x1000 has elements at 0x1000, 0x1004, 0x1008, etc.

### Type Conversions

Converting between data types requires careful handling to preserve or appropriately modify values.

**Narrowing conversion** (larger to smaller) truncates high-order bits:

- 32-bit 0x12345678 → 8-bit 0x78

**Widening conversion** (smaller to larger) requires either zero extension for unsigned or sign extension for signed values:

- Unsigned: 8-bit 0xFF → 32-bit 0x000000FF
- Signed: 8-bit 0xFF (-1) → 32-bit 0xFFFFFFFF (-1)

## Memory Organization and Addressing

Understanding memory organization is essential for effective assembly programming, as the programmer directly controls memory access.

### Memory Address Space

Memory is organized as a linear array of bytes, each with a unique address. In 32-bit ARM, addresses range from 0x00000000 to 0xFFFFFFFF (4GB). In 64-bit ARM, the address space is theoretically much larger, though practical implementations use fewer address bits.

Addresses are typically expressed in hexadecimal for readability. Memory access uses byte addressing—each address refers to one byte—but ARM can access larger units (halfwords, words) from aligned addresses.

### Memory Layout Segments

Programs organize memory into distinct segments with different purposes:

**Text Segment (Code Segment)**: Contains executable instructions. Typically marked read-only to prevent accidental modification. Located at lower addresses in many systems.

**Data Segment**: Contains initialized global and static variables. Includes:

- **.data**: Initialized writable data
- **.rodata**: Read-only initialized data (constants)

**BSS Segment**: Contains uninitialized global and static variables, automatically zeroed at program startup. Name derives from "Block Started by Symbol."

**Heap**: Dynamic memory region that grows upward (toward higher addresses). Managed through allocation functions. Programmer controls allocation and deallocation.

**Stack**: Automatic memory region that grows downward (toward lower addresses). Stores local variables, function parameters, return addresses, and saved registers. Managed automatically by function call/return mechanisms.

The heap and stack grow toward each other, and exhausting the space between them causes stack overflow or heap exhaustion.

### Stack Organization

The stack operates as a Last-In-First-Out (LIFO) data structure. The **Stack Pointer (SP)** register points to the current top of the stack.

In ARM, the stack typically uses a **full descending** model:

- Full: SP points to the last occupied location
- Descending: Stack grows toward lower addresses

Pushing a value: decrement SP, then store value at SP Popping a value: load value from SP, then increment SP

ARM provides PUSH and POP instructions that handle multiple registers efficiently:

```
PUSH {r4-r7, lr}  @ Save registers on stack
POP {r4-r7, pc}   @ Restore registers from stack
```

### Stack Frames

Function calls create **stack frames** (activation records) containing:

- Function parameters beyond those passed in registers
- Return address (link register value)
- Saved register values
- Local variables

The **Frame Pointer (FP)**, often R11 in ARM, optionally points to a fixed location within the frame, simplifying access to local variables and parameters when stack size varies.

### Addressing Modes

ARM provides several addressing modes for memory access, specifying how to calculate the effective address:

**Immediate Offset**: Address = base register ± immediate constant

```
LDR r0, [r1, #4]    @ Load from address (r1 + 4)
```

**Register Offset**: Address = base register ± offset register

```
LDR r0, [r1, r2]    @ Load from address (r1 + r2)
```

**Scaled Register Offset**: Address = base register ± (offset register × scale)

```
LDR r0, [r1, r2, LSL #2]  @ Load from address (r1 + r2×4)
```

**Pre-indexed**: Calculate address, access memory, update base register

```
LDR r0, [r1, #4]!   @ Load from (r1+4), then r1 = r1+4
```

**Post-indexed**: Access memory at base register, then update base register

```
LDR r0, [r1], #4    @ Load from r1, then r1 = r1+4
```

Pre-indexed and post-indexed modes are efficient for array traversal and stack operations.

### Cache Considerations

Modern ARM processors include cache hierarchies that automatically store copies of frequently accessed memory. Understanding cache behavior helps optimize code:

**Spatial Locality**: Accessing nearby memory locations benefits from cache lines (typically 32-64 bytes) that load multiple adjacent bytes together. Sequential memory access patterns are cache-friendly.

**Temporal Locality**: Reusing recently accessed data finds it in cache. Organizing code to reuse data while still cached improves performance.

[Inference] Pointer chasing (following pointers through memory) often exhibits poor cache performance because each access depends on the previous one, and addresses may be scattered unpredictably.

### Memory-Mapped I/O

Peripheral devices often appear as memory locations in the address space. Reading or writing specific addresses interacts with hardware rather than actual memory.

**Example** accessing a memory-mapped register at 0x40000000:

```
LDR r1, =0x40000000  @ Load peripheral base address
LDR r0, [r1]         @ Read from peripheral
ORR r0, r0, #0x01    @ Modify value
STR r0, [r1]         @ Write back to peripheral
```

Memory-mapped I/O addresses may require special considerations like volatile access semantics and memory barriers to ensure correct ordering of operations.

### Memory Protection

Modern ARM processors include Memory Management Units (MMUs) that provide:

**Virtual Memory**: Programs use virtual addresses translated by the MMU to physical addresses. This allows memory isolation between processes and enables features like demand paging.

**Access Permissions**: Memory regions can be marked read-only, read-write, or execute-only. Attempts to violate permissions trigger exceptions.

**Memory Attributes**: Regions can be marked as cacheable, bufferable, or requiring specific ordering guarantees, critical for device memory.

These features are typically managed by operating systems, but understanding them helps debug issues and optimize performance-critical code.

**Key Points**

Understanding computer architecture fundamentals, number systems, data types, and memory organization forms the foundation for effective ARM assembly programming. The RISC philosophy of ARM emphasizes simple instructions operating on registers, with explicit load-store operations for memory access. Binary representation using two's complement enables efficient arithmetic with unified hardware. Data types range from 8-bit bytes to 64-bit doublewords, each with specific alignment requirements. Memory organizes into segments including code, data, heap, and stack, with various addressing modes providing flexible access patterns. These concepts interconnect—for instance, understanding two's complement explains why the same addition hardware works for both positive and negative numbers, and recognizing alignment requirements prevents performance penalties or faults when accessing memory.

---

## ARM Architecture Overview and History

ARM (Advanced RISC Machine, originally Acorn RISC Machine) represents a family of reduced instruction set computing (RISC) architectures developed by ARM Holdings. The architecture originated in 1983 at Acorn Computers in Cambridge, England, where engineers Sophie Wilson and Steve Furber designed the first ARM processor for the BBC Micro computer.

The original ARM1 prototype appeared in 1985, followed by ARM2 in 1986, which became the first commercial ARM processor used in the Acorn Archimedes personal computer. ARM3, released in 1989, introduced the first integrated cache. In 1990, Acorn spun off ARM as a separate company (Advanced RISC Machines Ltd.) adopting a licensing business model rather than manufacturing chips directly.

This licensing model became ARM's defining characteristic. ARM designs processor architectures and licenses the intellectual property to semiconductor companies who manufacture the actual chips. This approach enabled widespread adoption across mobile devices, embedded systems, and increasingly servers and desktop computers.

The architecture evolved through several major generations. ARMv4 introduced the Thumb instruction set (16-bit instructions). ARMv5 added improved interworking between ARM and Thumb states. ARMv6 brought SIMD extensions and multiprocessor support. ARMv7 split into three profiles: ARMv7-A (Application, for complex operating systems), ARMv7-R (Real-time, for embedded systems), and ARMv7-M (Microcontroller, for deeply embedded systems).

ARMv8 marked a fundamental transition by introducing 64-bit computing support (AArch64 execution state) while maintaining backward compatibility with 32-bit code (AArch32 execution state). This architecture, announced in 2011 and first implemented in Apple's A7 chip (2013), represents the foundation of modern ARM processors. Subsequent versions (ARMv8.1 through ARMv8.6, and ARMv9 announced in 2021) added incremental features like enhanced virtualization, security extensions, scalable vector extensions (SVE), and machine learning capabilities.

ARM's RISC philosophy emphasizes simplified instructions that execute in a single cycle, load/store architecture where only specific instructions access memory, large register files to minimize memory access, and fixed-length instructions (though Thumb provides variable-length encoding for code density). This design prioritizes power efficiency and scalability, making ARM dominant in battery-powered devices where energy consumption determines usability.

The architecture's impact extends beyond mobile phones and tablets. ARM processors power embedded systems in automotive electronics, industrial controllers, IoT devices, networking equipment, and storage systems. Recent developments include ARM-based servers competing with x86 in data centers, and Apple's transition of Mac computers to custom ARM-based Apple Silicon processors, demonstrating ARM's capability in high-performance computing scenarios.

## ARM Processor Families and Variants

ARM processor families organize into distinct series, each targeting specific market segments and performance requirements. Understanding these families clarifies the architectural variations and implementation choices available to system designers.

**Classic ARM Cores (ARM1-ARM11)** represent the earliest generations. ARM7TDMI became widely adopted in early mobile phones and embedded systems, implementing ARMv4T with Thumb support. ARM9 families (ARM9TDMI, ARM926EJ-S) added five-stage pipelines and enhanced performance for feature phones and basic smartphones. ARM11 cores (ARM1136, ARM1176) extended to ARMv6 with SIMD instructions and cache improvements, appearing in devices like early Raspberry Pi models and original iPhone.

**Cortex-A Series (Application Processors)** targets devices running complex operating systems like Linux, Android, iOS, and Windows. These processors implement ARMv7-A or ARMv8-A/ARMv9-A architectures with features supporting virtual memory, multiple privilege levels, and sophisticated cache hierarchies.

Cortex-A5 through A17 represent 32-bit cores with varying performance points. Cortex-A7 prioritized energy efficiency for entry-level smartphones. Cortex-A9 and A15 offered higher performance with out-of-order execution and multi-core configurations. Cortex-A53 and A57 introduced 64-bit computing as the first ARMv8-A implementations, often paired in big.LITTLE configurations where efficient A53 cores handle light workloads while powerful A57 cores activate for demanding tasks.

Modern Cortex-A cores (A55, A65, A76, A77, A78, A710, A715, A720) continue evolving with deeper pipelines, wider execution units, larger caches, and enhanced instruction sets. Cortex-A510/A710/A715 reflect ARM's DynamIQ architecture enabling heterogeneous computing with different core types in a single cluster. Cortex-X series (X1, X2, X3, X4) provides maximum performance variants optimized for flagship devices where power consumption constraints relax compared to efficiency cores.

**Cortex-R Series (Real-Time Processors)** addresses deterministic embedded systems requiring guaranteed response times. These implement ARMv7-R or ARMv8-R with tightly-coupled memory, error correction, and memory protection units without virtual memory's unpredictability. Applications include automotive systems (engine control, brake systems), industrial controllers, medical devices, and storage controllers where timing predictability matters more than maximum throughput.

Cortex-R4, R5, R7, R8, R52 represent successive generations with increasing performance and safety features. Cortex-R5 particularly dominates automotive applications with dual-core lockstep configurations detecting hardware faults. Cortex-R52 brought ARMv8-R architecture with optional 64-bit support and enhanced virtualization for next-generation real-time systems.

**Cortex-M Series (Microcontroller Processors)** serves deeply embedded applications where cost, power, and deterministic interrupt response dominate requirements. These implement ARMv6-M, ARMv7-M, ARMv8-M, or ARMv8.1-M architectures with simplified programmer's models, single-cycle multiply, hardware divide, and bit-banding capabilities.

Cortex-M0 and M0+ provide minimal-cost implementations for simple control tasks, replacing 8-bit and 16-bit microcontrollers. Cortex-M3 added Thumb-2 instruction set for improved code density while maintaining deterministic behavior. Cortex-M4 extended with digital signal processing instructions and optional floating-point unit, targeting sensor fusion and audio processing. Cortex-M7 achieved substantially higher performance with caches and branch prediction while preserving deterministic interrupt latency. Cortex-M23 and M33 introduced ARMv8-M with TrustZone security for IoT devices. Cortex-M55 and M85 brought ARMv8.1-M with Helium vector processing for ML inference at the edge.

**Specialized Variants** include Cortex-A32 (smallest 64-bit application processor), SecurCore (security-focused cores for smart cards and SIM cards), and custom implementations by licensees. Architecture licenses permit companies to design custom cores implementing ARM instruction sets. Apple designs custom cores (Avalanche, Blizzard, Firestorm, Icestorm) for iPhone and Mac. Qualcomm developed Kryo cores. Samsung created Mongoose/Exynos-M cores. These custom designs differentiate performance while maintaining software compatibility.

**Neoverse Series** targets infrastructure computing including servers, networking, and edge computing. Neoverse-N cores (N1, N2) balance performance and efficiency for scale-out workloads. Neoverse-V cores (V1, V2) maximize single-thread performance for compute-intensive applications. These implement ARMv8.2-A through ARMv9-A with features like scalable vector extensions (SVE/SVE2) for high-performance computing.

## Registers and Register Conventions

ARM processors provide general-purpose registers for computation, special-purpose registers for processor control, and system registers for privileged operations. The register organization differs between 32-bit (AArch32) and 64-bit (AArch64) execution states.

### AArch64 Register Organization

AArch64 provides thirty-one 64-bit general-purpose registers named X0 through X30, plus a zero register (XZR) and stack pointer (SP). Each 64-bit register can also be accessed as a 32-bit register using W naming (W0-W30, WZR). When accessing the 32-bit form, the upper 32 bits are typically zero-extended (though this behavior depends on the specific instruction).

**X0-X7** serve as argument registers for function calls and return values. X0 holds the first argument and primary return value. X1-X7 hold subsequent arguments (second through eighth). Functions returning small structures use multiple registers starting from X0. Large structures use pointer passing.

**X8** serves as the indirect result location register. When a function returns a structure too large for registers, the caller allocates space and passes the address in X8. The callee writes the result to this location.

**X9-X15** function as temporary registers (caller-saved). Functions may freely modify these without preservation. Callers expecting to use values across function calls must save them explicitly.

**X16-X17** (IP0, IP1) are intra-procedure-call temporary registers. Linkers and dynamic linkers may use these for veneers and trampolines during function calls. Normal application code treats them as additional temporary registers.

**X18** has platform-specific usage. Some platforms reserve it as a platform register for special purposes (like accessing thread-local storage or shadow call stacks). Other platforms treat it as an additional temporary register. [Inference: Code portability benefits from avoiding X18 unless platform conventions are well-understood.]

**X19-X28** serve as callee-saved registers. Functions must preserve these values if they modify them, typically by saving to the stack in the function prologue and restoring in the epilogue. Callers can rely on these registers maintaining values across function calls.

**X29** (FP) serves as the frame pointer in debugging builds or when required by calling conventions. It points to the previous frame pointer on the stack, creating a linked list of stack frames for debugger stack unwinding. Optimized builds may omit frame pointers and repurpose X29 as a general callee-saved register.

**X30** (LR) holds the link register containing the return address. The BL (branch with link) instruction automatically stores the return address here. Functions typically save LR to the stack if they call other functions, since nested calls would overwrite it.

**SP** (Stack Pointer) maintains the current stack position. The stack grows downward (toward lower addresses) on ARM. Instructions cannot use SP and XZR interchangeably; the encoding determines which applies based on context.

**XZR/WZR** (Zero Register) reads as zero and discards writes. This simplifies certain operations like testing values (by subtracting from zero) or clearing registers (by moving zero).

**PC** (Program Counter) is not directly accessible as a general-purpose register in AArch64 (unlike AArch32). Instruction encoding no longer permits reading PC directly, improving pipeline efficiency. PC-relative addressing remains available through specific instruction forms.

### Vector and Floating-Point Registers

AArch64 provides thirty-two 128-bit SIMD and floating-point registers named V0-V31. These support multiple access modes:

**B0-B31** access the lowest 8 bits (byte)
**H0-H31** access the lowest 16 bits (half-word, for 16-bit float)
**S0-S31** access the lowest 32 bits (single-precision float)
**D0-D31** access the lowest 64 bits (double-precision float)
**Q0-Q31** access the full 128 bits (quad-word, for SIMD operations)

Calling conventions designate V0-V7 as argument and return value registers for floating-point and SIMD operations. V0 returns scalar floating-point values. V8-V15 are callee-saved (lower 64 bits must be preserved). V16-V31 are caller-saved temporaries.

Scalable Vector Extension (SVE) introduces variable-length vector registers Z0-Z31 (extending V registers) and predicate registers P0-P15 for masked operations. SVE2 enhances these capabilities further. [Unverified: Specific SVE availability varies by processor implementation.]

### System Registers

System registers control processor operation and are accessible only at appropriate privilege levels. Common system registers include:

**SPSR_EL1, SPSR_EL2, SPSR_EL3** (Saved Program Status Register) preserve processor state during exception handling at each exception level.

**ELR_EL1, ELR_EL2, ELR_EL3** (Exception Link Register) store return addresses for exceptions at each level.

**SCTLR_EL1** (System Control Register) configures MMU, caches, alignment checking, and other system features.

**TTBR0_EL1, TTBR1_EL1** (Translation Table Base Register) point to page tables for virtual memory translation. TTBR0 typically maps user space; TTBR1 maps kernel space.

**VBAR_EL1, VBAR_EL2, VBAR_EL3** (Vector Base Address Register) specify exception vector table locations for each exception level.

**CurrentEL** indicates the current exception level (EL0-EL3).

**FPCR** (Floating-Point Control Register) and **FPSR** (Floating-Point Status Register) control floating-point behavior and report floating-point exceptions.

### AArch32 Register Organization

AArch32 provides sixteen 32-bit general-purpose registers R0-R15. R0-R12 serve general purposes. R13 functions as the stack pointer (SP). R14 serves as the link register (LR). R15 is the program counter (PC) and can be read/written directly in many contexts (though this practice is discouraged for portability).

AArch32 has multiple register banks that switch depending on processor mode. For example, FIQ mode has banked R8-R14, giving it private copies of these registers for fast interrupt handling. Other modes have fewer banked registers. [Inference: This banking complexity contributed to ARM's architectural simplification in AArch64.]

The Current Program Status Register (CPSR) contains condition flags (N, Z, C, V), processor mode bits, interrupt disable flags, and instruction set state (ARM, Thumb, ThumbEE, Jazelle). Each exception mode has a Saved Program Status Register (SPSR) preserving CPSR during exception entry.

### Calling Convention Summary

The Procedure Call Standard for the ARM Architecture (AAPCS) standardizes register usage for interoperability between compilers and libraries. Key principles include:

Arguments pass in X0-X7 (or V0-V7 for floating-point). Additional arguments spill to the stack. Variadic functions follow special rules where the variadic portion always uses the stack.

Return values use X0 (or X0-X1 for 128-bit values, or V0 for floating-point). Structures up to 16 bytes return in registers; larger structures use indirect return via X8.

Stack alignment requires 16-byte alignment at public interfaces. The stack pointer must be 16-byte aligned when calling functions.

Callee-saved registers (X19-X28, bottom 64 bits of V8-V15) must be preserved. Caller-saved registers (X0-X18, X30, V0-V7, V16-V31) may be modified freely.

Red zone: [Unverified: AArch64 may not guarantee a red zone below SP in all environments]. Interrupt handlers and signal handlers can corrupt stack memory below SP.

## Instruction Set Architecture Concepts

ARM's instruction set architecture embodies RISC principles while incorporating pragmatic extensions for code density, performance, and specialized workloads. Understanding the ISA's fundamental concepts enables effective assembly programming and comprehension of compiler-generated code.

### Load/Store Architecture

ARM implements a pure load/store architecture where arithmetic and logical operations execute only on registers. Memory access occurs exclusively through dedicated load and store instructions. This separation simplifies pipeline design and enables aggressive instruction reordering for performance.

Load instructions transfer data from memory to registers (LDR, LDRB, LDRH, LDRSB, LDRSH). Store instructions transfer from registers to memory (STR, STRB, STRH). Multiple-register variants (LDP, STP, LDM, STM) transfer multiple registers in single operations, improving efficiency for stack operations and structure copying.

Addressing modes determine how memory addresses calculate. AArch64 supports base register addressing, base plus offset (immediate or register), pre-indexed (update base before access), and post-indexed (update base after access). PC-relative addressing enables position-independent code.

### Instruction Encoding and Formats

AArch64 instructions uniformly encode as 32-bit values, simplifying instruction fetch and decode. This fixed width contrasts with variable-length instruction sets like x86. The 32-bit encoding partitions into fields specifying opcode, operands, condition codes, and modifiers.

Most instructions follow a three-operand format: destination register, first source register, second source (register or immediate). For example, `ADD X0, X1, X2` computes X0 = X1 + X2. Some instructions use two-operand format where destination overwrites a source: `ADDS X0, X0, #1` increments X0.

Immediate values embed directly in instructions with encoding constraints. Logical immediates (AND, ORR, EOR) support specific patterns derived from rotating and replicating small bit patterns. Arithmetic immediates (ADD, SUB) support 12-bit values with optional 12-bit left shift, enabling constants 0-4095 or multiples of 4096. Large constants require loading from literal pools or constructing through multiple instructions (MOVZ, MOVK sequences).

### Conditional Execution

AArch64 eliminated the pervasive conditional execution present in AArch32 where almost every instruction could execute conditionally based on flags. Instead, AArch64 provides conditional select instructions (CSEL, CSINC, CSINV, CSNEG) and conditional compare instructions (CCMP, CCMN) for branchless conditional logic.

Condition codes derive from the NZCV flags in the processor status:
- **N (Negative)**: Result's most significant bit is 1 (signed negative)
- **Z (Zero)**: Result is zero
- **C (Carry)**: Unsigned overflow occurred (or borrow didn't occur for subtraction)
- **V (oVerflow)**: Signed overflow occurred

Common condition codes include EQ (equal, Z=1), NE (not equal, Z=0), LT (signed less than, N≠V), LE (signed less than or equal, Z=1 or N≠V), GT (signed greater than, Z=0 and N=V), GE (signed greater than or equal, N=V), LO/CC (unsigned lower/carry clear, C=0), HS/CS (unsigned higher or same/carry set, C=1), HI (unsigned higher, C=1 and Z=0), LS (unsigned lower or same, C=0 or Z=1).

Conditional branches (B.cond) test condition codes and branch if true. Compare and branch instructions (CBZ, CBNZ) compare register with zero and branch atomically without affecting flags. Test and branch instructions (TBZ, TBNZ) test individual bits and branch.

### Shift and Extend Operations

Many data-processing instructions accept shifted or extended operands as the second source, eliminating separate shift instructions. Available shifts include LSL (logical shift left), LSR (logical shift right), ASR (arithmetic shift right preserving sign), and ROR (rotate right).

For example, `ADD X0, X1, X2, LSL #2` computes X0 = X1 + (X2 << 2), useful for array indexing with element size 4. The shift amount can be immediate (0-31 for W registers, 0-63 for X registers) or register-specified in some instruction forms.

Extension operations sign-extend or zero-extend smaller values when mixing 32-bit and 64-bit operands. SXTB/SXTH/SXTW extend signed bytes/halfwords/words to register width. UXTB/UXTH extend unsigned values. Instructions like `ADD X0, X1, W2, SXTW` add sign-extended W2 to X1.

### Memory Ordering and Barriers

ARM implements a weakly-ordered memory model where memory accesses may complete out of program order unless explicitly constrained. This flexibility enables performance optimizations in hardware but requires careful synchronization in concurrent code.

Memory barrier instructions enforce ordering constraints. DMB (Data Memory Barrier) ensures memory accesses before the barrier complete before accesses after it. DSB (Data Synchronization Barrier) additionally waits for barriers to complete before proceeding. ISB (Instruction Synchronization Barrier) flushes pipelines and ensures subsequent instructions fetch with new context.

Load-acquire and store-release instructions (LDAR, STLR) provide acquire/release semantics matching C11/C++11 atomic operations. LDAR prevents subsequent memory accesses from being observed before the load. STLR prevents prior memory accesses from being observed after the store. These enable efficient lock-free programming patterns.

Exclusive access instructions (LDXR/LDAXR, STXR/STLXR) implement atomic read-modify-write sequences for synchronization primitives. The load-exclusive marks a memory address for exclusive access. The subsequent store-exclusive succeeds only if the exclusive state persists, returning status in a register. Pairing these implements compare-and-swap and other atomic operations.

### SIMD and Vector Processing

Advanced SIMD (NEON) instructions operate on vector registers performing multiple operations simultaneously. Operations support 8-bit, 16-bit, 32-bit, and 64-bit element sizes, with vector lengths of 64 bits (8 bytes) or 128 bits (16 bytes, quad-word).

Vector instructions commonly append shape specifiers indicating element size and count: .8B (8 bytes), .16B (16 bytes), .4H (4 halfwords), .8H (8 halfwords), .2S (2 single-precision floats), .4S (4 single-precision floats), .2D (2 double-precision floats).

NEON provides arithmetic (ADD, SUB, MUL, MLA, MLS), logical (AND, ORR, EOR, BIC), comparison (CMEQ, CMGT, CMGE), and specialized operations (absolute difference, saturating arithmetic, polynomial multiply). Vector load/store instructions transfer multiple elements between memory and vector registers (LD1, LD2, LD3, LD4, ST1, ST2, ST3, ST4) supporting various interleaving patterns for structure-of-arrays and array-of-structures access.

Scalable Vector Extension (SVE) introduces vector-length-agnostic programming where code operates on vectors of implementation-defined length (128 to 2048 bits in 128-bit increments). Programs query vector length at runtime and process data in vector-length-sized chunks, automatically utilizing wider vectors on processors that provide them. Predicate registers enable per-element masking for handling partial vectors and conditional operations. [Unverified: SVE availability and vector length vary by specific processor implementation.]

### Floating-Point Architecture

ARM floating-point follows IEEE 754 standards supporting single-precision (32-bit, S registers), double-precision (64-bit, D registers), and half-precision (16-bit, H registers) formats. Floating-point instructions include arithmetic (FADD, FSUB, FMUL, FDIV), multiply-accumulate (FMADD, FMSUB, FNMADD, FNMSUB), square root (FSQRT), conversion (FCVT), and comparison (FCMP, FCCMP).

The FPCR controls rounding modes (round to nearest, round toward positive/negative infinity, round toward zero), flush-to-zero behavior for denormals, and exception trapping. The FPSR reports exceptional conditions (invalid operation, division by zero, overflow, underflow, inexact result).

Fused multiply-add instructions compute (a × b) + c with single rounding, providing better accuracy and performance than separate multiply and add. These instructions are fundamental to matrix operations, signal processing, and numerical computation.

### Exception and Interrupt Handling

ARM defines four exception levels (EL0-EL3) representing increasing privilege. EL0 runs unprivileged application code. EL1 runs operating system kernels. EL2 runs hypervisors. EL3 runs secure monitors. Transitions between levels occur through synchronous exceptions (system calls, instruction faults) or asynchronous exceptions (interrupts).

Exception vectors organize into a table pointed to by VBAR_EL registers. Each exception level has vectors for synchronous exceptions, IRQ (normal interrupts), FIQ (fast interrupts), and SError (system errors), with separate vectors for different exception sources (same EL, lower EL with same execution state, lower EL with different execution state).

When exceptions occur, the processor saves PC to ELR_EL, saves processor state to SPSR_EL, updates PC to the appropriate vector address, and switches to the target exception level. Exception handlers examine syndrome registers (ESR_EL) to determine exception causes and address registers (FAR_EL) to identify faulting addresses for data/instruction aborts.

Exception return instructions (ERET) restore processor state from SPSR_EL and return address from ELR_EL, switching back to the exception origin.

### Position-Independent and Thread-Local Storage

Position-independent code (PIC) executes correctly regardless of absolute memory addresses, essential for shared libraries and ASLR security. ARM supports PIC through PC-relative addressing for instruction and data references. ADRP/ADD instruction pairs compute addresses relative to page boundaries. GOT (Global Offset Table) and PLT (Procedure Linkage Table) enable dynamic symbol resolution.

Thread-local storage (TLS) provides per-thread variables in multi-threaded programs. ARM implements TLS through special registers or reserved registers pointing to thread control blocks. Access models (Local Exec, Initial Exec, General Dynamic, Local Dynamic) balance performance and flexibility depending on whether code is executable, shared library, or requires runtime symbol resolution.

**Key Points:**
- ARM employs load/store architecture restricting memory access to dedicated instructions while computation occurs solely in registers
- AArch64 uses uniform 32-bit instruction encoding with three-operand format, contrasting with AArch32's variable-width instructions and two-operand bias
- Condition codes (NZCV flags) control conditional execution through conditional select and conditional branch instructions
- Weakly-ordered memory model requires explicit barriers (DMB, DSB, ISB) or acquire/release instructions (LDAR, STLR) for correct concurrent behavior
- SIMD/NEON processes multiple data elements simultaneously with support for 8/16/32/64-bit element sizes in 64-bit or 128-bit vectors
- Four exception levels (EL0-EL3) provide hierarchical privilege separation for applications, OS, hypervisor, and secure monitor

**Important related topics**: ARM assembly syntax and directives, instruction reference with detailed encodings, optimization techniques for ARM processors, exception vector table configuration, memory management unit (MMU) programming, TrustZone security architecture, cache maintenance operations, atomic operations and lock-free algorithms.

---

# Development Environment

The ARM assembly development environment consists of essential tools that transform human-readable assembly code into executable machine code. This environment includes assemblers for translation, linkers for combining object files, and debuggers for testing and troubleshooting.

## Toolchain Installation and Setup

### GNU Toolchain for ARM

The GNU ARM Embedded Toolchain provides a complete development suite for ARM processors. It includes the compiler, assembler, linker, and binary utilities necessary for bare-metal and embedded development.

**Installation on Linux:**

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install gcc-arm-none-eabi binutils-arm-none-eabi gdb-multiarch

# Verify installation
arm-none-eabi-as --version
arm-none-eabi-ld --version
```

**Installation on macOS:**

```bash
# Using Homebrew
brew install --cask gcc-arm-embedded

# Alternative: ARM official release
brew tap ArmMbed/homebrew-formulae
brew install arm-none-eabi-gcc
```

**Installation on Windows:**

- Download from ARM's official website (GNU Arm Embedded Toolchain)
- Run the installer and add to system PATH
- Verify in Command Prompt: `arm-none-eabi-gcc --version`

### ARM Compiler Toolchain

ARM provides its proprietary compiler suite (armclang) with advanced optimization capabilities. This requires a license for commercial use but offers superior code generation for ARM architectures.

**Components:**

- armclang: C/C++ compiler with integrated assembler
- armasm: Standalone assembler
- armlink: Linker
- fromelf: Object file converter

### Cross-Compilation Setup

Cross-compilation allows development on one architecture (x86-64) while targeting ARM processors.

**Target Triple Specification:**

```bash
# Format: <arch>-<vendor>-<os>-<abi>
arm-none-eabi      # Bare-metal ARM (no OS)
arm-linux-gnueabi  # ARM Linux with soft float
arm-linux-gnueabihf # ARM Linux with hard float
aarch64-linux-gnu  # ARM 64-bit Linux
```

**Environment Configuration:**

```bash
# Set cross-compiler prefix
export CROSS_COMPILE=arm-none-eabi-
export ARCH=arm

# For specific target CPU
export CFLAGS="-mcpu=cortex-m4 -mthumb"
```

### QEMU for Emulation

QEMU provides ARM processor emulation for testing without physical hardware.

```bash
# Install QEMU
sudo apt-get install qemu-system-arm

# Run ARM binary
qemu-arm -L /usr/arm-linux-gnueabihf/ ./program

# Debug with GDB
qemu-arm -g 1234 ./program
# In another terminal
gdb-multiarch program
(gdb) target remote localhost:1234
```

## Assemblers (GNU AS, ARM Assembler)

### GNU Assembler (GAS)

The GNU Assembler (as) is part of the GNU Binutils package and uses AT&T or unified ARM syntax.

**Basic Invocation:**

```bash
# Assemble to object file
arm-none-eabi-as -mcpu=cortex-m4 -mthumb -o output.o input.s

# Common options
-g              # Include debugging information
-march=armv7-m  # Specify architecture
-mfpu=fpv4-sp-d16 # Floating-point unit
-W              # Suppress warnings
```

**Syntax Modes:**

```assembly
# Unified Assembly Language (UAL) - recommended
.syntax unified
.thumb

add r0, r1, r2    # Same syntax for ARM and Thumb

# Divided syntax (legacy)
.syntax divided
.arm
add r0, r1, r2    # ARM mode
.thumb
adds r0, r1, r2   # Thumb mode (note 's' suffix)
```

**Directives:**

```assembly
.section .text    # Code section
.section .data    # Initialized data
.section .bss     # Uninitialized data

.global _start    # Export symbol
.extern func      # External symbol reference

.align 4          # Align to 4-byte boundary
.word 0x12345678  # 32-bit constant
.byte 0xFF        # 8-bit constant
.ascii "text"     # String without null terminator
.asciz "text"     # String with null terminator

.equ CONSTANT, 100  # Define constant
.set VALUE, 0x20    # Alternative constant definition
```

**Conditional Assembly:**

```assembly
.ifdef DEBUG
    mov r0, #1
.else
    mov r0, #0
.endif

.if PLATFORM == 1
    bl platform1_init
.elseif PLATFORM == 2
    bl platform2_init
.endif
```

**Macros:**

```assembly
.macro PUSH_REGS reg1, reg2, reg3
    push {\reg1, \reg2, \reg3}
.endm

.macro DELAY cycles
    mov r0, #\cycles
loop_\@:
    subs r0, r0, #1
    bne loop_\@
.endm

# Usage
PUSH_REGS r4, r5, r6
DELAY 1000
```

### ARM Assembler (armasm)

ARM's proprietary assembler with advanced features and optimizations.

**Invocation:**

```bash
# Basic assembly
armasm --cpu=Cortex-M4 -g input.s -o output.o

# Options
--cpu=<name>        # Specify processor
--fpu=<name>        # Specify FPU
--apcs=/interwork   # ARM/Thumb interworking
--debug             # Debug tables
--keep              # Keep intermediate files
```

**Syntax Differences:**

```assembly
; armasm uses semicolons for comments
; Area directive instead of .section
        AREA MyCode, CODE, READONLY
        
        EXPORT _start
        ENTRY

_start  PROC
        MOV     r0, #10
        BL      function
        BX      lr
        ENDP

function PROC
        ADD     r0, r0, #5
        BX      lr
        ENDP

        AREA MyData, DATA, READWRITE
buffer  SPACE   256         ; Reserve 256 bytes
value   DCD     0x12345678  ; Define 32-bit word

        END
```

**Advanced Features:**

```assembly
; Frame directives for stack unwinding
function PROC
        FRAME PUSH {r4-r7, lr}
        FRAME ADDRESS sp, 20
        ; Function body
        FRAME POP {r4-r7, pc}
        ENDP

; PRESERVE8 directive for 8-byte stack alignment
        PRESERVE8

; REQUIRE8 - require 8-byte aligned stack
        REQUIRE8

; ROUT - local label scope
loop    ROUT
%F1     ; Forward reference to local label 1
        B       %F1
1       ; Local label 1
        B       %B1  ; Backward reference to label 1
```

### Inline Assembly in C

Embedding assembly in C code using GCC or armclang.

**GCC Inline Assembly:**

```c
// Basic template
asm volatile (
    "assembly code"
    : output operands
    : input operands
    : clobbered registers
);

// Example: Add two numbers
int add(int a, int b) {
    int result;
    asm volatile (
        "add %[out], %[in1], %[in2]"
        : [out] "=r" (result)
        : [in1] "r" (a), [in2] "r" (b)
    );
    return result;
}

// Memory barrier
#define memory_barrier() asm volatile("dmb" ::: "memory")

// Disable interrupts
static inline void disable_irq(void) {
    asm volatile ("cpsid i" ::: "memory");
}

// Enable interrupts
static inline void enable_irq(void) {
    asm volatile ("cpsie i" ::: "memory");
}
```

**Constraint Characters:**

```c
"r"  // General register
"l"  // Low register (r0-r7)
"h"  // High register (r8-r15)
"m"  // Memory operand
"i"  // Immediate constant
"I"  // Immediate 0-255
"J"  // Immediate -255 to -1
"K"  // Immediate shifted constant
"=r" // Write-only register
"+r" // Read-write register
"&r" // Early clobber register
```

## Linkers and Linking Process

### Linker Basics

The linker combines object files and libraries into a final executable, resolving symbols and organizing memory layout.

**GNU Linker (ld) Invocation:**

```bash
# Basic linking
arm-none-eabi-ld -o program.elf file1.o file2.o -T linker_script.ld

# Common options
-T script.ld    # Use linker script
-Map=output.map # Generate map file
-nostdlib       # Don't link standard libraries
--gc-sections   # Remove unused sections
-L/path         # Add library search path
-lfoo           # Link library libfoo.a
```

**Linking Process Stages:**

1. **Symbol Resolution**: Match symbol references with definitions
2. **Relocation**: Adjust addresses based on final memory layout
3. **Section Merging**: Combine similar sections from multiple files
4. **Memory Allocation**: Assign final addresses according to linker script

### Linker Scripts

Linker scripts control memory layout and section placement.

**Basic Structure:**

```ld
/* Memory regions definition */
MEMORY
{
    FLASH (rx)  : ORIGIN = 0x08000000, LENGTH = 256K
    RAM (rwx)   : ORIGIN = 0x20000000, LENGTH = 64K
}

/* Entry point */
ENTRY(Reset_Handler)

/* Section definitions */
SECTIONS
{
    /* Vector table in Flash */
    .isr_vector :
    {
        . = ALIGN(4);
        KEEP(*(.isr_vector))
        . = ALIGN(4);
    } >FLASH

    /* Code section */
    .text :
    {
        . = ALIGN(4);
        *(.text)
        *(.text*)
        *(.rodata)
        *(.rodata*)
        . = ALIGN(4);
    } >FLASH

    /* ARM exception unwinding */
    .ARM.extab :
    {
        *(.ARM.extab* .gnu.linkonce.armextab.*)
    } >FLASH

    .ARM.exidx :
    {
        __exidx_start = .;
        *(.ARM.exidx* .gnu.linkonce.armexidx.*)
        __exidx_end = .;
    } >FLASH

    /* Initialized data - Load address in FLASH, runtime in RAM */
    .data :
    {
        . = ALIGN(4);
        _sdata = .;        /* Start of data in RAM */
        *(.data)
        *(.data*)
        . = ALIGN(4);
        _edata = .;        /* End of data in RAM */
    } >RAM AT>FLASH
    
    _sidata = LOADADDR(.data);  /* Load address in FLASH */

    /* Uninitialized data */
    .bss :
    {
        . = ALIGN(4);
        _sbss = .;
        __bss_start__ = _sbss;
        *(.bss)
        *(.bss*)
        *(COMMON)
        . = ALIGN(4);
        _ebss = .;
        __bss_end__ = _ebss;
    } >RAM

    /* Stack section */
    ._user_heap_stack :
    {
        . = ALIGN(8);
        PROVIDE (end = .);
        PROVIDE (_end = .);
        . = . + 0x400;     /* Heap size */
        . = . + 0x800;     /* Stack size */
        . = ALIGN(8);
    } >RAM
}
```

**Advanced Linker Script Features:**

```ld
/* Define symbols */
_stack_size = 0x1000;
_heap_size = 0x800;

/* Alignment functions */
. = ALIGN(4);           /* Align to 4 bytes */
. = ALIGN(. != 0 ? 4 : 1);  /* Conditional alignment */

/* Fill unused memory */
.text :
{
    *(.text)
    FILL(0xFF)          /* Fill with 0xFF */
} >FLASH

/* Assertions */
ASSERT(_edata < 0x20010000, "Data section overflow")
ASSERT((_ebss - _sbss) < 0x4000, "BSS too large")

/* Keep sections from garbage collection */
KEEP(*(.isr_vector))
KEEP(*(.init))
KEEP(*(.fini))

/* Discard sections */
/DISCARD/ :
{
    *(.note.GNU-stack)
    *(.gnu_debuglink)
    *(.comment)
}
```

### Symbol Resolution

**Symbol Types:**

- **Global symbols**: Visible across all files (`.global` directive)
- **Local symbols**: Visible within single file only
- **Weak symbols**: Can be overridden by strong symbols (`.weak` directive)
- **Common symbols**: Uninitialized global variables

**Symbol Visibility:**

```assembly
.global _start      # Export symbol
.weak weak_func     # Weak symbol
.local local_var    # Local symbol (not exported)
.hidden hidden_sym  # Hidden from dynamic linking
```

**Weak Symbol Example:**

```assembly
# Default implementation (weak)
.weak default_handler
.type default_handler, %function
default_handler:
    b default_handler    # Infinite loop

# Strong implementation overrides weak
.global irq_handler
.type irq_handler, %function
irq_handler:
    push {lr}
    bl handle_interrupt
    pop {pc}
```

### Relocation

Relocation adjusts addresses after final memory layout is determined.

**Relocation Types:**

- `R_ARM_ABS32`: Absolute 32-bit address
- `R_ARM_REL32`: PC-relative 32-bit offset
- `R_ARM_THM_CALL`: Thumb branch relocation
- `R_ARM_THM_JUMP24`: Thumb long jump

**Viewing Relocations:**

```bash
# Display relocation entries
arm-none-eabi-objdump -r file.o

# Example output:
# RELOCATION RECORDS FOR [.text]:
# OFFSET   TYPE              VALUE
# 00000004 R_ARM_THM_CALL    function
# 0000000c R_ARM_ABS32       variable
```

### Map Files

Map files show the final memory layout and symbol addresses.

```bash
# Generate map file
arm-none-eabi-ld -Map=output.map -o program.elf file.o

# Map file contents include:
# - Memory configuration
# - Section addresses and sizes
# - Symbol table with addresses
# - Cross-reference table
```

**Map File Sections:**

```
Memory Configuration:
Name    Origin      Length      Attributes
FLASH   0x08000000  0x00040000  xr
RAM     0x20000000  0x00010000  xrw

Linker script and memory map:
.text           0x08000000     0x1234
 *(.text)
 .text          0x08000000      0x100 file1.o
                0x08000000      _start
                0x08000020      main
```

## Debuggers (GDB, LLDB)

### GDB for ARM

GNU Debugger with ARM cross-debugging support.

**Starting GDB:**

```bash
# Debug local ARM executable
arm-none-eabi-gdb program.elf

# Connect to remote target (OpenOCD, QEMU, hardware debugger)
arm-none-eabi-gdb program.elf
(gdb) target remote localhost:3333
(gdb) monitor reset halt
(gdb) load
(gdb) continue
```

**Essential GDB Commands:**

```gdb
# Loading and running
file program.elf          # Load executable
load                      # Load program to target
run                       # Start execution
continue (c)              # Continue execution
step (s)                  # Step into
next (n)                  # Step over
finish                    # Step out
until *0x8000100         # Run until address

# Breakpoints
break main                # Break at function
break *0x8000100         # Break at address
break file.c:42          # Break at line
break func if r0==5      # Conditional breakpoint
watch variable           # Data watchpoint
rwatch address           # Read watchpoint
awatch address           # Access watchpoint
info breakpoints         # List breakpoints
delete 1                 # Delete breakpoint 1
disable 2                # Disable breakpoint 2
enable 2                 # Enable breakpoint 2

# Examining registers
info registers           # All registers
info all-registers       # Including system registers
print $r0                # Single register
set $r1 = 10            # Modify register
info registers pc sp lr  # Specific registers

# Examining memory
x/10x 0x20000000        # 10 hex words
x/10i $pc               # 10 instructions at PC
x/s 0x20000100          # String
x/10b 0x20000000        # 10 bytes
print *(int*)0x20000000 # Dereference pointer
dump binary memory out.bin 0x20000000 0x20001000

# Stack examination
backtrace (bt)           # Stack trace
frame 2                  # Select frame
info frame               # Frame details
info locals              # Local variables
info args                # Function arguments

# Disassembly
disassemble main         # Disassemble function
disassemble /r          # Include raw bytes
set disassembly-flavor intel  # Syntax style
```

**ARM-Specific GDB Features:**

```gdb
# Set ARM/Thumb mode
set arm force-mode thumb
set arm force-mode arm

# Vector Floating Point
info float               # FPU registers
set $s0 = 1.5           # Set FP register

# ARM system registers
print $cpsr             # Current Program Status Register
print $spsr             # Saved Program Status Register
set $cpsr = 0x13        # Set to supervisor mode

# Memory-mapped peripherals
set *(int*)0x40021000 = 0x1  # Write to peripheral
print/x *(int*)0x40021000     # Read peripheral
```

**GDB Initialization Script (.gdbinit):**

```gdb
# .gdbinit for ARM Cortex-M4
target remote localhost:3333
set mem inaccessible-by-default off
set architecture arm
set arm force-mode thumb

# Custom commands
define reset
    monitor reset halt
    load
end

define flash
    monitor reset halt
    load
    monitor reset run
end

# Layout
layout regs              # Show registers
layout asm               # Show assembly
layout split             # Show source and assembly

# Pretty printing
set print pretty on
set print array on
set print array-indexes on
```

### OpenOCD Integration

OpenOCD provides debugging interface for ARM hardware.

**Starting OpenOCD:**

```bash
# Start OpenOCD with board config
openocd -f board/st_nucleo_f4.cfg

# Or with interface and target
openocd -f interface/stlink-v2.cfg -f target/stm32f4x.cfg
```

**OpenOCD Commands in GDB:**

```gdb
# Reset commands
monitor reset halt       # Reset and halt
monitor reset run        # Reset and run
monitor reset init       # Reset and init

# Flash operations
monitor flash write_image erase program.elf
monitor flash erase_sector 0 0 last
monitor flash info 0

# Debugging
monitor mdw 0x20000000 10    # Read memory
monitor mww 0x20000000 0x12  # Write memory
monitor reg                   # Show registers
monitor halt                  # Halt target
monitor resume                # Resume execution
```

### LLDB for ARM

LLVM debugger with similar functionality to GDB.

**Basic LLDB Commands:**

```lldb
# Starting
lldb program.elf
(lldb) target create program.elf
(lldb) gdb-remote localhost:3333

# Breakpoints
breakpoint set --name main
breakpoint set --address 0x8000100
breakpoint set --file main.c --line 42
breakpoint list
breakpoint delete 1

# Execution
run
continue
step
next
finish
process kill

# Registers
register read
register read r0 r1 r2
register write r0 42

# Memory
memory read 0x20000000
memory read --size 4 --format x --count 10 0x20000000
memory write 0x20000000 0x12345678

# Disassembly
disassemble --name main
disassemble --start-address 0x8000000 --count 20
```

**LLDB Python Scripting:**

```python
# .lldbinit
script
import lldb

def reset_target(debugger, command, result, internal_dict):
    target = debugger.GetSelectedTarget()
    process = target.GetProcess()
    process.SendCommand("monitor reset halt")
    
lldb.debugger.HandleCommand('command script add -f script.reset_target reset')
end
```

### Hardware Debugging Interfaces

**JTAG (Joint Test Action Group):**

- Full boundary scan capability
- 4-5 signal lines (TMS, TCK, TDI, TDO, optional TRST)
- Supports multiple devices in daisy chain
- Industry standard for ARM debugging

**SWD (Serial Wire Debug):**

- 2-wire interface (SWDIO, SWCLK)
- ARM-specific alternative to JTAG
- Lower pin count, suitable for small packages
- Similar functionality to JTAG

**Debug Probe Connection:**

```bash
# Check connected probes
openocd -f interface/stlink.cfg -c "adapter list"

# Probe-specific configs
-f interface/jlink.cfg        # Segger J-Link
-f interface/stlink.cfg       # ST-Link
-f interface/cmsis-dap.cfg    # ARM CMSIS-DAP
-f interface/ftdi/openocd-usb.cfg  # FTDI-based
```

### Semihosting

Semihosting allows debugging through host I/O operations.

**Enabling Semihosting:**

```gdb
# In GDB
monitor arm semihosting enable

# In code (OpenOCD)
extern void initialise_monitor_handles(void);
initialise_monitor_handles();  # Enable semihosting

printf("Debug output\n");  # Output via debugger
```

**Semihosting System Calls:**

```assembly
# SYS_WRITE (0x05)
mov r0, #0x05          @ SYS_WRITE
ldr r1, =params        @ Parameter block
bkpt #0xAB             @ Semihosting breakpoint

params:
    .word 1            @ File handle (stdout)
    .word message      @ Buffer address
    .word msg_len      @ Length
```

**Key Points:**

- ARM development requires cross-compilation toolchains targeting specific processor architectures
- GNU tools (GAS, ld, GDB) provide free, open-source development capabilities across all platforms
- Linker scripts precisely control memory layout, critical for bare-metal and embedded systems
- Modern debuggers support remote debugging via JTAG/SWD interfaces through tools like OpenOCD
- Semihosting enables printf-style debugging through the debugger connection without UART hardware

---

## Emulators and Simulators

### QEMU (Quick Emulator)

QEMU is a widely-used open-source machine emulator and virtualizer that supports full system emulation for ARM architectures. It allows developers to run ARM binaries on x86 hosts without requiring physical ARM hardware.

**System Emulation Mode**

QEMU's system emulation mode virtualizes an entire ARM machine, including CPU, memory, peripherals, and devices. This mode is useful for testing bootloaders, operating systems, and bare-metal applications. The emulator supports various ARM boards such as versatilepb, vexpress-a9, vexpress-a15, and raspi2/raspi3 for Raspberry Pi emulation.

When running in system mode, QEMU can emulate different ARM processor variants including ARM926, ARM1176, Cortex-A9, Cortex-A15, and Cortex-A53/A57. The choice of CPU model affects available instruction sets and architectural features.

**User Mode Emulation**

User mode emulation allows running ARM Linux binaries directly on x86 Linux hosts by translating ARM instructions to host architecture instructions on-the-fly. This mode is faster than system emulation and suitable for testing userspace applications. The qemu-arm binary handles translation and system call forwarding between guest ARM programs and the host kernel.

**Debugging Capabilities**

QEMU integrates with GDB through its remote debugging stub. Developers can attach GDB to a running QEMU instance using the `-s` flag (opens port 1234) or `-S` flag (starts paused, waiting for debugger). This enables breakpoints, single-stepping, register inspection, and memory examination during program execution.

**Command Line Usage**

Basic QEMU invocation requires specifying the ARM variant and binary to execute. Common flags include `-M` for machine type, `-cpu` for processor model, `-m` for memory size, `-kernel` for kernel image, and `-append` for kernel command line arguments.

### ARM Fast Models

ARM Fast Models are instruction-accurate software models of ARM processors that execute faster than cycle-accurate simulators. These commercial tools from ARM provide high simulation speed while maintaining functional accuracy, making them suitable for software development before hardware availability.

### Cycle-Accurate Simulators

Cycle-accurate simulators model the exact timing behavior of ARM processors, including pipeline stages, cache behavior, and memory access latency. While slower than functional simulators, they provide precise performance analysis and timing verification for real-time systems and performance-critical code.

## Cross-Compilation Basics

### Toolchain Components

A cross-compilation toolchain consists of compilers, assemblers, linkers, and binary utilities that run on a host architecture but produce code for a target ARM architecture. The GNU toolchain (binutils, GCC, GDB) is the most common open-source option for ARM development.

**Compiler (arm-none-eabi-gcc / arm-linux-gnueabi-gcc)**

The cross-compiler translates C/C++ code into ARM assembly and machine code. Different toolchain prefixes target different environments: `arm-none-eabi-` for bare-metal (no operating system), `arm-linux-gnueabi-` for ARM Linux with soft-float ABI, and `arm-linux-gnueabihf-` for ARM Linux with hard-float ABI.

**Assembler (as)**

The GNU assembler converts ARM assembly source code into object files containing machine code and relocation information. It supports multiple syntax styles including GNU syntax and UAL (Unified Assembler Language).

**Linker (ld)**

The linker combines multiple object files into a single executable or library, resolves symbol references, and arranges code and data according to linker scripts. For bare-metal development, custom linker scripts define memory layout, section placement, and entry points.

**Binary Utilities (objcopy, objdump, nm, size)**

Binary utilities analyze and manipulate object files and executables. `objcopy` extracts raw binary images from ELF files, `objdump` disassembles code and displays section information, `nm` lists symbols, and `size` reports section sizes.

### Target Specifications

Cross-compilation requires specifying the target architecture, processor variant, ABI (Application Binary Interface), and floating-point configuration. Architecture flags like `-march=armv7-a` define instruction set availability, while `-mcpu=cortex-a9` optimizes for specific processors. The `-mfpu` flag selects floating-point hardware (VFPv3, NEON), and `-mfloat-abi` chooses soft, softfp, or hard float calling conventions.

### Sysroot Configuration

For Linux userspace development, the cross-compiler needs access to target architecture headers and libraries through a sysroot. The sysroot is a directory tree containing ARM versions of system libraries and headers, specified with `--sysroot` compiler flag or configured during toolchain build.

### Installation Methods

Cross-compilation toolchains can be installed through package managers (apt, yum, pacman), downloaded as pre-built packages from ARM or Linaro, or built from source using tools like crosstool-ng or buildroot. Package manager installation is simplest but may provide older versions, while building from source offers customization but requires more time and expertise.

## Build Systems and Makefiles

### Traditional Make

GNU Make uses makefiles to automate the build process by defining rules for transforming source files into targets. Makefiles specify dependencies between files and shell commands to execute when targets are out of date.

**Makefile Structure**

A makefile consists of variables, rules, and directives. Variables store values like compiler names, flags, and file lists. Rules define targets, prerequisites, and recipes (commands to execute). Pattern rules use wildcards to handle multiple files with similar transformations.

**Variables and Flags**

Common makefile variables include `CC` (compiler), `AS` (assembler), `LD` (linker), `CFLAGS` (compiler flags), `ASFLAGS` (assembler flags), and `LDFLAGS` (linker flags). For ARM cross-compilation, these typically reference cross-toolchain executables with appropriate architecture flags.

**Automatic Variables**

Make provides automatic variables that represent parts of the current rule: `$@` is the target filename, `$<` is the first prerequisite, `$^` is all prerequisites, and `$*` is the stem of a pattern rule match. These reduce repetition in recipes.

**Phony Targets**

Phony targets are not actual files but represent commands to execute, marked with `.PHONY` directive. Common phony targets include `all` (default build), `clean` (remove generated files), `install` (copy files to destination), and `debug` (build with debug symbols).

### CMake for ARM Projects

CMake is a cross-platform build system generator that produces makefiles, ninja files, or IDE project files from high-level configuration. For ARM cross-compilation, CMake uses toolchain files that specify the cross-compiler, target architecture, and sysroot location.

**Toolchain Files**

A CMake toolchain file sets variables like `CMAKE_SYSTEM_NAME`, `CMAKE_SYSTEM_PROCESSOR`, `CMAKE_C_COMPILER`, `CMAKE_CXX_COMPILER`, and `CMAKE_FIND_ROOT_PATH`. This file is passed to cmake with `-DCMAKE_TOOLCHAIN_FILE` option to configure cross-compilation.

**Architecture-Specific Flags**

CMakeLists.txt files can conditionally add ARM-specific compiler flags, select assembly source files, and configure target properties based on the processor architecture detected from the toolchain file.

### Build Automation

Modern build systems support parallel compilation, incremental builds, dependency tracking, and out-of-source builds. Parallel compilation with `make -j` utilizes multiple CPU cores. Out-of-source builds separate build artifacts from source code, enabling multiple configurations from the same source tree.

## Integrated Development Environments

### VS Code with ARM Extensions

Visual Studio Code provides ARM assembly development support through extensions and configuration. The C/C++ extension offers IntelliSense, syntax highlighting, and debugging integration. ARM-specific extensions add instruction documentation and assembly syntax support.

**Configuration Files**

VS Code uses JSON configuration files: `c_cpp_properties.json` configures IntelliSense with cross-compiler paths and include directories, `tasks.json` defines build tasks for compilation, and `launch.json` configures debugging with GDB remote targets or QEMU.

**Debugging Integration**

VS Code integrates with GDB for debugging ARM binaries running on QEMU or remote hardware. The debugging interface provides breakpoints, variable inspection, register views, memory dumps, and call stack visualization through the native debug UI.

### Eclipse with GNU ARM Plugin

Eclipse IDE with GNU ARM Eclipse plugin (now GNU MCU Eclipse) provides comprehensive embedded ARM development features including project templates, toolchain management, peripheral registers view, and integrated debugging with OpenOCD or J-Link.

### Vim/Emacs for Assembly

Text editors like Vim and Emacs offer ARM assembly development through syntax highlighting, ctags integration for symbol navigation, and integration with external build systems. Plugins like ALE or Syntastic provide real-time syntax checking using assembler output.

### Platform-Specific IDEs

ARM Keil MDK (Microcontroller Development Kit) is a commercial IDE with ARM compiler, debugger, and extensive device support. IAR Embedded Workbench is another commercial option with advanced optimization and certification for safety-critical development. These IDEs provide comprehensive toolchains but require licenses.

### Debugging Tools Integration

IDEs integrate with hardware debugging tools like J-Link, ST-Link, and OpenOCD for on-chip debugging of ARM microcontrollers. These tools support JTAG and SWD protocols for breakpoints, flash programming, and real-time trace capture.

**Key Points:**

- QEMU provides both system and user mode emulation for ARM development without physical hardware
- Cross-compilation toolchains include compiler, assembler, linker, and utilities configured for target ARM architecture
- Makefiles and CMake automate the build process with dependency tracking and parallel compilation
- Modern IDEs integrate cross-compilation, QEMU emulation, and GDB debugging in unified interfaces
- Toolchain selection depends on target environment: bare-metal uses `arm-none-eabi`, Linux uses `arm-linux-gnueabi/hf`

**Important related topics:** Linker scripts for bare-metal development, OpenOCD configuration for hardware debugging, QEMU device tree customization, Docker containers for reproducible ARM build environments, Remote debugging protocols (GDB remote serial protocol), Build system integration with continuous integration pipelines.

---

# Basic Instructions in ARM Assembly

## Data Movement Instructions

Data movement instructions transfer data between registers, memory, and immediate values without performing arithmetic or logical operations. These instructions form the foundation of all ARM programs by loading operands, storing results, and copying values between locations.

### MOV - Move Register or Immediate

The MOV instruction copies a value from one register to another or loads an immediate value into a register. It provides the simplest form of data transfer within the processor's register file.

**Syntax and Variants**

Basic syntax follows the pattern `MOV destination, source` where destination is always a register and source can be a register or immediate value. The instruction updates the destination register without affecting the source when copying between registers.

**Immediate Value Limitations**

ARM processors have encoding restrictions on immediate values in MOV instructions. The ARM32 instruction set can only encode certain immediate values directly - specifically those that can be represented by rotating an 8-bit value by an even number of positions. Values like 255, 0x100, 0xFF00 are encodable, but arbitrary 32-bit values like 0x12345678 cannot be loaded with a single MOV instruction.

**MOVW and MOVT for Large Immediates**

ARMv7 and later architectures provide MOVW (move wide) and MOVT (move top) instructions to load arbitrary 16-bit values into the lower or upper half of a register. Loading a full 32-bit immediate requires two instructions: MOVW loads the lower 16 bits, then MOVT loads the upper 16 bits without affecting the lower half.

**Example:**

```assembly
MOV r0, r1           @ Copy r1 to r0
MOV r2, #42          @ Load immediate 42 into r2
MOV r3, #0xFF00      @ Valid rotated immediate
MOVW r4, #0x5678     @ Load lower 16 bits
MOVT r4, #0x1234     @ Load upper 16 bits, r4 = 0x12345678
```

### LDR - Load Register from Memory

LDR instructions read data from memory into registers. Memory addresses are calculated using base registers with optional offsets, enabling access to data structures, arrays, and stack variables.

**Addressing Modes**

ARM supports multiple addressing modes for memory access. Offset addressing adds a constant or register to the base address without modifying the base register. Pre-indexed addressing adds the offset to the base and writes the result back to the base register before accessing memory. Post-indexed addressing uses the base address for memory access, then adds the offset to the base register afterward.

**Syntax Variations**

Offset mode: `LDR rd, [rn, #offset]` loads from address rn + offset without changing rn. Pre-indexed mode: `LDR rd, [rn, #offset]!` updates rn to rn + offset before loading. Post-indexed mode: `LDR rd, [rn], #offset` loads from rn then updates rn to rn + offset.

**Data Size Variants**

Different instruction suffixes load different data sizes. LDR loads 32-bit words, LDRH loads 16-bit halfwords, LDRB loads 8-bit bytes. Signed variants LDRSH and LDRSB sign-extend smaller values to 32 bits, while unsigned variants zero-extend.

**PC-Relative Loading**

LDR with PC as the base register enables position-independent code and access to constants stored in literal pools. The assembler pseudo-instruction `LDR rd, =value` loads arbitrary 32-bit constants by either using MOV/MOVW/MOVT or loading from a nearby literal pool.

**Example:**

```assembly
LDR r0, [r1]         @ Load word from address in r1
LDR r2, [r3, #16]    @ Load from r3 + 16
LDR r4, [r5, #4]!    @ Pre-indexed: r5 = r5 + 4, load from new r5
LDR r6, [r7], #8     @ Post-indexed: load from r7, then r7 = r7 + 8
LDRB r8, [r9]        @ Load byte (8-bit)
LDRH r10, [r11]      @ Load halfword (16-bit)
LDRSB r12, [sp, #-4] @ Load signed byte, negative offset
LDR r0, =0x12345678  @ Load 32-bit constant (pseudo-instruction)
```

### STR - Store Register to Memory

STR instructions write register contents to memory locations. Addressing modes and variants mirror those of LDR instructions, maintaining symmetry between load and store operations.

**Basic Store Operations**

Store instructions transfer data from registers to memory addresses calculated using the same addressing modes as loads. The register being stored provides the value, while the addressing expression determines the destination memory location.

**Size Variants**

STR stores 32-bit words, STRH stores 16-bit halfwords (lower 16 bits of register), and STRB stores 8-bit bytes (lower 8 bits of register). Upper bits are ignored when storing smaller data sizes.

**Stack Operations**

STR with pre-indexed or post-indexed addressing modes implements stack operations. Pushing to a full descending stack uses `STR rd, [sp, #-4]!` which decrements sp then stores. Popping uses `LDR rd, [sp], #4` which loads then increments sp.

**Example:**

```assembly
STR r0, [r1]         @ Store word to address in r1
STR r2, [r3, #20]    @ Store to r3 + 20
STR r4, [r5, #-8]!   @ Pre-indexed: r5 = r5 - 8, store to new r5
STR r6, [r7], #12    @ Post-indexed: store to r7, then r7 = r7 + 12
STRB r8, [r9]        @ Store byte (lower 8 bits)
STRH r10, [r11]      @ Store halfword (lower 16 bits)
```

### Multiple Register Transfers

LDM (load multiple) and STM (store multiple) instructions transfer multiple registers in a single instruction, improving code density and potentially performance. These instructions are particularly useful for function prologue/epilogue, context switching, and bulk data transfers.

**Stack Variants**

PUSH and POP are pseudo-instructions that expand to STMDB (store multiple decrement before) and LDMIA (load multiple increment after) operating on the stack pointer. `PUSH {r0-r3, lr}` stores five registers to the stack, `POP {r0-r3, pc}` restores them and returns from a function by loading the return address into PC.

## Immediate Values and Operands

### Immediate Value Encoding

ARM instructions are fixed-width (32 bits in ARM state, 16 bits in Thumb state), limiting the space available for encoding immediate values within instructions. The instruction format must accommodate the opcode, destination register, source registers, and immediate value.

**ARM32 Encoding Scheme**

In ARM32 state, data processing instructions encode immediates as an 8-bit value and a 4-bit rotation. The immediate is formed by rotating the 8-bit value right by twice the 4-bit rotation value (0-30 in steps of 2). This allows encoding values like 0xFF (rotate 0), 0xFF00 (rotate by 8), 0xFF000000 (rotate by 24), but not arbitrary 32-bit values.

**Assembler Transformation**

When an immediate cannot be encoded directly, the assembler may substitute equivalent instructions. For example, `MOV r0, #-1` might become `MVN r0, #0` (move NOT 0). Addition of negative values may become subtraction: `ADD r0, r1, #-8` becomes `SUB r0, r1, #8`.

**Literal Pools**

For truly arbitrary 32-bit constants, the assembler creates literal pools - areas of read-only data embedded in the code section. The pseudo-instruction `LDR rd, =constant` loads from these pools using PC-relative addressing. The assembler places literal pools at convenient locations, typically after function boundaries or when forced by `.ltorg` directive.

### Operand Types

ARM instructions support different operand combinations depending on the instruction type. Data processing instructions typically accept two source operands and produce one result, while memory instructions use one register operand and one address expression.

**Register Operands**

Register operands refer directly to one of the 16 general-purpose registers (r0-r15). Most instructions can use any register, though some have restrictions: r13 (SP) should only be used for stack operations, r15 (PC) has special behavior when read or written, and certain instructions prohibit specific registers.

**Immediate Operands**

Immediate operands are constant values encoded within the instruction. They must satisfy encoding constraints specific to each instruction type. The assembler reports errors for values that cannot be encoded and may suggest alternatives.

**Shifted Register Operands**

Many ARM data processing instructions accept a shifted register as the second operand. The register value is shifted or rotated before use in the operation. Available shifts include LSL (logical shift left), LSR (logical shift right), ASR (arithmetic shift right), ROR (rotate right), and RRX (rotate right extended through carry).

**Example:**

```assembly
ADD r0, r1, r2           @ r0 = r1 + r2 (register operand)
ADD r0, r1, #100         @ r0 = r1 + 100 (immediate operand)
ADD r0, r1, r2, LSL #2   @ r0 = r1 + (r2 << 2), shifted register
SUB r0, r1, r2, LSR #4   @ r0 = r1 - (r2 >> 4)
MOV r0, r1, ROR #8       @ r0 = r1 rotated right by 8 bits
```

## Arithmetic Instructions

Arithmetic instructions perform mathematical operations on integer values. Results are stored in destination registers, and optional status flags indicate properties like overflow, carry, and sign.

### ADD - Addition

ADD sums two operands and stores the result in a destination register. The instruction supports register, immediate, and shifted register operands as the second source.

**Basic Addition**

The basic form adds two registers or a register and immediate value: `ADD rd, rn, operand2` computes rd = rn + operand2. The destination can be the same as one of the sources for accumulation operations.

**Flag Updates**

The 'S' suffix (ADDS) updates condition flags based on the result. The N (negative) flag is set if the result is negative (bit 31 = 1), Z (zero) if result is zero, C (carry) if unsigned overflow occurred, and V (overflow) if signed overflow occurred. These flags enable conditional execution and overflow detection.

**Add with Carry**

ADC (add with carry) includes the carry flag in the addition, useful for multi-precision arithmetic. When adding two 64-bit numbers stored in register pairs, ADD handles the lower 32 bits and ADC adds the upper 32 bits plus any carry from the lower addition.

**Example:**

```assembly
ADD r0, r1, r2           @ r0 = r1 + r2
ADD r3, r3, #1           @ r3 = r3 + 1 (increment)
ADDS r4, r5, r6          @ r4 = r5 + r6, update flags
ADD r7, r8, r9, LSL #3   @ r7 = r8 + (r9 * 8)
ADC r10, r11, r12        @ r10 = r11 + r12 + carry
```

### SUB - Subtraction

SUB subtracts the second operand from the first and stores the result. It provides the complement to ADD and supports the same operand types.

**Basic Subtraction**

The syntax `SUB rd, rn, operand2` computes rd = rn - operand2. Like ADD, the destination can alias a source register for in-place modification.

**Reverse Subtraction**

RSB (reverse subtract) computes rd = operand2 - rn, swapping the operand order. This is useful when the minuend is an immediate or shifted register rather than the base register. RSB enables negation: `RSB r0, r0, #0` computes r0 = 0 - r0 = -r0.

**Subtract with Carry**

SBC (subtract with carry) performs rd = rn - operand2 - NOT(carry), implementing borrow for multi-precision subtraction. The inverted carry flag represents borrow: if carry is clear (0), one is subtracted; if carry is set (1), nothing extra is subtracted.

**Example:**

```assembly
SUB r0, r1, r2           @ r0 = r1 - r2
SUB r3, r3, #5           @ r3 = r3 - 5 (decrement)
SUBS r4, r5, r6          @ r4 = r5 - r6, update flags
RSB r7, r8, #100         @ r7 = 100 - r8
SBC r9, r10, r11         @ r9 = r10 - r11 - borrow
RSB r12, r12, #0         @ r12 = -r12 (negation)
```

### MUL - Multiplication

Multiplication instructions compute the product of two registers. ARM provides several multiplication variants for different result sizes and accumulation patterns.

**Basic Multiplication**

MUL multiplies two 32-bit registers and stores the lower 32 bits of the 64-bit product. The syntax `MUL rd, rn, rm` computes rd = rn * rm, discarding the upper 32 bits. This is sufficient when the product fits in 32 bits or only the lower bits are needed.

**Multiply-Accumulate**

MLA (multiply-accumulate) adds a third operand to the product: `MLA rd, rn, rm, ra` computes rd = (rn * rm) + ra. This single instruction replaces separate multiply and add operations, commonly used in digital signal processing and matrix operations.

**Long Multiplication**

UMULL (unsigned multiply long) and SMULL (signed multiply long) produce full 64-bit results from 32-bit operands. The syntax `UMULL rdlo, rdhi, rn, rm` stores the lower 32 bits in rdlo and upper 32 bits in rdhi, with rdlo and rdhi being consecutive or separate registers.

**Long Multiply-Accumulate**

UMLAL and SMLAL accumulate into a 64-bit destination: `UMLAL rdlo, rdhi, rn, rm` computes {rdhi, rdlo} = {rdhi, rdlo} + (rn * rm), useful for accumulating multiple products without overflow.

**Example:**

```assembly
MUL r0, r1, r2           @ r0 = r1 * r2 (lower 32 bits)
MLA r3, r4, r5, r6       @ r3 = (r4 * r5) + r6
UMULL r7, r8, r9, r10    @ {r8, r7} = r9 * r10 (unsigned 64-bit)
SMULL r11, r12, r13, r14 @ {r12, r11} = r13 * r14 (signed 64-bit)
```

### Division

Integer division support varies by ARM architecture. Earlier ARM cores lack hardware division, requiring software implementation. ARMv7-R and ARMv7-M with divide extensions, and all ARMv8-A cores include SDIV (signed divide) and UDIV (unsigned divide) instructions.

**Hardware Division Instructions**

Where available, SDIV and UDIV divide two registers and store the quotient. The syntax `SDIV rd, rn, rm` computes rd = rn / rm using signed division, truncating toward zero. UDIV performs unsigned division. Division by zero behavior is [Inference: likely sets the result to zero or an unpredictable value, though specific behavior may be implementation-defined].

**Remainder Calculation**

ARM division instructions only produce quotients, not remainders. The remainder must be calculated separately: `MLS rd, quotient, divisor, dividend` computes rd = dividend - (quotient * divisor), providing the modulo result.

**Software Division**

On cores without hardware division, software routines implement division using shifts, subtracts, and comparisons. These routines are typically provided by compiler runtime libraries and are significantly slower than hardware division.

**Example:**

```assembly
@ [Inference: Assuming hardware divide available]
SDIV r0, r1, r2          @ r0 = r1 / r2 (signed quotient)
UDIV r3, r4, r5          @ r3 = r4 / r5 (unsigned quotient)
MLS r6, r0, r2, r1       @ r6 = r1 - (r0 * r2) = remainder
```

## Logical Instructions

Logical instructions perform bitwise boolean operations on register values. These operations manipulate individual bits for masking, testing, combining, and inverting data.

### AND - Bitwise AND

AND performs bitwise conjunction of two operands, setting result bits to 1 only where both input bits are 1. This operation is used for masking, clearing bits, and testing bit patterns.

**Bit Masking**

AND with a mask immediate or register isolates specific bits by clearing all others. For example, `AND r0, r1, #0xFF` extracts the lower 8 bits of r1, zeroing the upper 24 bits. Complex masks can be applied using register operands.

**Testing Bits**

TST (test) performs AND without storing the result, only updating flags. `TST r0, #0x10` sets the Z flag if bit 4 of r0 is clear, enabling conditional branches based on bit values. TST is equivalent to ANDS with a discarded result.

**Example:**

```assembly
AND r0, r1, r2           @ r0 = r1 & r2
AND r3, r3, #0x0F        @ r3 = r3 & 0x0F (mask lower 4 bits)
ANDS r4, r5, r6          @ r4 = r5 & r6, update flags
TST r7, #0x80            @ Test bit 7, set flags
AND r8, r9, r10, LSL #2  @ r8 = r9 & (r10 << 2)
```

### ORR - Bitwise OR

ORR (or OR) performs bitwise disjunction, setting result bits to 1 where either input bit is 1. This operation combines bit patterns and sets specific bits without affecting others.

**Setting Bits**

ORR with a mask sets specific bits in a register while preserving others. `ORR r0, r0, #0x80` sets bit 7 without changing bits 0-6 or 8-31. Multiple bits can be set in a single operation using appropriate masks.

**Combining Values**

ORR combines separate bit fields into a single value. After isolating different fields in separate registers, ORR merges them: upper bits from one register OR'd with lower bits from another produce a combined result.

**Example:**

```assembly
ORR r0, r1, r2           @ r0 = r1 | r2
ORR r3, r3, #0x100       @ r3 = r3 | 0x100 (set bit 8)
ORRS r4, r5, r6          @ r4 = r5 | r6, update flags
ORR r7, r8, r9, LSR #4   @ r7 = r8 | (r9 >> 4)
```

### EOR - Bitwise Exclusive OR

EOR (exclusive OR or XOR) sets result bits to 1 where input bits differ. This operation toggles bits, detects differences, and performs bit swapping.

**Toggling Bits**

EOR with a mask flips specific bits: `EOR r0, r0, #0x40` toggles bit 6, changing 0 to 1 or 1 to 0. Repeated EOR with the same mask returns the original value, providing reversible bit manipulation.

**Zeroing Registers**

EOR of a register with itself produces zero: `EOR r0, r0, r0` clears r0. This may be more efficient than `MOV r0, #0` on some microarchitectures, though both achieve the same result.

**Comparison Detection**

EOR detects differences between values. After `EOR r0, r1, r2`, bits set in r0 indicate positions where r1 and r2 differ. Testing if the result is zero (using flags) determines if two values are identical.

**Example:**

```assembly
EOR r0, r1, r2           @ r0 = r1 ^ r2
EOR r3, r3, #0x01        @ r3 = r3 ^ 0x01 (toggle bit 0)
EORS r4, r5, r6          @ r4 = r5 ^ r6, update flags
EOR r7, r7, r7           @ r7 = 0 (register cleared)
```

### BIC - Bit Clear

BIC (bit clear) clears bits in the first operand where corresponding bits in the second operand are 1. It performs AND with the complement of the second operand: rd = rn AND NOT(operand2).

**Clearing Specific Bits**

BIC selectively clears bits indicated by a mask. `BIC r0, r0, #0x0F` clears the lower 4 bits while preserving all others. Unlike AND with an inverted mask, BIC simplifies bit clearing with direct mask specification.

**Alignment Operations**

BIC is commonly used for address alignment. `BIC r0, r0, #3` clears the lower 2 bits, aligning r0 to a 4-byte boundary by rounding down to the nearest multiple of 4.

**Example:**

```assembly
BIC r0, r1, r2           @ r0 = r1 & ~r2
BIC r3, r3, #0x07        @ r3 = r3 & ~0x07 (clear lower 3 bits)
BICS r4, r5, r6          @ r4 = r5 & ~r6, update flags
BIC sp, sp, #7           @ Align stack pointer to 8 bytes
```

### MVN - Move NOT

MVN moves the bitwise complement of the source operand to the destination. It performs NOT operation, inverting all bits: 0 becomes 1 and 1 becomes 0.

**Bitwise Negation**

MVN provides efficient bit inversion. `MVN r0, r1` stores the inverted value of r1 in r0. This is particularly useful for generating inverted constants when the positive value is not encodable but the negative is.

**Generating Constants**

MVN enables loading immediates that cannot be encoded directly in MOV. For example, if 0xFFFFFFFE is not encodable but 1 is, `MVN r0, #1` loads 0xFFFFFFFE. The assembler may automatically substitute MVN when MOV with a particular immediate is requested but not encodable.

**Example:**

```assembly
MVN r0, r1               @ r0 = ~r1
MVN r2, #0               @ r2 = 0xFFFFFFFF (all bits set)
MVNS r3, r4              @ r3 = ~r4, update flags
MVN r5, r6, LSL #2       @ r5 = ~(r6 << 2)
```

**Key Points:**

- MOV copies values between registers or loads immediates with encoding restrictions requiring MOVW/MOVT for arbitrary 32-bit values
- LDR and STR support multiple addressing modes (offset, pre-indexed, post-indexed) with size variants for bytes, halfwords, and words
- Immediate values in ARM32 instructions are limited to rotated 8-bit patterns; larger constants require literal pools or multiple instructions
- Arithmetic instructions include ADD/SUB for basic operations, ADC/SBC for multi-precision, and MUL variants for different multiplication types
- Division instructions (SDIV/UDIV) are only available on ARMv7-R, ARMv7-M with divide extensions, and ARMv8-A architectures [Inference: requiring software emulation on earlier cores]
- Logical operations (AND, ORR, EOR, BIC, MVN) manipulate individual bits for masking, combining, toggling, and clearing
- Instructions with 'S' suffix update condition flags based on operation results, enabling conditional execution and overflow detection
- Shifted register operands (LSL, LSR, ASR, ROR) extend instruction capabilities by preprocessing values before arithmetic or logical operations

**Important related topics:** Condition flags and conditional execution, barrel shifter operation details, multi-precision arithmetic implementation, memory alignment requirements and unaligned access handling, literal pool management and placement strategies, Thumb and Thumb-2 instruction encoding differences.

---

## Shift and Rotate Operations

Shift and rotate operations manipulate bit patterns within registers, enabling efficient multiplication/division by powers of 2, bit field extraction, and data packing operations.

### Logical Shift Left (LSL)

Logical Shift Left shifts bits toward the most significant bit position, filling vacated least significant bits with zeros. Bits shifted out from the MSB are discarded (or captured in the carry flag).

```
LSL r0, r1, #3        @ r0 = r1 << 3
```

**Example**
```
Initial:  r1 = 0b00010110 (22 decimal)
LSL #2:   r0 = 0b01011000 (88 decimal)
```

Each left shift by one position multiplies the value by 2. Shifting left by n positions multiplies by 2^n, providing efficient multiplication when the multiplier is a power of 2.

Overflow occurs when significant bits are shifted out. For unsigned values, this means the result exceeds the register size. For signed values, the sign bit may be lost, changing the number's sign unexpectedly.

### Logical Shift Right (LSR)

Logical Shift Right shifts bits toward the least significant bit position, filling vacated most significant bits with zeros. This operation is appropriate for unsigned values.

```
LSR r0, r1, #2        @ r0 = r1 >> 2 (logical)
```

**Example**
```
Initial:  r1 = 0b10110100 (180 decimal unsigned)
LSR #2:   r0 = 0b00101101 (45 decimal)
```

Each right shift by one position divides unsigned values by 2 (rounding toward zero). Shifting right by n positions divides by 2^n.

For signed negative values, LSR produces incorrect results because it fills with zeros rather than preserving the sign bit.

### Arithmetic Shift Right (ASR)

Arithmetic Shift Right shifts bits rightward while replicating the sign bit into vacated positions. This operation correctly divides signed two's complement values by powers of 2.

```
ASR r0, r1, #2        @ r0 = r1 >> 2 (arithmetic)
```

**Example** with negative value:
```
Initial:  r1 = 0b11110100 (-12 decimal in 8-bit)
ASR #2:   r0 = 0b11111101 (-3 decimal)
```

**Example** with positive value:
```
Initial:  r1 = 0b00110100 (52 decimal)
ASR #2:   r0 = 0b00001101 (13 decimal)
```

Sign extension during the shift maintains the correct sign of the result. ASR rounds toward negative infinity for negative numbers, which differs slightly from C language division that rounds toward zero.

### Rotate Right (ROR)

Rotate Right shifts bits rightward with bits shifted out from the LSB position re-entering at the MSB position. No information is lost—the operation is reversible.

```
ROR r0, r1, #4        @ r0 = r1 rotated right by 4
```

**Example**
```
Initial:     r1 = 0b10110011
ROR #3:      r0 = 0b01110110
                        ^^^--- these bits wrapped around
```

Rotation has no direct arithmetic interpretation but is useful for:
- Circular bit manipulation
- Cryptographic operations
- Extracting bit fields that wrap around word boundaries

### Rotate Right Extended (RRX)

Rotate Right Extended performs a 33-bit rotation through the carry flag. The carry flag becomes the new MSB, and the LSB shifts into the carry flag. This always rotates by exactly one bit position.

```
RRX r0, r1           @ r0 = (C:r1) >> 1, C = r1[0]
```

**Example** with carry = 1:
```
Carry: 1
Initial:     r1 = 0b10110011
RRX:         r0 = 0b11011001
             Carry = 1 (from LSB of r1)
```

RRX is useful for multi-word shifts where the carry propagates between words, and for implementing efficient division algorithms.

### Flexible Second Operand

ARM instructions often incorporate shifts into operand processing without requiring separate shift instructions. The second operand can include an inline shift operation.

```
ADD r0, r1, r2, LSL #2    @ r0 = r1 + (r2 << 2)
SUB r0, r1, r2, ASR #4    @ r0 = r1 - (r2 >> 4)
MOV r0, r1, ROR #8        @ r0 = r1 rotated right by 8
```

The shift amount can be specified as:
- Immediate constant (0-31 for 32-bit, 0-63 for 64-bit)
- Register value (bottom byte only)

```
ADD r0, r1, r2, LSL r3    @ r0 = r1 + (r2 << r3[7:0])
```

This flexible operand mechanism reduces instruction count by combining operations. [Inference] It exploits the barrel shifter hardware in ARM processors that can perform shifts in the same cycle as ALU operations.

### Barrel Shifter Integration

ARM's barrel shifter is hardware that performs shift operations in parallel with other operations. When a shift is specified as part of another instruction's operand, [Inference] the shift occurs without consuming additional execution time in most cases.

This architectural feature makes shifted operands essentially free, encouraging their use for:
- Array indexing: `LDR r0, [r1, r2, LSL #2]` loads from r1 + r2×4
- Efficient multiplication by constants: `ADD r0, r1, r1, LSL #2` computes r1×5
- Bit field manipulation

### Practical Applications

**Multiplication by constants**: Multiplying by 10 can be decomposed as (x × 8) + (x × 2):
```
ADD r0, r1, r1, LSL #3    @ r0 = r1 + (r1 × 8) = r1 × 9
```

Or more efficiently:
```
ADD r0, r1, r1, LSL #2    @ r0 = r1 + (r1 × 4) = r1 × 5
LSL r0, r0, #1            @ r0 = (r1 × 5) × 2 = r1 × 10
```

**Array indexing**: Accessing element i in a 4-byte word array at base address in r1:
```
LDR r0, [r1, r2, LSL #2]  @ Load array[r2] where each element is 4 bytes
```

**Bit field extraction**: Extracting bits [7:4] from a register:
```
LSR r0, r1, #4            @ Shift right to position bit 4 at bit 0
AND r0, r0, #0xF          @ Mask to keep only lower 4 bits
```

**Flag packing**: Combining multiple boolean flags into a single byte:
```
ORR r0, r0, r1, LSL #0    @ Pack flag1 (r1) at bit 0
ORR r0, r0, r2, LSL #1    @ Pack flag2 (r2) at bit 1
ORR r0, r0, r3, LSL #2    @ Pack flag3 (r3) at bit 2
```

## Comparison Instructions

Comparison instructions evaluate conditions by performing operations and updating status flags without storing results. These flags control conditional execution and branching.

### Status Flags (CPSR/APSR)

The Current Program Status Register (CPSR) in ARMv7, or Application Program Status Register (APSR) in ARMv8, contains condition flags in the upper bits:

**N (Negative)**: Set when the result's most significant bit is 1, indicating a negative value in two's complement representation.

**Z (Zero)**: Set when the result equals zero, with all bits zero.

**C (Carry)**: Set when an unsigned operation produces a carry out or requires a borrow. For addition, set when the result exceeds the register size. For subtraction, set when no borrow is required (first operand ≥ second operand).

**V (Overflow)**: Set when a signed operation produces a result outside the representable range. Occurs when adding two positive numbers yields a negative result, or adding two negative numbers yields a positive result.

These flags enable conditional operations without explicit branching, supporting efficient condition evaluation.

### CMP - Compare

CMP performs subtraction (first operand minus second operand) and updates flags without storing the result. It is equivalent to SUBS but discards the result, keeping only the flag updates.

```
CMP r0, r1            @ Set flags based on (r0 - r1)
CMP r0, #100          @ Set flags based on (r0 - 100)
```

**Example**: Comparing r0 = 5 and r1 = 3:
```
CMP r0, r1            @ Computes 5 - 3 = 2
```
Result flags: N=0 (positive), Z=0 (non-zero), C=1 (no borrow needed), V=0 (no overflow)

**Example**: Comparing r0 = 3 and r1 = 5:
```
CMP r0, r1            @ Computes 3 - 5 = -2
```
Result flags: N=1 (negative), Z=0 (non-zero), C=0 (borrow needed), V=0 (no overflow)

**Example**: Comparing r0 = 7 and r1 = 7:
```
CMP r0, r1            @ Computes 7 - 7 = 0
```
Result flags: N=0, Z=1 (zero), C=1 (no borrow), V=0

### CMN - Compare Negative

CMN performs addition (first operand plus second operand) and updates flags without storing the result. It is equivalent to ADDS but discards the result. CMN is useful for comparing against negative values without explicitly negating.

```
CMN r0, r1            @ Set flags based on (r0 + r1)
CMN r0, #-10          @ Effectively compares r0 with 10
```

Comparing r0 with -5 can be done as `CMN r0, #5` instead of `CMP r0, #-5`, which may be more efficient depending on immediate value encoding.

### TST - Test Bits

TST performs bitwise AND and updates flags without storing the result. It is equivalent to ANDS but discards the result. TST checks whether specific bits are set.

```
TST r0, r1            @ Set flags based on (r0 AND r1)
TST r0, #0x80         @ Test if bit 7 is set
```

Common uses:
- Testing if a specific bit is set: `TST r0, #(1<<5)` tests bit 5
- Checking if value is zero: `TST r0, r0`
- Testing multiple bits: `TST r0, #0x0F` checks if any of lower 4 bits are set

**Example**: r0 = 0b10110100, testing bit 7:
```
TST r0, #0x80         @ AND with 0b10000000
```
Result flags: N=1 (bit 31 of result in 32-bit), Z=0 (non-zero result)

**Example**: r0 = 0b01110100, testing bit 7:
```
TST r0, #0x80         @ AND with 0b10000000
```
Result flags: N=0, Z=1 (zero result, bit 7 not set)

### TEQ - Test Equivalence

TEQ performs bitwise XOR and updates flags without storing the result. It is equivalent to EORS but discards the result. TEQ checks whether values are equal or identifies differing bits.

```
TEQ r0, r1            @ Set flags based on (r0 XOR r1)
```

If r0 equals r1, the XOR result is zero and the Z flag sets. If they differ, the Z flag clears and the result shows which bits differ.

**Example**: Comparing r0 = 0b10110100 and r1 = 0b10110100:
```
TEQ r0, r1            @ XOR produces 0b00000000
```
Result flags: Z=1 (values are equal)

**Example**: Comparing r0 = 0b10110100 and r1 = 0b10010110:
```
TEQ r0, r1            @ XOR produces 0b00100010
```
Result flags: Z=0 (values differ), with differing bits shown in result

TEQ can efficiently check equality without affecting the carry flag, unlike CMP.

### Conditional Execution Suffixes

ARM supports conditional execution where instructions execute only when specified conditions are true based on current flags. Each instruction can include a two-letter condition suffix.

Condition codes:
- **EQ** (Equal): Z=1
- **NE** (Not Equal): Z=0
- **CS/HS** (Carry Set/Unsigned Higher or Same): C=1
- **CC/LO** (Carry Clear/Unsigned Lower): C=0
- **MI** (Minus/Negative): N=1
- **PL** (Plus/Positive or Zero): N=0
- **VS** (Overflow Set): V=1
- **VC** (Overflow Clear): V=0
- **HI** (Unsigned Higher): C=1 and Z=0
- **LS** (Unsigned Lower or Same): C=0 or Z=1
- **GE** (Signed Greater or Equal): N=V
- **LT** (Signed Less Than): N≠V
- **GT** (Signed Greater Than): Z=0 and N=V
- **LE** (Signed Less or Equal): Z=1 or N≠V
- **AL** (Always): unconditional (default)

**Example** conditional execution:
```
CMP r0, #10
ADDGT r1, r1, #1      @ Execute only if r0 > 10 (signed)
MOVLE r1, #0          @ Execute only if r0 <= 10 (signed)
```

**Example** avoiding branches:
```
CMP r0, r1
MOVGT r2, r0          @ r2 = r0 if r0 > r1
MOVLE r2, r1          @ r2 = r1 if r0 <= r1
@ r2 now contains max(r0, r1)
```

[Inference] Conditional execution reduces branch instructions, improving performance on pipelined processors by avoiding pipeline flushes. However, ARMv8 AArch64 mode removed most conditional execution, retaining only conditional branches and select operations.

### Signed vs Unsigned Comparisons

Signed and unsigned comparisons require different condition codes because the flag interpretations differ.

**Unsigned comparison** uses carry flag:
```
CMP r0, r1
BHI label             @ Branch if r0 > r1 (unsigned)
BLS label             @ Branch if r0 <= r1 (unsigned)
```

**Signed comparison** uses negative and overflow flags:
```
CMP r0, r1
BGT label             @ Branch if r0 > r1 (signed)
BLE label             @ Branch if r0 <= r1 (signed)
```

**Example** where signed/unsigned matters: r0 = 0xFFFFFFFF, r1 = 0x00000001
- Unsigned: r0 (4,294,967,295) > r1 (1), so HI condition is true
- Signed: r0 (-1) < r1 (1), so LT condition is true

Using the wrong condition code for the data type produces incorrect comparisons.

### Flag-Setting Variants

Most ALU instructions have two forms: one that updates flags (with 'S' suffix) and one that does not.

```
ADD r0, r1, r2        @ r0 = r1 + r2, flags unchanged
ADDS r0, r1, r2       @ r0 = r1 + r2, update N,Z,C,V flags

SUB r0, r1, r2        @ r0 = r1 - r2, flags unchanged
SUBS r0, r1, r2       @ r0 = r1 - r2, update flags
```

CMP, CMN, TST, and TEQ implicitly set flags, as they exist specifically for condition evaluation. In ARMv8 AArch64, the 'S' suffix is explicit in the mnemonic (ADDS, SUBS) rather than optional.

## Instruction Encoding and Formats

ARM instructions are encoded as fixed-width binary patterns that the processor decodes and executes. Understanding encoding helps optimize code and explains instruction limitations.

### Fixed-Width Instruction Encoding

ARMv7 uses 32-bit fixed-width instruction encoding in ARM mode. Every instruction occupies exactly 4 bytes in memory, simplifying instruction fetch and pipeline design. [Inference] Fixed-width encoding allows the processor to predict instruction locations and fetch multiple instructions efficiently.

ARMv7 also supports Thumb mode with 16-bit instructions (and Thumb-2 with mixed 16/32-bit instructions), providing better code density at some cost to functionality per instruction.

ARMv8 AArch64 uses 32-bit fixed-width encoding uniformly, abandoning variable-width encoding.

### Instruction Format Components

ARM instructions encode several fields within the 32-bit word:

**Condition field** (bits 31-28): 4-bit field specifying execution condition. 0b1110 (14) represents "always" (AL), the unconditional default.

**Opcode field**: Specifies the operation type (ADD, SUB, LDR, etc.). Location and size vary by instruction type.

**Register fields**: Specify source and destination registers. Each register field is typically 4 bits (allowing selection of 16 registers R0-R15).

**Operand fields**: Encode immediate values, shift amounts, or addressing mode parameters. Size and interpretation depend on instruction type.

**Modifier bits**: Control flag updates (S bit), addressing modes, data sizes, etc.

### Data Processing Format

Data processing instructions (arithmetic, logical, comparison) follow a common encoding structure:

```
[Cond][00][I][Opcode][S][Rn][Rd][Operand2]
```

- **Cond** (4 bits): Condition code
- **I** (1 bit): Immediate operand flag (0=register, 1=immediate)
- **Opcode** (4 bits): Operation (ADD=0100, SUB=0010, etc.)
- **S** (1 bit): Set condition flags
- **Rn** (4 bits): First source register
- **Rd** (4 bits): Destination register
- **Operand2** (12 bits): Second operand encoding

**Example** encoding of `ADD r0, r1, r2`:
```
Cond=1110 (AL), I=0 (register), Opcode=0100 (ADD), S=0
Rn=0001 (r1), Rd=0000 (r0), Rm=0010 (r2), Shift=00000000
Result: 0xE0810002
```

### Immediate Value Encoding

Immediate values face encoding constraints within the 12-bit Operand2 field. ARM uses a special encoding scheme with 8-bit value and 4-bit rotation.

An immediate is encoded as: 8-bit value rotated right by (2 × 4-bit rotate field) positions.

This allows representing various useful constants efficiently:
- 0-255: directly encodable
- 0xFF00, 0xFF0000, 0xFF000000: rotations of 0xFF
- Powers of 2 and nearby values

**Example** valid immediates:
- #100: 0x64, directly encodable
- #0x300: 0x03 rotated right by 24 (rotate=12)
- #0xFF000000: 0xFF rotated right by 8 (rotate=4)

**Example** invalid immediates:
- #0x101: Cannot be represented as rotated 8-bit value
- #0xFFFF: Cannot be represented in the encoding scheme

[Inference] For invalid immediates, the assembler may use alternate approaches like loading from a literal pool, or decomposing into multiple operations.

### Load/Store Format

Load and store instructions encode memory addressing modes:

```
[Cond][01][I][P][U][B][W][L][Rn][Rd][Offset]
```

- **Cond** (4 bits): Condition code
- **I** (1 bit): Immediate offset (0) or register offset (1)
- **P** (1 bit): Pre-indexed (1) or post-indexed (0)
- **U** (1 bit): Add offset (1) or subtract offset (0)
- **B** (1 bit): Byte transfer (1) or word transfer (0)
- **W** (1 bit): Write-back (update base register)
- **L** (1 bit): Load (1) or store (0)
- **Rn** (4 bits): Base address register
- **Rd** (4 bits): Source/destination register
- **Offset** (12 bits): Offset encoding

**Example** `LDR r0, [r1, #8]`:
```
Cond=1110 (AL), I=0, P=1 (pre-indexed), U=1 (add), B=0 (word)
W=0 (no writeback), L=1 (load), Rn=0001 (r1), Rd=0000 (r0)
Offset=8
```

### Branch Format

Branch instructions encode target addresses as PC-relative offsets:

```
[Cond][101][L][Offset]
```

- **Cond** (4 bits): Condition code
- **L** (1 bit): Link bit (save return address for BL)
- **Offset** (24 bits): Signed offset in words (×4 for byte offset)

The 24-bit offset allows branching ±32MB from the current instruction. Offsets are multiplied by 4 because instructions are word-aligned.

**Example** branch calculation:
```
Current PC = 0x8000
Offset = 0x000010 (16 decimal)
Target = PC + (Offset << 2) + 8
       = 0x8000 + (16 × 4) + 8
       = 0x8048
```

The "+8" accounts for ARM's pipeline where PC points two instructions ahead during execution.

### Thumb Encoding

Thumb mode uses 16-bit instruction encoding for improved code density. Thumb instructions have limited functionality compared to ARM instructions:

- Access to only lower 8 registers (R0-R7) in most instructions
- Limited immediate ranges
- Fewer addressing modes
- Most instructions unconditional

Thumb-2 extends Thumb with 32-bit instructions (encoded as two 16-bit halfwords) that provide functionality approaching ARM mode while maintaining code density benefits.

**Example** Thumb instruction `ADDS r0, r1, r2` (16-bit):
```
Format: [000110][0][Rm][Rn][Rd]
Opcode=000110 (ADD), S=0, Rm=010 (r2), Rn=001 (r1), Rd=000 (r0)
```

Thumb-2 allows mixed 16-bit and 32-bit instructions in the same code, with [Inference] the processor detecting instruction width from specific bit patterns in the first halfword.

### Literal Pool

When immediate values cannot be encoded directly, assemblers place them in a literal pool—a region of constant data near the code. Instructions load these values using PC-relative addressing.

```
LDR r0, =0x12345678   @ Assembler directive
@ Becomes:
LDR r0, [PC, #offset] @ Load from literal pool

@ Later in code:
.ltorg                @ Literal pool location
.word 0x12345678      @ Constant value
```

The assembler automatically manages literal pool placement and generates appropriate load instructions. Programmers can explicitly place pools using `.ltorg` directives to ensure they remain within the ±4KB offset range of referring instructions.

### Instruction Alignment

Instructions must be aligned according to their size:
- ARM mode: 4-byte alignment (addresses divisible by 4)
- Thumb mode: 2-byte alignment (addresses divisible by 2)

[Unverified] Executing an instruction from a misaligned address typically triggers an alignment fault. The processor may use the lowest bits of the PC to determine instruction set mode, so proper alignment is critical.

## Instruction Cycle and Execution

Understanding how processors execute instructions helps optimize code and reason about performance characteristics.

### Classical Fetch-Decode-Execute Cycle

Processors execute instructions through a repeating cycle:

**Fetch**: The processor reads the instruction from memory at the address stored in the Program Counter (PC). The instruction is transferred from memory to the instruction register.

**Decode**: The control unit interprets the instruction encoding, identifying the operation, operands, and addressing modes. It generates control signals for subsequent stages.

**Execute**: The ALU or relevant functional unit performs the specified operation on the operands.

**Memory Access**: If the instruction requires memory access (load/store), the processor reads from or writes to memory at the calculated address.

**Writeback**: The result is written to the destination register or memory location. The PC is updated to point to the next instruction.

This cycle repeats continuously during program execution. Each cycle processes one instruction in a simple processor implementation.

### Pipelining

Modern ARM processors implement instruction pipelining, where multiple instructions occupy different stages simultaneously. While one instruction executes, the next instruction decodes, and another fetches.

Classical ARM pipeline stages:

**Fetch (F)**: Retrieve instruction from memory
**Decode (D)**: Interpret instruction and read registers
**Execute (E)**: Perform ALU operation or calculate address
**Memory (M)**: Access memory for load/store
**Writeback (W)**: Write result to register

**Example** pipeline execution:
```
Cycle:   1    2    3    4    5    6    7
Inst1:   F    D    E    M    W
Inst2:        F    D    E    M    W
Inst3:             F    D    E    M    W
Inst4:                  F    D    E    M    W
```

[Inference] Pipelining increases throughput—multiple instructions complete per cycle—though individual instruction latency remains multiple cycles. Ideal pipelining achieves one instruction completion per cycle in steady state.

### Pipeline Hazards

Pipelining encounters hazards that prevent the next instruction from executing in the following cycle:

**Data hazards** occur when an instruction depends on the result of a previous instruction still in the pipeline:
```
ADD r0, r1, r2        @ Cycle 1: F
SUB r3, r0, r4        @ Cycle 2: F, but needs r0 from ADD not yet available
```

**Solutions** include:
- **Stalling**: Delay the dependent instruction until the result is ready
- **Forwarding**: Route the result directly from execute/memory stage to the next instruction's execute stage, bypassing writeback
- **Instruction reordering**: Compiler or programmer rearranges independent instructions to separate dependent ones

**Control hazards** occur at branches where the next instruction address is unknown until the branch resolves:
```
CMP r0, #10
BEQ label             @ Branch target unknown until CMP completes
```

**Solutions** include:
- **Branch prediction**: [Inference] Guess the branch outcome and speculatively execute predicted path
- **Delayed branch slots**: Execute instruction(s) following the branch regardless of outcome
- **Pipeline flush**: Discard speculatively executed instructions if prediction was wrong

**Structural hazards** occur when hardware resources are insufficient for simultaneous instruction requirements. [Inference] ARM processors typically avoid structural hazards through resource duplication or scheduling constraints.

### Branch Prediction

Modern processors predict branch outcomes to maintain pipeline flow. [Inference] Predictors use various strategies:

**Static prediction**: Always predict taken or not-taken based on branch type (backward branches often taken in loops, forward branches often not taken).

**Dynamic prediction**: Track branch history and predict based on past behavior. Simple 2-bit saturating counters predict each branch individually. More sophisticated predictors use global history or correlation.

**Example** loop performance with prediction:
```
MOV r0, #0
MOV r1, #100
loop:
    ADD r0, r0, #1
    CMP r0, r1
    BNE loop          @ Predicted taken 99 times, not-taken once
```

[Inference] Correct predictions maintain full pipeline throughput. Mispredictions cause pipeline flushes, losing several cycles. Well-predicted loops execute efficiently; unpredictable branches cause performance problems.

### Conditional Execution and Predication

ARM's conditional execution avoids some branches entirely by making instruction execution dependent on condition flags:

```
CMP r0, r1
MOVGT r2, r0          @ Conditionally executed
MOVLE r2, r1          @ Conditionally executed
```

[Inference] Conditional instructions enter the pipeline and proceed through decode and execute stages, but writeback is suppressed if the condition is false. This avoids branch misprediction penalties for simple conditional operations but consumes pipeline bandwidth even when not executing.

[Inference] Conditional execution is most beneficial for short code sequences where branching overhead exceeds the cost of executing instructions conditionally. For longer sequences or highly predictable branches, conventional branching may be more efficient.

ARMv8 AArch64 removed most conditional execution, retaining only conditional select (CSEL) instructions and conditional branches, relying instead on improved branch prediction.

### Instruction Timing

Instruction execution time varies by operation complexity and pipeline state:

**Simple ALU operations** (ADD, SUB, AND, OR, etc.): [Inference] Typically 1 cycle in the execute stage, completing in 1 cycle throughput when pipelined.

**Shifts and rotates**: [Inference] Usually incorporated into other operations through the barrel shifter without additional cycles, or 1 cycle when standalone.

**Multiply instructions**: [Inference] Historically required multiple cycles (2-4 cycles) depending on operand values. Modern processors may complete multiplication in 1-2 cycles or pipeline multiplies for 1 cycle throughput.

**Division instructions**: [Inference] Significantly slower than multiplication, potentially requiring 10-20+ cycles. Software division may be competitive with hardware division for some processors.

**Memory access** (LDR, STR): [Inference] 1 cycle if data is in L1 cache, potentially 10-100+ cycles for cache misses reaching main memory. Load-use latency (cycles until loaded data is available) affects dependent instructions.

**Branches**: [Inference] 1 cycle when correctly predicted, 3-20+ cycles when mispredicted depending on pipeline depth and branch resolution point.

[Unverified] These timings are general characteristics that vary significantly across ARM processor implementations (Cortex-A, Cortex-R, Cortex-M series) and generations.

### Superscalar Execution

Advanced ARM processors implement superscalar execution, issuing multiple instructions per cycle. [Inference] The processor includes multiple execution units (ALUs, load/store units, branch units) that can operate in parallel.

**Example** dual-issue processor:
```
Cycle:   1    2    3
Inst1:   F    D    E      @ ALU operation
Inst2:   F    D    E      @ ALU operation (parallel with Inst1)
Inst3:        F    D      @ Load operation
```

[Inference] The processor analyzes instruction dependencies and resource requirements to determine which instructions can execute simultaneously. Out-of-order execution allows later independent instructions to execute before earlier dependent instructions complete.

These advanced features are largely invisible to assembly programmers but understanding them helps appreciate why instruction ordering and independence affect performance.

### Memory Ordering and Barriers

ARM processors may reorder memory accesses for performance, executing them out of program order when dependencies allow. [Inference] This optimization is generally safe for single-threaded code but can cause unexpected behavior in multi-threaded or memory-mapped I/O contexts.

Memory barrier instructions enforce ordering:

**DMB (Data Memory Barrier)**: Ensures memory accesses before the barrier are observed before accesses after the barrier.

**DSB (Data Synchronization Barrier)**: Ensures all memory accesses complete before subsequent instructions execute.

**ISB (Instruction Synchronization Barrier)**: Flushes pipeline and ensures subsequent instructions are fetched after the barrier.

```
STR r0, [r1]          @ Write data
DMB                   @ Ensure write completes
STR r2, [r3]          @ Write flag
```

These barriers are essential for synchronization primitives, device driver code, and any situation where memory access order is semantically significant.

### Cache Interaction

Cache organization significantly affects instruction execution performance. [Inference] The processor checks cache levels for both instruction and data accesses:

**Instruction cache (I-cache)**: Stores recently fetched instructions. Cache hits provide fast instruction fetch. Cache misses stall the pipeline while fetching from slower memory levels.

**Data cache (D-cache)**: Stores recently accessed data. Load hits return data quickly. Load misses stall dependent instructions. Store operations may buffer in write buffers.

### Cache Line Fills and Write Policies

Cache operates on cache lines (typically 32, 64, or 128 bytes). When a cache miss occurs, the processor fetches an entire cache line from the next memory level, not just the requested byte or word. [Inference] This exploits spatial locality—nearby addresses are often accessed soon after.

**Write-through cache**: Writes update both cache and main memory simultaneously. [Inference] This ensures memory consistency but increases write latency.

**Write-back cache**: Writes update only cache initially, marking the line as dirty. [Inference] The dirty line is written to main memory only when evicted. This reduces memory traffic but requires cache coherency protocols in multi-core systems.

**Write-combining buffer**: Accumulates multiple writes to adjacent addresses before committing them to memory. [Inference] This is particularly useful for memory-mapped I/O regions where burst writes are more efficient than individual writes.

### Cache Associativity

Cache associativity determines where a memory address can be stored:

**Direct-mapped cache**: Each memory address maps to exactly one cache line location. Simple and fast but suffers from conflicts when multiple frequently-accessed addresses map to the same line.

**N-way set-associative cache**: Each memory address can be stored in any of N cache lines within a specific set. [Inference] 4-way or 8-way associativity balances conflict reduction with lookup complexity.

**Fully associative cache**: Any memory address can be stored in any cache line. [Inference] Eliminates conflict misses but requires complex hardware to search all entries. Typically used only for small caches like TLBs.

[Inference] Understanding cache behavior helps explain why reordering data accesses or padding data structures can significantly impact performance, even when total work remains constant.

### Instruction Prefetching

Modern processors prefetch instructions ahead of current execution to hide memory latency. [Inference] Hardware prefetchers detect sequential access patterns and speculatively fetch upcoming cache lines.

**Sequential prefetch**: Automatically fetches the next sequential cache line when accessing instructions.

**Branch prediction-driven prefetch**: [Inference] Prefetches instructions from predicted branch targets.

Prefetching improves performance for predictable code flow but wastes memory bandwidth for unpredictable control flow. [Inference] Tight loops that fit in cache benefit maximally from prefetching and caching.

### Data Prefetching

ARM provides explicit prefetch instructions for data:

```
PLD [r0]              @ Prefetch data at address in r0
PLDW [r0]             @ Prefetch for write
```

[Inference] These are hints to the memory system—they never cause faults and may be ignored by the processor. Strategic prefetching can hide memory latency:

```
    LDR r1, [r0]          @ Current load
    PLD [r0, #64]         @ Prefetch next cache line
    @ Process r1...
    ADD r0, r0, #64
    LDR r1, [r0]          @ Likely cache hit due to prefetch
```

[Inference] Overusing prefetch can pollute cache with unneeded data or waste memory bandwidth. Effective prefetch requires understanding access patterns and memory latency.

### Load-Store Unit Operation

The load-store unit handles memory access instructions independently from ALU operations in superscalar processors. [Inference] Multiple loads/stores can be in flight simultaneously:

**Store buffer**: Holds pending stores that can complete out of order. [Inference] The processor doesn't wait for store completion before proceeding, hiding store latency.

**Load forwarding**: [Inference] If a load reads from an address with a pending store, the data forwards directly from the store buffer, avoiding a cache access.

**Example** load-store sequence:

```
STR r0, [r2]          @ Store enters store buffer
LDR r1, [r3]          @ Load proceeds independently
LDR r4, [r2]          @ May forward from store buffer if addresses match
```

[Inference] This allows loads and stores to execute out of program order when addresses don't conflict, increasing instruction throughput.

### Memory Access Latency

Memory access latency varies dramatically by location:

**L1 cache**: [Inference] ~1-4 cycles typical latency **L2 cache**: [Inference] ~10-20 cycles typical latency **L3 cache**: [Inference] ~20-40 cycles typical latency (if present) **Main memory (DRAM)**: [Inference] ~100-300 cycles typical latency **Page fault to disk**: [Inference] Millions of cycles

[Inference] The processor continues executing independent instructions during load latency until a dependent instruction requires the loaded value (load-use penalty).

**Example** showing load-use latency:

```
LDR r0, [r1]          @ Load starts, takes N cycles
ADD r2, r2, #1        @ Independent, executes during load
MUL r3, r3, r4        @ Independent, executes during load
ADD r5, r0, r6        @ Dependent on r0, stalls if load incomplete
```

Reordering code to separate loads from dependent instructions reduces stalls.

### Multi-Core Considerations

Modern ARM systems are multi-core, with each core having private L1 caches and shared L2/L3 caches. [Inference] This creates cache coherency challenges when multiple cores access the same memory.

**Cache coherency protocols**: [Inference] Hardware automatically maintains consistency between cores' caches, using protocols like MESI or MOESI. When one core writes to a cache line, other cores with copies are notified to invalidate or update their versions.

**False sharing**: Occurs when different cores access different variables that reside in the same cache line. [Inference] Each write invalidates the line in other cores' caches even though they access different data, causing performance degradation.

**Example** false sharing:

```c
struct {
    int counter1;     // Used by core 0
    int counter2;     // Used by core 1, but same cache line
} data;
```

[Inference] Padding structures to separate frequently-written fields onto different cache lines prevents false sharing.

### Atomic Operations

Multi-threaded code requires atomic operations for synchronization. ARM provides exclusive load/store instructions:

```
LDREX r0, [r1]        @ Exclusive load
@ Modify r0...
STREX r2, r0, [r1]    @ Exclusive store, r2=0 if successful
CMP r2, #0
BNE retry             @ Retry if store failed
```

**LDREX** marks a memory location for exclusive access. **STREX** succeeds only if no other core has accessed that location since the LDREX. [Inference] The processor monitors the address using cache coherency hardware.

This implements **Load-Linked/Store-Conditional** semantics for lock-free algorithms and mutex implementation.

ARMv8.1+ adds **atomic operations** like:

```
LDADD r0, r1, [r2]    @ Atomically: r1 = [r2], [r2] += r0
CAS r0, r1, [r2]      @ Compare-and-swap
```

[Inference] These execute atomically without explicit loops, potentially improving performance for contended synchronization.

### Pipeline Depth and Branch Misprediction Cost

Pipeline depth—the number of stages from fetch to writeback—directly affects branch misprediction penalty. [Inference] Deeper pipelines achieve higher clock frequencies but increase misprediction costs.

**Shallow pipeline** (5-8 stages): [Inference] Lower misprediction penalty (3-8 cycles), lower clock frequency

**Deep pipeline** (12-20+ stages): [Inference] Higher misprediction penalty (12-20+ cycles), higher clock frequency potential

**Example** comparing branches:

```
@ Predictable loop - low misprediction rate
MOV r0, #0
loop:
    ADD r0, r0, #1
    CMP r0, #1000
    BLT loop          @ 999 correct predictions, 1 misprediction

@ Unpredictable branch - 50% misprediction rate
LDR r0, [r1]
TST r0, #1
BEQ label             @ Depends on runtime data
```

[Inference] The predictable loop costs approximately (1000 instructions + 1 misprediction penalty), while unpredictable branches cost (instruction + 50% × misprediction penalty) on average.

### Instruction Fusion and Macro-op Fusion

Advanced processors detect common instruction sequences and fuse them into single operations internally. [Inference] This reduces pressure on pipeline resources.

**Example** commonly fused sequences:

```
CMP r0, #0            @ Compare and branch
BEQ label             @ May fuse into single operation

MOV r0, #value
@ Load immediate sequences may optimize internally
```

[Inference] Fusion is transparent to the programmer but can make certain instruction sequences unexpectedly efficient. Compilers sometimes generate code to exploit known fusion patterns.

### Execution Units and Instruction Dispatch

Superscalar processors contain multiple specialized execution units:

**Integer ALU units**: [Inference] Often 2-4 units for basic arithmetic and logical operations

**Complex integer unit**: [Inference] Handles multiplication, division, potentially at lower throughput

**Load-store units**: [Inference] Typically 1-2 units for memory operations

**Branch unit**: [Inference] Resolves branch conditions and calculates targets

**Floating-point/SIMD units**: [Inference] Dedicated units for FP and vector operations

[Inference] The instruction dispatch logic examines upcoming instructions and routes them to available execution units. Instructions compete for limited resources—having 4 ALU operations that can execute in parallel is beneficial, but 4 multiplies may bottleneck on a single multiply unit.

### Speculative Execution

Modern processors execute instructions speculatively before knowing they're definitely needed:

**Branch speculation**: Execute predicted branch path before confirming branch outcome. Discard results and restart if prediction was wrong.

**Memory disambiguation**: [Inference] Execute loads before knowing whether earlier store addresses conflict. Roll back if conflict detected.

**Example** speculative execution:

```
    CMP r0, #10
    BGE skip
    LDR r1, [r2]      @ Speculatively executes before BGE resolves
    ADD r3, r1, #5
skip:
    @ Continue...
```

[Inference] If BGE is predicted not-taken, the processor executes LDR and ADD before knowing the branch outcome. If prediction is correct, work completes earlier. If incorrect, speculative work is discarded.

[Inference] Speculative execution enables high performance but complicates security (as demonstrated by Spectre/Meltdown vulnerabilities) since speculative instructions can affect micro-architectural state like caches.

### Instruction Retirement and Reorder Buffer

Out-of-order processors use a reorder buffer to track in-flight instructions:

[Inference] Instructions execute out of program order based on operand availability and resource availability. However, instructions **retire** (commit results permanently) in program order to maintain precise architectural state for exceptions.

**Example** out-of-order execution:

```
Program order:
    LDR r0, [r1]      @ 1: Cache miss, long latency
    ADD r2, r3, r4    @ 2: Independent, executes immediately
    MUL r5, r6, r7    @ 3: Independent, executes immediately  
    ADD r8, r0, r9    @ 4: Depends on instruction 1, waits

Execution order: 2, 3, 1, 4
Retirement order: 1, 2, 3, 4 (program order)
```

[Inference] The reorder buffer allows execution flexibility while preserving program semantics and enabling precise exception handling.

### Exception and Interrupt Handling During Execution

Exceptions interrupt normal execution flow, requiring the processor to save state and transfer control to exception handlers:

**Precise exceptions**: The processor ensures all instructions before the exception complete, and no instructions after the exception have modified architectural state. [Inference] This requires flushing speculative work from the pipeline.

**Exception types**:

- **Synchronous exceptions**: Caused by executing instruction (undefined instruction, data abort, alignment fault)
- **Asynchronous exceptions (interrupts)**: Caused by external events (timer, peripheral)

When an exception occurs:

1. [Inference] The processor saves the PC and CPSR to exception-specific registers (LR_exception, SPSR_exception)
2. Switches to appropriate exception mode
3. [Inference] Updates PC to the exception vector address
4. Exception handler executes
5. Handler returns using special instruction (MOVS PC, LR or ERET in ARMv8), restoring saved state

**Example** exception vector table (ARMv7):

```
0x00000000: Reset
0x00000004: Undefined Instruction
0x00000008: Supervisor Call (SVC)
0x0000000C: Prefetch Abort
0x00000010: Data Abort
0x00000014: Reserved
0x00000018: IRQ (Interrupt Request)
0x0000001C: FIQ (Fast Interrupt Request)
```

### Performance Counters

ARM processors include performance monitoring units (PMU) with hardware counters tracking events:

[Unverified] Typical counters include:

- Cycle count
- Instruction count
- Cache hits/misses (I-cache, D-cache, L2, L3)
- Branch predictions/mispredictions
- Pipeline stalls
- Load/store unit events
- TLB hits/misses

These counters help developers measure and optimize performance, identifying bottlenecks like cache misses or branch mispredictions.

[Unverified] Accessing counters typically requires privileged mode, and configuration/read mechanisms are processor-specific.

### Power Management and Execution

Modern ARM processors implement dynamic power management affecting execution:

**Dynamic voltage and frequency scaling (DVFS)**: [Inference] Adjusts clock frequency and voltage based on workload, reducing power when high performance isn't needed but affecting instruction execution rate.

**Clock gating**: [Inference] Disables clock to idle execution units, saving power without affecting active units.

**Power domains**: [Inference] Individual cores or clusters can be powered down completely when unused, with wake latency when work arrives.

**WFI (Wait For Interrupt)** instruction: Puts the processor in low-power state until an interrupt occurs:

```
WFI                   @ Enter idle state
@ Execution resumes here after interrupt
```

[Inference] Understanding power states is important for embedded systems and helps explain why real-time measurements may show performance variations.

### Deterministic Execution Considerations

Some ARM processors (particularly Cortex-R series for real-time systems) emphasize deterministic execution:

[Inference] Features supporting determinism:

- Predictable cache behavior (or no cache)
- Tightly coupled memory (TCM) with guaranteed latency
- Simplified pipelines with predictable timing
- Disabled or controlled speculation

[Inference] Real-time software requires knowing worst-case execution time (WCET), making predictability more valuable than average-case performance.

**Key Points**

Instruction execution involves a sophisticated pipeline processing multiple instructions simultaneously through fetch, decode, execute, memory, and writeback stages. Pipelining increases throughput but introduces hazards requiring solutions like forwarding, stalling, or reordering. Branch prediction maintains pipeline flow for control instructions, with mispredictions causing significant penalties. ARM's conditional execution can avoid branches for simple conditional operations. Memory access interacts with a cache hierarchy where hits complete quickly but misses incur substantial latency. Modern superscalar processors execute multiple instructions per cycle using multiple execution units and out-of-order execution, though instructions retire in program order for correctness. Understanding these execution mechanisms—including cache behavior, speculation, atomic operations for multi-core systems, and performance monitoring—enables writing efficient assembly code and reasoning about performance characteristics. The complexity of modern processors means [Inference] actual behavior varies significantly across implementations, making performance measurement and profiling essential for optimization work.

---

# Memory and Addressing

ARM processors implement a load/store architecture where arithmetic and logical operations occur exclusively on registers, with separate instructions dedicated to transferring data between registers and memory. This design philosophy distinguishes ARM from x86's complex instruction set, where operations can directly access memory operands.

## Load and Store Architecture

### Architecture Principles

Load/store architecture enforces a clear separation between computation and memory access. All data processing instructions operate only on register operands, while dedicated load and store instructions handle memory transfers.

**Architecture Characteristics:**

- Memory access only through load (LDR) and store (STR) instructions
- All arithmetic/logical operations use register operands exclusively
- Predictable instruction timing and pipeline efficiency
- Simplified instruction decoding and execution
- Reduced instruction complexity enables higher clock frequencies

**Comparison with CISC:**

```assembly
# ARM (Load/Store Architecture)
ldr r0, [r1]          @ Load from memory to register
add r0, r0, #5        @ Arithmetic on registers
str r0, [r1]          @ Store back to memory

# x86 (CISC - direct memory operations)
add [ebx], 5          @ Arithmetic directly on memory
```

### Load Instructions

Load instructions transfer data from memory into registers.

**Basic Load Syntax:**

```assembly
ldr r0, [r1]          @ Load word (32-bit) from address in r1
ldrb r0, [r1]         @ Load byte (8-bit), zero-extend to 32-bit
ldrh r0, [r1]         @ Load halfword (16-bit), zero-extend
ldrsb r0, [r1]        @ Load signed byte, sign-extend to 32-bit
ldrsh r0, [r1]        @ Load signed halfword, sign-extend
```

**Data Size Suffixes:**

- No suffix or `W`: Word (32-bit)
- `B`: Byte (8-bit)
- `H`: Halfword (16-bit)
- `SB`: Signed byte
- `SH`: Signed halfword
- `D`: Doubleword (64-bit, loads two consecutive registers)

**Sign Extension Example:**

```assembly
# Memory at 0x20000000 contains: 0xFF
ldrb r0, [r1]         @ r0 = 0x000000FF (zero-extended)
ldrsb r0, [r1]        @ r0 = 0xFFFFFFFF (sign-extended)

# Memory at 0x20000000 contains: 0x8000
ldrh r0, [r1]         @ r0 = 0x00008000 (zero-extended)
ldrsh r0, [r1]        @ r0 = 0xFFFF8000 (sign-extended)
```

**Multiple Register Loads:**

```assembly
ldm r0, {r1-r5}       @ Load multiple: r1, r2, r3, r4, r5 from [r0]
ldmia r0, {r1-r5}     @ Increment After (same as ldm)
ldmib r0, {r1-r5}     @ Increment Before
ldmda r0, {r1-r5}     @ Decrement After
ldmdb r0, {r1-r5}     @ Decrement Before

# Stack operations (aliases)
pop {r0-r3}           @ Same as ldmia sp!, {r0-r3}
```

**Doubleword Load:**

```assembly
# Load 64-bit value into r0 and r1
ldrd r0, r1, [r2]     @ r0 = [r2], r1 = [r2+4]
                      @ r0 must be even-numbered register
```

### Store Instructions

Store instructions transfer data from registers into memory.

**Basic Store Syntax:**

```assembly
str r0, [r1]          @ Store word (32-bit) to address in r1
strb r0, [r1]         @ Store byte (lower 8 bits of r0)
strh r0, [r1]         @ Store halfword (lower 16 bits of r0)
```

**Partial Register Storage:**

```assembly
# r0 contains 0x12345678
strb r0, [r1]         @ Memory at [r1] = 0x78 (lower byte only)
strh r0, [r1]         @ Memory at [r1] = 0x5678 (lower halfword)
str r0, [r1]          @ Memory at [r1] = 0x12345678 (full word)
```

**Multiple Register Stores:**

```assembly
stm r0, {r1-r5}       @ Store multiple: r1, r2, r3, r4, r5 to [r0]
stmia r0, {r1-r5}     @ Increment After
stmib r0, {r1-r5}     @ Increment Before
stmda r0, {r1-r5}     @ Decrement After
stmdb r0, {r1-r5}     @ Decrement Before

# Stack operations (aliases)
push {r0-r3}          @ Same as stmdb sp!, {r0-r3}
```

**Doubleword Store:**

```assembly
# Store r0 and r1 as 64-bit value
strd r0, r1, [r2]     @ [r2] = r0, [r2+4] = r1
                      @ r0 must be even-numbered register
```

### Memory Alignment

ARM processors have alignment requirements for efficient memory access.

**Alignment Rules:**

- Word (32-bit) accesses must be 4-byte aligned
- Halfword (16-bit) accesses must be 2-byte aligned
- Byte (8-bit) accesses can be unaligned
- Doubleword (64-bit) accesses must be 8-byte aligned

**Unaligned Access Behavior:**

```assembly
# Address 0x20000001 (not word-aligned)
ldr r0, [r1]          @ ARMv6 and later: supports unaligned access
                      @ ARMv5 and earlier: undefined or fault
                      @ Cortex-M0/M0+: fault (no unaligned support)
                      @ Cortex-M3/M4/M7: allowed but slower
```

**Alignment Configuration:**

```assembly
# Cortex-M: CCR.UNALIGN_TRP bit controls unaligned access
ldr r0, =0xE000ED14   @ Configuration Control Register
ldr r1, [r0]
orr r1, r1, #8        @ Set UNALIGN_TRP bit
str r1, [r0]          @ Unaligned access will fault
```

### Endianness

ARM supports both little-endian and big-endian byte ordering.

**Little-Endian (default and most common):**

```assembly
# Store 0x12345678 at address 0x1000
str r0, [r1]

# Memory layout:
# 0x1000: 0x78  (LSB)
# 0x1001: 0x56
# 0x1002: 0x34
# 0x1003: 0x12  (MSB)
```

**Big-Endian:**

```assembly
# Same value stored big-endian
# 0x1000: 0x12  (MSB)
# 0x1001: 0x34
# 0x1002: 0x56
# 0x1003: 0x78  (LSB)
```

**Byte Reversal:**

```assembly
# Convert between endianness
rev r0, r1            @ Reverse byte order in word
rev16 r0, r1          @ Reverse bytes in each halfword
revsh r0, r1          @ Reverse bytes in bottom halfword, sign-extend
```

## Addressing Modes (Immediate, Register, Indexed)

Addressing modes specify how to calculate the effective memory address for load/store operations.

### Immediate Offset

The effective address is the base register plus or minus an immediate constant.

**Syntax:**

```assembly
ldr r0, [r1, #offset]     @ Address = r1 + offset
ldr r0, [r1, #-offset]    @ Address = r1 - offset
```

**Examples:**

```assembly
# Access structure members
# struct Point { int x; int y; int z; };
# r0 points to Point structure
ldr r1, [r0, #0]      @ r1 = point.x (offset 0)
ldr r2, [r0, #4]      @ r2 = point.y (offset 4)
ldr r3, [r0, #8]      @ r3 = point.z (offset 8)

# Array access with known index
# int array[10]; r0 points to array
ldr r1, [r0, #0]      @ array[0]
ldr r2, [r0, #12]     @ array[3] (3 * 4 bytes)
ldr r3, [r0, #36]     @ array[9] (9 * 4 bytes)

# Negative offset
ldr r1, [r0, #-4]     @ Access data before base address
```

**Offset Range:**

- Word/Halfword/Byte: -4095 to +4095 (12-bit offset)
- Thumb-2 encoding: -255 to +4095
- Thumb 16-bit encoding: 0 to 31 (limited range)

**Assembler Pseudo-Instruction:**

```assembly
# Large offsets use pseudo-instruction
ldr r0, [r1, #8192]   @ Assembler generates:
                      @ add r_temp, r1, #8192
                      @ ldr r0, [r_temp]
```

### Register Offset

The effective address is the base register plus or minus another register.

**Syntax:**

```assembly
ldr r0, [r1, r2]      @ Address = r1 + r2
ldr r0, [r1, -r2]     @ Address = r1 - r2
```

**Examples:**

```assembly
# Array access with variable index
# r0 = array base, r1 = index
lsl r2, r1, #2        @ r2 = index * 4 (word size)
ldr r3, [r0, r2]      @ Load array[index]

# Pointer arithmetic
# r0 = base address, r1 = byte offset
ldr r2, [r0, r1]      @ Load from base + offset

# Negative offset
sub r2, r0, r1        @ Calculate offset
ldr r3, [r4, -r2]     @ Load using negative offset
```

### Scaled Register Offset (Indexed Addressing)

The offset register can be scaled using a shift operation before being added to the base.

**Syntax:**

```assembly
ldr r0, [r1, r2, LSL #shift]   @ Address = r1 + (r2 << shift)
ldr r0, [r1, r2, LSR #shift]   @ Address = r1 + (r2 >> shift)
ldr r0, [r1, r2, ASR #shift]   @ Address = r1 + (r2 arithmetic >> shift)
ldr r0, [r1, r2, ROR #shift]   @ Address = r1 + (r2 rotated shift)
```

**Shift Operations:**

- `LSL` (Logical Shift Left): Multiply by power of 2
- `LSR` (Logical Shift Right): Unsigned divide by power of 2
- `ASR` (Arithmetic Shift Right): Signed divide by power of 2
- `ROR` (Rotate Right): Circular bit rotation

**Array Indexing Examples:**

```assembly
# Word array access (4 bytes per element)
# r0 = array base, r1 = index
ldr r2, [r0, r1, LSL #2]   @ Load array[index], shift index by 2

# Halfword array (2 bytes per element)
ldr r2, [r0, r1, LSL #1]   @ Load array[index], shift index by 1

# Doubleword array (8 bytes per element)
ldr r2, [r0, r1, LSL #3]   @ Load array[index], shift index by 3

# Structure array access
# struct { int a; int b; int c; } array[]; (12 bytes per struct)
# r0 = array base, r1 = index, access member 'b' (offset 4)
add r2, r1, r1, LSL #1     @ r2 = index * 3
ldr r3, [r0, r2, LSL #2]   @ Base + (index * 12)
ldr r4, [r3, #4]           @ Load member 'b'
```

**Matrix Access:**

```assembly
# 2D array: matrix[rows][cols], element size = 4 bytes
# r0 = matrix base
# r1 = row index
# r2 = column index
# r3 = number of columns
# Calculate: address = base + (row * cols + col) * 4

mul r4, r1, r3        @ r4 = row * cols
add r4, r4, r2        @ r4 = row * cols + col
ldr r5, [r0, r4, LSL #2]   @ Load matrix[row][col]
```

**Extended Register Offset (ARMv8-A 64-bit):**

```assembly
# AArch64 addressing modes
ldr x0, [x1, x2]           @ Base + offset
ldr x0, [x1, x2, SXTW #3]  @ Base + sign-extend(w2) << 3
ldr x0, [x1, w2, UXTW #2]  @ Base + zero-extend(w2) << 2
```

### PC-Relative Addressing

Load data relative to the Program Counter for position-independent code.

**Syntax:**

```assembly
ldr r0, [pc, #offset]     @ Address = PC + offset + 8 (ARM)
                          @ Address = PC + offset + 4 (Thumb)
ldr r0, label             @ Assembler calculates offset
```

**Examples:**

```assembly
# Load constant from literal pool
ldr r0, =0x12345678   @ Pseudo-instruction, assembler generates:
                      @ ldr r0, [pc, #offset]
                      @ ...
                      @ .word 0x12345678

# Access data near code
.text
function:
    ldr r0, constant_value   @ PC-relative load
    bx lr

constant_value:
    .word 0xDEADBEEF

# Position-independent data access
    adr r0, data_table       @ Load address of data_table into r0
    ldr r1, [r0]             @ Access data

data_table:
    .word 100, 200, 300
```

**ADR Pseudo-Instruction:**

```assembly
# ADR generates address relative to PC
adr r0, label         @ r0 = address of label
                      @ Assembler generates: add r0, pc, #offset

# ADRL for larger offsets (pseudo-instruction)
adrl r0, far_label    @ May generate two instructions
```

### Literal Pool

The literal pool stores constants that can be loaded with PC-relative addressing.

**Automatic Literal Pool:**

```assembly
# Assembler creates literal pool automatically
ldr r0, =0x12345678   @ Load immediate value
ldr r1, =0x20000000   @ Load address

# Assembler generates:
# ldr r0, [pc, #offset1]
# ldr r1, [pc, #offset2]
# ...
# .ltorg               @ Literal pool placement
# .word 0x12345678
# .word 0x20000000
```

**Manual Literal Pool Placement:**

```assembly
.text
function:
    ldr r0, =value1
    ldr r1, =value2
    # ... more code ...
    
    b skip_literals   @ Branch over literal pool
    .ltorg            @ Explicit literal pool placement
skip_literals:
    # ... continue execution ...

# Without .ltorg, assembler places pool automatically
# at end of section or when out of range
```

## Pre-Indexed and Post-Indexed Addressing

These addressing modes update the base register automatically during load/store operations.

### Pre-Indexed Addressing (Write-Back)

The base register is updated with the calculated address before the memory access, and this updated value is written back.

**Syntax:**

```assembly
ldr r0, [r1, #offset]!    @ r1 = r1 + offset, then r0 = [r1]
ldr r0, [r1, r2]!         @ r1 = r1 + r2, then r0 = [r1]
```

**The exclamation mark (!) indicates write-back.**

**Examples:**

```assembly
# Increment pointer before access
ldr r0, [r1, #4]!     @ r1 += 4, then load from [r1]
                      @ Equivalent to:
                      @ add r1, r1, #4
                      @ ldr r0, [r1]

# Array traversal
mov r0, #0            @ Sum accumulator
ldr r1, =array        @ Array pointer
mov r2, #10           @ Counter

loop:
    ldr r3, [r1, #4]! @ Pre-increment pointer, load element
    add r0, r0, r3    @ Add to sum
    subs r2, r2, #1   @ Decrement counter
    bne loop          @ Loop if not zero

# Stack operations (growing downward)
str r0, [sp, #-4]!    @ Pre-decrement sp, store r0 (push)
ldr r0, [sp], #4      @ Load r0, post-increment sp (pop)
```

**String/Buffer Operations:**

```assembly
# Copy string with pre-increment
# r0 = source, r1 = destination
copy_loop:
    ldrb r2, [r0, #1]!    @ Pre-increment source, load byte
    strb r2, [r1, #1]!    @ Pre-increment dest, store byte
    cmp r2, #0            @ Check for null terminator
    bne copy_loop
```

### Post-Indexed Addressing

The memory access occurs using the current base register value, then the base register is updated afterward.

**Syntax:**

```assembly
ldr r0, [r1], #offset     @ r0 = [r1], then r1 = r1 + offset
ldr r0, [r1], r2          @ r0 = [r1], then r1 = r1 + r2
```

**No exclamation mark; the update happens automatically after access.**

**Examples:**

```assembly
# Access then increment pointer
ldr r0, [r1], #4      @ Load from [r1], then r1 += 4
                      @ Equivalent to:
                      @ ldr r0, [r1]
                      @ add r1, r1, #4

# Array traversal with post-increment
ldr r0, =array        @ Array pointer
mov r1, #10           @ Counter
mov r2, #0            @ Sum

loop:
    ldr r3, [r0], #4  @ Load element, post-increment pointer
    add r2, r2, r3    @ Add to sum
    subs r1, r1, #1   @ Decrement counter
    bne loop

# Sequential memory access
mov r0, #0x20000000   @ Buffer start
ldr r1, [r0], #4      @ Load word 0, advance to word 1
ldr r2, [r0], #4      @ Load word 1, advance to word 2
ldr r3, [r0], #4      @ Load word 2, advance to word 3
# r0 now points to word 3
```

**Buffer Processing:**

```assembly
# Process buffer elements
# r0 = buffer pointer, r1 = count
process_buffer:
    push {r4, lr}
    mov r4, #0            @ Running total

process_loop:
    ldr r2, [r0], #4      @ Load element, advance pointer
    add r4, r4, r2        @ Process element
    subs r1, r1, #1       @ Decrement count
    bne process_loop
    
    mov r0, r4            @ Return total
    pop {r4, pc}
```

### Pre-Indexed vs Post-Indexed Comparison

**Pre-Indexed:**

```assembly
# Update happens BEFORE memory access
ldr r0, [r1, #4]!     
# 1. r1 = r1 + 4
# 2. r0 = memory[r1]

# Use when you want to:
# - Access next element and keep pointer there
# - Skip first element
# - Implement pre-increment loops
```

**Post-Indexed:**

```assembly
# Update happens AFTER memory access
ldr r0, [r1], #4      
# 1. r0 = memory[r1]
# 2. r1 = r1 + 4

# Use when you want to:
# - Access current element then advance
# - Sequential processing
# - Implement post-increment loops
```

**Example Comparison:**

```assembly
# Pre-indexed: Skip first element
ldr r0, =array
ldr r1, [r0, #4]!     @ r0 now points to array[1]
ldr r2, [r0, #4]!     @ r0 now points to array[2]

# Post-indexed: Start from first element
ldr r0, =array
ldr r1, [r0], #4      @ r1 = array[0], r0 points to array[1]
ldr r2, [r0], #4      @ r2 = array[1], r0 points to array[2]
```

### Stack Operations Using Indexed Addressing

**Full Descending Stack (ARM standard):**

```assembly
# Push (store with pre-decrement)
str r0, [sp, #-4]!    @ sp -= 4, then [sp] = r0
stmdb sp!, {r0-r3}    @ Push multiple (decrement before)

# Pop (load with post-increment)
ldr r0, [sp], #4      @ r0 = [sp], then sp += 4
ldmia sp!, {r0-r3}    @ Pop multiple (increment after)
```

**Other Stack Types:**

```assembly
# Full Ascending (sp points to last used)
stmib sp!, {r0-r3}    @ Increment before store
ldmda sp!, {r0-r3}    @ Decrement after load

# Empty Descending (sp points to next free)
stmda sp!, {r0-r3}    @ Decrement after store
ldmib sp!, {r0-r3}    @ Increment before load

# Empty Ascending (sp points to next free)
stmia sp!, {r0-r3}    @ Increment after store
ldmdb sp!, {r0-r3}    @ Decrement before load
```

## Offset Addressing

Offset addressing provides flexible memory access patterns without modifying the base register.

### Simple Offset

The base register remains unchanged; only the effective address is calculated.

**Syntax:**

```assembly
ldr r0, [r1, #offset]     @ Address = r1 + offset, r1 unchanged
ldr r0, [r1, r2]          @ Address = r1 + r2, r1 unchanged
```

**Examples:**

```assembly
# Structure member access
# struct { int x; int y; int z; } point;
# r0 points to structure
ldr r1, [r0, #0]      @ r1 = point.x
ldr r2, [r0, #4]      @ r2 = point.y
ldr r3, [r0, #8]      @ r3 = point.z
# r0 still points to structure base

# Multiple accesses to same structure
ldr r0, =my_struct
ldr r1, [r0, #0]      @ Load member 1
add r1, r1, #10       @ Process
str r1, [r0, #0]      @ Store back
ldr r2, [r0, #4]      @ Load member 2
mul r2, r2, r1        @ Process
str r2, [r0, #4]      @ Store back
# r0 unchanged throughout
```

### Complex Data Structure Access

**Nested Structure Example:**

```assembly
# struct Outer {
#     int id;              @ offset 0
#     struct Inner {
#         int x;           @ offset 4
#         int y;           @ offset 8
#     } inner;
#     int value;           @ offset 12
# };

# r0 points to Outer structure
ldr r1, [r0, #0]      @ outer.id
ldr r2, [r0, #4]      @ outer.inner.x
ldr r3, [r0, #8]      @ outer.inner.y
ldr r4, [r0, #12]     @ outer.value
```

**Array of Structures:**

```assembly
# struct Point { int x; int y; } points[10];
# Each structure is 8 bytes
# r0 = array base, r1 = index

# Calculate structure address
mov r2, #8            @ Structure size
mul r3, r1, r2        @ Offset = index * size
add r4, r0, r3        @ Address of points[index]

# Access members
ldr r5, [r4, #0]      @ points[index].x
ldr r6, [r4, #4]      @ points[index].y

# Alternative: direct calculation
lsl r3, r1, #3        @ index * 8 (shift left by 3)
ldr r5, [r0, r3]      @ points[index].x
add r3, r3, #4
ldr r6, [r0, r3]      @ points[index].y
```

### Base Register with Multiple Offsets

**Accessing Multiple Array Elements:**

```assembly
# Process adjacent array elements
ldr r0, =array
ldr r1, [r0, #0]      @ array[0]
ldr r2, [r0, #4]      @ array[1]
ldr r3, [r0, #8]      @ array[2]
ldr r4, [r0, #12]     @ array[3]

# Calculate sum
add r5, r1, r2
add r5, r5, r3
add r5, r5, r4
```

**Sliding Window Operations:**

```assembly
# 3-element moving average
# r0 = array pointer, r1 = index
lsl r2, r1, #2        @ Convert index to byte offset
ldr r3, [r0, r2]      @ current element
sub r2, r2, #4
ldr r4, [r0, r2]      @ previous element
add r2, r2, #8
ldr r5, [r0, r2]      @ next element

add r6, r3, r4
add r6, r6, r5
mov r7, #3
udiv r6, r6, r7       @ Average = (prev + curr + next) / 3
```

### Bit Field Access

**Loading and Extracting Bit Fields:**

```assembly
# Extract bits [15:8] from word at [r0]
ldr r1, [r0]          @ Load full word
ubfx r2, r1, #8, #8   @ Extract 8 bits starting at bit 8

# Insert bit field
ldr r1, [r0]          @ Load current value
bfi r1, r3, #8, #8    @ Insert r3 bits [7:0] into r1 bits [15:8]
str r1, [r0]          @ Store modified value
```

### Memory-Mapped I/O Access

**Peripheral Register Access:**

```assembly
# GPIO peripheral access
.equ GPIO_BASE, 0x40020000
.equ GPIO_IDR,  0x10      @ Input Data Register offset
.equ GPIO_ODR,  0x14      @ Output Data Register offset
.equ GPIO_BSRR, 0x18      @ Bit Set/Reset Register offset

# Read input
ldr r0, =GPIO_BASE
ldr r1, [r0, #GPIO_IDR]   @ Read input register

# Write output
ldr r2, =0xFF
str r2, [r0, #GPIO_ODR]   @ Write output register

# Set specific bits
mov r3, #(1 << 5)
str r3, [r0, #GPIO_BSRR]  @ Set bit 5
```

**Timer Configuration:**

```assembly
# TIM2 peripheral configuration
.equ TIM2_BASE, 0x40000000
.equ TIM_CR1,   0x00      @ Control register 1
.equ TIM_PSC,   0x28      @ Prescaler
.equ TIM_ARR,   0x2C      @ Auto-reload register

ldr r0, =TIM2_BASE
mov r1, #1000
str r1, [r0, #TIM_PSC]    @ Set prescaler
mov r1, #10000
str r1, [r0, #TIM_ARR]    @ Set auto-reload value
mov r1, #1
str r1, [r0, #TIM_CR1]    @ Enable timer
```

### DMA Buffer Operations

**Setting Up DMA Transfer:**

```assembly
# Configure DMA for memory-to-memory transfer
.equ DMA1_BASE,     0x40020000
.equ DMA_CCR,       0x08      @ Configuration register
.equ DMA_CNDTR,     0x0C      @ Number of data register
.equ DMA_CPAR,      0x10      @ Peripheral address register
.equ DMA_CMAR,      0x14      @ Memory address register

ldr r0, =DMA1_BASE
ldr r1, =source_buffer
str r1, [r0, #DMA_CPAR]       @ Source address
ldr r1, =dest_buffer
str r1, [r0, #DMA_CMAR]       @ Destination address
mov r1, #1024
str r1, [r0, #DMA_CNDTR]      @ Transfer size
ldr r1, =0x00004081           @ Configuration flags
str r1, [r0, #DMA_CCR]        @ Start DMA
```

### Volatile Access

**Ensuring Memory Access Ordering:**

```assembly
# Volatile read (force memory access)
ldr r0, [r1]          @ Load from memory
dmb                   @ Data Memory Barrier

# Volatile write (ensure completion)
str r0, [r1]          @ Store to memory
dsb                   @ Data Synchronization Barrier

# Critical section with barriers
dmb                   @ Barrier before critical section
ldr r0, [r1]          @ Protected read
add r0, r0, #1
str r0, [r1]          @ Protected write
dmb                   @ Barrier after critical section
```

**Memory Barrier Types:**

```assembly
dmb                   @ Data Memory Barrier (full system)
dmb sy                @ System-wide
dmb ish               @ Inner Shareable domain
dmb osh               @ Outer Shareable domain
dmb nsh               @ Non-shareable domain

dsb                   @ Data Synchronization Barrier
isb                   @ Instruction Synchronization Barrier
```

### Optimization Considerations

**Cache Line Alignment:**

```assembly
# Align data to cache line boundary (typically 32 or 64 bytes)
.align 6              @ Align to 64-byte boundary
buffer:
    .space 1024

# Access aligned data more efficiently
ldr r0, =buffer       @ Already cache-aligned
ldm r0, {r1-r8}       @ Load 32 bytes (likely single cache line)
```

**Prefetching:**

```assembly
# Hint processor to prefetch data
pld [r0, #64]         @ Prefetch data at r0 + 64
pldw [r0, #128]       @ Prefetch for write
pli [r0, #32]         @ Prefetch instruction

# Prefetch in loop
mov r1, #1000
loop:
    pld [r0, #64]     @ Prefetch next iteration
    ldr r2, [r0], #4  @ Load current element
    # Process r2
    subs r1, r1, #1
    bne loop
```

**Burst Access Optimization:**

```assembly
# Sequential loads are more efficient
ldm r0!, {r1-r8}      @ 8 loads in one instruction
                      @ More efficient than individual loads

# Compare with individual loads
ldr r1, [r0], #4      @ Less efficient
ldr r2, [r0], #4
ldr r3, [r0], #4
ldr r4, [r0], #4
ldr r5, [r0], #4
ldr r6, [r0], #4
ldr r7, [r0], #4
ldr r8, [r0], #4
```

### Write-Combining and Write Buffers

**Write Buffer Behavior:**

```assembly
# Sequential writes may be combined
str r1, [r0, #0]      @ Write 1
str r2, [r0, #4]      @ Write 2
str r3, [r0, #8]      @ Write 3
str r4, [r0, #12]     @ Write 4
# Processor may combine these into burst write

# Ensure writes complete
dsb                   @ Wait for write buffer to drain

# Memory-mapped I/O requires ordering
ldr r0, =PERIPHERAL_BASE
str r1, [r0, #REG1]   @ Write register 1
dsb                   @ Ensure write completes
str r2, [r0, #REG2]   @ Write register 2
```

### Exclusive Access for Synchronization

**Load-Exclusive and Store-Exclusive:**

```assembly
# Atomic increment using exclusive access
ldrex r1, [r0]        @ Load exclusive
add r1, r1, #1        @ Increment
strex r2, r1, [r0]    @ Store exclusive, r2 = success flag
cmp r2, #0            @ Check if store succeeded
bne .-12              @ Retry if failed

# Clear exclusive monitor
clrex                 @ Clear exclusive access state
```

**Semaphore Implementation:**

```assembly
# Acquire semaphore
acquire_semaphore:
    mov r1, #1
retry_acquire:
    ldrex r2, [r0]        @ Load semaphore value
    cmp r2, #0            @ Check if available
    itt ne                @ If-Then-Then block
    strexne r3, r1, [r0]  @ Try to acquire
    cmpne r3, #0          @ Check success
    bne retry_acquire     @ Retry if failed
    dmb                   @ Memory barrier
    bx lr

# Release semaphore
release_semaphore:
    dmb                   @ Memory barrier before release
    mov r1, #0
    str r1, [r0]          @ Release semaphore
    dsb                   @ Ensure write completes
    sev                   @ Signal event (wake other cores)
    bx lr
```

### Compare and Swap (CAS)

**Atomic Compare-and-Swap:**

```assembly
# bool cas(int *ptr, int old_val, int new_val)
# Returns true if swap succeeded
compare_and_swap:
    # r0 = ptr, r1 = old_val, r2 = new_val
cas_retry:
    ldrex r3, [r0]        @ Load current value
    cmp r3, r1            @ Compare with expected
    bne cas_fail          @ Exit if not equal
    strex r3, r2, [r0]    @ Try to store new value
    cmp r3, #0            @ Check if store succeeded
    bne cas_retry         @ Retry if failed
    dmb                   @ Memory barrier
    mov r0, #1            @ Success
    bx lr

cas_fail:
    clrex                 @ Clear exclusive monitor
    mov r0, #0            @ Failure
    bx lr
```

### Lock-Free Data Structures

**Lock-Free Stack Push:**

```assembly
# struct Node { void *data; struct Node *next; };
# void push(struct Node **head, struct Node *node)
# r0 = head pointer, r1 = new node
lock_free_push:
push_retry:
    ldr r2, [r0]          @ Load current head
    str r2, [r1, #4]      @ node->next = current head
    dmb                   @ Memory barrier
    ldrex r3, [r0]        @ Load-exclusive head
    cmp r3, r2            @ Check if head changed
    bne push_retry        @ Retry if changed
    strex r3, r1, [r0]    @ Try to update head
    cmp r3, #0
    bne push_retry        @ Retry if failed
    dmb
    bx lr
```

**Lock-Free Stack Pop:**

```assembly
# struct Node *pop(struct Node **head)
# r0 = head pointer, returns node in r0
lock_free_pop:
pop_retry:
    ldrex r1, [r0]        @ Load-exclusive head
    cmp r1, #0            @ Check if empty
    beq pop_empty         @ Return NULL if empty
    ldr r2, [r1, #4]      @ Load next pointer
    strex r3, r2, [r0]    @ Try to update head
    cmp r3, #0
    bne pop_retry         @ Retry if failed
    dmb
    mov r0, r1            @ Return popped node
    bx lr

pop_empty:
    clrex
    mov r0, #0            @ Return NULL
    bx lr
```

### Memory Region Attributes

**Memory Type Configuration:**

```assembly
# Cortex-M Memory Protection Unit (MPU) configuration
.equ MPU_BASE,  0xE000ED90
.equ MPU_TYPE,  0x00      @ Type register
.equ MPU_CTRL,  0x04      @ Control register
.equ MPU_RNR,   0x08      @ Region number register
.equ MPU_RBAR,  0x0C      @ Region base address
.equ MPU_RASR,  0x10      @ Region attribute and size

configure_mpu_region:
    # r0 = region number, r1 = base address, r2 = attributes
    ldr r3, =MPU_BASE
    str r0, [r3, #MPU_RNR]    @ Select region
    str r1, [r3, #MPU_RBAR]   @ Set base address
    str r2, [r3, #MPU_RASR]   @ Set attributes
    bx lr

# Memory attributes
.equ TEX_NORMAL,    0x01  @ Normal memory
.equ TEX_DEVICE,    0x02  @ Device memory
.equ CACHEABLE,     0x08  @ Cacheable
.equ BUFFERABLE,    0x04  @ Bufferable
.equ SHAREABLE,     0x10  @ Shareable
```

### Advanced Addressing Examples

**Circular Buffer Implementation:**

```assembly
# Circular buffer with wrapping
# r0 = buffer base, r1 = buffer size, r2 = index
circular_buffer_access:
    # Calculate wrapped index
    udiv r3, r2, r1       @ r3 = index / size
    mls r4, r3, r1, r2    @ r4 = index % size (modulo)
    
    # Access element
    ldr r5, [r0, r4, LSL #2]  @ Load buffer[wrapped_index]
    bx lr

# Optimized version for power-of-2 sizes
circular_buffer_opt:
    # r1 = size (must be power of 2)
    sub r3, r1, #1        @ Create mask (size - 1)
    and r4, r2, r3        @ index & mask = wrapped index
    ldr r5, [r0, r4, LSL #2]
    bx lr
```

**Bit Array Access:**

```assembly
# Set bit in bit array
# r0 = bit array base, r1 = bit index
set_bit:
    lsr r2, r1, #5        @ Word index = bit_index / 32
    and r3, r1, #31       @ Bit position = bit_index % 32
    mov r4, #1
    lsl r4, r4, r3        @ Create bit mask
    ldr r5, [r0, r2, LSL #2]  @ Load word
    orr r5, r5, r4        @ Set bit
    str r5, [r0, r2, LSL #2]  @ Store back
    bx lr

# Clear bit in bit array
clear_bit:
    lsr r2, r1, #5
    and r3, r1, #31
    mov r4, #1
    lsl r4, r4, r3
    mvn r4, r4            @ Invert mask
    ldr r5, [r0, r2, LSL #2]
    and r5, r5, r4        @ Clear bit
    str r5, [r0, r2, LSL #2]
    bx lr

# Test bit in bit array
test_bit:
    lsr r2, r1, #5
    and r3, r1, #31
    ldr r4, [r0, r2, LSL #2]
    lsr r4, r4, r3
    and r0, r4, #1        @ Return bit value
    bx lr
```

**Hash Table Access:**

```assembly
# Simple hash table lookup
# r0 = hash table base, r1 = key, r2 = table size
hash_table_lookup:
    # Calculate hash (simple modulo)
    udiv r3, r1, r2
    mls r3, r3, r2, r1    @ hash = key % size
    
    # Each entry is 8 bytes (key + value)
    lsl r3, r3, #3        @ Multiply by 8
    
    # Load key and value
    ldr r4, [r0, r3]      @ Load key
    cmp r4, r1            @ Compare with search key
    bne hash_miss
    
    ldr r0, [r0, r3, #4]  @ Load value
    bx lr

hash_miss:
    mov r0, #0            @ Return 0 for miss
    bx lr
```

**Sparse Matrix Access (COO format):**

```assembly
# Coordinate (COO) sparse matrix format
# struct Entry { int row; int col; int value; };
# r0 = entries array, r1 = num_entries, r2 = target_row, r3 = target_col
sparse_matrix_lookup:
    mov r4, #0            @ Index counter
    mov r12, #12          @ Entry size (3 words)

search_loop:
    cmp r4, r1            @ Check if done
    bge not_found
    
    mul r5, r4, r12       @ Calculate offset
    ldr r6, [r0, r5]      @ Load row
    cmp r6, r2            @ Compare row
    bne next_entry
    
    add r5, r5, #4
    ldr r6, [r0, r5]      @ Load column
    cmp r6, r3            @ Compare column
    bne next_entry
    
    add r5, r5, #4
    ldr r0, [r0, r5]      @ Load value
    bx lr                 @ Found

next_entry:
    add r4, r4, #1
    b search_loop

not_found:
    mov r0, #0            @ Return 0 for not found
    bx lr
```

### String Operations with Addressing Modes

**String Length (strlen):**

```assembly
# size_t strlen(const char *str)
# r0 = string pointer
strlen:
    mov r1, r0            @ Save start address
    
strlen_loop:
    ldrb r2, [r0], #1     @ Load byte, post-increment
    cmp r2, #0            @ Check for null terminator
    bne strlen_loop
    
    sub r0, r0, r1        @ Calculate length
    sub r0, r0, #1        @ Adjust for extra increment
    bx lr
```

**String Copy (strcpy):**

```assembly
# char *strcpy(char *dest, const char *src)
# r0 = dest, r1 = src
strcpy:
    push {r4, lr}
    mov r4, r0            @ Save dest for return

strcpy_loop:
    ldrb r2, [r1], #1     @ Load from src, post-increment
    strb r2, [r0], #1     @ Store to dest, post-increment
    cmp r2, #0            @ Check for null
    bne strcpy_loop
    
    mov r0, r4            @ Return original dest
    pop {r4, pc}
```

**String Compare (strcmp):**

```assembly
# int strcmp(const char *s1, const char *s2)
# r0 = s1, r1 = s2, returns: <0, 0, >0
strcmp:
strcmp_loop:
    ldrb r2, [r0], #1     @ Load from s1
    ldrb r3, [r1], #1     @ Load from s2
    cmp r2, r3            @ Compare characters
    bne strcmp_diff
    cmp r2, #0            @ Check for end
    bne strcmp_loop
    
    mov r0, #0            @ Strings equal
    bx lr

strcmp_diff:
    sub r0, r2, r3        @ Return difference
    bx lr
```

**Memory Copy (memcpy) - Optimized:**

```assembly
# void *memcpy(void *dest, const void *src, size_t n)
# r0 = dest, r1 = src, r2 = n
memcpy:
    push {r4-r9, lr}
    mov r3, r0            @ Save dest for return
    
    # Copy 32 bytes at a time
    cmp r2, #32
    blt memcpy_small

memcpy_large:
    ldmia r1!, {r4-r9, r12, r14}  @ Load 8 words
    stmia r0!, {r4-r9, r12, r14}  @ Store 8 words
    sub r2, r2, #32
    cmp r2, #32
    bge memcpy_large

memcpy_small:
    cmp r2, #4
    blt memcpy_bytes

memcpy_words:
    ldr r4, [r1], #4      @ Copy word
    str r4, [r0], #4
    sub r2, r2, #4
    cmp r2, #4
    bge memcpy_words

memcpy_bytes:
    cmp r2, #0
    beq memcpy_done

memcpy_byte_loop:
    ldrb r4, [r1], #1     @ Copy byte
    strb r4, [r0], #1
    subs r2, r2, #1
    bne memcpy_byte_loop

memcpy_done:
    mov r0, r3            @ Return original dest
    pop {r4-r9, pc}
```

### SIMD-Style Operations

**Parallel Byte Processing:**

```assembly
# Process 4 bytes in parallel using word operations
# Add constant to each byte (saturating)
parallel_byte_add:
    # r0 = input word (4 bytes), r1 = constant to add
    and r1, r1, #0xFF     @ Ensure constant is single byte
    orr r1, r1, r1, LSL #8
    orr r1, r1, r1, LSL #16   @ Replicate to all 4 bytes
    
    # Saturating add
    uqadd8 r0, r0, r1     @ Parallel saturating add
    bx lr
```

**SIMD Instructions (ARMv7 and later):**

```assembly
# Parallel arithmetic on packed data
usad8 r0, r1, r2      @ Sum of absolute differences (4 bytes)
usada8 r0, r1, r2, r3 @ SAD with accumulate
uhadd8 r0, r1, r2     @ Parallel unsigned halving add
sel r0, r1, r2        @ Select bytes based on GE flags

# Example: Alpha blending of 4 pixels
alpha_blend_4pixels:
    # r0 = src (4 bytes), r1 = dst (4 bytes), r2 = alpha
    uxtb r3, r2           @ Extract alpha
    rsb r4, r3, #256      @ 256 - alpha
    
    # Multiply source by alpha
    mul r5, r0, r3
    lsr r5, r5, #8
    
    # Multiply dest by (256-alpha)
    mul r6, r1, r4
    lsr r6, r6, #8
    
    # Add results
    uqadd8 r0, r5, r6     @ Saturating add
    bx lr
```

**Key Points:**

- ARM's load/store architecture separates memory access from computation, improving pipeline efficiency and predictability
- Addressing modes provide flexible memory access patterns: immediate offsets for structure members, register offsets for arrays, and scaled offsets for efficient indexing
- Pre-indexed addressing updates the base register before access (useful for skipping elements), while post-indexed updates after (useful for sequential processing)
- Write-back addressing modes (indicated by `!`) automatically update base registers, simplifying pointer manipulation in loops
- Exclusive access instructions (LDREX/STREX) enable atomic operations and synchronization primitives without locks
- Memory barriers (DMB, DSB, ISB) ensure proper ordering of memory accesses in multi-core and memory-mapped I/O scenarios
- Optimized memory operations use LDM/STM for burst transfers, which are more efficient than sequential individual loads/stores
- Understanding memory alignment requirements prevents faults and improves performance on different ARM implementations

---

## Stack Operations

The stack provides temporary storage for function local variables, saved registers, return addresses, and function arguments that don't fit in registers. ARM uses a full descending stack where the stack pointer points to the last occupied location and the stack grows toward lower memory addresses.

### Stack Pointer Management

In AArch64, the stack pointer (SP) must maintain 16-byte alignment at public interfaces (function calls). The hardware may enforce this alignment, triggering alignment faults for misaligned stack access. Within a function body, the stack pointer can temporarily lose alignment, but must restore 16-byte alignment before calling other functions.

Functions typically allocate stack space in the prologue by subtracting from SP and deallocate in the epilogue by adding back to SP:

```
function_prologue:
    stp x29, x30, [sp, #-32]!   // Pre-index: decrement SP by 32, store FP and LR
    mov x29, sp                  // Set up frame pointer
    stp x19, x20, [sp, #16]      // Save callee-saved registers
```

The pre-indexed addressing mode `[sp, #-32]!` atomically decrements SP by 32 bytes and uses the new value as the store address. This single instruction allocates stack space and saves registers. Post-indexed addressing would update SP after using its current value.

The corresponding epilogue restores registers and deallocates stack space:

```
function_epilogue:
    ldp x19, x20, [sp, #16]      // Restore callee-saved registers
    ldp x29, x30, [sp], #32      // Post-index: load FP and LR, increment SP by 32
    ret                          // Return using address in LR (x30)
```

Stack allocation sizes must be multiples of 16 bytes to maintain alignment. For odd-sized local variables, the compiler rounds up allocation. For example, allocating 20 bytes of locals requires subtracting 32 from SP (20 bytes for locals plus 12 bytes padding to reach the next 16-byte boundary, considering any saved registers).

### Stack Frame Layout

A typical stack frame contains several components in order from higher to lower addresses:

1. **Previous frame's data** (belongs to caller)
2. **Saved frame pointer (X29)** - points to previous frame's FP location, creating linked list
3. **Saved link register (X30)** - return address to caller
4. **Saved callee-saved registers** (X19-X28, V8-V15 lower 64 bits)
5. **Local variables** - function's automatic variables
6. **Alignment padding** - ensures 16-byte alignment
7. **Outgoing argument space** - arguments beyond X0-X7 for called functions
8. **Current SP location** - points here at function call points

The frame pointer (X29) enables stack unwinding by debuggers and exception handlers. Each frame's FP points to the previous frame's FP location, creating a linked chain back through all active function calls. Release builds often omit frame pointers (compiling with `-fomit-frame-pointer`) to free X29 for general use, improving performance but complicating debugging.

### Variable-Length Stack Allocation

Functions allocating variable-length arrays (VLAs) or using `alloca()` require dynamic stack adjustment. Since the allocation size isn't known at compile time, these functions must establish a frame pointer:

```
    stp x29, x30, [sp, #-16]!
    mov x29, sp                  // FP now marks frame base
    
    // Compute allocation size in x0
    add x0, x0, #15              // Round up to 16-byte boundary
    and x0, x0, #-16             // Clear low 4 bits
    sub sp, sp, x0               // Allocate variable space
    
    // Use allocated space...
    
    mov sp, x29                  // Restore SP from FP
    ldp x29, x30, [sp], #16
    ret
```

The frame pointer remains constant throughout function execution, allowing SP to move freely for dynamic allocations. At function exit, copying FP back to SP deallocates all variable-sized allocations in one instruction.

### Stack Probing for Large Allocations

Large stack allocations (typically >4KB) risk extending past the stack guard page, potentially causing silent stack overflow where the stack grows into other memory regions. Operating systems typically protect a page below the committed stack region; accessing this guard page triggers automatic stack extension.

For large allocations, code must probe the stack at page intervals to ensure proper guard page triggering:

```
large_allocation:
    stp x29, x30, [sp, #-16]!
    mov x29, sp
    
    // Allocate 16KB (exceeds page size, requires probing)
    mov x9, #4096               // Page size
    
probe_loop:
    sub sp, sp, x9              // Decrease SP by one page
    str xzr, [sp]               // Touch the page (probe)
    subs x10, x10, x9           // Decrement remaining size
    b.gt probe_loop             // Continue if more to allocate
```

Modern compilers automatically insert probing code when detecting large stack allocations. The exact threshold and mechanism depend on the operating system's stack management. [Inference: Stack probing behavior varies across operating systems and may be configured through compiler flags.]

### Red Zone Considerations

Some ABIs define a "red zone" - a small region below the stack pointer that functions can use without adjusting SP, avoiding stack pointer manipulation for leaf functions with minimal local storage. [Unverified: AArch64 does not guarantee a red zone in all environments.] Signal handlers and interrupt service routines can corrupt memory below SP, making red zone usage unsafe in contexts where asynchronous events occur. [Inference: Portable code should avoid assuming red zone availability.]

### Stack Unwinding

Exception handling mechanisms (C++ exceptions, setjmp/longjmp) require stack unwinding - traversing the stack to find exception handlers and execute cleanup code. Unwinding can use frame pointer chains (following X29 links) or DWARF unwind information embedded in executables.

DWARF unwind info describes how to restore registers at each instruction address without requiring frame pointers. This allows optimized builds to omit FP while preserving exception handling capability. The information specifies canonical frame address (CFA) computation and register restore locations for each function at each instruction offset.

### Stack Protection Mechanisms

Stack buffer overflow attacks historically exploited overwrites of saved return addresses. Modern systems employ several countermeasures:

**Stack canaries** place random values between local variables and saved return addresses. Function epilogues verify canary integrity before returning; modification indicates buffer overflow and triggers termination. The compiler inserts canary checking code controlled by flags like `-fstack-protector-strong`.

**Stack address randomization (ASLR)** randomizes stack base addresses, making exploitation harder by preventing attackers from predicting return address locations.

**Shadow call stacks** maintain a separate stack storing only return addresses, protected from buffer overflows in regular stack data. [Unverified: Shadow call stack support varies by platform and requires specific compiler/runtime support.]

**Non-executable stacks** mark stack pages non-executable, preventing injected shellcode execution. ARM's Execute Never (XN) page attribute implements this at the hardware level through page tables.

## Heap and Static Memory

Heap and static memory serve different purposes in program memory organization. Static memory exists for the program's entire lifetime with fixed addresses, while heap memory provides dynamic allocation and deallocation controlled by program logic.

### Static Memory Organization

Static memory encompasses several categories of data with different characteristics:

**.text section** contains executable code. The linker places all function instructions here. This section is marked read-only and executable in page tables. Position-independent executables use PC-relative addressing to reference code within this section. The section typically loads at fixed addresses (for static executables) or randomized addresses (with ASLR enabled).

**.rodata section** stores read-only data including string literals, constant arrays, and const-qualified variables. Page protection marks this non-writable, causing faults if code attempts modification. Compilers often merge identical string literals across translation units to reduce size.

**.data section** contains initialized static and global variables with non-zero initial values. The loader copies initial values from the executable file to memory at program startup. This section has read-write page permissions.

**.bss section** (Block Started by Symbol) holds uninitialized or zero-initialized static and global variables. The section doesn't consume space in the executable file - only metadata describing its size. The loader allocates and zero-fills this region at program startup. Large zero-initialized arrays use .bss rather than .data to avoid bloating executable size.

**Thread-local storage (TLS)** sections (.tdata, .tbss) store per-thread static variables. The system creates separate copies of these sections for each thread. Access requires special instruction sequences or runtime library calls to locate the current thread's TLS block.

### Accessing Static Memory

PC-relative addressing enables position-independent access to static data. The ADRP instruction computes the address of a 4KB page relative to the current PC:

```
    adrp x0, variable       // x0 = page address of 'variable'
    ldr x1, [x0, :lo12:variable]  // Load from page offset
```

ADRP loads bits [63:12] of the target address into X0, with bits [11:0] cleared. The `:lo12:` relocation adds the low 12 bits (page offset). This two-instruction sequence can address any symbol within ±4GB of current PC.

For data within ±1MB, a single ADR instruction suffices:

```
    adr x0, nearby_data     // x0 = address of 'nearby_data'
```

Global Offset Table (GOT) provides indirection for shared library symbols and dynamic linking. The GOT contains absolute addresses of global symbols, updated by the dynamic linker at load time:

```
    adrp x0, :got:global_var     // Page of GOT entry
    ldr x0, [x0, :got_lo12:global_var]  // Load address from GOT
    ldr w1, [x0]                  // Load actual value
```

This three-instruction sequence first locates the GOT entry, loads the actual variable address from the GOT, then accesses the variable. [Inference: The GOT indirection overhead motivates link-time optimization to convert GOT access to direct PC-relative access when possible.]

### Heap Memory Management

The heap provides dynamically allocated memory managed through allocator interfaces (malloc/free in C, new/delete in C++). The heap typically grows upward from low addresses toward higher addresses, opposite to the stack's downward growth.

**System allocators** (malloc implementations) request large memory regions from the operating system through system calls (mmap on Linux, VirtualAlloc on Windows) and subdivide these regions for application allocations. Allocators maintain metadata tracking free and allocated blocks, implementing various strategies (first-fit, best-fit, segregated free lists, buddy allocation) balancing performance, fragmentation, and overhead.

**Alignment requirements** constrain allocator behavior. Standard allocators guarantee alignment suitable for any standard data type - typically 16 bytes on AArch64 to satisfy SIMD requirements. Allocating smaller objects still returns 16-byte aligned addresses; the allocator's internal fragmentation wastes the unused bytes.

**Metadata overhead** includes block headers storing size information and allocation status. Allocators often store metadata immediately before allocated blocks:

```
Block layout:
[Header: 8-16 bytes][User data: requested size][Padding for next alignment]
                    ^
                    malloc returns this address
```

The header might contain the block size, allocation flags, and pointers for free list management. On free(), the allocator accesses the header by subtracting from the user pointer. [Inference: Writing before allocated blocks corrupts allocator metadata, causing crashes or vulnerabilities during subsequent allocations/deallocations.]

**Fragmentation** occurs in two forms. External fragmentation leaves free memory scattered in small unusable pieces. Internal fragmentation wastes space within allocated blocks due to alignment and minimum size constraints. Allocators employ various strategies to mitigate fragmentation, including coalescing adjacent free blocks and segregating allocations by size class.

### Memory Pools and Arena Allocation

Applications with specific allocation patterns often implement custom allocators:

**Memory pools** pre-allocate fixed-size blocks, eliminating allocation overhead and fragmentation for uniform-sized objects. Allocation returns the next available block from a free list; deallocation returns blocks to the free list. This provides O(1) allocation/deallocation and excellent cache locality.

**Arena allocators** (bump allocators, linear allocators) allocate sequentially from a large buffer by incrementing a pointer. Individual deallocations are not supported - the entire arena releases at once. This suits temporary allocations with bulk deallocation (parser nodes, per-frame game objects).

**Stack allocators** function like arena allocators but support last-in-first-out deallocation. Allocations increment a stack pointer; deallocations must occur in reverse order, decrementing the pointer back.

### Cache Considerations for Memory Layout

Memory access patterns significantly impact performance on modern processors with multi-level caches. [Inference: Understanding cache behavior helps optimize data structure layout, though specific cache parameters vary by processor implementation.]

**Spatial locality** benefits from storing related data contiguously. Accessing one element brings nearby elements into cache. Array traversal exhibits excellent spatial locality; linked list traversal exhibits poor spatial locality.

**Temporal locality** benefits from reusing recently accessed data while still cache-resident. Algorithms processing the same data repeatedly should minimize working set size to fit in cache.

**Cache line size** (typically 64 bytes on ARM) determines granularity of data transfer between memory and cache. Straddling cache line boundaries may require two cache line loads. Aligning frequently accessed structures to cache line boundaries can improve performance.

**False sharing** occurs when threads write to different variables occupying the same cache line, causing cache coherency traffic. Padding structures to occupy full cache lines prevents false sharing in multi-threaded code.

## Memory Alignment Requirements

Memory alignment refers to the address constraints for accessing data of various types. Properly aligned data can be accessed efficiently in single operations; misaligned data may require multiple memory cycles, trap to software handlers, or cause faults depending on processor configuration and access type.

### Natural Alignment

ARM data types have natural alignment requirements matching their size:

- 8-bit (byte): 1-byte aligned (any address)
- 16-bit (halfword): 2-byte aligned (address divisible by 2)
- 32-bit (word): 4-byte aligned (address divisible by 4)
- 64-bit (doubleword): 8-byte aligned (address divisible by 8)
- 128-bit (quadword): 16-byte aligned (address divisible by 16)

Accessing naturally aligned data guarantees optimal performance. For example, loading a 32-bit word from address 0x1000 executes efficiently since 0x1000 is divisible by 4. Loading from 0x1002 constitutes misaligned access with platform-dependent behavior.

### Misalignment Handling

AArch64 provides configurable misalignment handling through the SCTLR_EL1 system register. The alignment check (A) and alignment fault checking (A/SA) bits control behavior:

**Alignment checking disabled** (typical for user-space): Most load/store instructions tolerate misalignment with performance penalties. The processor may execute multiple memory transactions to access misaligned data. [Unverified: Specific performance penalties depend on processor microarchitecture and memory system implementation.]

**Alignment checking enabled**: Misaligned access triggers alignment faults, causing exceptions. Operating system handlers may emulate the access (slowly) or terminate the process. Kernel code often enables alignment checking to catch bugs.

**Exclusive access restrictions**: Load-exclusive and store-exclusive instructions (LDXR, STXR family) require natural alignment regardless of alignment checking settings. Misaligned exclusive access causes alignment faults unconditionally.

**SIMD/vector restrictions**: Vector load/store instructions accessing multiple elements may impose stricter alignment requirements. [Unverified: Specific SIMD alignment requirements vary by instruction and processor implementation.] Unaligned SIMD access either performs poorly or faults depending on instruction variant and processor.

### Structure Padding and Packing

Compilers insert padding in structures to satisfy alignment requirements:

```c
struct Example {
    char c;      // 1 byte at offset 0
    // 3 bytes padding inserted here
    int i;       // 4 bytes at offset 4
    char d;      // 1 byte at offset 8
    // 7 bytes padding inserted here
    double f;    // 8 bytes at offset 16
};
// Total size: 24 bytes
```

The compiler aligns each member to its natural alignment. The structure's overall alignment equals the maximum member alignment (8 bytes for the double). Structure size rounds up to a multiple of its alignment, ensuring proper alignment in arrays.

Reordering members can reduce padding:

```c
struct Optimized {
    double f;    // 8 bytes at offset 0
    int i;       // 4 bytes at offset 8
    char c;      // 1 byte at offset 12
    char d;      // 1 byte at offset 13
    // 2 bytes padding
};
// Total size: 16 bytes (vs. 24 bytes)
```

The `__attribute__((packed))` directive (GCC/Clang) eliminates padding, placing members contiguously:

```c
struct Packed {
    char c;
    int i;       // Misaligned at offset 1
    char d;
    double f;    // Misaligned at offset 6
} __attribute__((packed));
// Total size: 14 bytes
```

Packed structures save space but incur performance penalties or faults when accessing misaligned members. [Inference: Packed structures suit serialization and hardware register mapping where layout must match external specifications, despite performance costs.]

### Array and Pointer Arithmetic

Arrays of aligned elements maintain alignment if the element size is a power of two. Array element access computes addresses as `base + (index * element_size)`. If base aligns to element_size and element_size is a power of two, all elements align naturally.

Pointer arithmetic preserves alignment relationships. Adding a multiple of N to an N-aligned pointer yields an N-aligned pointer. Incrementing a properly aligned pointer by sizeof(type) maintains alignment.

### Stack Alignment

Function call conventions mandate 16-byte stack alignment on AArch64. The stack pointer must be 16-byte aligned immediately before executing call instructions (BL, BLR). This alignment requirement:

- Enables efficient SIMD operations on stack data
- Simplifies compiler optimizations assuming aligned stack access
- Matches cache line boundaries on some implementations

Functions allocate stack space in multiples of 16 bytes. Local variables requiring stricter alignment (like 128-bit vectors) must consider their position within the frame:

```
    sub sp, sp, #32          // Allocate 32 bytes (maintaining alignment)
    // SP now 16-byte aligned
    // Can safely store 128-bit vectors at [sp, #0] and [sp, #16]
```

The compiler calculates offsets ensuring properly aligned local variables given the guaranteed 16-byte aligned frame base.

### Alignment Attributes and Directives

C11/C++11 provide standardized alignment control through `_Alignas` (C) and `alignas` (C++):

```c
alignas(64) int cache_aligned_array[16];  // 64-byte aligned
```

GCC/Clang support `__attribute__((aligned(N)))`:

```c
struct __attribute__((aligned(32))) AlignedStruct {
    int data[8];
};
```

Assembly directives specify alignment:

```asm
.align 4                    // Align to 2^4 = 16 bytes
.balign 64                  // Align to 64 bytes (byte alignment)
.p2align 6                  // Align to 2^6 = 64 bytes (power-of-2 alignment)
```

Dynamic memory allocators provide alignment-specifying interfaces:

```c
void *aligned_alloc(size_t alignment, size_t size);  // C11
void *memalign(size_t alignment, size_t size);       // POSIX
int posix_memalign(void **memptr, size_t alignment, size_t size);
```

These return pointers aligned to the specified boundary, enabling allocation of SIMD buffers or DMA-suitable memory.

### Over-Alignment

Over-aligned types specify alignment stricter than their size requires. SIMD vector types typically require 16-byte alignment despite being used with various sizes. DMA buffers may require cache line (64-byte) alignment to avoid coherency issues.

Compilers propagate alignment requirements through pointer types. Casting between pointer types with different alignment requirements may cause warnings or undefined behavior:

```c
char *p = malloc(64);                        // Typically 16-byte aligned
alignas(64) char buffer[64];                 // 64-byte aligned
int *q = (int *)buffer;                      // OK: 64-byte > 4-byte requirement

int *r = (int *)p;                           // Potentially problematic
// 'r' assumes 4-byte alignment, but compiler may assume 16-byte
```

[Inference: Casting between pointer types with different alignment assumptions requires care to avoid triggering undefined behavior or compiler mis-optimizations.]

## Endianness

Endianness specifies the byte ordering for multi-byte values in memory. Little-endian stores the least significant byte at the lowest address; big-endian stores the most significant byte at the lowest address. ARM processors support both orderings, though little-endian predominates in modern systems.

### Little-Endian Layout

In little-endian ordering, a 32-bit value 0x12345678 stored at address 0x1000 appears in memory as:

```
Address:  0x1000  0x1001  0x1002  0x1003
Value:    0x78    0x56    0x34    0x12
          (LSB)                   (MSB)
```

The least significant byte (0x78) occupies the lowest address. Reading sequentially from low to high addresses yields bytes in increasing significance.

A 64-bit value 0x0123456789ABCDEF at 0x2000:

```
Address:  0x2000  0x2001  0x2002  0x2003  0x2004  0x2005  0x2006  0x2007
Value:    0xEF    0xCD    0xAB    0x89    0x67    0x45    0x23    0x01
```

### Big-Endian Layout

Big-endian reverses the byte order. The same 32-bit value 0x12345678 at 0x1000:

```
Address:  0x1000  0x1001  0x1002  0x1003
Value:    0x12    0x34    0x56    0x78
          (MSB)                   (LSB)
```

The most significant byte (0x12) occupies the lowest address, matching human-readable hexadecimal notation.

### ARM Endianness Configuration

ARMv8 processors operate in little-endian mode by default but support configurable endianness through exception level controls. The SCTLR_ELx registers contain endianness bits:

**EE bit (Exception Endianness)**: Controls endianness for explicit data accesses at the current exception level. When cleared (0), little-endian; when set (1), big-endian.

**E0E bit (Endianness of EL0)**: Controls EL0 (user-space) endianness independently of kernel endianness. This allows big-endian applications running on little-endian kernels (or vice versa).

Modern ARM systems overwhelmingly use little-endian mode. Linux on ARM defaults to little-endian for both kernel and user-space. [Inference: Big-endian ARM systems exist primarily in specialized embedded applications requiring big-endian network protocols or legacy compatibility.]

### Endianness and Data Interpretation

Endianness affects interpretation of multi-byte sequences but not individual bytes. A character array appears identical regardless of endianness:

```c
char str[] = "ABCD";
// Memory always contains: 'A' 'B' 'C' 'D' 0x00
// at sequential addresses regardless of endianness
```

Only when interpreting these bytes as multi-byte integers does endianness matter:

```c
int *p = (int *)str;
// Little-endian: *p = 0x44434241 (assuming ASCII)
// Big-endian:    *p = 0x41424344
```

### Network Byte Order

Network protocols standardize on big-endian byte order ("network byte order") for protocol fields. Systems must convert between host byte order and network byte order when sending/receiving network data.

Standard conversion functions handle this:

```c
uint32_t htonl(uint32_t hostlong);    // Host to network long (32-bit)
uint16_t htons(uint16_t hostshort);   // Host to network short (16-bit)
uint32_t ntohl(uint32_t netlong);     // Network to host long
uint16_t ntohs(uint16_t netshort);    // Network to host short
```

On little-endian systems, these functions byte-swap. On big-endian systems, they compile to no-ops since host order matches network order.

ARM provides byte-swapping instructions for efficient endianness conversion:

```asm
rev w0, w1      // Reverse bytes in 32-bit register
rev x0, x1      // Reverse bytes in 64-bit register
rev16 w0, w1    // Reverse bytes within each halfword
rev32 x0, x1    // Reverse bytes within each word (64-bit register)
```

REV swaps byte order completely. For a 32-bit value 0x12345678, REV produces 0x78563412. For network byte order conversion on little-endian systems, htonl/ntohl compile to REV instructions.

### File Formats and Serialization

File formats must specify endianness for multi-byte values. Some formats mandate specific endianness:

**Little-endian formats**: PE/COFF executables (Windows), DWARF debug info, most modern binary formats

**Big-endian formats**: Java class files, many legacy formats

**Endianness markers**: Some formats include magic numbers that appear different depending on endianness. Readers examine these markers to detect the file's endianness and adapt accordingly. For example, UTF-16 byte order marks (BOM) indicate text encoding direction.

**Endianness-neutral formats**: Text-based formats (JSON, XML, CSV) avoid endianness issues by representing numbers as character sequences. Protocol Buffers and similar serialization libraries handle endianness conversion automatically.

### Structure and Union Interpretation

Endianness affects structure member interpretation in mixed-type contexts:

```c
union EndianTest {
    uint32_t word;
    uint8_t bytes[4];
};

union EndianTest test;
test.word = 0x12345678;

// Little-endian:
// bytes[0] = 0x78, bytes[1] = 0x56, bytes[2] = 0x34, bytes[3] = 0x12

// Big-endian:
// bytes[0] = 0x12, bytes[1] = 0x34, bytes[2] = 0x56, bytes[3] = 0x78
```

Code relying on specific byte orderings within unions or accessing structures through different pointer types creates endianness dependencies. [Inference: Portable code should avoid assuming specific byte orderings within multi-byte types accessed through unions or type-punned pointers.]

### Bit Fields and Endianness

Bit field layout within integers exhibits compiler-dependent and endianness-dependent behavior:

```c
struct BitField {
    unsigned int a : 4;
    unsigned int b : 4;
    unsigned int c : 8;
};
```

The ordering of bit fields within the underlying integer varies by compiler and endianness. [Unverified: Specific bit field layout depends on compiler implementation and may not follow predictable endianness-based rules.] Portable code avoids relying on bit field memory layout, using bit fields only for in-memory access through the defined structure type.

### Detecting Endianness at Runtime

Code can detect system endianness at runtime:

```c
int is_little_endian(void) {
    uint16_t value = 0x0001;
    uint8_t *byte_ptr = (uint8_t *)&value;
    return byte_ptr[0] == 0x01;  // True if little-endian
}
```

This checks whether the least significant byte occupies the lowest address. Compilers often optimize this into constant folding, eliminating runtime checks.

Preprocessor defines may indicate endianness:

```c
#ifdef __BYTE_ORDER__
#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__
    // Little-endian code
#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
    // Big-endian code
#endif
#endif
```

### Mixed-Endian Systems

Some ARM configurations support mixed-endian operation where different exception levels or software contexts operate in different endianness modes. The E0E bit enables big-endian user-space applications on little-endian kernels.

[Unverified: Mixed-endian support varies by operating system and requires careful system software implementation.] System calls crossing endianness boundaries must convert pointer arguments and data structures between endianness representations.

### Performance Implications

Little-endian matches the natural byte ordering of modern peripherals and memory systems. [Inference: This alignment contributed to little-endian becoming the dominant choice for ARM systems, though no inherent performance advantage exists at the processor instruction level.] Big-endian systems require byte swapping when interfacing with little-endian devices or protocols, incurring REV instruction overhead.

**Key Points:**
- Stack grows downward (toward lower addresses) with SP pointing to the last occupied location; AArch64 requires 16-byte alignment at function call boundaries
- Static memory divides into .text (code), .rodata (constants), .data (initialized globals), .bss (zero-initialized globals), and TLS (thread-local) sections accessed via PC-relative addressing
- Heap provides dynamic allocation through system allocators that request memory from the OS and subdivide it, with implementations balancing fragmentation, performance, and metadata overhead
- ARM data types require natural alignment (N-byte data on N-byte boundaries) for optimal performance; misalignment handling is configurable with potential performance penalties or faults
- Little-endian (LSB at lowest address) predominates in ARM systems; big-endian support exists but mainly for specialized applications requiring network protocol compatibility
- REV family instructions provide efficient byte-swapping for endianness conversion, compiling to no-ops on systems where host order matches required order

**Important related topics**: Function calling conventions and parameter passing, dynamic linking and GOT/PLT mechanics, page table management and virtual memory translation, cache architecture and coherency protocols, SIMD memory access patterns and alignment requirements, atomics and memory ordering for concurrent programming, executable file formats (ELF structure), stack unwinding mechanisms for exception handling.

---

# Control Flow in ARM Assembly

## Conditional Execution

ARM's distinctive feature is its ability to conditionally execute almost any instruction based on processor status flags. This capability reduces branch instructions, improves code density, and can enhance performance by avoiding pipeline disruption.

### Condition Code Suffixes

Instructions can be made conditional by appending a two-letter condition code suffix. The processor evaluates the condition against current flag states before executing the instruction. If the condition is false, the instruction behaves as a NOP (no operation), consuming one cycle but changing no architectural state.

**Condition Code Mnemonics**

Each condition code tests specific combinations of status flags. EQ (equal) tests if Z flag is set, NE (not equal) tests if Z is clear, GT (greater than) tests Z clear AND N equals V for signed comparison, and LT (less than) tests N not equal to V. The complete set includes 15 usable conditions plus AL (always), which is the default when no suffix is specified.

**Syntax Application**

The condition suffix appears between the instruction mnemonic and any size/update suffixes. `ADDEQ` adds if equal, `LDRNEB` loads byte if not equal, `STRHS` stores if unsigned higher or same. The 'S' flag-update suffix comes after the condition code: `ADDNES` adds if not equal and updates flags.

**Example:**

```assembly
CMP r0, r1              @ Compare r0 and r1, set flags
MOVEQ r2, #1            @ r2 = 1 if r0 == r1
MOVNE r2, #0            @ r2 = 0 if r0 != r1
ADDGT r3, r3, #1        @ r3++ if r0 > r1 (signed)
LDRLE r4, [r5]          @ Load if r0 <= r1 (signed)
STRCC r6, [r7]          @ Store if carry clear (unsigned less than)
```

### Predication Benefits

Conditional execution eliminates short forward branches, reducing code size and avoiding branch prediction penalties. A simple if-then-else selecting between two values requires no branches: the comparison sets flags, then conditional moves select the appropriate value based on those flags.

**Branch Avoidance**

Traditional architectures require a conditional branch to skip instructions in an if statement. ARM can predicate the instructions themselves, executing them only when conditions are met. This is particularly effective for short conditional sequences of 1-4 instructions.

**Pipeline Efficiency**

[Inference: Predicated instructions that fail their condition checks typically allow the pipeline to continue smoothly, potentially avoiding the multi-cycle penalty of a mispredicted branch, though the exact performance characteristics depend on the specific ARM implementation].

**Example:**

```assembly
@ Traditional approach with branches
CMP r0, #10
BLT skip_block
ADD r1, r1, #5
MOV r2, #1
skip_block:

@ Predicated approach without branches
CMP r0, #10
ADDGE r1, r1, #5        @ Only execute if r0 >= 10
MOVGE r2, #1            @ Only execute if r0 >= 10
```

### Limitations and Considerations

Not all ARM architectures support predication equally. ARMv8-A in AArch64 state removes most conditional execution, retaining only conditional branches and conditional select instructions. Thumb mode provides limited conditional execution through IT (If-Then) blocks rather than per-instruction conditions.

**IT Blocks in Thumb**

Thumb-2 uses IT (If-Then) instruction to create conditional execution blocks. The IT instruction specifies a condition and up to four following instructions that execute conditionally. Syntax like `ITTTE EQ` means: if equal, then (execute), then (execute), then (execute), else (don't execute the fourth).

## Condition Flags (N, Z, C, V)

The ARM processor maintains condition flags in the Current Program Status Register (CPSR). These flags reflect properties of the most recent flag-updating operation and control conditional execution.

### Flag Register Organization

The CPSR contains four condition flags in bits 31-28: N (Negative) in bit 31, Z (Zero) in bit 30, C (Carry) in bit 29, and V (Overflow) in bit 28. Instructions with the 'S' suffix or comparison instructions update these flags based on operation results.

### N Flag - Negative

The N flag reflects the sign bit (bit 31) of the result. It is set (1) when the result is negative in two's complement representation, clear (0) when positive or zero. This flag enables signed comparison conditions.

**Setting Conditions**

Any flag-updating instruction sets N if the result's most significant bit is 1. For 32-bit operations, this means bit 31. Arithmetic operations set N based on the actual sign of the mathematical result, while logical operations set N based on the output pattern.

**Usage in Comparisons**

Signed comparison conditions (LT, LE, GT, GE) examine the N flag in combination with V to determine relative magnitude. After a subtraction or comparison, N indicates whether the first operand is less than the second (considering signed values).

### Z Flag - Zero

The Z flag indicates whether the result of an operation is zero. It is set (1) when all result bits are 0, clear (0) otherwise. This flag is fundamental to equality testing and loop termination.

**Equality Testing**

Comparison instructions subtract operands and set flags without storing the result. If the operands are equal, the subtraction yields zero, setting the Z flag. EQ condition tests Z set, NE tests Z clear, enabling equality-based conditional execution.

**Loop Control**

Decrement-and-test patterns use Z flag for loop termination. `SUBS r0, r0, #1` decrements a counter and sets flags; branching with BNE (branch if not equal, Z clear) continues the loop until the counter reaches zero.

**Example:**

```assembly
SUBS r0, r0, #1         @ Decrement and set flags
BNE loop_start          @ Branch if r0 != 0 (Z clear)
```

### C Flag - Carry

The C flag indicates unsigned overflow in arithmetic operations and bit shift outcomes. Its meaning varies by operation type: addition sets C on unsigned overflow, subtraction sets C when no borrow occurs (inverted borrow), and shifts set C to the last bit shifted out.

**Addition Carry**

ADD and ADC set C when the unsigned sum exceeds 32 bits. For example, adding 0xFFFFFFFF and 0x00000001 produces 0x00000000 with C set, indicating the true result is 0x100000000. This enables multi-precision arithmetic by propagating carry between word additions.

**Subtraction Borrow**

SUB and SBC set C when no borrow is needed: C is set (1) when first operand >= second operand unsigned, clear (0) when borrow occurs. This inverted logic matches ARM's subtract-with-carry implementation where SBC subtracts NOT(C) rather than C itself. After SUB, C set means no underflow occurred.

**Shift Operations**

Shift instructions (LSL, LSR, ASR, ROR) set C to the last bit shifted out of the register. Left shift sets C to the bit shifted out of bit 31, right shift sets C to the bit shifted out of bit 0. This allows recovering lost bits and enables extended precision shifts.

**Unsigned Comparisons**

Unsigned comparison conditions (HI, HS/CS, LO/CC, LS) test C and Z flags. After CMP (which performs subtraction), C set means first operand >= second operand unsigned. HI (higher) tests C set AND Z clear, HS (higher or same) tests C set, LO (lower) tests C clear, LS (lower or same) tests C clear OR Z set.

**Example:**

```assembly
@ Multi-precision addition: {r1,r0} = {r1,r0} + {r3,r2}
ADDS r0, r0, r2         @ Add lower words, set C if overflow
ADC r1, r1, r3          @ Add upper words plus carry

@ Unsigned comparison
CMP r4, r5              @ Compare r4 and r5
BHI higher              @ Branch if r4 > r5 (unsigned)
BLO lower               @ Branch if r4 < r5 (unsigned)
```

### V Flag - Overflow

The V flag indicates signed overflow: when an arithmetic operation produces a result that cannot be represented in two's complement format. Overflow occurs when adding two positive numbers yields a negative result, or adding two negative numbers yields a positive result.

**Signed Overflow Detection**

Addition sets V when operands have the same sign but the result has opposite sign. Subtracting a negative from a positive yielding negative, or a positive from a negative yielding positive, also sets V. The flag enables detection of results that exceed the representable range of -2,147,483,648 to 2,147,483,647 for 32-bit signed integers.

**Mathematical Conditions**

V is set in these cases: (positive + positive = negative), (negative + negative = positive), (positive - negative = negative), (negative - positive = positive). In all cases, the mathematical result exceeds the representable range, and the actual stored value wraps around through two's complement arithmetic.

**Signed Comparisons**

Signed comparison conditions (GT, GE, LT, LE) test N and V together. After subtraction, N indicates the sign of the result, but overflow can invert this. The condition N equals V means greater-or-equal (no overflow, or overflow from both directions), N not-equal-to V means less-than.

**Example:**

```assembly
@ Signed overflow example
MOV r0, #0x7FFFFFFF     @ r0 = 2,147,483,647 (max positive)
ADDS r0, r0, #1         @ r0 = 0x80000000 (wraps to -2,147,483,648)
                        @ V flag set: signed overflow occurred
                        @ N flag set: result is negative
                        @ C flag clear: no unsigned overflow

@ Signed comparison using V
CMP r1, r2              @ Compare r1 and r2
BGT signed_greater      @ Branch if r1 > r2 (signed, tests N == V and Z clear)
BLT signed_less         @ Branch if r1 < r2 (signed, tests N != V)
```

### Flag-Setting Instructions

Instructions update flags only when specified. Data processing instructions require 'S' suffix: ADDS, SUBS, ANDS. Comparison and test instructions always set flags: CMP, CMN, TST, TEQ. Memory access instructions (LDR, STR) never affect flags. Multiplication instructions can update flags with 'S' suffix, though flag values for multiplication have [Inference: implementation-defined behavior for some flag bits on certain architectures].

**Explicit Comparisons**

CMP (compare) performs subtraction without storing the result, only updating flags. `CMP rn, operand2` computes rn - operand2, setting flags for conditional branches or predicated instructions. CMN (compare negative) adds operands: rn + operand2, testing equality to negative values.

**Bit Testing**

TST (test) performs bitwise AND without storing results, only setting flags. `TST rn, operand2` computes rn AND operand2, with Z set if result is zero (no common bits set). TEQ (test equivalence) performs XOR similarly, with Z set if operands are equal (XOR yields zero).

**Example:**

```assembly
CMP r0, #100            @ Compare r0 with 100
CMN r1, #5              @ Compare r1 with -5 (test r1 + 5)
TST r2, #0x01           @ Test if bit 0 is set
TEQ r3, r4              @ Test if r3 equals r4
```

## Branch Instructions

Branch instructions alter program flow by loading new values into the program counter (PC). They enable loops, function calls, conditional execution blocks, and returns. ARM provides several branch variants for different control flow patterns.

### B - Branch

The B instruction performs an unconditional or conditional relative branch. It adds a signed offset to PC, enabling forward or backward jumps within approximately ±32MB in ARM state (±16MB in Thumb-2).

**Relative Addressing**

Branch targets are encoded as PC-relative offsets rather than absolute addresses. The assembler calculates the offset from the branch instruction to the target label. This makes code position-independent, executable anywhere in memory without relocation.

**Conditional Branching**

Adding condition codes creates conditional branches: BEQ branches if equal (Z set), BNE if not equal (Z clear), BGT if greater (signed), BHI if higher (unsigned). The branch executes only if the condition is true based on current flag values.

**Range Limitations**

[Inference: The PC-relative offset is encoded in a limited number of bits within the instruction, constraining branch range. For branches beyond this range, the assembler may generate long-branch sequences using multiple instructions or veneer code, though this is typically transparent to the programmer].

**Example:**

```assembly
B target                @ Unconditional branch to target
BEQ equal_case          @ Branch if Z flag set
BNE not_equal           @ Branch if Z flag clear
BGT greater_than        @ Branch if signed greater than
BHI higher              @ Branch if unsigned higher

loop_start:
    @ Loop body
    SUBS r0, r0, #1     @ Decrement counter
    BNE loop_start      @ Continue if not zero
```

### BL - Branch with Link

BL (branch with link) performs a function call by branching to a target address while saving the return address. Before branching, BL stores the address of the next instruction (PC + 4 in ARM state) into the link register (LR, r14).

**Function Call Mechanism**

The saved return address enables returning to the calling location. A function returns by copying LR back to PC, typically using `MOV PC, LR` or `BX LR`. The link register provides a simple single-level return mechanism without stack manipulation for leaf functions.

**Return Address Handling**

Non-leaf functions (those that call other functions) must preserve LR because the nested call overwrites it. Standard practice pushes LR onto the stack in the function prologue and pops it (often directly into PC) in the epilogue. This creates the call stack structure.

**Example:**

```assembly
BL my_function          @ Call function, LR = return address

my_function:
    PUSH {r4-r7, lr}    @ Save registers and return address
    @ Function body
    BL nested_call      @ Call another function (overwrites LR)
    @ More function body
    POP {r4-r7, pc}     @ Restore registers and return
```

### BX - Branch and Exchange

BX (branch and exchange) branches to an address in a register and can switch between ARM and Thumb instruction sets. The target address is loaded from a register rather than encoded as an immediate offset.

**Instruction Set Switching**

Bit 0 of the target address determines the instruction set after branching: 0 selects ARM state, 1 selects Thumb state. The actual branch target is the register value with bit 0 cleared (forced to even address). This enables interworking between ARM and Thumb code.

**Register-Based Branching**

BX enables computed branches, calling function pointers, and returning from functions. `BX LR` is the standard return instruction, compatible with both ARM and Thumb code. The instruction automatically switches to the appropriate instruction set based on the stored return address.

**Example:**

```assembly
BX lr                   @ Return from function, switch modes if needed
BX r0                   @ Branch to address in r0

@ Function pointer call
LDR r3, =function_ptr   @ Load pointer to function pointer
LDR r3, [r3]            @ Load actual function address
BLX r3                  @ Call function through pointer
```

### BLX - Branch with Link and Exchange

BLX combines the features of BL and BX: it saves the return address in LR and can switch instruction sets. Two forms exist: BLX with immediate offset (ARM-to-Thumb calls in ARM state) and BLX with register (general register-based call).

**Register Form**

`BLX rn` branches to the address in register rn, saves the return address in LR, and switches instruction sets based on bit 0 of rn. This is the standard instruction for calling functions through pointers, including virtual function calls and callback invocations.

**Immediate Form**

In ARM state, `BLX label` calls a Thumb function at a known location. The assembler encodes the offset and automatically sets bit 0 to indicate Thumb mode. This form is less common in hand-written assembly but generated by compilers for ARM-to-Thumb calls.

**Example:**

```assembly
BLX r4                  @ Call function in r4, save return address
BLX thumb_function      @ Call Thumb function from ARM code
```

### Return Mechanisms

Functions return by restoring PC to the saved return address. Several methods exist depending on function complexity and register usage.

**Simple Returns**

Leaf functions that don't modify LR can return with `BX LR` or `MOV PC, LR`. The BX form is preferred for interworking compatibility. These single-instruction returns are efficient for simple functions.

**Stack-Based Returns**

Functions that preserve registers on the stack can return by popping PC: `POP {r4-r7, pc}` restores saved registers and returns simultaneously. This is more efficient than separate POP and BX instructions and is standard in function epilogues.

**Conditional Returns**

Returns can be predicated like other instructions: `BXEQ LR` returns if equal, `POPNE {r4-r7, pc}` returns if not equal. This enables early returns based on conditions without additional branch instructions.

**Example:**

```assembly
@ Leaf function return
simple_function:
    @ Function body, LR unchanged
    BX lr                   @ Return

@ Non-leaf function return
complex_function:
    PUSH {r4-r7, lr}        @ Prologue
    @ Function body
    POP {r4-r7, pc}         @ Epilogue and return

@ Conditional return
check_value:
    CMP r0, #0
    BXEQ lr                 @ Return immediately if r0 == 0
    @ Continue processing
    BX lr                   @ Normal return
```

## Conditional Branches

Conditional branches combine branch instructions with condition codes to implement control flow structures like if-statements, loops, and case selections. They form the foundation of structured programming in assembly.

### Comparison-Based Branches

The typical pattern compares two values with CMP or TST, then branches based on the resulting flags. This implements conditional statements and loop tests.

**Equality and Inequality**

CMP followed by BEQ or BNE implements equality tests. `CMP r0, r1` then `BEQ equal_block` branches if r0 equals r1. BNE branches when operands differ. These are the most common conditional branches for if-statements and loop conditions.

**Relational Comparisons**

Signed comparisons use BGT (greater than), BGE (greater or equal), BLT (less than), BLE (less or equal). Unsigned comparisons use BHI (higher), BHS (higher or same), BLO (lower), BLS (lower or same). The signed variants test N and V flags together, unsigned variants test C and Z flags.

**Example:**

```assembly
@ If-then-else structure
CMP r0, r1
BNE else_block          @ If r0 != r1, jump to else
    @ Then block
    MOV r2, #1
    B endif
else_block:
    @ Else block
    MOV r2, #0
endif:

@ For loop: for (i = 10; i > 0; i--)
MOV r0, #10             @ i = 10
for_loop:
    @ Loop body
    SUBS r0, r0, #1     @ i--, set flags
    BGT for_loop        @ Continue if i > 0
```

### Condition Code Reference

The complete set of condition codes and their flag tests:

**EQ (Equal)**: Z set. Used after comparison when operands are equal.

**NE (Not Equal)**: Z clear. Used when operands differ.

**CS/HS (Carry Set / Higher or Same)**: C set. Unsigned greater-or-equal after comparison.

**CC/LO (Carry Clear / Lower)**: C clear. Unsigned less-than after comparison.

**MI (Minus/Negative)**: N set. Result is negative.

**PL (Plus/Positive or Zero)**: N clear. Result is non-negative.

**VS (Overflow Set)**: V set. Signed overflow occurred.

**VC (Overflow Clear)**: V clear. No signed overflow.

**HI (Higher)**: C set AND Z clear. Unsigned greater-than after comparison.

**LS (Lower or Same)**: C clear OR Z set. Unsigned less-or-equal after comparison.

**GE (Greater or Equal)**: N equals V. Signed greater-or-equal after comparison.

**LT (Less Than)**: N not-equal-to V. Signed less-than after comparison.

**GT (Greater Than)**: Z clear AND N equals V. Signed greater-than after comparison.

**LE (Less or Equal)**: Z set OR N not-equal-to V. Signed less-or-equal after comparison.

**AL (Always)**: Always execute. Default when no condition specified. [Inference: AL is typically omitted as it's the default condition].

**Example:**

```assembly
@ Unsigned comparison
CMP r0, r1
BHI unsigned_greater    @ r0 > r1 (unsigned)
BLS unsigned_less_eq    @ r0 <= r1 (unsigned)

@ Signed comparison
CMP r2, r3
BGT signed_greater      @ r2 > r3 (signed)
BLE signed_less_eq      @ r2 <= r3 (signed)

@ Overflow checking
ADDS r4, r5, r6         @ Add with flag update
BVS overflow_handler    @ Branch if signed overflow

@ Bit testing
TST r7, #0x04           @ Test bit 2
BNE bit_set             @ Branch if bit is set
BEQ bit_clear           @ Branch if bit is clear
```

### Loop Structures

Common loop patterns in ARM assembly use conditional branches for iteration control.

**While Loop**

A while loop tests the condition before each iteration. The test occurs at the loop start, with a conditional branch exiting when the condition becomes false.

**Example:**

```assembly
@ While (r0 != 0)
while_loop:
    CMP r0, #0
    BEQ end_while       @ Exit if r0 == 0
    @ Loop body
    SUB r0, r0, #1
    B while_loop
end_while:
```

**Do-While Loop**

A do-while loop tests the condition after each iteration, guaranteeing at least one execution. The conditional branch appears at the loop end.

**Example:**

```assembly
@ Do { body } while (r0 != 0)
do_loop:
    @ Loop body
    SUBS r0, r0, #1     @ Decrement and set flags
    BNE do_loop         @ Continue if r0 != 0
```

**For Loop**

For loops combine initialization, test, and increment. The counter update often uses SUBS to simultaneously decrement and set flags for the branch condition.

**Example:**

```assembly
@ For (i = 0; i < 10; i++)
MOV r0, #0              @ i = 0
for_loop:
    CMP r0, #10
    BGE end_for         @ Exit if i >= 10
    @ Loop body
    ADD r0, r0, #1      @ i++
    B for_loop
end_for:
```

**Countdown Loop Optimization**

Counting down to zero is more efficient than counting up because SUBS sets the Z flag, eliminating a separate comparison instruction.

**Example:**

```assembly
@ Efficient countdown: for (i = 10; i != 0; i--)
MOV r0, #10
countdown:
    @ Loop body
    SUBS r0, r0, #1     @ Decrement and set flags
    BNE countdown       @ No separate CMP needed
```

### Switch/Case Implementation

Switch statements with multiple cases can be implemented using comparison chains or jump tables depending on case density and range.

**Comparison Chain**

Sequential comparisons test each case value, branching to the corresponding handler. This works for any case values but requires multiple comparisons.

**Example:**

```assembly
@ Switch (r0)
CMP r0, #1
BEQ case_1
CMP r0, #2
BEQ case_2
CMP r0, #5
BEQ case_5
B default_case

case_1:
    @ Handle case 1
    B end_switch
case_2:
    @ Handle case 2
    B end_switch
case_5:
    @ Handle case 5
    B end_switch
default_case:
    @ Handle default
end_switch:
```

**Jump Table**

For dense consecutive case values, a jump table provides O(1) lookup. An array of branch targets is indexed by the case value, with bounds checking for safety.

**Example:**

```assembly
@ Switch (r0) for cases 0-3
CMP r0, #3              @ Bounds check
BHI default_case        @ Jump to default if > 3

ADR r1, jump_table      @ Load jump table address
LDR pc, [r1, r0, LSL #2] @ Branch to jump_table[r0]

jump_table:
    .word case_0
    .word case_1
    .word case_2
    .word case_3

case_0:
    @ Handle case 0
    B end_switch
case_1:
    @ Handle case 1
    B end_switch
@ ... additional cases
end_switch:
```

### Branch Prediction Considerations

[Inference: Modern ARM processors use branch prediction to speculatively execute instructions before branch outcomes are known. Predictable branches (loop back-edges, consistently taken/not-taken branches) typically predict well, while unpredictable branches may cause pipeline flushes. Conditional execution can eliminate branches entirely for short sequences, potentially improving performance by avoiding prediction altogether].

**Key Points:**

- ARM supports conditional execution of most instructions through two-letter condition code suffixes, reducing branch instructions for short conditional sequences
- Four condition flags (N, Z, C, V) in CPSR encode result properties: negative, zero, carry/borrow, signed overflow
- C flag has inverted meaning for subtraction: set indicates no borrow (first operand >= second operand unsigned)
- Signed comparisons (GT, GE, LT, LE) test N and V flags together to handle overflow cases correctly
- Unsigned comparisons (HI, HS, LO, LS) test C and Z flags for magnitude relationships without sign considerations
- Branch instructions include B (branch), BL (branch with link for calls), BX (branch with instruction set exchange), and BLX (combined call and exchange)
- Return addresses are saved in link register (LR/r14) by BL and BLX, with functions returning via BX LR or POP {pc}
- Conditional branches combine CMP/TST with condition codes (BEQ, BNE, BGT, etc.) to implement control flow structures
- Countdown loops using SUBS are more efficient than count-up loops as they eliminate separate comparison instructions
- Jump tables provide efficient O(1) switch statement implementation for dense consecutive case values

**Important related topics:** IT (If-Then) blocks in Thumb-2 mode, CPSR and SPSR register organization, AArch64 conditional execution changes (elimination of most predication), branch prediction and pipeline effects, function calling conventions and register preservation requirements, long branch veneer generation by assemblers/linkers, performance comparison between predicated execution and branching.

---

## Branch Instructions

### Unconditional Branches

Branch instructions transfer control to a different location in the program.

**Basic Branch Types:**

```assembly
b label               @ Branch to label (relative, ±16MB range)
bl function          @ Branch with Link (call function, saves return in LR)
bx r0                @ Branch and Exchange (can switch ARM/Thumb modes)
blx r0               @ Branch with Link and Exchange

# Example
main:
    bl function1     @ Call function1
    b next           @ Jump to next
    # This code never executes
    mov r0, #1

next:
    mov r0, #0
    bx lr            @ Return
```

**Long Branches:**

```assembly
# For addresses beyond ±16MB
ldr pc, =far_label   @ Load address into PC
# or
adr r0, far_label    @ Load address
bx r0                @ Branch to it
```

### Conditional Branches

Conditional branches depend on condition flags in the CPSR (Current Program Status Register).

**Condition Codes:**

```assembly
beq label            @ Branch if Equal (Z=1)
bne label            @ Branch if Not Equal (Z=0)
bgt label            @ Branch if Greater Than (signed)
bge label            @ Branch if Greater or Equal (signed)
blt label            @ Branch if Less Than (signed)
ble label            @ Branch if Less or Equal (signed)
bhi label            @ Branch if Higher (unsigned)
bhs/bcs label        @ Branch if Higher or Same / Carry Set (unsigned)
blo/bcc label        @ Branch if Lower / Carry Clear (unsigned)
bls label            @ Branch if Lower or Same (unsigned)
bmi label            @ Branch if Minus (N=1)
bpl label            @ Branch if Plus (N=0)
bvs label            @ Branch if Overflow Set (V=1)
bvc label            @ Branch if Overflow Clear (V=0)
```

**Condition Flags (CPSR):**

- **N (Negative)**: Set if result is negative
- **Z (Zero)**: Set if result is zero
- **C (Carry)**: Set on unsigned overflow or borrow
- **V (Overflow)**: Set on signed overflow

**Setting Condition Flags:**

```assembly
cmp r0, r1           @ Compare: sets flags based on r0 - r1
cmn r0, r1           @ Compare negative: flags based on r0 + r1
tst r0, r1           @ Test: flags based on r0 AND r1
teq r0, r1           @ Test equivalence: flags based on r0 XOR r1

# Data processing with 'S' suffix
adds r0, r1, r2      @ Add and set flags
subs r0, r1, r2      @ Subtract and set flags
ands r0, r1, r2      @ AND and set flags
```

**Conditional Execution (ARM mode, pre-ARMv8):**

```assembly
# Instructions can be conditionally executed
cmp r0, #10
addgt r1, r1, #1     @ Execute only if r0 > 10
movle r2, #0         @ Execute only if r0 <= 10

# Without branches
mov r0, #5
cmp r0, #3
movgt r1, #1         @ r1 = 1 if r0 > 3
movle r1, #0         @ r1 = 0 if r0 <= 3
```

**IT Blocks (Thumb-2):**

```assembly
# IT (If-Then) provides conditional execution in Thumb
cmp r0, #10
ite gt               @ If-Then-Else
movgt r1, #1         @ Execute if greater
movle r1, #0         @ Execute if less or equal

# Multiple instructions
cmp r0, r1
ittt eq              @ If-Then-Then-Then (all equal condition)
addeq r2, r2, #1
moveq r3, #0
streq r2, [r4]

# Mixed conditions
cmp r0, #5
ittee gt             @ If-Then-Then-Else-Else
addgt r1, r1, #1     @ Execute if GT
subgt r2, r2, #1     @ Execute if GT
addle r1, r1, #2     @ Execute if LE
subsle r2, r2, #2    @ Execute if LE
```

## Loops (For, While, Do-While Patterns)

### While Loop

A while loop tests the condition before executing the loop body.

**Pattern:**

```c
// C code
while (condition) {
    // body
}
```

**Assembly Implementation:**

```assembly
# while (i < 10) { sum += i; i++; }
# r0 = i, r1 = sum
    mov r0, #0           @ i = 0
    mov r1, #0           @ sum = 0

while_loop:
    cmp r0, #10          @ Compare i with 10
    bge while_end        @ Exit if i >= 10
    
    add r1, r1, r0       @ sum += i
    add r0, r0, #1       @ i++
    b while_loop         @ Continue loop

while_end:
    # r1 contains final sum
```

**Complex While Loop:**

```assembly
# while (arr[i] != 0 && i < size) { process(arr[i]); i++; }
# r0 = array base, r1 = size, r2 = i
    mov r2, #0           @ i = 0

while_array:
    cmp r2, r1           @ Check i < size
    bge while_array_end
    
    ldr r3, [r0, r2, LSL #2]  @ Load arr[i]
    cmp r3, #0           @ Check arr[i] != 0
    beq while_array_end
    
    push {r0-r2}         @ Save registers
    mov r0, r3           @ Argument for process
    bl process
    pop {r0-r2}          @ Restore registers
    
    add r2, r2, #1       @ i++
    b while_array
    
while_array_end:
```

### Do-While Loop

A do-while loop executes the body at least once before testing the condition.

**Pattern:**

```c
// C code
do {
    // body
} while (condition);
```

**Assembly Implementation:**

```assembly
# do { sum += i; i++; } while (i < 10);
# r0 = i, r1 = sum
    mov r0, #0           @ i = 0
    mov r1, #0           @ sum = 0

do_while_loop:
    add r1, r1, r0       @ sum += i
    add r0, r0, #1       @ i++
    cmp r0, #10          @ Compare i with 10
    blt do_while_loop    @ Continue if i < 10

    # r1 contains final sum
```

**Input Validation Example:**

```assembly
# do { input = read(); } while (input < 0 || input > 100);
# r0 = input
validate_input:
    bl read_input        @ Get input in r0
    cmp r0, #0           @ Check lower bound
    blt validate_input
    cmp r0, #100         @ Check upper bound
    bgt validate_input
    
    # r0 contains valid input
```

### For Loop

A for loop has initialization, condition, and increment sections.

**Pattern:**

```c
// C code
for (init; condition; increment) {
    // body
}
```

**Basic For Loop:**

```assembly
# for (i = 0; i < 10; i++) { sum += i; }
# r0 = i, r1 = sum
    mov r0, #0           @ i = 0 (initialization)
    mov r1, #0           @ sum = 0

for_loop:
    cmp r0, #10          @ condition: i < 10
    bge for_end
    
    add r1, r1, r0       @ body: sum += i
    
    add r0, r0, #1       @ increment: i++
    b for_loop

for_end:
    # r1 contains final sum
```

**Array Iteration:**

```assembly
# for (i = 0; i < n; i++) { result += array[i]; }
# r0 = array base, r1 = n, r2 = i, r3 = result
    mov r2, #0           @ i = 0
    mov r3, #0           @ result = 0

for_array:
    cmp r2, r1           @ i < n
    bge for_array_end
    
    ldr r4, [r0, r2, LSL #2]  @ Load array[i]
    add r3, r3, r4       @ result += array[i]
    
    add r2, r2, #1       @ i++
    b for_array

for_array_end:
    mov r0, r3           @ Return result
```

**Nested For Loop (Matrix):**

```assembly
# for (i = 0; i < rows; i++)
#     for (j = 0; j < cols; j++)
#         sum += matrix[i][j];
# r0 = matrix base, r1 = rows, r2 = cols
# r4 = i, r5 = j, r6 = sum
    push {r4-r7}
    mov r4, #0           @ i = 0
    mov r6, #0           @ sum = 0

outer_loop:
    cmp r4, r1           @ i < rows
    bge nested_end
    
    mov r5, #0           @ j = 0

inner_loop:
    cmp r5, r2           @ j < cols
    bge inner_end
    
    # Calculate offset: (i * cols + j) * 4
    mul r7, r4, r2       @ i * cols
    add r7, r7, r5       @ i * cols + j
    ldr r3, [r0, r7, LSL #2]  @ Load matrix[i][j]
    add r6, r6, r3       @ sum += matrix[i][j]
    
    add r5, r5, #1       @ j++
    b inner_loop

inner_end:
    add r4, r4, #1       @ i++
    b outer_loop

nested_end:
    mov r0, r6           @ Return sum
    pop {r4-r7}
    bx lr
```

**Countdown Loop (More Efficient):**

```assembly
# for (i = n-1; i >= 0; i--) { process(array[i]); }
# Counting down to zero is more efficient
# r0 = array, r1 = n
    mov r2, r1           @ i = n

countdown_loop:
    subs r2, r2, #1      @ i--, set flags
    blt countdown_end    @ Exit if negative
    
    ldr r3, [r0, r2, LSL #2]  @ Load array[i]
    # Process r3
    
    b countdown_loop

countdown_end:
```

**Optimized Loop (Test at End):**

```assembly
# Reduces branches by testing at loop end
# for (i = 0; i < n; i++) { work; }
# r0 = n, r1 = i
    mov r1, #0
    cmp r0, #0           @ Check if n > 0
    ble loop_skip        @ Skip if n <= 0

loop_start:
    # Loop body here
    
    add r1, r1, #1       @ i++
    cmp r1, r0           @ i < n
    blt loop_start       @ Continue if less

loop_skip:
```

### Loop Unrolling

Unrolling reduces loop overhead by processing multiple iterations per loop.

**Manual Unrolling:**

```assembly
# Unrolled by factor of 4
# for (i = 0; i < n; i++) { sum += array[i]; }
# r0 = array, r1 = n, r2 = sum
    mov r2, #0           @ sum = 0
    mov r3, #0           @ i = 0
    
    # Check if at least 4 iterations
    sub r4, r1, #4
    cmp r3, r4
    bgt unroll_remainder

unrolled_loop:
    ldr r5, [r0, r3, LSL #2]      @ array[i]
    ldr r6, [r0, r3, LSL #2]      @ array[i+1]
    ldr r7, [r0, r3, LSL #2]      @ array[i+2]
    ldr r8, [r0, r3, LSL #2]      @ array[i+3]
    
    add r2, r2, r5
    add r2, r2, r6
    add r2, r2, r7
    add r2, r2, r8
    
    add r3, r3, #4       @ i += 4
    cmp r3, r4
    ble unrolled_loop

unroll_remainder:
    cmp r3, r1
    bge unroll_end

remainder_loop:
    ldr r5, [r0, r3, LSL #2]
    add r2, r2, r5
    add r3, r3, #1
    cmp r3, r1
    blt remainder_loop

unroll_end:
    mov r0, r2           @ Return sum
```

### Software Pipelining

Overlapping loop iterations for better instruction scheduling.

**Example:**

```assembly
# Pipelined loop for better performance
# Process array: output[i] = process(input[i])
# r0 = input, r1 = output, r2 = count
    subs r2, r2, #1
    blt pipeline_end
    
    ldr r3, [r0], #4     @ Load first element (prologue)
    
pipeline_loop:
    bl process           @ Process current (r3 -> r3)
    subs r2, r2, #1      @ Decrement counter
    ldr r4, [r0], #4     @ Load next element (overlap)
    str r3, [r1], #4     @ Store result
    mov r3, r4           @ Move next to current
    bge pipeline_loop
    
    bl process           @ Process last element (epilogue)
    str r3, [r1]         @ Store last result

pipeline_end:
```

## Switch/Case Implementations

Switch statements can be implemented using different strategies depending on case density and range.

### Binary Search (Sparse Cases)

For sparse cases, a binary search tree of comparisons.

**Pattern:**

```c
switch(value) {
    case 1: ...
    case 10: ...
    case 100: ...
    case 1000: ...
}
```

**Assembly Implementation:**

```assembly
# r0 = switch value
switch_binary:
    cmp r0, #50          @ Mid-point between 1-10 and 100-1000
    blt switch_low
    
switch_high:
    cmp r0, #100
    beq case_100
    cmp r0, #1000
    beq case_1000
    b switch_default

switch_low:
    cmp r0, #1
    beq case_1
    cmp r0, #10
    beq case_10
    b switch_default

case_1:
    # Handle case 1
    b switch_end

case_10:
    # Handle case 10
    b switch_end

case_100:
    # Handle case 100
    b switch_end

case_1000:
    # Handle case 1000
    b switch_end

switch_default:
    # Default case
    
switch_end:
```

### Jump Table (Dense Cases)

For consecutive or dense cases, use a jump table for O(1) lookup.

**Pattern:**

```c
switch(value) {
    case 0: ...
    case 1: ...
    case 2: ...
    case 3: ...
}
```

**Assembly Implementation:**

```assembly
# r0 = switch value (0-3)
switch_jump_table:
    cmp r0, #3           @ Range check
    bhi switch_default   @ Out of range
    
    # Load jump address from table
    adr r1, jump_table   @ Get table address
    ldr pc, [r1, r0, LSL #2]  @ Jump to case

    .align 2
jump_table:
    .word case_0
    .word case_1
    .word case_2
    .word case_3

case_0:
    # Handle case 0
    b switch_end

case_1:
    # Handle case 1
    b switch_end

case_2:
    # Handle case 2
    b switch_end

case_3:
    # Handle case 3
    b switch_end

switch_default:
    # Default case

switch_end:
```

**Alternative: TBB/TBH Instructions (Thumb-2):**

```assembly
# Table Branch Byte - for small offsets
switch_tbb:
    cmp r0, #3
    bhi switch_default
    
    tbb [pc, r0]         @ Table branch byte
    
branch_table:
    .byte (case_0 - branch_table) / 2
    .byte (case_1 - branch_table) / 2
    .byte (case_2 - branch_table) / 2
    .byte (case_3 - branch_table) / 2
    .align 2

# Table Branch Halfword - for larger offsets
switch_tbh:
    cmp r0, #3
    bhi switch_default
    
    tbh [pc, r0, LSL #1] @ Table branch halfword
    
branch_table_h:
    .hword (case_0 - branch_table_h) / 2
    .hword (case_1 - branch_table_h) / 2
    .hword (case_2 - branch_table_h) / 2
    .hword (case_3 - branch_table_h) / 2
    .align 2
```

### Hybrid Approach (Non-Contiguous Dense Cases)

For cases like 0, 1, 2, 10, 11, 12, use multiple jump tables.

**Assembly Implementation:**

```assembly
# r0 = switch value
switch_hybrid:
    cmp r0, #2
    bls group_0_2        @ Cases 0-2
    cmp r0, #10
    blt switch_default   @ Between groups
    cmp r0, #12
    bls group_10_12      @ Cases 10-12
    b switch_default

group_0_2:
    adr r1, table_0_2
    ldr pc, [r1, r0, LSL #2]

group_10_12:
    sub r0, r0, #10      @ Normalize to 0-2
    adr r1, table_10_12
    ldr pc, [r1, r0, LSL #2]

    .align 2
table_0_2:
    .word case_0
    .word case_1
    .word case_2

table_10_12:
    .word case_10
    .word case_11
    .word case_12

case_0:
    mov r0, #100
    b switch_end

case_1:
    mov r0, #200
    b switch_end

case_2:
    mov r0, #300
    b switch_end

case_10:
    mov r0, #1000
    b switch_end

case_11:
    mov r0, #1100
    b switch_end

case_12:
    mov r0, #1200
    b switch_end

switch_default:
    mov r0, #-1

switch_end:
    bx lr
```

### Character Classification Switch

Common pattern for parsing and text processing.

**Assembly Implementation:**

```assembly
# Classify character type
# r0 = character, returns r0 = type (0=other, 1=digit, 2=lower, 3=upper)
classify_char:
    # Check digit (0-9, ASCII 48-57)
    sub r1, r0, #'0'
    cmp r1, #9
    movls r0, #1
    bxls lr
    
    # Check lowercase (a-z, ASCII 97-122)
    sub r1, r0, #'a'
    cmp r1, #25
    movls r0, #2
    bxls lr
    
    # Check uppercase (A-Z, ASCII 65-90)
    sub r1, r0, #'A'
    cmp r1, #25
    movls r0, #3
    bxls lr
    
    # Other
    mov r0, #0
    bx lr
```

## Function Calls and Returns

### ARM Procedure Call Standard (AAPCS)

The AAPCS defines register usage and calling conventions.

**Register Usage:**

- **r0-r3**: Argument registers (first 4 arguments)
- **r0**: Return value
- **r4-r11**: Callee-saved (must preserve)
- **r12 (IP)**: Intra-procedure call scratch register
- **r13 (SP)**: Stack pointer
- **r14 (LR)**: Link register (return address)
- **r15 (PC)**: Program counter

**Stack Alignment:**

- Stack must be 8-byte aligned at public interfaces
- Caller allocates space for arguments beyond r0-r3

### Simple Function Call

**Caller:**

```assembly
# Call function with arguments
    mov r0, #10          @ First argument
    mov r1, #20          @ Second argument
    bl add_numbers       @ Call function
    # r0 contains return value
```

**Callee:**

```assembly
# int add_numbers(int a, int b)
add_numbers:
    add r0, r0, r1       @ result = a + b
    bx lr                @ Return
```

### Leaf Function (No Nested Calls)

A leaf function doesn't call other functions.

**Example:**

```assembly
# int square(int x)
square:
    mul r0, r0, r0       @ x * x
    bx lr
```

### Non-Leaf Function (Saves LR)

Functions that call other functions must preserve LR.

**Example:**

```assembly
# int calculate(int a, int b)
# Returns: (a + b) * 2
calculate:
    push {lr}            @ Save return address
    
    add r0, r0, r1       @ a + b
    bl double_value      @ Call another function
    
    pop {pc}             @ Return (pop into PC)

double_value:
    lsl r0, r0, #1       @ x * 2
    bx lr
```

### Preserving Registers

**Callee-Saved Registers:**

```assembly
# Function using callee-saved registers
process_array:
    push {r4-r7, lr}     @ Save registers we'll use
    
    mov r4, r0           @ Save array pointer
    mov r5, r1           @ Save count
    mov r6, #0           @ Initialize sum
    mov r7, #0           @ Initialize index

loop:
    cmp r7, r5
    bge done
    
    ldr r0, [r4, r7, LSL #2]
    bl process_element   @ Can modify r0-r3, r12
    add r6, r6, r0       @ Accumulate result
    add r7, r7, #1
    b loop

done:
    mov r0, r6           @ Return sum
    pop {r4-r7, pc}      @ Restore and return
```

### Stack Frame

Functions with local variables create a stack frame.

**Stack Frame Layout:**

```
High addresses
+------------------+
| Saved LR         |  <- [FP, #4]
| Saved FP         |  <- FP (r11)
| Local var 1      |  <- [FP, #-4]
| Local var 2      |  <- [FP, #-8]
| Saved r4         |
| Saved r5         |
+------------------+  <- SP
Low addresses
```

**Function with Stack Frame:**

```assembly
# function(int a, int b)
# Local variables: int x, y, z
function_with_locals:
    push {r4-r5, fp, lr} @ Save registers
    mov fp, sp           @ Setup frame pointer
    sub sp, sp, #12      @ Allocate space for 3 local vars
    
    # Access parameters (in r0, r1)
    # Access local variables
    str r0, [fp, #-4]    @ x = a
    str r1, [fp, #-8]    @ y = b
    
    # Calculate z = x + y
    ldr r4, [fp, #-4]    @ Load x
    ldr r5, [fp, #-8]    @ Load y
    add r4, r4, r5       @ x + y
    str r4, [fp, #-12]   @ z = x + y
    
    # Return z
    ldr r0, [fp, #-12]
    
    mov sp, fp           @ Restore stack pointer
    pop {r4-r5, fp, pc}  @ Restore and return
```

### Variadic Functions

Functions with variable number of arguments.

**Example: printf-like function:**

```assembly
# int sum_varargs(int count, ...)
# r0 = count, r1-r3 = first three values, rest on stack
sum_varargs:
    push {r4, r5, lr}
    
    mov r4, #0           @ sum = 0
    mov r5, r0           @ counter = count
    
    # Process r1 if count >= 1
    cmp r5, #0
    ble varargs_done
    add r4, r4, r1
    sub r5, r5, #1
    
    # Process r2 if count >= 2
    cmp r5, #0
    ble varargs_done
    add r4, r4, r2
    sub r5, r5, #1
    
    # Process r3 if count >= 3
    cmp r5, #0
    ble varargs_done
    add r4, r4, r3
    sub r5, r5, #1
    
    # Process stack arguments
    add r0, sp, #12      @ Point to first stack arg
    
varargs_loop:
    cmp r5, #0
    ble varargs_done
    
    ldr r1, [r0], #4     @ Load and advance
    add r4, r4, r1
    sub r5, r5, #1
    b varargs_loop

varargs_done:
    mov r0, r4           @ Return sum
    pop {r4, r5, pc}
```

### Tail Call Optimization

When a function's last action is calling another function, optimize by jumping instead of calling.

**Without Optimization:**

```assembly
function_a:
    push {lr}
    # ... some work ...
    bl function_b        @ Call
    pop {pc}             @ Return
```

**With Tail Call Optimization:**

```assembly
function_a:
    # ... some work ...
    b function_b         @ Jump directly (no push/pop)
                         @ function_b will return to our caller
```

**Tail Recursion Example:**

```assembly
# int factorial_tail(int n, int accumulator)
factorial_tail:
    cmp r0, #1
    bls factorial_base   @ if n <= 1
    
    mul r1, r1, r0       @ accumulator *= n
    sub r0, r0, #1       @ n--
    b factorial_tail     @ Tail call (becomes loop)

factorial_base:
    mov r0, r1           @ Return accumulator
    bx lr
```

### Recursive Functions

**Direct Recursion:**

```assembly
# int factorial(int n)
factorial:
    cmp r0, #1
    bls factorial_base
    
    push {r0, lr}        @ Save n and return address
    sub r0, r0, #1       @ n - 1
    bl factorial         @ Recursive call
    
    pop {r1, lr}         @ Restore n
    mul r0, r0, r1       @ n * factorial(n-1)
    bx lr

factorial_base:
    mov r0, #1
    bx lr
```

**Mutual Recursion:**

```assembly
# int is_even(int n);
# int is_odd(int n);

is_even:
    cmp r0, #0
    moveq r0, #1         @ 0 is even
    bxeq lr
    
    push {lr}
    sub r0, r0, #1
    bl is_odd            @ is_even(n) = is_odd(n-1)
    pop {pc}

is_odd:
    cmp r0, #0
    moveq r0, #0         @ 0 is not odd
    bxeq lr
    
    push {lr}
    sub r0, r0, #1
    bl is_even           @ is_odd(n) = is_even(n-1)
    pop {pc}
```

### Function Pointers

Calling functions through pointers.

**Direct Call Through Register:**

```assembly
# void (*func_ptr)(int, int) = my_function;
# func_ptr(10, 20);

    ldr r0, =my_function @ Load function address
    mov r1, #10          @ First argument
    mov r2, #20          @ Second argument
    
    # Call through register
    push {lr}
    blx r0               @ Branch with link and exchange
    pop {pc}
```

**Function Pointer Array:**

```assembly
# Array of function pointers
    .data
function_table:
    .word func0
    .word func1
    .word func2
    .word func3

    .text
# Call function by index
# r0 = index
call_by_index:
    cmp r0, #3
    movhi r0, #-1
    bxhi lr              @ Return error
    
    ldr r1, =function_table
    ldr r2, [r1, r0, LSL #2]  @ Load function address
    
    push {lr}
    blx r2               @ Call function
    pop {pc}

func0:
    mov r0, #0
    bx lr

func1:
    mov r0, #1
    bx lr

func2:
    mov r0, #2
    bx lr

func3:
    mov r0, #3
    bx lr
```

## Jump Tables

Jump tables provide efficient multi-way branching using computed addresses.

### Basic Jump Table

**Implementation:**

```assembly
# Dispatch based on command index
# r0 = command (0-4)
command_dispatcher:
    cmp r0, #4
    movhi r0, #-1
    bxhi lr              @ Invalid command
    
    adr r1, command_table
    ldr pc, [r1, r0, LSL #2]

    .align 2
command_table:
    .word cmd_start
    .word cmd_stop
    .word cmd_pause
    .word cmd_resume
    .word cmd_reset

cmd_start:
    # Start command implementation
    mov r0, #0
    bx lr

cmd_stop:
    # Stop command implementation
    mov r0, #0
    bx lr

cmd_pause:
    # Pause command implementation
    mov r0, #0
    bx lr

cmd_resume:
    # Resume command implementation
    mov r0, #0
    bx lr

cmd_reset:
    # Reset command implementation
    mov r0, #0
    bx lr
```

### Position-Independent Jump Table

For position-independent code, use offsets instead of absolute addresses.

**Implementation:**

```assembly
# Position-independent jump table
dispatch_pic:
    cmp r0, #4
    bhi dispatch_invalid
    
    adr r1, offset_table
    ldr r2, [r1, r0, LSL #2]  @ Load offset
    add pc, r1, r2            @ PC = table_base + offset

    .align 2
offset_table:
    .word (handler_0 - offset_table)
    .word (handler_1 - offset_table)
    .word (handler_2 - offset_table)
    .word (handler_3 - offset_table)
    .word (handler_4 - offset_table)

handler_0:
    mov r0, #100
    bx lr

handler_1:
    mov r0, #101
    bx lr

handler_2:
    mov r0, #102
    bx lr

handler_3:
    mov r0, #103
    bx lr

handler_4:
    mov r0, #104
    bx lr

dispatch_invalid:
    mov r0, #-1
    bx lr
```

### Computed Goto (State Machine)

Jump tables are ideal for implementing state machines.

**State Machine Example:**

```assembly
# Simple state machine with 4 states
# r4 = current state, r5 = input, r6 = output
# States: 0=IDLE, 1=RUNNING, 2=PAUSED, 3=ERROR

state_machine_loop:
    cmp r4, #3
    bhi state_error
    
    # Dispatch to current state handler
    adr r0, state_table
    ldr pc, [r0, r4, LSL #2]

    .align 2
state_table:
    .word state_idle
    .word state_running
    .word state_paused
    .word state_error

state_idle:
    # Check input for start command
    cmp r5, #1           @ Start command?
    moveq r4, #1         @ Transition to RUNNING
    beq state_machine_loop
    
    # Stay in IDLE
    b state_machine_loop

state_running:
    # Check for pause or stop
    cmp r5, #2           @ Pause command?
    moveq r4, #2         @ Transition to PAUSED
    beq state_machine_loop
    
    cmp r5, #3           @ Stop command?
    moveq r4, #0         @ Transition to IDLE
    beq state_machine_loop
    
    # Process work
    add r6, r6, #1       @ Increment output
    b state_machine_loop

state_paused:
    # Check for resume or stop
    cmp r5, #4           @ Resume command?
    moveq r4, #1         @ Transition to RUNNING
    beq state_machine_loop
    
    cmp r5, #3           @ Stop command?
    moveq r4, #0         @ Transition to IDLE
    beq state_machine_loop
    
    # Stay paused
    b state_machine_loop

state_error:
    # Error handling
    mov r6, #-1          @ Set error output
    mov r4, #0           @ Reset to IDLE
    bx lr                @ Exit state machine
```

### Virtual Function Table (vtable)

Implementing C++ virtual functions using jump tables.

**Class Structure:**

```assembly
# struct Animal {
#     vtable_ptr;
#     int age;
# };
# 
# vtable = { speak, move, eat };

    .data
    .align 2
# Dog vtable
dog_vtable:
    .word dog_speak
    .word dog_move
    .word dog_eat

# Cat vtable
cat_vtable:
    .word cat_speak
    .word cat_move
    .word cat_eat

    .text
# Create dog object
# Returns pointer in r0
create_dog:
    push {r4, lr}
    
    mov r0, #8           @ Size: vtable_ptr + age
    bl malloc            @ Allocate memory
    
    mov r4, r0           @ Save object pointer
    ldr r1, =dog_vtable
    str r1, [r4]         @ Set vtable pointer
    mov r1, #0
    str r1, [r4, #4]     @ Initialize age = 0
    
    mov r0, r4
    pop {r4, pc}

# Call virtual function
# r0 = object pointer, r1 = method index (0=speak, 1=move, 2=eat)
call_virtual:
    push {r4, lr}
    
    mov r4, r0           @ Save object pointer
    ldr r2, [r0]         @ Load vtable pointer
    ldr r3, [r2, r1, LSL #2]  @ Load method address
    
    mov r0, r4           @ Pass object as 'this'
    blx r3               @ Call virtual method
    
    pop {r4, pc}

# Virtual function implementations
dog_speak:
    # this pointer in r0
    push {lr}
    ldr r0, =dog_bark_msg
    bl printf
    pop {pc}

dog_move:
    push {lr}
    ldr r0, =dog_walk_msg
    bl printf
    pop {pc}

dog_eat:
    push {lr}
    ldr r0, =dog_eat_msg
    bl printf
    pop {pc}

cat_speak:
    push {lr}
    ldr r0, =cat_meow_msg
    bl printf
    pop {pc}

cat_move:
    push {lr}
    ldr r0, =cat_walk_msg
    bl printf
    pop {pc}

cat_eat:
    push {lr}
    ldr r0, =cat_eat_msg
    bl printf
    pop {pc}

    .section .rodata
dog_bark_msg: .asciz "Woof!\n"
dog_walk_msg: .asciz "Dog walks\n"
dog_eat_msg:  .asciz "Dog eats\n"
cat_meow_msg: .asciz "Meow!\n"
cat_walk_msg: .asciz "Cat walks\n"
cat_eat_msg:  .asciz "Cat eats\n"
```

### Interrupt Vector Table

Jump tables are fundamental to interrupt handling.

**Vector Table Example:**

```assembly
# Cortex-M vector table
    .section .isr_vector, "a"
    .align 2
    .global vector_table

vector_table:
    .word _estack            @ 0: Initial stack pointer
    .word Reset_Handler      @ 1: Reset
    .word NMI_Handler        @ 2: Non-maskable interrupt
    .word HardFault_Handler  @ 3: Hard fault
    .word MemManage_Handler  @ 4: Memory management
    .word BusFault_Handler   @ 5: Bus fault
    .word UsageFault_Handler @ 6: Usage fault
    .word 0                  @ 7: Reserved
    .word 0                  @ 8: Reserved
    .word 0                  @ 9: Reserved
    .word 0                  @ 10: Reserved
    .word SVC_Handler        @ 11: Supervisor call
    .word DebugMon_Handler   @ 12: Debug monitor
    .word 0                  @ 13: Reserved
    .word PendSV_Handler     @ 14: Pendable service
    .word SysTick_Handler    @ 15: System tick timer
    .word EXTI0_Handler      @ 16: External interrupt 0
    .word EXTI1_Handler      @ 17: External interrupt 1
    # ... more interrupt handlers ...

    .text
    .thumb
    .thumb_func
Reset_Handler:
    # Initialize data section
    ldr r0, =_sdata
    ldr r1, =_edata
    ldr r2, =_sidata
    
copy_data:
    cmp r0, r1
    bge copy_data_done
    ldr r3, [r2], #4
    str r3, [r0], #4
    b copy_data

copy_data_done:
    # Zero BSS section
    ldr r0, =_sbss
    ldr r1, =_ebss
    mov r2, #0

zero_bss:
    cmp r0, r1
    bge zero_bss_done
    str r2, [r0], #4
    b zero_bss

zero_bss_done:
    # Call main
    bl main
    
    # Infinite loop if main returns
infinite_loop:
    b infinite_loop

    .weak NMI_Handler
    .thumb_set NMI_Handler, Default_Handler

    .weak HardFault_Handler
    .thumb_set HardFault_Handler, Default_Handler

Default_Handler:
    b Default_Handler
```

### Callback Registration System

Using function pointers for event-driven programming.

**Implementation:**

```assembly
# Event callback system
# Max 8 callbacks
    .data
    .align 2
callback_table:
    .space 32            @ 8 function pointers

callback_count:
    .word 0

    .text
# Register callback
# r0 = function pointer
register_callback:
    push {r4, r5, lr}
    
    ldr r4, =callback_count
    ldr r5, [r4]         @ Current count
    
    cmp r5, #8           @ Check if table full
    movge r0, #-1
    bge register_done
    
    ldr r1, =callback_table
    str r0, [r1, r5, LSL #2]  @ Store callback
    
    add r5, r5, #1       @ Increment count
    str r5, [r4]
    
    mov r0, #0           @ Success

register_done:
    pop {r4, r5, pc}

# Trigger all callbacks
# r0 = event data
trigger_callbacks:
    push {r4-r6, lr}
    
    mov r4, r0           @ Save event data
    ldr r5, =callback_table
    ldr r6, =callback_count
    ldr r6, [r6]         @ Load count
    
    mov r0, #0           @ Index

trigger_loop:
    cmp r0, r6
    bge trigger_done
    
    push {r0}            @ Save index
    ldr r1, [r5, r0, LSL #2]  @ Load callback address
    mov r0, r4           @ Pass event data
    blx r1               @ Call callback
    pop {r0}             @ Restore index
    
    add r0, r0, #1
    b trigger_loop

trigger_done:
    pop {r4-r6, pc}

# Example callbacks
callback_logger:
    push {lr}
    # Log event (r0 = event data)
    ldr r1, =log_msg
    bl printf
    pop {pc}

callback_counter:
    push {r4, lr}
    ldr r4, =event_counter
    ldr r1, [r4]
    add r1, r1, #1
    str r1, [r4]
    pop {r4, pc}

    .data
event_counter:
    .word 0

    .section .rodata
log_msg: .asciz "Event: %d\n"
```

### Dynamic Dispatch (Interpreter Pattern)

Interpreting bytecode using jump tables.

**Bytecode Interpreter:**

```assembly
# Simple stack-based VM
# Opcodes: 0=PUSH, 1=POP, 2=ADD, 3=SUB, 4=MUL, 5=HALT

    .data
    .align 2
vm_stack:
    .space 256           @ 64-element stack
vm_sp:
    .word vm_stack       @ Stack pointer

    .text
# Execute bytecode program
# r0 = bytecode array, r1 = length
vm_execute:
    push {r4-r7, lr}
    
    mov r4, r0           @ Bytecode pointer
    add r5, r0, r1       @ End of bytecode
    ldr r6, =vm_stack    @ Stack base
    mov r7, r6           @ Stack pointer

vm_fetch:
    cmp r4, r5           @ Check if done
    bge vm_halt
    
    ldrb r0, [r4], #1    @ Fetch opcode
    
    cmp r0, #5           @ Range check
    bhi vm_invalid_opcode
    
    adr r1, opcode_table
    ldr pc, [r1, r0, LSL #2]  @ Dispatch

    .align 2
opcode_table:
    .word op_push
    .word op_pop
    .word op_add
    .word op_sub
    .word op_mul
    .word op_halt

op_push:
    # PUSH immediate value (next byte)
    ldrb r0, [r4], #1    @ Fetch value
    str r0, [r7], #4     @ Push to stack
    b vm_fetch

op_pop:
    # POP (discard top)
    sub r7, r7, #4       @ Pop from stack
    b vm_fetch

op_add:
    # ADD: pop two values, push sum
    sub r7, r7, #4
    ldr r1, [r7]         @ Second operand
    sub r7, r7, #4
    ldr r0, [r7]         @ First operand
    add r0, r0, r1       @ Add
    str r0, [r7], #4     @ Push result
    b vm_fetch

op_sub:
    # SUB: pop two values, push difference
    sub r7, r7, #4
    ldr r1, [r7]         @ Second operand
    sub r7, r7, #4
    ldr r0, [r7]         @ First operand
    sub r0, r0, r1       @ Subtract
    str r0, [r7], #4     @ Push result
    b vm_fetch

op_mul:
    # MUL: pop two values, push product
    sub r7, r7, #4
    ldr r1, [r7]         @ Second operand
    sub r7, r7, #4
    ldr r0, [r7]         @ First operand
    mul r0, r0, r1       @ Multiply
    str r0, [r7], #4     @ Push result
    b vm_fetch

op_halt:
    # HALT: return top of stack
    sub r7, r7, #4
    ldr r0, [r7]         @ Get result
    b vm_done

vm_invalid_opcode:
    mov r0, #-1

vm_halt:
vm_done:
    pop {r4-r7, pc}
```

### Threaded Code Interpreter

More efficient interpreter using indirect threading.

**Direct Threaded Code:**

```assembly
# Direct threaded interpreter
# Each instruction contains address of next instruction

    .text
# Execute threaded code
# r0 = pointer to first instruction cell
execute_threaded:
    push {r4, lr}
    
    ldr r4, =data_stack
    mov r5, r4           @ Stack pointer
    
    ldr pc, [r0]         @ Jump to first instruction

# Threading primitives
next:
    .macro NEXT
    ldr r0, [r0], #4     @ Fetch next instruction address
    ldr pc, [r0]         @ Jump to it
    .endm

# Example instructions
t_push:
    ldr r1, [r0], #4     @ Fetch immediate value
    str r1, [r5], #4     @ Push to stack
    NEXT

t_add:
    sub r5, r5, #4
    ldr r2, [r5]         @ Pop second operand
    sub r5, r5, #4
    ldr r1, [r5]         @ Pop first operand
    add r1, r1, r2       @ Add
    str r1, [r5], #4     @ Push result
    NEXT

t_dup:
    ldr r1, [r5, #-4]    @ Get top
    str r1, [r5], #4     @ Push copy
    NEXT

t_halt:
    pop {r4, pc}

    .data
data_stack:
    .space 256

# Example program: 5 3 + (should give 8)
program:
    .word t_push
    .word 5
    .word t_push
    .word 3
    .word t_add
    .word t_halt
```

### Coroutine Switching

Jump tables for cooperative multitasking.

**Simple Coroutine System:**

```assembly
# Coroutine context structure:
# offset 0: SP
# offset 4: PC
# offset 8: r4-r11 (8 registers)

    .data
    .align 2
coroutine1_ctx:
    .space 40            @ Context size

coroutine2_ctx:
    .space 40

current_coroutine:
    .word coroutine1_ctx

    .text
# Yield to another coroutine
# r0 = next coroutine context
yield:
    # Save current context
    ldr r1, =current_coroutine
    ldr r2, [r1]         @ Current context
    
    # Save registers
    str sp, [r2, #0]     @ Save SP
    str lr, [r2, #4]     @ Save PC (return address)
    add r3, r2, #8
    stm r3, {r4-r11}     @ Save r4-r11
    
    # Switch to new coroutine
    str r0, [r1]         @ Update current
    
    # Restore new context
    ldr sp, [r0, #0]     @ Restore SP
    ldr lr, [r0, #4]     @ Restore PC
    add r3, r0, #8
    ldm r3, {r4-r11}     @ Restore r4-r11
    
    bx lr                @ Resume execution

# Initialize coroutine
# r0 = context, r1 = function, r2 = stack
init_coroutine:
    str r2, [r0, #0]     @ Set SP
    str r1, [r0, #4]     @ Set PC
    bx lr

# Example coroutine 1
coroutine1:
    mov r4, #0           @ Counter

coro1_loop:
    # Do work
    add r4, r4, #1
    
    # Yield to coroutine 2
    ldr r0, =coroutine2_ctx
    bl yield
    
    # Resumed
    cmp r4, #10
    blt coro1_loop
    
    # Done
    b coro1_loop

# Example coroutine 2
coroutine2:
    mov r4, #100

coro2_loop:
    # Do work
    sub r4, r4, #1
    
    # Yield to coroutine 1
    ldr r0, =coroutine1_ctx
    bl yield
    
    # Resumed
    cmp r4, #0
    bgt coro2_loop
    
    b coro2_loop
```

**Key Points:**

- Conditional branches test CPSR flags set by comparison or arithmetic operations; IT blocks enable conditional execution in Thumb mode
- Loop patterns (while, do-while, for) translate naturally to assembly using conditional branches, with countdown loops often more efficient
- Switch statements use binary search for sparse cases, jump tables for dense cases, and hybrid approaches for non-contiguous groups
- AAPCS defines r0-r3 for arguments and return values, r4-r11 as callee-saved, and requires functions to preserve registers they modify
- Non-leaf functions must save LR before calling other functions; tail call optimization eliminates unnecessary stack operations
- Jump tables enable O(1) dispatch for multi-way branches, essential for state machines, virtual functions, and interpreters
- Position-independent jump tables use offsets instead of absolute addresses for relocatable code
- Threaded code interpreters improve performance by eliminating the fetch-decode loop through direct jumps between instruction handlers

---

# Functions and Procedures

## ARM Procedure Call Standard (AAPCS)

The ARM Architecture Procedure Call Standard (AAPCS) defines conventions for function calls, ensuring interoperability between separately compiled code modules and different programming languages. Adhering to AAPCS allows assembly code to interface correctly with C/C++ and other high-level language code.

### Purpose and Scope

AAPCS standardizes the interface between caller and callee functions, specifying:

- Which registers pass parameters
- Which registers preserve values across calls
- How return values are communicated
- Stack organization and alignment requirements
- Special considerations for variadic functions

The standard exists in variants for different ARM profiles:

- AAPCS for ARMv7-A (32-bit application processors)
- AAPCS64 for ARMv8-A AArch64 (64-bit application processors)
- Embedded AAPCS variants for Cortex-R and Cortex-M processors

Following AAPCS is mandatory when interfacing with compiled code but optional for internal assembly-only code, though consistency with the standard simplifies maintenance.

### Register Usage Conventions (ARMv7 32-bit)

AAPCS defines specific roles for the 16 general-purpose registers (R0-R15):

**R0-R3 (Argument/Result registers)**: Pass the first four integer arguments to functions. R0 also returns integer results up to 32 bits. These registers are **caller-saved**—the caller cannot assume they preserve values across function calls.

**R4-R8, R10-R11 (Variable registers)**: Available for general use. These registers are **callee-saved**—a called function must preserve their values, saving them on entry and restoring them before returning if it uses them.

**R9 (Platform register)**: Role varies by platform. May be reserved for platform-specific purposes or available as an additional variable register. Portable code should avoid R9 or follow platform-specific conventions.

**R12 (IP - Intra-Procedure-call scratch register)**: Temporary register used by linker veneers and PLT code. May be corrupted by function calls, even if the called function doesn't explicitly use it. Treat as caller-saved.

**R13 (SP - Stack Pointer)**: Points to the current stack position. Must remain valid at all times and maintain 4-byte alignment (8-byte alignment at public interfaces). Callee-saved semantically—functions return with SP restored to entry value.

**R14 (LR - Link Register)**: Holds the return address when a function is called using BL or BLX. Caller-saved since the called function may call other functions, overwriting LR.

**R15 (PC - Program Counter)**: Contains the address of the current instruction plus 8 (or 4 in Thumb). Not directly manipulated for parameter passing but used implicitly in branches and returns.

### Register Usage Conventions (ARMv8 64-bit)

AAPCS64 defines conventions for 64-bit AArch64 mode with 31 general-purpose registers (X0-X30) plus special registers:

**X0-X7 (Argument/Result registers)**: Pass the first eight integer arguments. X0-X1 return integer results up to 128 bits. Caller-saved registers.

**X8 (Indirect result location)**: Holds the address for returning large structures that don't fit in registers. Caller-saved.

**X9-X15 (Temporary registers)**: Available for general use. Caller-saved—values not preserved across calls.

**X16-X17 (IP0, IP1 - Intra-Procedure-call temporary registers)**: Used by linker veneers and PLT code. May be corrupted by function calls. Caller-saved.

**X18 (Platform register)**: Role varies by platform. On some platforms reserved; on others available as temporary. Portable code should avoid X18.

**X19-X28 (Callee-saved registers)**: Must be preserved across function calls. A function using these registers must save them on entry and restore before returning.

**X29 (FP - Frame Pointer)**: Optional frame pointer. When used, points to the saved frame record (FP, LR pair) on the stack. Callee-saved.

**X30 (LR - Link Register)**: Holds the return address. Callee must save if calling other functions.

**SP (Stack Pointer)**: Must maintain 16-byte alignment at all times in AArch64. Points to the lowest used address on the stack.

**PC (Program Counter)**: Not directly accessible as a general-purpose register in AArch64. Branch and return instructions manipulate it implicitly.

Additionally, 32-bit **W0-W30** registers are the lower 32 bits of corresponding X registers.

### Floating-Point and SIMD Register Conventions

ARM processors with floating-point and NEON/SIMD support use separate register files:

**ARMv7 VFP registers**:

- S0-S31: 32 single-precision (32-bit) registers
- D0-D31: 32 double-precision (64-bit) registers (D0-D15 overlap with S0-S31)
- S0-S15 (D0-D7): Pass floating-point arguments and return values. Caller-saved.
- S16-S31 (D8-D15 lower halves): Callee-saved, must be preserved if used
- D16-D31: Caller-saved (if present on the processor)

**ARMv8 SIMD/FP registers**:

- V0-V31: 128-bit SIMD/FP registers
- Can be accessed as B (8-bit), H (16-bit), S (32-bit), D (64-bit), or Q (128-bit)
- V0-V7: Pass floating-point/SIMD arguments and return values. Caller-saved.
- V8-V15: Callee-saved—lower 64 bits (D8-D15) must be preserved if used
- V16-V31: Caller-saved

The distinction between caller-saved and callee-saved determines optimization opportunities—caller-saved registers allow the called function to use them freely, while callee-saved registers require save/restore overhead but allow the caller to maintain values across calls.

### Subroutine Call Instruction

Function calls use branch-with-link instructions that save the return address:

**ARMv7**:

```
BL function_name      @ Branch to function, LR = return address
BLX r0                @ Branch to address in r0, LR = return address
                      @ BLX can also switch ARM/Thumb modes
```

**ARMv8**:

```
BL function_name      @ Branch to function, X30 = return address
BLR x0                @ Branch to address in x0, X30 = return address
```

The return address stored in LR (R14 or X30) points to the instruction immediately following the call. Functions return by branching to this address.

### Function Return Mechanisms

Functions return control to the caller by restoring the program counter to the saved return address:

**ARMv7 simple return** (when LR not overwritten):

```
BX lr                 @ Return, can switch ARM/Thumb modes
MOV pc, lr            @ Alternative return (ARM mode only)
```

**ARMv7 return with register restoration**:

```
POP {r4-r8, pc}       @ Restore saved registers and return in one operation
```

**ARMv8 simple return**:

```
RET                   @ Return to address in X30 (LR)
RET x5                @ Return to address in specified register (rare)
```

**ARMv8 return with register restoration**:

```
LDP x29, x30, [sp], #16   @ Restore FP and LR from stack
RET                       @ Return to caller
```

When a function calls other functions, it must save LR before the nested call overwrites it, typically by pushing it onto the stack.

### Interworking Between ARM and Thumb

ARMv7 supports both ARM (32-bit instructions) and Thumb (16-bit instructions) modes. AAPCS defines interworking conventions:

The LSB of a function pointer or return address indicates the target mode:

- LSB = 0: ARM mode
- LSB = 1: Thumb mode

**BX** and **BLX** instructions examine the target address LSB and switch modes appropriately:

```
BX r0                 @ Branch to address in r0, switch to mode indicated by r0[0]
BLX label             @ Call function, switch modes if necessary
```

When taking function addresses, the assembler or compiler sets the LSB according to the function's mode:

```
.thumb_func           @ Declares following function as Thumb
function:
    @ Function body...
    BX lr

@ Later, loading this address produces an odd address (LSB=1)
LDR r0, =function     @ r0 gets address with LSB=1
```

ARMv8 AArch64 uses only 64-bit fixed-width instructions, eliminating mode switching complexity.

### C Language Type Mapping

AAPCS defines how C types map to registers and memory:

**ARMv7 (32-bit)**:

- `char`: 8 bits, zero or sign-extended to 32 bits in registers
- `short`: 16 bits, zero or sign-extended to 32 bits in registers
- `int`, `long`: 32 bits, occupy one register
- `long long`: 64 bits, occupy two consecutive registers (even-odd pair)
- Pointers: 32 bits, one register
- `float`: 32 bits, S register (or core register if no VFP)
- `double`: 64 bits, D register (or two core registers if no VFP)
- Structures: Passed by value using multiple registers or stack

**ARMv8 (64-bit)**:

- `char`: 8 bits, zero or sign-extended to 32/64 bits in registers
- `short`: 16 bits, zero or sign-extended to 32/64 bits in registers
- `int`: 32 bits, occupy lower 32 bits of register (W register)
- `long`, `long long`: 64 bits, occupy one X register
- Pointers: 64 bits, occupy one X register
- `float`: 32 bits, S register
- `double`: 64 bits, D register
- Structures: Passed by value using multiple registers or stack

### Endianness

ARM supports both little-endian and big-endian byte ordering, though little-endian is predominant in modern systems. AAPCS defines that:

- Multi-byte values stored in memory follow the configured endianness
- Register values are endian-neutral (bits have fixed positions)
- Data structures passed between functions must use consistent endianness

Code must not assume specific endianness unless targeting a known platform. Most modern ARM systems (including all Cortex-A application processors) use little-endian mode.

### Variadic Functions

Functions with variable numbers of arguments (variadic functions, like `printf`) follow special conventions:

**ARMv7**: Arguments are passed in the same manner as fixed-argument functions. The first four arguments use R0-R3, with additional arguments on the stack. The callee doesn't know which registers contain valid arguments without examining the format string or other metadata.

**ARMv8**: Similar to ARMv7, using X0-X7 for the first eight arguments. Variadic functions may need to save all argument registers to support `va_start`/`va_arg` operations.

The caller is responsible for stack cleanup in variadic functions, ensuring alignment and removing arguments after the call.

### Struct and Union Passing

Small structures may be passed in registers; larger structures use memory:

**ARMv7**:

- Structures ≤ 32 bits: Passed in one register
- Structures 33-64 bits: Passed in two registers
- Larger structures: Caller allocates memory, passes pointer in register

**ARMv8**:

- Homogeneous structures (all elements same type, ≤ 4 elements): May be passed in consecutive registers of appropriate type
- Structures ≤ 16 bytes: Passed in registers if sufficient registers available
- Larger structures: Passed by reference (pointer in X8 or stack)

The exact rules are complex, accounting for structure alignment and register availability. [Inference] Compilers handle these details automatically, but assembly programmers must follow the rules precisely for correct interoperation.

### Position-Independent Code (PIC)

AAPCS defines conventions for position-independent code, allowing code to execute correctly regardless of its load address. This is essential for shared libraries:

**ARMv7**: Uses R9 as the static base register (SB) or R10 as the global offset table (GOT) pointer, depending on the PIC variant. Function calls may indirect through a procedure linkage table (PLT).

**ARMv8**: Uses PC-relative addressing for position-independent access. The `ADRP` instruction calculates page addresses relative to PC, and subsequent instructions add page offsets.

**Example** ARMv8 position-independent global access:

```
ADRP x0, global_var       @ x0 = page address of global_var
LDR x1, [x0, :lo12:global_var]  @ Load from page offset
```

Position-independent code follows standard AAPCS register conventions but adds constraints on global data access patterns.

## Stack Frames

Stack frames (activation records) organize stack memory for each function invocation, storing local variables, saved registers, parameters, and linkage information.

### Stack Growth Direction

ARM stacks typically grow downward (toward lower addresses):

- Stack pointer starts at high address
- Pushing values decrements SP
- Popping values increments SP

The stack pointer must always point to valid, allocated stack memory. AAPCS specifies a "full descending" stack where SP points to the last occupied location.

### Stack Alignment Requirements

AAPCS mandates strict stack alignment:

**ARMv7**:

- SP must maintain 4-byte alignment at all times
- SP must maintain 8-byte alignment at public interfaces (function entry/exit)
- [Inference] 8-byte alignment ensures efficient double-word access and compatibility with LDRD/STRD instructions

**ARMv8**:

- SP must maintain 16-byte alignment at all times
- Violations may cause alignment faults or degraded performance

Proper alignment is the responsibility of both caller and callee. Functions that allocate stack space must ensure alignment is maintained.

### Frame Structure

A typical stack frame contains, from high to low addresses:

1. **Incoming arguments** (beyond register-passed arguments)
2. **Saved LR** (return address)
3. **Saved FP** (if frame pointer is used)
4. **Saved callee-saved registers** (R4-R11 in ARMv7, X19-X28 in ARMv8)
5. **Local variables**
6. **Temporary storage** (for spilled values, intermediate results)
7. **Outgoing arguments** (for nested function calls, beyond register-passed arguments)

The exact structure varies based on what the function needs. Simple leaf functions (functions that don't call other functions) may not create a frame at all.

### Frame Pointer Usage

The frame pointer (FP) is an optional feature that simplifies local variable access and debugging:

**ARMv7**: R11 conventionally serves as FP (though R7 is used in some contexts)

**ARMv8**: X29 is the designated frame pointer

When using a frame pointer:

1. On entry, save the previous FP and LR:

```
@ ARMv7
PUSH {fp, lr}         @ Save frame pointer and return address
MOV fp, sp            @ Establish new frame pointer
SUB sp, sp, #local_space  @ Allocate space for locals
```

```
@ ARMv8
STP x29, x30, [sp, #-16]!  @ Save FP and LR, pre-decrement SP
MOV x29, sp                @ Establish new frame pointer
SUB sp, sp, #local_space   @ Allocate space for locals
```

2. Throughout the function, access locals via FP-relative addressing with fixed offsets, even if SP changes
    
3. On exit, restore FP and return:
    

```
@ ARMv7
MOV sp, fp            @ Restore stack pointer
POP {fp, pc}          @ Restore FP and return

@ ARMv8
MOV sp, x29           @ Restore stack pointer
LDP x29, x30, [sp], #16  @ Restore FP and LR
RET
```

**Without frame pointer**, functions access locals via SP-relative addressing, using variable offsets when SP changes. This is more efficient (saves one register, eliminates FP setup) but complicates debugging and exception handling.

### Function Prologue

The function prologue sets up the stack frame on entry:

**ARMv7 example** with saved registers and locals:

```
function:
    PUSH {r4-r8, lr}      @ Save callee-saved registers and return address
    SUB sp, sp, #32       @ Allocate 32 bytes for local variables
    @ Function body...
```

**ARMv8 example**:

```
function:
    STP x29, x30, [sp, #-16]!   @ Save FP and LR
    MOV x29, sp                  @ Set up frame pointer
    STP x19, x20, [sp, #-16]!    @ Save callee-saved registers
    SUB sp, sp, #48              @ Allocate local variable space
    @ Function body...
```

The prologue must maintain stack alignment. If the total push/allocation is not alignment-compliant, adjust accordingly.

### Function Epilogue

The epilogue reverses the prologue, deallocating the frame and restoring saved values:

**ARMv7 example**:

```
    ADD sp, sp, #32       @ Deallocate local variables
    POP {r4-r8, pc}       @ Restore registers and return
```

**ARMv8 example**:

```
    ADD sp, sp, #48               @ Deallocate locals
    LDP x19, x20, [sp], #16       @ Restore callee-saved registers
    LDP x29, x30, [sp], #16       @ Restore FP and LR
    RET                            @ Return to caller
```

Alternatively, using the frame pointer:

```
@ ARMv7
    MOV sp, fp            @ Deallocate everything below FP
    POP {fp, pc}          @ Restore FP and return

@ ARMv8
    MOV sp, x29           @ Restore SP to frame base
    LDP x29, x30, [sp], #16
    RET
```

### Leaf Function Optimization

Leaf functions (functions that don't call other functions) don't need to save LR and may avoid creating a formal stack frame entirely if they don't need local storage or callee-saved registers:

```
@ ARMv7 minimal leaf function
leaf_add:
    ADD r0, r0, r1        @ r0 = r0 + r1
    BX lr                 @ Return (LR unchanged)

@ ARMv8 minimal leaf function
leaf_add:
    ADD x0, x0, x1        @ x0 = x0 + x1
    RET                   @ Return (X30 unchanged)
```

This optimization eliminates all stack overhead for simple functions.

### Stack Frame Example

**Example** C function:

```c
int compute(int a, int b, int c, int d, int e) {
    int x = a + b;
    int y = c * d;
    return x + y + e;
}
```

**ARMv7 implementation**:

```
@ Arguments: a=r0, b=r1, c=r2, d=r3, e=[sp, #0] (fifth arg on stack)
compute:
    PUSH {r4, lr}         @ Save r4 (for local y) and return address
    @ SP now 8 bytes lower, e is at [sp, #8]
    
    ADD r4, r0, r1        @ x = a + b (using r4 for x)
    MUL r0, r2, r3        @ r0 = c * d (y)
    LDR r1, [sp, #8]      @ r1 = e (load fifth argument)
    
    ADD r0, r4, r0        @ r0 = x + y
    ADD r0, r0, r1        @ r0 = x + y + e (result)
    
    POP {r4, pc}          @ Restore r4 and return
```

**ARMv8 implementation**:

```
@ Arguments: a=x0, b=x1, c=x2, d=x3, e=x4
compute:
    @ No need to save registers - using only argument registers
    ADD x5, x0, x1        @ x = a + b
    MUL x6, x2, x3        @ y = c * d
    ADD x0, x5, x6        @ result = x + y
    ADD x0, x0, x4        @ result += e
    RET                   @ Return
```

The ARMv8 version is simpler because all five arguments fit in registers.

### Dynamic Stack Allocation

Functions that require variable stack space (e.g., using `alloca()` or variable-length arrays) must use a frame pointer:

```
@ ARMv7 with variable allocation
function:
    PUSH {fp, lr}
    MOV fp, sp            @ Establish frame pointer
    @ r0 contains requested size
    SUB sp, sp, r0        @ Allocate variable amount
    BIC sp, sp, #7        @ Realign to 8 bytes
    @ Use allocated space via SP...
    MOV sp, fp            @ Restore SP using FP
    POP {fp, pc}
```

Without a frame pointer, the function cannot reliably determine how to restore SP because the allocation amount may vary at runtime.

### Nested Function Support

Some language implementations support nested functions (functions defined inside other functions). [Inference] These typically require a static link or display pointer to access the enclosing function's local variables:

[Inference] One approach uses an additional register (often the platform register) to point to the parent frame, allowing nested functions to access outer scope variables through this link. AAPCS doesn't mandate specific nested function support, leaving implementation to language-specific conventions.

### Red Zone

Some ABIs define a "red zone"—a fixed area below the stack pointer that functions can use without adjusting SP. [Inference] However, AAPCS does not define a red zone. Functions must not use memory below SP because asynchronous events (interrupts, signals) may overwrite it.

### Stack Unwinding and Exception Handling

Stack frames must be constructed to support exception handling and debugging:

[Inference] Frame pointers enable stack unwinding—traversing the call stack by following the chain of saved FP values. Each frame's saved FP points to the previous frame, creating a linked list.

[Inference] Exception handling mechanisms may insert unwind information into the executable, describing how to restore register state and deallocate each function's frame. This allows throwing exceptions through multiple stack frames, properly cleaning up each frame.

## Parameter Passing Conventions

AAPCS defines precise rules for passing arguments to functions, optimizing for register usage while supporting arbitrary parameter lists.

### Register-Based Parameter Passing (ARMv7)

The first four integer or pointer arguments use R0-R3 in order:

```
void func(int a, int b, int c, int d);
@ Called with: a=r0, b=r1, c=r2, d=r3
```

**Example** function call:

```
MOV r0, #10           @ First argument
MOV r1, #20           @ Second argument  
MOV r2, #30           @ Third argument
MOV r3, #40           @ Fourth argument
BL func
```

Arguments beyond the fourth are passed on the stack in reverse order (rightmost argument at lowest address), though AAPCS specifies they appear in forward order above the saved frame:

```
void func(int a, int b, int c, int d, int e, int f);
@ a=r0, b=r1, c=r2, d=r3, e=[sp, #0], f=[sp, #4]
```

**Example** calling with six arguments:

```
MOV r0, #10           @ a
MOV r1, #20           @ b
MOV r2, #30           @ c
MOV r3, #40           @ d
MOV r4, #50           @ e (temporary)
MOV r5, #60           @ f (temporary)
PUSH {r5}             @ Push f (sixth arg)
PUSH {r4}             @ Push e (fifth arg)
BL func
ADD sp, sp, #8        @ Clean up stack arguments
```

### Register-Based Parameter Passing (ARMv8)

The first eight integer or pointer arguments use X0-X7:

```
void func(long a, long b, long c, long d, long e, long f, long g, long h);
@ a=x0, b=x1, c=x2, d=x3, e=x4, f=x5, g=x6, h=x7
```

Arguments beyond the eighth are passed on the stack:

```
void func(long a, ..., long i, long j);
@ a-h in x0-x7, i=[sp, #0], j=[sp, #8]
```

**Example** calling with ten arguments:

```
MOV x0, #10           @ First argument
@ ... set x1-x7 ...
MOV x8, #90           @ Ninth argument (temporary)
MOV x9, #100          @ Tenth argument (temporary)
STP x8, x9, [sp, #-16]!  @ Push both onto stack
BL func
ADD sp, sp, #16       @ Clean up stack arguments
```

### Floating-Point Parameter Passing

Floating-point and SIMD arguments use separate register files:

**ARMv7 with VFP**:

- First 16 single-precision floats use S0-S15
- First 8 double-precision floats use D0-D7
- Additional FP arguments go on stack, 8-byte aligned

```
float func(float a, double b, float c);
@ a=s0, b=d1 (d1 overlaps s2-s3), c=s1
```

**ARMv8**:

- First 8 FP/SIMD arguments use V0-V7 (accessed as S, D, or Q depending on size)
- Additional FP arguments go on stack, aligned to their natural size

```
double func(double a, double b, float c);
@ a=d0, b=d1, c=s2
```

Mixed integer and FP arguments each use their respective register sets independently:

```
void func(int a, float b, int c, double d);
@ ARMv7: a=r0, b=s0, c=r1, d=d1
@ ARMv8: a=x0, b=s0, c=x1, d=d0
```

### Composite Type Parameter Passing

Structures and arrays follow complex rules based on size and composition:

**Small structures (ARMv7)**:

- Structures ≤ 32 bits: Passed in one register, fields packed according to endianness
- Structures 33-64 bits: Passed in two consecutive registers
- Larger structures: Passed by reference (pointer in register)

**Example** small structure:

```c
struct Point {
    short x;
    short y;
};

void plot(struct Point p);
@ p passed in r0: x in lower 16 bits, y in upper 16 bits (little-endian)
```

**Example** calling with structure:

```
MOV r0, #10           @ x = 10
ORR r0, r0, #20, LSL #16  @ y = 20 (packed into upper 16 bits)
BL plot
```

**Small structures (ARMv8)**:

- Structures ≤ 16 bytes: May be passed in registers if sufficient registers available
- Register allocation depends on member types and alignment

**Example** structure in multiple registers:

```c
struct Data {
    long a;
    long b;
};

void process(struct Data d);
@ ARMv8: d.a in x0, d.b in x1
```

**Homogeneous aggregates**: Structures containing 2-4 elements of the same floating-point or vector type may be passed in consecutive FP/SIMD registers:

```c
struct Vec4 {
    float x, y, z, w;
};

void transform(struct Vec4 v);
@ ARMv8: v.x=s0, v.y=s1, v.z=s2, v.w=s3
```

**Large structures**: Structures exceeding register capacity are passed by copying to stack, with the caller placing the copy and passing a pointer. In ARMv8, X8 may point to caller-allocated space for the return value.

### 64-bit Integer Parameter Passing (ARMv7)

64-bit integers require two 32-bit registers and must use an even-odd register pair:

```
void func(int a, long long b, int c);
@ a=r0, b=r2:r3 (skips r1 for alignment), c=[sp, #0]
```

If a 64-bit argument would use R3 and need R4, but R4 isn't available for parameter passing, the entire 64-bit value goes on the stack:

```
void func(int a, int b, int c, long long d);
@ a=r0, b=r1, c=r2, d=[sp, #0]:[sp, #4] (8-byte aligned on stack)
```

The register pair represents little-endian: lower 32 bits in lower-numbered register.

**Example** calling with 64-bit parameter:

```
MOV r0, #1            @ First argument
MOV r2, #0x89ABCDEF   @ Lower 32 bits of second argument
MOV r3, #0x01234567   @ Upper 32 bits of second argument
BL func
```

In ARMv8, 64-bit values simply use one X register without alignment concerns.

### Variable Argument Lists (va_list)

Variadic functions receive parameters identically to fixed-argument functions through the first N arguments. The function then uses `va_start`, `va_arg`, and `va_end` macros (or equivalent assembly) to access additional arguments.

**ARMv7 variadic function**:

```
int printf(const char *format, ...);
@ format in r0, additional arguments in r1-r3 and stack
```

The callee may need to save all argument registers to memory to create a contiguous argument list for `va_arg` traversal:

```
varfunc:
    PUSH {r0-r3}          @ Save all argument registers
    MOV r0, sp            @ Base of saved arguments
    @ Use r0 to access arguments sequentially...
    ADD sp, sp, #16       @ Clean up
    BX lr
```

**ARMv8 variadic function**: Similar approach, potentially saving X0-X7 to create an argument save area.

### Stack Argument Ordering and Alignment

Stack arguments appear at increasing addresses in the order they appear in the parameter list:

```
void func(int a, int b, int c, int d, int e, int f);
@ Stack layout after call (ARMv7):
@ [SP + 0]: e (fifth argument)
@ [SP + 4]: f (sixth argument)
```

Each stack argument is sized and aligned according to its type:

- 1-byte types: occupy 1 byte but typically padded to 4-byte alignment
- 2-byte types: occupy 2 bytes, aligned to 2-byte boundaries
- 4-byte types: occupy 4 bytes, aligned to 4-byte boundaries
- 8-byte types: occupy 8 bytes, aligned to 8-byte boundaries

The total stack space for arguments must maintain the overall stack alignment requirement (8 bytes for ARMv7, 16 bytes for ARMv8).

### Argument Evaluation Order

AAPCS does not mandate argument evaluation order—this is specified by the programming language. In C/C++, evaluation order is unspecified (except for specific operators), so the compiler may evaluate arguments in any order for optimization.

However, arguments must appear in registers and stack positions according to their source code position, regardless of evaluation order.

### Caller vs Callee Stack Cleanup

In AAPCS, the **caller** is responsible for removing stack arguments after the function returns:

```
@ Place arguments on stack
PUSH {r4-r5}          @ Two stack arguments
BL func
ADD sp, sp, #8        @ Caller removes arguments
```

This differs from some other calling conventions (like stdcall) where the callee cleans up. Caller cleanup allows variadic functions to work correctly since the callee doesn't know how many arguments were passed.

### Register Shadowing

Some ABIs define "register save areas" or "shadow space" where the callee can spill register arguments. Standard AAPCS does not require this, but some variants (particularly for Windows on ARM) include shadow space:

[Inference] Shadow space reserves stack locations corresponding to register arguments, even though those arguments were passed in registers. The callee may optionally store register arguments to these locations, simplifying debugging and variadic function implementation.

## Return Values

AAPCS defines conventions for returning results from functions, ensuring consistent handling of various data types across the caller-callee interface.

### Integer and Pointer Return Values

Simple scalar return values use specific registers:

**ARMv7**:
- 32-bit or smaller integers: R0
- 64-bit integers: R0 (lower 32 bits) and R1 (upper 32 bits)
- Pointers: R0

**ARMv8**:
- Up to 32-bit integers: W0 (lower 32 bits of X0)
- 64-bit integers and pointers: X0
- 128-bit integers: X0 (lower 64 bits) and X1 (upper 64 bits)

**Example** ARMv7 function returning int:
```
int add(int a, int b) {
    return a + b;
}

add:
    ADD r0, r0, r1        @ Result in r0
    BX lr                 @ Return
```

**Example** ARMv7 function returning 64-bit value:
```
long long multiply(int a, int b) {
    return (long long)a * b;
}

multiply:
    SMULL r0, r1, r0, r1  @ r0:r1 = r0 * r1 (signed 64-bit result)
    BX lr                 @ Return r0:r1
```

**Example** ARMv8 function returning pointer:
```
char* get_string(void) {
    return "Hello";
}

get_string:
    ADRP x0, string_literal
    ADD x0, x0, :lo12:string_literal
    RET                   @ Return pointer in x0

string_literal:
    .asciz "Hello"
```

### Boolean Return Values

Boolean values conventionally use 0 for false and non-zero (typically 1) for true, returned in R0/W0:

```
int is_positive(int x) {
    return x > 0;
}

is_positive:
    CMP r0, #0            @ Compare x with 0
    MOVGT r0, #1          @ r0 = 1 if x > 0
    MOVLE r0, #0          @ r0 = 0 if x <= 0
    BX lr

@ Or more efficiently in ARMv8:
is_positive:
    CMP w0, #0
    CSET w0, GT           @ Set w0 to 1 if greater, else 0
    RET
```

### Floating-Point Return Values

Floating-point results use the FP/SIMD register file:

**ARMv7 with VFP**:
- `float` (32-bit): S0
- `double` (64-bit): D0
- If no VFP, floats use R0, doubles use R0:R1

**ARMv8**:
- `float`: S0 (32-bit view of V0)
- `double`: D0 (64-bit view of V0)
- 128-bit vectors: Q0 (full V0)

**Example** ARMv8 function returning float:
```
float compute_ratio(float a, float b) {
    return a / b;
}

compute_ratio:
    FDIV s0, s0, s1       @ s0 = s0 / s1
    RET                   @ Return s0
```

**Example** mixed return types:
```c
double complex_calc(int x, float y);
@ ARMv8: x in w0, y in s0, returns double in d0
```

### Small Structure Return Values

Small structures that fit in registers are returned by value:

**ARMv7**:
- Structures ≤ 32 bits: Returned in R0
- Structures 33-64 bits: Returned in R0:R1
- Larger structures: Returned via memory (see below)

**Example** ARMv7 returning small structure:
```c
struct Point {
    short x;
    short y;
};

struct Point make_point(short x, short y) {
    struct Point p;
    p.x = x;
    p.y = y;
    return p;
}

make_point:
    @ x in r0 (lower 16 bits), y in r1 (lower 16 bits)
    ORR r0, r0, r1, LSL #16   @ Pack: r0 = (y << 16) | x
    BX lr                      @ Return packed structure in r0
```

**ARMv8**:
- Structures ≤ 16 bytes: Returned in X0 (and X1 if needed)
- Homogeneous aggregates: Returned in consecutive FP registers

**Example** ARMv8 returning structure in two registers:
```c
struct Pair {
    long first;
    long second;
};

struct Pair make_pair(long a, long b) {
    struct Pair p = {a, b};
    return p;
}

make_pair:
    @ a already in x0, b already in x1
    @ Structure members naturally occupy correct registers
    RET                   @ Return x0:x1
```

**Example** ARMv8 homogeneous aggregate:
```c
struct Vec4 {
    float x, y, z, w;
};

struct Vec4 create_vector(float val) {
    struct Vec4 v = {val, val, val, val};
    return v;
}

create_vector:
    @ val in s0
    FMOV s1, s0           @ Copy to s1
    FMOV s2, s0           @ Copy to s2
    FMOV s3, s0           @ Copy to s3
    RET                   @ Return s0:s1:s2:s3
```

### Large Structure Return Values

Structures too large for register return use indirect return via memory:

**ARMv7**: The caller allocates space for the return value and passes a pointer in R0. The function writes the result to this location and returns the pointer in R0.

**Example** ARMv7 large structure return:
```c
struct Large {
    int data[10];
};

struct Large create_large(void);

@ Caller:
caller:
    SUB sp, sp, #40       @ Allocate 40 bytes for return value
    MOV r0, sp            @ Pass pointer in r0
    BL create_large       @ Call function
    @ r0 points to result at [sp]
    @ Use result...
    ADD sp, sp, #40       @ Deallocate

@ Callee:
create_large:
    @ r0 contains pointer to caller-allocated space
    @ Write result to [r0]...
    @ r0 already contains correct pointer
    BX lr                 @ Return r0 unchanged
```

**ARMv8**: Similar mechanism using X8 as the indirect result location register. The caller passes the result buffer address in X8, and the function writes the result there.

**Example** ARMv8 large structure return:
```c
struct Matrix {
    double elements[4][4];
};

struct Matrix create_identity(void);

@ Caller:
caller:
    SUB sp, sp, #128      @ Allocate 128 bytes (16 doubles)
    MOV x8, sp            @ Pass pointer in x8
    BL create_identity    @ Call function
    @ Result now at [sp]
    @ Use result...
    ADD sp, sp, #128      @ Deallocate

@ Callee:
create_identity:
    @ x8 contains pointer to result buffer
    @ Write identity matrix to [x8]...
    @ No need to modify x8 or return value
    RET
```

The indirect return mechanism allows arbitrarily large structures without copying overhead beyond the initial write.

### Returning Multiple Values

C/C++ functions return a single value, but assembly can simulate multiple returns using conventions:

**Option 1**: Pack multiple values into registers:
```
@ Return two 16-bit values in r0
two_values:
    MOV r0, #10           @ First value (lower 16 bits)
    ORR r0, r0, #20, LSL #16  @ Second value (upper 16 bits)
    BX lr

@ Caller extracts values:
    BL two_values
    UXTH r1, r0           @ Extract lower 16 bits
    LSR r2, r0, #16       @ Extract upper 16 bits
```

**Option 2**: Use structure return (as shown above)

**Option 3**: Return one value normally, pass pointer for second:
```c
int divide_with_remainder(int a, int b, int *remainder);
@ Returns quotient in r0/x0, writes remainder to address in r1/x1
```

**Example** implementation:
```
@ ARMv7
divide_with_remainder:
    @ r0 = a, r1 = b, r2 = pointer to remainder
    SDIV r3, r0, r1       @ r3 = a / b (quotient) - ARMv7 with IDIV extension
    MLS r4, r3, r1, r0    @ r4 = a - (quotient * b) = remainder
    STR r4, [r2]          @ Store remainder
    MOV r0, r3            @ Return quotient
    BX lr
```

### Void Returns

Functions with `void` return type don't set any return value registers:

```
void print_message(void);

print_message:
    @ Function body...
    BX lr                 @ Simply return, no value in r0
```

However, R0-R3 (X0-X7 in ARMv8) are caller-saved, so their contents after a void function call are undefined. The caller must not rely on these registers preserving values across void function calls.

### Returning Status Codes

Many system-level functions return status codes indicating success or error:

```
int file_open(const char *path);
@ Returns file descriptor (>=0) on success, -1 on error

file_open:
    @ Attempt to open file...
    @ On success:
    MOV r0, #3            @ Return file descriptor
    BX lr
    
    @ On error:
error:
    MVN r0, #0            @ r0 = -1 (bitwise NOT of 0)
    BX lr
```

The caller checks the return value to determine success:
```
    LDR r0, =filename
    BL file_open
    CMP r0, #0
    BLT error_handler     @ Branch if r0 < 0 (error)
    @ Success, r0 contains valid file descriptor
```

### Returning Errno-Style Errors

Some conventions return success/failure boolean and set a global error variable:

```
int operation(void);
@ Returns 1 on success, 0 on failure
@ On failure, sets errno to error code

operation:
    @ Attempt operation...
    @ On failure:
fail:
    LDR r1, =errno        @ Load address of errno
    MOV r2, #EINVAL       @ Error code
    STR r2, [r1]          @ Set errno
    MOV r0, #0            @ Return failure
    BX lr
    
    @ On success:
success:
    MOV r0, #1            @ Return success
    BX lr
```

### Optimizing Return Value Handling

Efficient code often arranges computation so results naturally appear in return registers:

**Example** inefficient:
```
compute:
    @ Compute result in r4
    MOV r4, #42
    MOV r0, r4            @ Extra move to return register
    BX lr
```

**Example** efficient:
```
compute:
    @ Compute result directly in r0
    MOV r0, #42
    BX lr
```

Similarly, when calling a function and using its return value, avoid unnecessary moves:

```
    BL get_value          @ Returns in r0
    @ Use r0 directly without moving to another register
    ADD r1, r1, r0        @ Use return value
```

### Tail Call Optimization

When a function's final action is calling another function and returning its result, a tail call optimization replaces the call-return sequence with a direct branch:

**Example** without optimization:
```
wrapper:
    @ Prepare arguments...
    BL inner_func         @ Call inner function
    BX lr                 @ Return inner_func's result
```

**Example** with tail call optimization:
```
wrapper:
    @ Prepare arguments...
    B inner_func          @ Branch directly (not BL)
                          @ inner_func will return directly to our caller
```

This eliminates the extra return, saves stack space, and improves performance. [Inference] The optimization requires that the wrapper doesn't modify the stack frame or rely on saved registers, since control transfers directly to and from inner_func.

**Example** ARMv8 tail call:
```
wrapper:
    @ Set up arguments in x0-x7...
    B inner_func          @ Tail call - inner_func returns to our caller
```

### Return Value Register Preservation

Return value registers (R0-R1 or X0-X1) are caller-saved, meaning:

1. The caller cannot assume they preserve values across calls
2. The callee can freely modify them without saving
3. The callee must place return values in these registers before returning

**Example** caller handling return value:
```
caller:
    PUSH {r4, lr}
    MOV r4, #important_value  @ Save important data in callee-saved register
    
    BL some_function          @ Call - r0-r3 may be destroyed
    @ r0 now contains return value
    @ r4 still contains important_value
    
    ADD r0, r0, r4            @ Use return value with preserved data
    POP {r4, pc}              @ Return
```

### Returning Floating-Point and Integer Simultaneously

Some functions need to return both FP and integer results:

```c
// ARMv8
float process(int *status);
@ Returns float in s0, writes status to *status address passed in x0
```

**Example** implementation:
```
process:
    @ Compute float result...
    FMOV s0, #1.5         @ Float return value
    
    @ Compute status...
    MOV w1, #0            @ Status = success
    STR w1, [x0]          @ Write to caller's status variable
    
    RET                   @ Return s0 and modified *status
```

The caller receives the FP result in S0 and the integer status via the pointer.

### Complex Number Return Values

Complex numbers (with real and imaginary parts) use two FP registers:

**Example** ARMv8:
```c
// Complex float: real in s0, imaginary in s1
// Complex double: real in d0, imaginary in d1

float _Complex complex_sqrt(float _Complex z);
```

**Example** implementation:
```
complex_sqrt:
    @ Input: real in s0, imaginary in s1
    @ Compute result...
    @ Output: real in s0, imaginary in s1
    RET
```

### Returning Nothing vs Returning Undefined

Void functions don't specify a return value, but registers are in defined states:

**Void function**: Returns normally, R0/X0 contains undefined value but is in a valid state (not corrupted).

**No-return function**: Functions declared `noreturn` (like `exit()`) never return control to the caller. They may terminate the program, enter infinite loops, or perform non-local jumps.

```
void exit(int status) __attribute__((noreturn));

@ Implementation doesn't need return instruction:
exit:
    @ Terminate program through system call...
    @ No BX lr or RET - execution never continues
```

The `noreturn` attribute allows compilers to optimize away code after the call, since execution never continues.

### Preserving Return Values Across Cleanup

When returning a computed value, ensure cleanup code doesn't destroy it:

**Example** preserving return value during cleanup:
```
function:
    PUSH {r4-r6, lr}
    @ Allocate and use resources...
    
    @ Compute return value
    MOV r0, #42           @ Result in r0
    
    @ Clean up (don't modify r0)
    @ Deallocate resources, restore registers...
    POP {r4-r6, pc}       @ Return with r0 preserved
```

If cleanup code needs to use R0, save the return value temporarily:

```
function:
    PUSH {r4-r6, lr}
    @ Compute return value
    MOV r4, #42           @ Save return value in callee-saved register
    
    @ Cleanup that uses r0
    BL cleanup_function   @ May destroy r0
    
    MOV r0, r4            @ Restore return value
    POP {r4-r6, pc}       @ Return
```

### Returning From Nested Calls

Functions that call other functions must preserve LR before the call:

```
outer:
    PUSH {lr}             @ Save return address
    BL inner              @ Call inner (destroys LR)
    @ Process inner's return value in r0...
    POP {pc}              @ Return to original caller
```

Alternatively, combine saving and restoring with other registers:

```
outer:
    PUSH {r4, lr}         @ Save callee-saved register and LR
    BL inner
    @ Use r4 and process return value...
    POP {r4, pc}          @ Restore r4 and return
```

### Return Value Guarantees

AAPCS guarantees that return values are placed in designated registers, but doesn't specify:

- **Timing**: When during epilogue the return value is placed in the register
- **Unused bits**: Values in unused portions of registers (e.g., upper 16 bits when returning a 16-bit value)

**Example** 16-bit return value:
```
get_short:
    MOV r0, #0x1234       @ Return 16-bit value
    BX lr
```

The caller should mask or sign-extend as needed:
```
    BL get_short
    UXTH r0, r0           @ Zero-extend to ensure upper bits are clear
```

However, most calling conventions specify that narrow values are appropriately extended (zero or sign extended) to fill the register.

**Key Points**

The ARM Procedure Call Standard (AAPCS) defines comprehensive conventions for function interfaces, specifying register roles, parameter passing mechanisms, return value handling, and stack organization. Register usage divides into caller-saved registers (which callees may freely modify) and callee-saved registers (which must be preserved). Parameters pass through registers when possible (R0-R3 or X0-X7 for integers, S/D registers for floating-point), with excess parameters on the stack. Stack frames organize memory for each function invocation, containing saved registers, local variables, and linkage information, with strict alignment requirements (8-byte for ARMv7, 16-byte for ARMv8). Return values use designated registers (R0/R1 or X0/X1 for integers, S0/D0 for floating-point), with large structures returned indirectly via memory pointers. Following AAPCS precisely ensures assembly code interoperates correctly with compiled code from any conforming compiler, enabling modular development and library reuse. The standard balances performance (preferring register-based communication) with flexibility (supporting arbitrary parameter counts and types), while maintaining clear responsibility boundaries between caller and callee for register preservation and stack management.

---

## Calling Conventions and Register Usage

ARM defines standard calling conventions that govern how functions communicate through registers, how the stack is managed, and which registers must be preserved. These conventions enable interoperability between separately compiled code and provide predictable behavior across toolchains.

### AAPCS - ARM Architecture Procedure Call Standard

The ARM Architecture Procedure Call Standard (AAPCS) specifies register allocation, parameter passing, return value handling, and stack management rules. Different variants exist for different environments: AAPCS for bare-metal and Linux, AAPCS64 for AArch64, and specialized versions for specific operating systems.

**Register Roles**

The AAPCS assigns specific roles to registers. r0-r3 pass the first four integer arguments and return values, r4-r11 are callee-saved general purpose registers, r12 (IP) is the intra-procedure-call scratch register, r13 (SP) is the stack pointer, r14 (LR) holds the return address, and r15 (PC) is the program counter. Floating-point and SIMD registers have separate allocation rules.

**Stack Alignment**

The stack pointer must maintain 8-byte alignment at public interfaces (function calls). Internal to a function, 4-byte alignment is sufficient, but calling other functions requires restoring 8-byte alignment. This alignment requirement affects how many registers are pushed in function prologues.

**Example:**

```assembly
@ Function with proper AAPCS compliance
my_function:
    @ Prologue: save callee-saved registers
    PUSH {r4-r7, lr}        @ Save r4-r7 and return address
                             @ SP now 8-byte aligned (pushed 5 words = 20 bytes)
    SUB sp, sp, #4          @ Adjust for 8-byte alignment (total 24 bytes)
    
    @ Function body uses r0-r3 freely (caller-saved)
    @ Must preserve r4-r7 if used
    MOV r4, r0              @ Save argument in callee-saved register
    BL other_function       @ Call preserves r4-r7
    ADD r0, r4, r0          @ Combine saved and returned values
    
    @ Epilogue: restore registers and return
    ADD sp, sp, #4          @ Remove alignment padding
    POP {r4-r7, pc}         @ Restore registers and return
```

## Caller-Saved vs Callee-Saved Registers

Register preservation responsibilities are divided between calling and called functions. This division balances flexibility for short functions against register availability for complex functions.

### Caller-Saved Registers (r0-r3, r12)

Caller-saved registers are not preserved across function calls. The calling function must save their values before calling if those values are needed afterward. The called function can freely modify these registers without restoration.

**Argument Passing Registers (r0-r3)**

The first four integer or pointer arguments pass through r0-r3 in order. A function receiving three arguments finds them in r0, r1, and r2. Return values occupy r0 for single 32-bit results or r0-r1 for 64-bit results. These registers are caller-saved because they serve as communication channels between caller and callee.

**Scratch Register (r12/IP)**

r12 serves as an intra-procedure-call scratch register. Linker veneers (long-branch trampolines) and position-independent code implementations may corrupt r12 during function calls, even if the target function itself doesn't use it. Therefore, r12 must be treated as caller-saved and cannot hold values across calls.

**Usage Patterns**

Functions use r0-r3 for local computations without saving them. If a value in r0 must survive a function call, the caller either moves it to a callee-saved register before calling or pushes it to the stack. The called function assumes r0-r3 contain garbage upon entry (except for passed arguments) and can overwrite them freely.

**Example:**

```assembly
@ Caller must preserve r0-r3 if needed after call
caller:
    PUSH {lr}
    MOV r0, #5              @ First argument
    MOV r1, #10             @ Second argument
    MOV r4, r0              @ Save r0 value in callee-saved r4
    BL callee               @ r0-r3, r12 may be corrupted
    ADD r0, r4, r0          @ Use saved value from r4, return value from r0
    POP {pc}

callee:
    @ Can freely use r0-r3, r12 without saving
    ADD r0, r0, r1          @ r0 = arg1 + arg2 (return value)
    BX lr
```

### Callee-Saved Registers (r4-r11)

Callee-saved registers must be preserved by any function that uses them. If a function modifies r4-r11, it must save the original values in its prologue and restore them in its epilogue. This guarantees the calling function finds these registers unchanged after the call.

**Variable Preservation**

Long-lived local variables and values that must survive nested function calls are typically stored in callee-saved registers. This avoids repeatedly loading from memory or the stack across multiple operations and function calls.

**Prologue and Epilogue**

The function prologue saves registers at function entry, the epilogue restores them before return. Typically, PUSH saves multiple registers to the stack efficiently, and POP restores them. Including LR in the push and PC in the pop combines register preservation with call/return mechanics.

**Selective Preservation**

Functions need only save callee-saved registers they actually modify. A function using only r0-r3 requires no prologue or epilogue for register preservation. A function using r4 and r5 saves only those two registers, not all of r4-r11.

**Example:**

```assembly
@ Callee preserves r4-r7 which it uses
preserve_example:
    PUSH {r4-r7, lr}        @ Save registers to be modified
    
    MOV r4, r0              @ Use callee-saved registers
    MOV r5, r1              @ for local variables
    MOV r6, r2
    
    BL helper_function      @ r4-r7 survive this call
    
    ADD r0, r4, r5          @ Callee-saved values still available
    ADD r0, r0, r6
    
    POP {r4-r7, pc}         @ Restore and return
```

### Stack Frame Structure

Functions that use local variables beyond available registers or need to save many registers create stack frames. The stack frame holds saved registers, local variables, and sometimes saved arguments for nested calls.

**Frame Pointer (r11/FP)**

Some calling conventions use r11 as a frame pointer that points to a fixed location in the current stack frame. This simplifies debugging and stack unwinding but reduces available registers. Many embedded systems omit frame pointers to maximize register availability.

**Stack Layout**

A typical stack frame contains (from high to low addresses): previous frame data, saved callee-saved registers including LR, local variables, and space for outgoing arguments beyond r0-r3. The stack pointer points to the lowest used address (full descending stack convention).

**Example:**

```assembly
@ Function with local stack variables
stack_frame_example:
    PUSH {r4-r7, lr}        @ Save registers
    SUB sp, sp, #16         @ Allocate 16 bytes for local variables
                             @ [sp+0]  = local var 1
                             @ [sp+4]  = local var 2
                             @ [sp+8]  = local var 3
                             @ [sp+12] = local var 4
    
    MOV r0, #42
    STR r0, [sp, #0]        @ Store to local variable 1
    
    LDR r1, [sp, #0]        @ Load from local variable 1
    ADD r0, r1, #10
    
    ADD sp, sp, #16         @ Deallocate local variables
    POP {r4-r7, pc}         @ Restore and return
```

### Argument Passing Beyond r0-r3

When functions require more than four integer arguments, additional arguments pass through the stack. The caller pushes arguments beyond the fourth in reverse order before calling, and the callee accesses them relative to the stack pointer.

**Stack Argument Access**

After pushing r4-r7 and lr (5 words = 20 bytes), stack arguments are at sp + 20 or higher. The fifth argument is at [sp + 20], sixth at [sp + 24], etc. These offsets account for registers saved in the callee's prologue.

**Variadic Functions**

Functions with variable argument counts (like printf) must save all argument registers to the stack to create a contiguous argument array. The stdarg macros then access arguments sequentially from this stack location.

**Example:**

```assembly
@ Calling function with 6 arguments
caller_many_args:
    PUSH {lr}
    MOV r0, #1              @ Arg 1
    MOV r1, #2              @ Arg 2
    MOV r2, #3              @ Arg 3
    MOV r3, #4              @ Arg 4
    MOV r4, #5
    MOV r5, #6
    PUSH {r4, r5}           @ Args 5-6 on stack
    BL callee_many_args
    ADD sp, sp, #8          @ Clean up stack arguments
    POP {pc}

callee_many_args:
    PUSH {r4-r5, lr}
    @ r0-r3 contain args 1-4
    LDR r4, [sp, #12]       @ Arg 5 (sp+12: 3 saved regs * 4 bytes)
    LDR r5, [sp, #16]       @ Arg 6
    @ Use all six arguments
    POP {r4-r5, pc}
```

## Leaf and Non-Leaf Functions

Functions are classified as leaf or non-leaf based on whether they call other functions. This classification affects register preservation requirements and prologue/epilogue complexity.

### Leaf Functions

Leaf functions do not call other functions. They represent the terminal nodes of the call tree. Leaf functions have simpler requirements because they don't corrupt the link register through nested calls.

**Simplified Prologue/Epilogue**

Leaf functions that only use caller-saved registers (r0-r3, r12) require no prologue or epilogue. The return address in LR remains valid throughout execution, and the function simply executes `BX LR` to return.

**Register Constraints**

Leaf functions can use r0-r3 freely and return via BX LR. If they need additional registers, they must save and restore callee-saved registers (r4-r11) but still don't need to save LR because no nested call overwrites it.

**Example:**

```assembly
@ Simple leaf function: no prologue/epilogue needed
leaf_simple:
    ADD r0, r0, r1          @ Use only r0-r3
    MUL r0, r0, r2
    BX lr                   @ Return immediately

@ Leaf function using callee-saved registers
leaf_with_saved:
    PUSH {r4-r5}            @ Save only registers being used
                             @ No need to save LR
    MOV r4, r0              @ Use callee-saved for locals
    MOV r5, r1
    ADD r0, r4, r5
    MUL r0, r0, r4
    POP {r4-r5}             @ Restore saved registers
    BX lr                   @ Return (LR still valid)
```

**Performance Benefits**

Leaf functions execute faster because they avoid memory accesses for pushing and popping the link register. Optimizing functions to be leaves when possible improves performance, especially for frequently called small functions.

### Non-Leaf Functions

Non-leaf functions call other functions, requiring them to preserve the link register since nested calls overwrite it. Every non-leaf function must save LR in its prologue and restore it (or directly load it into PC) in its epilogue.

**Mandatory LR Preservation**

When BL or BLX executes, the current LR value is lost. Non-leaf functions must save LR before any nested call. The standard pattern pushes LR in the prologue and pops PC in the epilogue, combining return address restoration with the return branch.

**Prologue Pattern**

The typical non-leaf prologue is `PUSH {r4-r7, lr}` or similar, saving any callee-saved registers the function uses plus LR. The push order doesn't matter functionally, but convention places LR last for consistency.

**Epilogue Pattern**

The epilogue `POP {r4-r7, pc}` restores saved registers and returns in one instruction. Loading the saved LR value into PC branches to the return address. This is more efficient than separate `POP {r4-r7, lr}` followed by `BX lr`.

**Example:**

```assembly
@ Non-leaf function must save LR
non_leaf_function:
    PUSH {r4-r5, lr}        @ Must save LR before nested calls
    
    MOV r4, r0              @ Save argument
    MOV r0, r1
    BL helper               @ Nested call overwrites LR
    
    ADD r0, r4, r0          @ Combine results
    
    POP {r4-r5, pc}         @ Restore registers and return
                             @ (loads saved LR into PC)

helper:
    ADD r0, r0, #1
    BX lr
```

### Tail Call Optimization

A tail call occurs when a function's last action is calling another function and returning that function's result unchanged. Tail calls can be optimized to branches, avoiding stack frame creation for the tail-called function.

**Optimization Technique**

Instead of using BL which pushes a return address, then popping and returning, a tail call uses B to branch directly to the target function. The target function returns directly to the original caller, bypassing the intermediate frame.

**Register Restoration**

Before the tail call branch, the function must restore callee-saved registers but not LR. The saved LR value points to the original caller, and the tail-called function will return there. Arguments for the tail call must be arranged in r0-r3 before branching.

**Example:**

```assembly
@ Tail call optimization
tail_call_example:
    PUSH {r4, lr}
    
    MOV r4, r0              @ Save original argument
    BL first_function       @ Regular call
    
    @ Last action is calling another function
    MOV r0, r4              @ Prepare argument
    POP {r4, lr}            @ Restore registers including LR
    B second_function       @ Tail call: branch instead of BL
                             @ second_function returns directly to our caller

@ Without optimization (less efficient):
tail_call_naive:
    PUSH {r4, lr}
    
    MOV r4, r0
    BL first_function
    
    MOV r0, r4
    BL second_function      @ Normal call
    
    POP {r4, pc}            @ Separate return
```

**Tail Recursion**

Tail-recursive functions where the recursive call is the last operation can be optimized similarly. Instead of creating new stack frames for each recursion level, the optimized version branches back to the function start with updated arguments, transforming recursion into iteration.

## Nested Function Calls

Nested function calls occur when a function calls another function, which may itself call others, creating a chain of active stack frames. Managing nested calls requires careful preservation of return addresses and register values.

### Call Chain Management

Each function call pushes a new return address onto the stack (explicitly or implicitly via saved LR). The chain of return addresses forms the call stack, enabling each function to return to its caller in sequence.

**Stack Growth**

Each nested call that saves registers and allocates local variables grows the stack downward (on full descending stacks). The stack pointer decreases with each call and increases during returns. Running out of stack space causes stack overflow.

**Register Window**

At any point in nested calls, only the most recent function's registers are visible. Previous functions' register values are saved on the stack. As functions return, saved register values are restored, recreating previous contexts.

**Example:**

```assembly
@ Function A calls B, which calls C
function_a:
    PUSH {r4, lr}           @ Save A's registers, LR points to A's caller
    MOV r4, #10
    BL function_b           @ Call B, new LR points to A's code after BL
    ADD r0, r4, r0          @ After B returns, r4 still contains 10
    POP {r4, pc}            @ Return to A's caller

function_b:
    PUSH {r4, lr}           @ Save B's registers, LR points to A
    MOV r4, #20
    BL function_c           @ Call C, new LR points to B's code after BL
    ADD r0, r4, r0          @ After C returns, r4 still contains 20
    POP {r4, pc}            @ Return to A (saved LR)

function_c:
    MOV r0, #30             @ Leaf function, no save needed
    BX lr                   @ Return to B (current LR)

@ Call sequence creates stack frames:
@ [High addresses]
@ A's saved r4, A's saved LR (points to A's caller)
@ B's saved r4, B's saved LR (points to A)
@ [Low addresses - current SP]
```

### Deep Call Stacks

Deeply nested function calls accumulate many stack frames. Each frame occupies memory, and excessive nesting can exhaust available stack space. [Inference: Stack overflow typically results in undefined behavior, corrupting other data or causing program crashes, depending on the system's memory protection mechanisms].

**Stack Depth Considerations**

Embedded systems often have limited stack space (kilobytes rather than megabytes). Deep recursion or many nested calls with large local variable allocations can exhaust this space. Iterative algorithms are preferred over recursive ones in stack-constrained environments.

**Frame Size Impact**

Functions that save many registers and allocate large local variable arrays create large stack frames. Multiplied by call depth, this can quickly consume stack space. Minimizing saved registers and local variable size reduces per-frame overhead.

### Register Pressure in Nested Calls

Functions in the middle of call chains experience register pressure: they need registers for their own computations while preserving values across nested calls to functions lower in the chain.

**Callee-Saved Register Usage**

Using callee-saved registers (r4-r11) allows values to survive nested calls without manual preservation. A function stores important values in r4-r7, calls helpers that might use r0-r3, and finds r4-r7 unchanged afterward. This is the primary advantage of the caller/callee-saved division.

**Spilling to Stack**

When all suitable registers are occupied, values must be spilled to the stack. Load and store instructions around computations and function calls transfer values between registers and stack slots. Excessive spilling indicates high register pressure and may suggest refactoring.

**Example:**

```assembly
@ Function with register pressure
complex_function:
    PUSH {r4-r8, lr}        @ Need many callee-saved registers
    
    MOV r4, r0              @ Save arguments and intermediates
    MOV r5, r1              @ in callee-saved registers
    MOV r6, r2
    MOV r7, r3
    
    MOV r0, r4
    BL process_1            @ r4-r7 survive this call
    MOV r8, r0              @ Save result
    
    MOV r0, r5
    BL process_2            @ r4-r8 survive this call
    
    ADD r0, r8, r0          @ Combine results from saved registers
    ADD r0, r0, r6
    ADD r0, r0, r7
    
    POP {r4-r8, pc}
```

## Recursion Implementation

Recursive functions call themselves, creating multiple active instances simultaneously. Each invocation requires its own stack frame to maintain separate local state and return addresses.

### Direct Recursion

Direct recursion occurs when a function explicitly calls itself. Each recursive call creates a new stack frame, and stack frames accumulate until the base case is reached.

**Base Case**

Every recursive function must have a base case that stops recursion. Without a base case, recursion continues until stack overflow occurs. The base case typically tests arguments and returns directly without recursing.

**Recursive Case**

The recursive case modifies arguments to progress toward the base case, then calls the function recursively. The return value from the recursive call is typically used to compute the current invocation's return value.

**Example:**

```assembly
@ Recursive factorial: factorial(n) = n * factorial(n-1)
@ Base case: factorial(0) = 1
factorial:
    PUSH {r4, lr}           @ Save registers
    
    @ Base case: if (n == 0) return 1
    CMP r0, #0
    MOVEQ r0, #1
    POPEQ {r4, pc}          @ Conditional return for base case
    
    @ Recursive case: n * factorial(n-1)
    MOV r4, r0              @ Save n in callee-saved register
    SUB r0, r0, #1          @ n - 1
    BL factorial            @ Recursive call: r0 = factorial(n-1)
    MUL r0, r4, r0          @ n * factorial(n-1)
    
    POP {r4, pc}            @ Return

@ Calling factorial(5) creates stack frames:
@ factorial(5) → factorial(4) → factorial(3) → factorial(2) → factorial(1) → factorial(0)
@ Then returns: 1 → 1 → 2 → 6 → 24 → 120
```

### Stack Frame Accumulation

Each recursive call adds a stack frame containing saved registers and local variables. The stack grows until recursion reaches the base case, then shrinks as each invocation returns.

**Memory Usage**

Recursive depth determines memory usage. Recursion depth of N with F bytes per frame consumes N × F stack bytes. For factorial(5), six frames are active simultaneously. Deep recursion with large frames quickly exhausts stack space.

**Frame Content**

Each frame contains at minimum the return address (saved LR) and any callee-saved registers used. Additional local variables increase frame size. Minimizing per-frame size reduces total stack consumption for a given recursion depth.

**Example Analysis:**

```assembly
@ Stack state during factorial(3) execution:

@ Initial state: factorial(3) called from main
@ [High addresses]
@ main's saved registers
@ [factorial(3) frame]
@   saved r4 = (garbage)
@   saved LR = return to main
@   r4 = 3

@ After calling factorial(2):
@ [High addresses]  
@ main's saved registers
@ [factorial(3) frame]
@   saved r4 = 3
@   saved LR = return to main
@ [factorial(2) frame]
@   saved r4 = 2
@   saved LR = return to factorial(3)

@ After calling factorial(1):
@ [All previous frames...]
@ [factorial(1) frame]
@   saved r4 = 1
@   saved LR = return to factorial(2)

@ After calling factorial(0):
@ [All previous frames...]
@ [factorial(0) frame]
@   saved r4 = (unused in base case)
@   saved LR = return to factorial(1)
@ [Low addresses - current SP]

@ Then frames are popped in reverse order as functions return
```

### Tail Recursion Optimization

Tail-recursive functions where the recursive call is the last operation can be optimized to avoid stack frame accumulation. The optimization transforms recursion into iteration.

**Identification**

A function is tail-recursive if its recursive call is in tail position: the call's return value is immediately returned without further computation. For example, `return recursive_call(args)` is tail-recursive, but `return n * recursive_call(args)` is not because multiplication occurs after the call.

**Optimization Strategy**

Instead of calling the function recursively with BL (creating a new frame), the optimized version updates arguments and branches back to the function start with B. This reuses the current stack frame, maintaining constant stack usage regardless of recursion depth.

**Example:**

```assembly
@ Tail-recursive sum: sum(n, acc) = sum(n-1, acc+n) with base sum(0, acc) = acc
@ Not optimized:
sum_recursive:
    PUSH {r4-r5, lr}
    
    @ Base case: if (n == 0) return acc
    CMP r0, #0
    MOVEQ r0, r1
    POPEQ {r4-r5, pc}
    
    @ Recursive case: sum(n-1, acc+n)
    MOV r4, r0              @ Save n
    SUB r0, r0, #1          @ First arg: n-1
    ADD r1, r1, r4          @ Second arg: acc+n
    BL sum_recursive        @ Recursive call
    
    POP {r4-r5, pc}

@ Tail-call optimized version:
sum_optimized:
    @ Base case: if (n == 0) return acc
    CMP r0, #0
    MOVEQ r0, r1
    BXEQ lr
    
    @ Update arguments in place
    ADD r1, r1, r0          @ acc = acc + n
    SUB r0, r0, #1          @ n = n - 1
    B sum_optimized         @ Branch (not call) back to start
                             @ Reuses the same stack frame

@ sum_optimized(5, 0):
@ Iteration 1: n=5, acc=0  → n=4, acc=5
@ Iteration 2: n=4, acc=5  → n=3, acc=9
@ Iteration 3: n=3, acc=9  → n=2, acc=12
@ Iteration 4: n=2, acc=12 → n=1, acc=14
@ Iteration 5: n=1, acc=14 → n=0, acc=15
@ Base case reached: return 15
@ Only one stack frame exists throughout
```

**Accumulator Pattern**

Many recursive algorithms can be converted to tail-recursive form using an accumulator parameter. The accumulator carries intermediate results, allowing the function to compute the final result by the time it reaches the base case, eliminating post-recursion computation.

### Mutual Recursion

Mutual recursion occurs when function A calls function B, which calls function A, creating a cycle. Each function must be non-leaf and preserve LR because both participate in recursive calling.

**Forward Declarations**

Assembly doesn't require forward declarations like C, but branch targets must exist. Mutually recursive functions can reference each other freely as long as both are defined before program execution.

**Stack Behavior**

Mutually recursive functions alternate stack frames. The call pattern might be A → B → A → B → ... until base cases are reached in either A or B. Total stack depth is the sum of all active frames across both functions.

**Example:**

```assembly
@ Mutual recursion: even/odd checking
@ is_even(n) = (n == 0) ? true : is_odd(n-1)
@ is_odd(n)  = (n == 0) ? false : is_even(n-1)

is_even:
    PUSH {r4, lr}
    
    @ Base case: is_even(0) = true
    CMP r0, #0
    MOVEQ r0, #1            @ Return true
    POPEQ {r4, pc}
    
    @ Recursive case: is_odd(n-1)
    SUB r0, r0, #1
    BL is_odd               @ Call mutually recursive partner
    
    POP {r4, pc}

is_odd:
    PUSH {r4, lr}
    
    @ Base case: is_odd(0) = false
    CMP r0, #0
    MOVEQ r0, #0            @ Return false
    POPEQ {r4, pc}
    
    @ Recursive case: is_even(n-1)
    SUB r0, r0, #1
    BL is_even              @ Call mutually recursive partner
    
    POP {r4, pc}

@ Calling is_even(4) creates alternating frames:
@ is_even(4) → is_odd(3) → is_even(2) → is_odd(1) → is_even(0)
@ Returns: true ← true ← true ← false ← true
```

### Recursion vs Iteration Trade-offs

Recursive algorithms are often more elegant and easier to understand than iterative equivalents, but they consume more stack space and may execute slower due to function call overhead.

**When to Use Recursion**

Recursion is appropriate when: the algorithm is naturally recursive (tree traversals, divide-and-conquer), maximum depth is small and bounded, code clarity outweighs performance concerns, or tail-call optimization can eliminate stack growth.

**When to Prefer Iteration**

Iteration is preferable when: stack space is limited (embedded systems), recursion depth could be large, performance is critical, or the iterative version is equally clear. Most recursive algorithms can be converted to iterative form using explicit stacks or state machines.

**Iterative Conversion Example:**

```assembly
@ Iterative factorial (no recursion, no stack frames)
factorial_iterative:
    MOV r1, #1              @ result = 1
    CMP r0, #0              @ Handle n = 0
    MOVEQ r0, r1
    BXEQ lr
    
loop:
    MUL r1, r1, r0          @ result *= n
    SUBS r0, r0, #1         @ n--
    BNE loop                @ Continue if n != 0
    
    MOV r0, r1              @ Return result
    BX lr

@ Constant stack usage, no function call overhead
@ Executes faster and uses less memory than recursive version
```

**Key Points:**

- AAPCS defines r0-r3 as caller-saved argument/return registers, r4-r11 as callee-saved, and requires 8-byte stack alignment at function boundaries
- Caller-saved registers (r0-r3, r12) can be freely modified by callees; callers must preserve needed values before calling
- Callee-saved registers (r4-r11) must be preserved by callees if modified; this guarantees callers find them unchanged after calls
- Leaf functions that use only r0-r3 need no prologue/epilogue and return directly via BX LR
- Non-leaf functions must save LR in prologue and restore it (typically by popping into PC) in epilogue to handle nested calls
- Stack frames grow downward with each nested call, containing saved registers, return addresses, and local variables
- Tail calls can be optimized from BL to B, allowing the tail-called function to return directly to the original caller
- Recursive functions create stack frame accumulation with depth equal to recursion depth; each frame consumes stack space
- Tail recursion can be optimized to iteration by updating arguments and branching back to function start, maintaining constant stack usage
- Arguments beyond r0-r3 pass through the stack; the caller pushes them before calling, and the callee accesses them via stack pointer offsets

**Important related topics:** Stack unwinding for exception handling and debugging, frame pointer usage for debuggability vs register availability trade-offs, Position-Independent Code (PIC) and register usage implications, interrupt handler special calling conventions, naked functions without automatic prologue/epilogue, software stack checking and protection mechanisms, profiling and call graph analysis, trampolines and thunks for indirect calls.

---

# Advanced Data Operations

ARM assembly provides specialized instructions for efficient data manipulation, bulk transfers, and type conversions. These operations enable optimized memory access patterns, structure handling, and data processing that would require multiple instructions using basic load/store operations.

## Multi-Register Operations (LDM, STM)

Load Multiple (LDM) and Store Multiple (STM) instructions transfer multiple registers to/from memory in a single operation, significantly improving performance for bulk data operations.

### Basic LDM/STM Syntax

**Load Multiple (LDM):**

```assembly
ldm r0, {r1-r5}          @ Load r1, r2, r3, r4, r5 from [r0]
ldm r0!, {r1-r5}         @ Load and update r0 (write-back)
ldmia r0, {r1-r5}        @ Increment After (same as ldm)
ldmib r0, {r1-r5}        @ Increment Before
ldmda r0, {r1-r5}        @ Decrement After
ldmdb r0, {r1-r5}        @ Decrement Before
```

**Store Multiple (STM):**

```assembly
stm r0, {r1-r5}          @ Store r1, r2, r3, r4, r5 to [r0]
stm r0!, {r1-r5}         @ Store and update r0 (write-back)
stmia r0, {r1-r5}        @ Increment After (same as stm)
stmib r0, {r1-r5}        @ Increment Before
stmda r0, {r1-r5}        @ Decrement After
stmdb r0, {r1-r5}        @ Decrement Before
```

### Addressing Modes for LDM/STM

**Increment After (IA):**

```assembly
# Address used, then incremented
# r0 = 0x1000, registers = {r1, r2, r3}
ldmia r0!, {r1-r3}

# Memory accesses:
# r1 = [0x1000]
# r2 = [0x1004]
# r3 = [0x1008]
# r0 = 0x100C (updated)
```

**Increment Before (IB):**

```assembly
# Address incremented, then used
# r0 = 0x1000
ldmib r0!, {r1-r3}

# Memory accesses:
# r1 = [0x1004]
# r2 = [0x1008]
# r3 = [0x100C]
# r0 = 0x100C (updated)
```

**Decrement After (DA):**

```assembly
# Address used, then decremented
# r0 = 0x1000
ldmda r0!, {r1-r3}

# Memory accesses:
# r1 = [0x1000]
# r2 = [0x0FFC]
# r3 = [0x0FF8]
# r0 = 0x0FF8 (updated)
```

**Decrement Before (DB):**

```assembly
# Address decremented, then used
# r0 = 0x1000
ldmdb r0!, {r1-r3}

# Memory accesses:
# r1 = [0x0FFC]
# r2 = [0x0FF8]
# r3 = [0x0FF4]
# r0 = 0x0FF4 (updated)
```

### Register List Specifications

**Contiguous Ranges:**

```assembly
ldm r0, {r1-r5}          @ r1, r2, r3, r4, r5
ldm r0, {r0-r7}          @ r0 through r7
```

**Non-Contiguous Lists:**

```assembly
ldm r0, {r1, r3, r5, r7} @ Only odd-numbered registers
ldm r0, {r0, r2, r4-r7}  @ Mixed: r0, r2, r4, r5, r6, r7
```

**Including Special Registers:**

```assembly
ldm r0, {r1-r5, lr}      @ Include link register
stm r0, {r4-r11, lr}     @ Common function prologue pattern
ldm sp!, {r4-r11, pc}    @ Common epilogue (restore and return)
```

**Register Order:** [Inference] LDM/STM operations process registers in ascending numerical order regardless of the order specified in the register list. Lower-numbered registers are transferred to/from lower memory addresses.

```assembly
# These are equivalent
ldm r0, {r7, r3, r5, r1}
ldm r0, {r1, r3, r5, r7}

# Both load in order:
# r1 = [r0 + 0]
# r3 = [r0 + 4]
# r5 = [r0 + 8]
# r7 = [r0 + 12]
```

### Stack Operations Using LDM/STM

**Full Descending Stack (ARM Standard):**

```assembly
# Push (store): decrement before, then store
stmfd sp!, {r0-r3, lr}   @ push {r0-r3, lr}
stmdb sp!, {r0-r3, lr}   @ Same as stmfd

# Pop (load): load, then increment after
ldmfd sp!, {r0-r3, pc}   @ pop {r0-r3, pc}
ldmia sp!, {r0-r3, pc}   @ Same as ldmfd
```

**Other Stack Types:**

```assembly
# Full Ascending (FA)
stmfa sp!, {r0-r3}       @ Increment before
ldmfa sp!, {r0-r3}       @ Decrement after
# Aliases: stmib, ldmda

# Empty Descending (ED)
stmed sp!, {r0-r3}       @ Decrement after
ldmed sp!, {r0-r3}       @ Increment before
# Aliases: stmda, ldmib

# Empty Ascending (EA)
stmea sp!, {r0-r3}       @ Increment after
ldmea sp!, {r0-r3}       @ Decrement before
# Aliases: stmia, ldmdb
```

**Function Prologue and Epilogue:**

```assembly
# Standard function entry
function:
    push {r4-r7, lr}     @ Save callee-saved registers
    # or
    stmdb sp!, {r4-r7, lr}
    
    # Function body
    # ...
    
    # Standard function exit
    pop {r4-r7, pc}      @ Restore registers and return
    # or
    ldmia sp!, {r4-r7, pc}
```

### Structure Operations

**Loading Structure Members:**

```assembly
# struct Point3D {
#     int x;  // offset 0
#     int y;  // offset 4
#     int z;  // offset 8
# };
# r0 points to Point3D structure

load_point:
    ldm r0, {r1-r3}      @ r1=x, r2=y, r3=z
    # Process coordinates in r1, r2, r3
    bx lr

# Array of structures
# r0 = array base, r1 = index
load_point_from_array:
    add r2, r1, r1, LSL #1   @ index * 3
    add r0, r0, r2, LSL #2   @ base + (index * 12)
    ldm r0, {r1-r3}          @ Load structure
    bx lr
```

**Storing Structure Members:**

```assembly
# Store Point3D structure
# r0 = destination, r1=x, r2=y, r3=z
store_point:
    stm r0, {r1-r3}      @ Store all three coordinates
    bx lr

# Copy structure
# r0 = dest, r1 = src
copy_point:
    ldm r1, {r2-r4}      @ Load from source
    stm r0, {r2-r4}      @ Store to destination
    bx lr
```

**Large Structure Copy:**

```assembly
# Copy large structure (16 words = 64 bytes)
# r0 = dest, r1 = src
copy_large_struct:
    ldm r1!, {r2-r9}     @ Load 8 words, update src
    stm r0!, {r2-r9}     @ Store 8 words, update dest
    ldm r1!, {r2-r9}     @ Load next 8 words
    stm r0!, {r2-r9}     @ Store next 8 words
    bx lr
```

### Array Operations

**Copy Array:**

```assembly
# Copy array of 8 words
# r0 = dest, r1 = src
copy_array_8:
    ldm r1, {r2-r9}      @ Load 8 elements
    stm r0, {r2-r9}      @ Store 8 elements
    bx lr

# Copy array with loop
# r0 = dest, r1 = src, r2 = count (in words)
copy_array_loop:
    push {r4-r11}
    
copy_loop:
    cmp r2, #8
    blt copy_remainder
    
    ldmia r1!, {r3-r10}  @ Load 8 words
    stmia r0!, {r3-r10}  @ Store 8 words
    sub r2, r2, #8
    b copy_loop

copy_remainder:
    cmp r2, #0
    beq copy_done
    
    ldr r3, [r1], #4     @ Copy remaining words one at a time
    str r3, [r0], #4
    subs r2, r2, #1
    bne copy_remainder

copy_done:
    pop {r4-r11}
    bx lr
```

**Initialize Array:**

```assembly
# Fill array with value
# r0 = array, r1 = value, r2 = count (in words)
fill_array:
    # Replicate value in multiple registers
    mov r3, r1
    mov r4, r1
    mov r5, r1
    mov r6, r1
    mov r7, r1
    mov r8, r1
    mov r9, r1
    mov r10, r1

fill_loop:
    cmp r2, #8
    blt fill_remainder
    
    stmia r0!, {r3-r10}  @ Store 8 copies
    sub r2, r2, #8
    b fill_loop

fill_remainder:
    cmp r2, #0
    beq fill_done
    
    str r1, [r0], #4     @ Store remaining words
    subs r2, r2, #1
    bne fill_remainder

fill_done:
    bx lr
```

### Context Switching

**Save Processor Context:**

```assembly
# Save full context for task switching
# r0 = context save area pointer
save_context:
    # Save general purpose registers
    stmia r0!, {r1-r12}  @ Save r1-r12
    
    # Save SP and LR
    mov r1, sp
    mov r2, lr
    stmia r0!, {r1-r2}
    
    # Save CPSR
    mrs r1, cpsr
    str r1, [r0]
    
    bx lr

# Restore processor context
# r0 = context save area pointer
restore_context:
    # Restore general purpose registers
    ldmia r0!, {r1-r12}
    
    # Restore SP and LR
    ldmia r0!, {r1-r2}
    mov sp, r1
    mov lr, r2
    
    # Restore CPSR
    ldr r1, [r0]
    msr cpsr_c, r1
    
    bx lr
```

### Buffer Management

**Ring Buffer Operations:**

```assembly
# Ring buffer read multiple
# r0 = buffer base, r1 = read_ptr, r2 = count, r3 = buffer_size
# Returns updated read_ptr in r0
ring_buffer_read:
    push {r4-r7}
    
    mov r4, r0           @ Save buffer base
    mov r5, r1           @ Current read position
    mov r6, r2           @ Items to read
    mov r7, r3           @ Buffer size

read_loop:
    cmp r6, #0
    beq read_done
    
    # Calculate wrapped position
    cmp r5, r7
    subge r5, r5, r7     @ Wrap if >= size
    
    # Read from buffer
    ldr r0, [r4, r5, LSL #2]
    # Process r0...
    
    add r5, r5, #1       @ Advance read pointer
    subs r6, r6, #1
    b read_loop

read_done:
    mov r0, r5           @ Return updated pointer
    pop {r4-r7}
    bx lr
```

## Block Data Transfers

Block data transfers efficiently move large amounts of data using optimized instruction sequences.

### Memory-to-Memory Copy

**Optimized memcpy:**

```assembly
# Fast memory copy
# r0 = dest, r1 = src, r2 = size (bytes)
fast_memcpy:
    push {r4-r11, lr}
    
    # Check alignment
    orr r3, r0, r1
    tst r3, #3
    bne memcpy_unaligned
    
    # Aligned copy - 32 bytes at a time
aligned_copy:
    cmp r2, #32
    blt aligned_remainder
    
    ldmia r1!, {r3-r10}  @ Load 32 bytes
    stmia r0!, {r3-r10}  @ Store 32 bytes
    sub r2, r2, #32
    b aligned_copy

aligned_remainder:
    # Handle 4-byte chunks
    cmp r2, #4
    blt byte_copy
    
    ldr r3, [r1], #4
    str r3, [r0], #4
    sub r2, r2, #4
    b aligned_remainder

byte_copy:
    cmp r2, #0
    beq memcpy_done
    
    ldrb r3, [r1], #1
    strb r3, [r0], #1
    subs r2, r2, #1
    bne byte_copy
    b memcpy_done

memcpy_unaligned:
    # Fallback to byte copy for unaligned
    cmp r2, #0
    beq memcpy_done
    
unaligned_loop:
    ldrb r3, [r1], #1
    strb r3, [r0], #1
    subs r2, r2, #1
    bne unaligned_loop

memcpy_done:
    pop {r4-r11, pc}
```

**Reverse Copy (for overlapping regions):**

```assembly
# Copy in reverse direction
# r0 = dest, r1 = src, r2 = size (bytes)
reverse_memcpy:
    push {r4-r11}
    
    # Start at end of buffers
    add r0, r0, r2
    add r1, r1, r2
    
    # 32-byte blocks
reverse_loop:
    cmp r2, #32
    blt reverse_remainder
    
    sub r1, r1, #32
    ldmia r1, {r3-r10}   @ Load from end
    sub r0, r0, #32
    stmia r0, {r3-r10}   @ Store to end
    sub r2, r2, #32
    b reverse_loop

reverse_remainder:
    cmp r2, #0
    beq reverse_done
    
    ldrb r3, [r1, #-1]!
    strb r3, [r0, #-1]!
    subs r2, r2, #1
    bne reverse_remainder

reverse_done:
    pop {r4-r11}
    bx lr
```

### Zero Fill (memset to 0)

**Fast zero initialization:**

```assembly
# Fast zero fill
# r0 = dest, r1 = size (bytes)
fast_memzero:
    push {r4-r9}
    
    mov r2, #0
    mov r3, #0
    mov r4, #0
    mov r5, #0
    mov r6, #0
    mov r7, #0
    mov r8, #0
    mov r9, #0

zero_loop:
    cmp r1, #32
    blt zero_remainder
    
    stmia r0!, {r2-r9}   @ Store 32 zero bytes
    sub r1, r1, #32
    b zero_loop

zero_remainder:
    cmp r1, #4
    blt zero_bytes
    
    str r2, [r0], #4
    sub r1, r1, #4
    b zero_remainder

zero_bytes:
    cmp r1, #0
    beq zero_done
    
    strb r2, [r0], #1
    subs r1, r1, #1
    bne zero_bytes

zero_done:
    pop {r4-r9}
    bx lr
```

### Pattern Fill

**Fill with repeating pattern:**

```assembly
# Fill buffer with 32-bit pattern
# r0 = dest, r1 = pattern, r2 = size (bytes)
pattern_fill:
    push {r4-r9}
    
    # Replicate pattern
    mov r3, r1
    mov r4, r1
    mov r5, r1
    mov r6, r1
    mov r7, r1
    mov r8, r1
    mov r9, r1

pattern_loop:
    cmp r2, #32
    blt pattern_remainder
    
    stmia r0!, {r1, r3-r9}  @ Store 32 bytes
    sub r2, r2, #32
    b pattern_loop

pattern_remainder:
    cmp r2, #4
    blt pattern_done
    
    str r1, [r0], #4
    sub r2, r2, #4
    b pattern_remainder

pattern_done:
    pop {r4-r9}
    bx lr
```

### DMA-Style Transfer

**Simulating DMA transfer with busy-wait:**

```assembly
# Transfer data in background-style operation
# r0 = dest, r1 = src, r2 = size, r3 = chunk_size
dma_style_transfer:
    push {r4-r11, lr}
    
transfer_chunks:
    cmp r2, #0
    ble transfer_complete
    
    # Determine chunk size for this iteration
    mov r4, r3
    cmp r2, r3
    movlt r4, r2         @ Use remaining size if less than chunk
    
    # Transfer one chunk
    mov r5, r4
    lsr r5, r5, #5       @ Number of 32-byte blocks
    
chunk_loop:
    cmp r5, #0
    beq chunk_remainder
    
    ldmia r1!, {r6-r13}
    stmia r0!, {r6-r13}
    
    subs r5, r5, #1
    bne chunk_loop

chunk_remainder:
    and r5, r4, #31      @ Remaining bytes
    
remainder_loop:
    cmp r5, #0
    beq chunk_done
    
    ldrb r6, [r1], #1
    strb r6, [r0], #1
    subs r5, r5, #1
    bne remainder_loop

chunk_done:
    sub r2, r2, r4       @ Update remaining
    
    # Simulate yielding to other tasks
    push {r0-r3}
    bl check_interrupts  @ Allow interrupt processing
    pop {r0-r3}
    
    b transfer_chunks

transfer_complete:
    pop {r4-r11, pc}
```

### Scatter-Gather Operations

**Gather: collect data from multiple locations:**

```assembly
# Gather operation: collect elements at specified indices
# r0 = dest, r1 = src array, r2 = index array, r3 = count
gather_operation:
    push {r4-r6}
    
    mov r4, #0           @ Index counter

gather_loop:
    cmp r4, r3
    bge gather_done
    
    ldr r5, [r2, r4, LSL #2]  @ Load index
    ldr r6, [r1, r5, LSL #2]  @ Load src[index]
    str r6, [r0, r4, LSL #2]  @ Store to dest[i]
    
    add r4, r4, #1
    b gather_loop

gather_done:
    pop {r4-r6}
    bx lr
```

**Scatter: distribute data to multiple locations:**

```assembly
# Scatter operation: distribute elements to specified indices
# r0 = dest array, r1 = src, r2 = index array, r3 = count
scatter_operation:
    push {r4-r6}
    
    mov r4, #0

scatter_loop:
    cmp r4, r3
    bge scatter_done
    
    ldr r5, [r2, r4, LSL #2]  @ Load index
    ldr r6, [r1, r4, LSL #2]  @ Load src[i]
    str r6, [r0, r5, LSL #2]  @ Store to dest[index]
    
    add r4, r4, #1
    b scatter_loop

scatter_done:
    pop {r4-r6}
    bx lr
```

## Byte and Halfword Operations

ARM provides specific instructions for efficient manipulation of sub-word data types.

### Byte Operations

**Basic Byte Load/Store:**

```assembly
# Load byte (zero-extended)
ldrb r0, [r1]        @ r0 = 8-bit value from [r1], zero-extended to 32-bit

# Load byte (sign-extended)
ldrsb r0, [r1]       @ r0 = 8-bit value from [r1], sign-extended to 32-bit

# Store byte
strb r0, [r1]        @ Store lower 8 bits of r0 to [r1]
```

**Example: Character Handling:**

```assembly
# Check if character is uppercase
# r0 = character, returns 1 if uppercase, 0 otherwise
is_uppercase:
    sub r1, r0, #'A'
    cmp r1, #25
    movls r0, #1
    movhi r0, #0
    bx lr

# Convert to uppercase
# r0 = character
to_uppercase:
    sub r1, r0, #'a'
    cmp r1, #25
    subls r0, r0, #32    @ Convert if lowercase
    bx lr

# Convert to lowercase
# r0 = character
to_lowercase:
    sub r1, r0, #'A'
    cmp r1, #25
    addls r0, r0, #32    @ Convert if uppercase
    bx lr
```

**String Length:**

```assembly
# strlen implementation
# r0 = string pointer, returns length in r0
strlen:
    mov r1, r0           @ Save start
    
strlen_loop:
    ldrb r2, [r0], #1    @ Load byte, advance
    cmp r2, #0           @ Check for null
    bne strlen_loop
    
    sub r0, r0, r1       @ Calculate length
    sub r0, r0, #1       @ Adjust for extra increment
    bx lr
```

**String Copy:**

```assembly
# strcpy implementation
# r0 = dest, r1 = src
strcpy:
    push {r4}
    mov r4, r0           @ Save dest for return

strcpy_loop:
    ldrb r2, [r1], #1    @ Load from src
    strb r2, [r0], #1    @ Store to dest
    cmp r2, #0           @ Check for null
    bne strcpy_loop
    
    mov r0, r4           @ Return original dest
    pop {r4}
    bx lr
```

**String Compare:**

```assembly
# strcmp implementation
# r0 = s1, r1 = s2, returns <0, 0, >0
strcmp:
strcmp_loop:
    ldrb r2, [r0], #1    @ Load from s1
    ldrb r3, [r1], #1    @ Load from s2
    cmp r2, r3           @ Compare bytes
    bne strcmp_diff
    cmp r2, #0           @ Check for end
    bne strcmp_loop
    
    mov r0, #0           @ Equal
    bx lr

strcmp_diff:
    sub r0, r2, r3       @ Return difference
    bx lr
```

**Byte Array Operations:**

```assembly
# Find byte in array
# r0 = array, r1 = byte to find, r2 = size
# Returns index in r0, or -1 if not found
find_byte:
    mov r3, #0           @ Index

find_loop:
    cmp r3, r2
    bge not_found
    
    ldrb r4, [r0, r3]    @ Load byte
    cmp r4, r1           @ Compare
    beq found
    
    add r3, r3, #1
    b find_loop

found:
    mov r0, r3           @ Return index
    bx lr

not_found:
    mov r0, #-1
    bx lr
```

### Halfword Operations

**Basic Halfword Load/Store:**

```assembly
# Load halfword (zero-extended)
ldrh r0, [r1]        @ r0 = 16-bit value from [r1], zero-extended

# Load halfword (sign-extended)
ldrsh r0, [r1]       @ r0 = 16-bit value from [r1], sign-extended

# Store halfword
strh r0, [r1]        @ Store lower 16 bits of r0 to [r1]
```

**Alignment Note:** Halfword accesses must be 2-byte aligned. Accessing unaligned halfwords may fault or produce unpredictable results depending on the processor.

```assembly
# Safe halfword access with alignment check
load_halfword_safe:
    tst r0, #1           @ Check if address is odd
    movne r0, #-1        @ Return error if unaligned
    bxne lr
    
    ldrh r0, [r0]        @ Load halfword
    bx lr
```

**Audio Sample Processing:**

```assembly
# Process 16-bit audio samples
# r0 = sample array, r1 = count, r2 = gain (fixed-point)
process_audio_samples:
    push {r4-r6}
    
    mov r4, #0           @ Index

sample_loop:
    cmp r4, r1
    bge sample_done
    
    ldrsh r5, [r0, r4, LSL #1]  @ Load signed 16-bit sample
    
    # Apply gain (fixed-point multiplication)
    smull r5, r6, r5, r2     @ 32x32 -> 64-bit result
    asr r5, r5, #16          @ Shift to get 16-bit result
    
    # Clamp to 16-bit range
    mov r6, #32767
    cmp r5, r6
    movgt r5, r6
    mvn r6, #32767           @ -32768
    cmp r5, r6
    movlt r5, r6
    
    strh r5, [r0, r4, LSL #1]  @ Store processed sample
    
    add r4, r4, #1
    b sample_loop

sample_done:
    pop {r4-r6}
    bx lr
```

**UTF-16 String Operations:**

```assembly
# UTF-16 string length
# r0 = string pointer (16-bit characters)
strlen_utf16:
    mov r1, r0
    
strlen_utf16_loop:
    ldrh r2, [r0], #2    @ Load 16-bit character
    cmp r2, #0
    bne strlen_utf16_loop
    
    sub r0, r0, r1       @ Calculate byte length
    lsr r0, r0, #1       @ Convert to character count
    sub r0, r0, #1       @ Adjust for null terminator
    bx lr
```

**Pixel Operations (RGB565):**

```assembly
# Extract RGB components from RGB565 format
# r0 = RGB565 value
# Returns: r0=R, r1=G, r2=B (8-bit values)
rgb565_to_rgb888:
    # Extract red (bits 15-11)
    lsr r1, r0, #11
    and r1, r1, #0x1F
    lsl r1, r1, #3       @ Scale to 8-bit
    
    # Extract green (bits 10-5)
    lsr r2, r0, #5
    and r2, r2, #0x3F
    lsl r2, r2, #2       @ Scale to 8-bit
    
    # Extract blue (bits 4-0)
    and r3, r0, #0x1F
    lsl r3, r3, #3       @ Scale to 8-bit
    
    # Return values
    mov r0, r1           @ R
    mov r1, r2           @ G
    mov r2, r3           @ B
    bx lr

# Pack RGB888 to RGB565
# r0=R, r1=G, r2=B (8-bit values)
# Returns RGB565 in r0
rgb888_to_rgb565:
    # Red: take top 5 bits, shift to position
    lsr r0, r0, #3
    lsl r0, r0, #11
    
    # Green: take top 6 bits, shift to position
    lsr r1, r1, #2
    lsl r1, r1, #5
    orr r0, r0, r1
    
    # Blue: take top 5 bits
    lsr r2, r2, #3
    orr r0, r0, r2
    
    bx lr
```

### Mixed-Size Operations

**Pack bytes into word:**

```assembly
# Pack 4 bytes into a word
# r0-r3 = byte values, returns packed word in r0
pack_bytes:
    and r0, r0, #0xFF    @ Ensure single byte
    and r1, r1, #0xFF
    and r2, r2, #0xFF
    and r3, r3, #0xFF
    
    orr r0, r0, r1, LSL #8
    orr r0, r0, r2, LSL #16
    orr r0, r0, r3, LSL #24
    
    bx lr
```

**Unpack word into bytes:**

```assembly
# Unpack word into 4 bytes
# r0 = packed word
# Returns: r0=byte0, r1=byte1, r2=byte2, r3=byte3
unpack_bytes:
    mov r4, r0           @ Save original
    
    and r0, r4, #0xFF             @ Byte 0
    lsr r1, r4, #8
    and r1, r1, #0xFF             @ Byte 1
    lsr r2, r4, #16
    and r2, r2, #0xFF             @ Byte 2
    lsr r3, r4, #24               @ Byte 3
    
    bx lr
```


**Pack halfwords into word:**

```assembly
# Pack 2 halfwords into a word
# r0 = low halfword, r1 = high halfword
# Returns packed word in r0
pack_halfwords:
    and r0, r0, #0xFFFF  @ Ensure lower 16 bits
    bfi r0, r1, #16, #16 @ Insert r1 into upper 16 bits
    # Alternative without BFI:
    # and r0, r0, #0xFFFF
    # orr r0, r0, r1, LSL #16
    bx lr
````

**Unpack word into halfwords:**

```assembly
# Unpack word into 2 halfwords
# r0 = packed word
# Returns: r0=low halfword, r1=high halfword
unpack_halfwords:
    mov r1, r0, LSR #16  @ High halfword
    and r0, r0, #0xFFFF  @ Low halfword
    # Alternative using UXTH:
    # uxth r0, r0          @ Zero-extend low halfword
    # lsr r1, r0, #16      @ Shift high halfword
    bx lr
```

## Sign Extension and Zero Extension

Extension operations convert smaller data types to larger ones while preserving value representation.

### Zero Extension

Zero extension fills the upper bits with zeros, used for unsigned values.

**Manual Zero Extension:**

```assembly
# Zero-extend byte to word
mov r0, #0xFF        @ Byte value
and r0, r0, #0xFF    @ Zero-extend (clear upper 24 bits)

# Zero-extend halfword to word
ldr r0, =0x1234      @ Halfword value
and r0, r0, #0xFFFF  @ Zero-extend (clear upper 16 bits)

# Using bit clear
mov r0, #0xFF
bic r0, r0, #0xFFFFFF00  @ Clear upper 24 bits
```

**UXTB - Unsigned Extend Byte:**

```assembly
# Zero-extend byte (bits 7:0) to word
uxtb r0, r1          @ r0 = zero-extend(r1[7:0])

# With rotation
uxtb r0, r1, ROR #8  @ Extract and extend byte 1
uxtb r0, r1, ROR #16 @ Extract and extend byte 2
uxtb r0, r1, ROR #24 @ Extract and extend byte 3

# Example: Extract individual bytes
mov r1, #0x12345678
uxtb r0, r1          @ r0 = 0x00000078
uxtb r0, r1, ROR #8  @ r0 = 0x00000056
uxtb r0, r1, ROR #16 @ r0 = 0x00000034
uxtb r0, r1, ROR #24 @ r0 = 0x00000012
```

**UXTH - Unsigned Extend Halfword:**

```assembly
# Zero-extend halfword (bits 15:0) to word
uxth r0, r1          @ r0 = zero-extend(r1[15:0])

# With rotation
uxth r0, r1, ROR #16 @ Extract and extend upper halfword

# Example
mov r1, #0x12345678
uxth r0, r1          @ r0 = 0x00005678
uxth r0, r1, ROR #16 @ r0 = 0x00001234
```

**LDRB/LDRH with Zero Extension:**

```assembly
# Load instructions automatically zero-extend
ldrb r0, [r1]        @ Load byte, zero-extend to 32 bits
ldrh r0, [r1]        @ Load halfword, zero-extend to 32 bits

# Example
# Memory at 0x1000: 0xFF
ldr r0, =0x1000
ldrb r1, [r0]        @ r1 = 0x000000FF (zero-extended)
```

### Sign Extension

Sign extension replicates the sign bit to preserve the numeric value of signed integers.

**Manual Sign Extension:**

```assembly
# Sign-extend byte to word (manual)
mov r0, #0x80        @ Negative byte value (-128)
lsl r0, r0, #24      @ Shift to MSB position
asr r0, r0, #24      @ Arithmetic shift right (sign extends)
# r0 = 0xFFFFFF80

# Sign-extend halfword to word (manual)
mov r0, #0x8000      @ Negative halfword value
lsl r0, r0, #16      @ Shift to MSB position
asr r0, r0, #16      @ Arithmetic shift right
# r0 = 0xFFFF8000
```

**SXTB - Signed Extend Byte:**

```assembly
# Sign-extend byte (bits 7:0) to word
sxtb r0, r1          @ r0 = sign-extend(r1[7:0])

# With rotation
sxtb r0, r1, ROR #8  @ Extract and sign-extend byte 1
sxtb r0, r1, ROR #16 @ Extract and sign-extend byte 2
sxtb r0, r1, ROR #24 @ Extract and sign-extend byte 3

# Example: Handle signed bytes
mov r1, #0x12345680  @ Contains negative byte in position 0
sxtb r0, r1          @ r0 = 0xFFFFFF80 (-128)
sxtb r0, r1, ROR #8  @ r0 = 0x00000056 (positive)
```

**SXTH - Signed Extend Halfword:**

```assembly
# Sign-extend halfword (bits 15:0) to word
sxth r0, r1          @ r0 = sign-extend(r1[15:0])

# With rotation
sxth r0, r1, ROR #16 @ Extract and sign-extend upper halfword

# Example
mov r1, #0x12348000  @ Contains negative halfword
sxth r0, r1          @ r0 = 0xFFFF8000 (-32768)
sxth r0, r1, ROR #16 @ r0 = 0x00001234 (positive)
```

**LDRSB/LDRSH with Sign Extension:**

```assembly
# Load instructions with automatic sign extension
ldrsb r0, [r1]       @ Load byte, sign-extend to 32 bits
ldrsh r0, [r1]       @ Load halfword, sign-extend to 32 bits

# Example
# Memory at 0x1000: 0xFF (represents -1 as signed byte)
ldr r0, =0x1000
ldrsb r1, [r0]       @ r1 = 0xFFFFFFFF (sign-extended)
ldrb r2, [r0]        @ r2 = 0x000000FF (zero-extended)
```

### Extension Examples

**Temperature Sensor Reading (8-bit signed):**

```assembly
# Read signed temperature value
# r0 = sensor register address
# Returns temperature in r0 (sign-extended)
read_temperature:
    ldrb r1, [r0]        @ Read raw byte
    sxtb r0, r1          @ Sign-extend to 32-bit
    # Now r0 contains proper signed value
    # 0x80 (128 as unsigned) becomes 0xFFFFFF80 (-128 as signed)
    bx lr
```

**Audio Sample Conversion:**

```assembly
# Convert 16-bit audio sample to 32-bit
# r0 = sample array (16-bit), r1 = count
# r2 = output array (32-bit)
convert_audio_16_to_32:
    push {r4-r5}
    mov r4, #0

convert_loop:
    cmp r4, r1
    bge convert_done
    
    ldrsh r5, [r0, r4, LSL #1]  @ Load and sign-extend
    str r5, [r2, r4, LSL #2]    @ Store as 32-bit
    
    add r4, r4, #1
    b convert_loop

convert_done:
    pop {r4-r5}
    bx lr
```

**Character to Integer Conversion:**

```assembly
# Convert ASCII digit to integer
# r0 = character ('0'-'9')
# Returns integer value (0-9) or -1 if invalid
char_to_digit:
    sub r1, r0, #'0'     @ Subtract '0'
    cmp r1, #9           @ Check if in range 0-9
    movls r0, r1         @ Valid digit
    movhi r0, #-1        @ Invalid
    bx lr

# Convert integer to ASCII digit
# r0 = value (0-9)
# Returns character or -1 if invalid
digit_to_char:
    cmp r0, #9
    movls r0, r0, ADD #'0'  @ Add '0'
    movhi r0, #-1        @ Invalid
    bx lr
```

### Bit Field Extraction and Insertion

**UBFX - Unsigned Bit Field Extract:**

```assembly
# Extract unsigned bit field
# ubfx rd, rn, #lsb, #width
ubfx r0, r1, #8, #8  @ Extract bits [15:8] into r0[7:0]

# Example: Extract RGB components from 32-bit color
# Format: 0xAARRGGBB
extract_color_components:
    # r0 = color value
    ubfx r1, r0, #16, #8 @ Extract red (bits 23:16)
    ubfx r2, r0, #8, #8  @ Extract green (bits 15:8)
    ubfx r3, r0, #0, #8  @ Extract blue (bits 7:0)
    ubfx r4, r0, #24, #8 @ Extract alpha (bits 31:24)
    bx lr
```

**SBFX - Signed Bit Field Extract:**

```assembly
# Extract signed bit field (with sign extension)
# sbfx rd, rn, #lsb, #width
sbfx r0, r1, #8, #8  @ Extract bits [15:8], sign-extend

# Example: Extract signed temperature from packed data
# Bits [15:8] contain signed 8-bit temperature
extract_temperature:
    sbfx r0, r0, #8, #8  @ Extract and sign-extend
    bx lr
```

**BFI - Bit Field Insert:**

```assembly
# Insert bit field
# bfi rd, rn, #lsb, #width
bfi r0, r1, #8, #8   @ Insert r1[7:0] into r0[15:8]

# Example: Pack RGB components into 32-bit color
pack_color_components:
    # r0=red, r1=green, r2=blue, r3=alpha
    mov r4, #0           @ Start with zero
    bfi r4, r3, #24, #8  @ Insert alpha at bits 31:24
    bfi r4, r0, #16, #8  @ Insert red at bits 23:16
    bfi r4, r1, #8, #8   @ Insert green at bits 15:8
    bfi r4, r2, #0, #8   @ Insert blue at bits 7:0
    mov r0, r4           @ Return packed color
    bx lr
```

**BFC - Bit Field Clear:**

```assembly
# Clear bit field
# bfc rd, #lsb, #width
bfc r0, #8, #8       @ Clear bits [15:8]

# Example: Clear specific flags
clear_status_flags:
    bfc r0, #0, #4       @ Clear lower 4 bits
    bx lr
```

### Practical Extension Applications

**Network Byte Order Conversion:**

```assembly
# Convert 16-bit value between host and network byte order (big-endian)
# r0 = value
htons:
    rev16 r0, r0         @ Reverse bytes in each halfword
    uxth r0, r0          @ Zero-extend to 32 bits
    bx lr

# Convert 32-bit value
htonl:
    rev r0, r0           @ Reverse all bytes
    bx lr
```

**Checksum Calculation with Extension:**

```assembly
# Calculate 16-bit checksum (Internet checksum)
# r0 = data pointer, r1 = length (bytes)
# Returns checksum in r0
calculate_checksum:
    push {r4-r5}
    
    mov r2, #0           @ Accumulator
    mov r4, r1           @ Save length

checksum_loop:
    cmp r1, #1
    ble checksum_last_byte
    
    ldrh r3, [r0], #2    @ Load halfword (zero-extended)
    add r2, r2, r3       @ Add to sum
    
    # Handle carry
    mov r5, r2, LSR #16  @ Extract carry
    and r2, r2, #0xFFFF  @ Keep lower 16 bits
    add r2, r2, r5       @ Add carry back
    
    sub r1, r1, #2
    b checksum_loop

checksum_last_byte:
    cmp r1, #0
    beq checksum_done
    
    ldrb r3, [r0]        @ Load last byte (zero-extended)
    add r2, r2, r3, LSL #8  @ Treat as high byte
    
checksum_done:
    # Final carry handling
    mov r5, r2, LSR #16
    and r2, r2, #0xFFFF
    add r2, r2, r5
    
    mvn r0, r2           @ One's complement
    uxth r0, r0          @ Keep lower 16 bits
    
    pop {r4-r5}
    bx lr
```

**Fixed-Point Arithmetic with Sign Extension:**

```assembly
# Multiply 16-bit fixed-point numbers (Q15 format)
# r0, r1 = Q15 values (16-bit signed)
# Returns Q15 result in r0
q15_multiply:
    sxth r0, r0          @ Sign-extend to 32-bit
    sxth r1, r1          @ Sign-extend to 32-bit
    
    smull r2, r3, r0, r1 @ 32x32 -> 64-bit result
    
    # Extract Q15 result (bits 30:15 of 64-bit product)
    lsr r2, r2, #15
    orr r0, r2, r3, LSL #17
    
    # Saturate to 16-bit
    mov r2, #0x7FFF
    cmp r0, r2
    movgt r0, r2         @ Clamp to max
    mvn r2, r2           @ -0x8000
    cmp r0, r2
    movlt r0, r2         @ Clamp to min
    
    sxth r0, r0          @ Final sign-extend
    bx lr
```

**Structure Packing with Bit Fields:**

```assembly
# Pack structure with bit fields
# struct Flags {
#     unsigned int a : 4;  // bits 0-3
#     signed int b : 8;    // bits 4-11 (signed)
#     unsigned int c : 12; // bits 12-23
# };

pack_flags_struct:
    # r0=a, r1=b, r2=c
    mov r3, #0           @ Result
    
    # Insert a (4 bits, unsigned)
    and r0, r0, #0xF
    bfi r3, r0, #0, #4
    
    # Insert b (8 bits, signed)
    sxtb r1, r1          @ Sign-extend
    and r1, r1, #0xFF    @ Mask to 8 bits
    bfi r3, r1, #4, #8
    
    # Insert c (12 bits, unsigned)
    and r2, r2, #0xFFF
    bfi r3, r2, #12, #12
    
    mov r0, r3
    bx lr

unpack_flags_struct:
    # r0 = packed value
    ubfx r1, r0, #0, #4  @ Extract a (unsigned)
    sbfx r2, r0, #4, #8  @ Extract b (signed, extended)
    ubfx r3, r0, #12, #12 @ Extract c (unsigned)
    
    # Return: r0=a, r1=b, r2=c
    mov r0, r1
    mov r1, r2
    mov r2, r3
    bx lr
```

**Saturation Operations:**

```assembly
# Saturate signed value to 8-bit range
# r0 = value, returns saturated value
saturate_s8:
    cmp r0, #127
    movgt r0, #127       @ Clamp to max
    cmn r0, #128
    movlt r0, #-128      @ Clamp to min
    sxtb r0, r0          @ Sign-extend result
    bx lr

# Saturate unsigned value to 8-bit range
saturate_u8:
    cmp r0, #0
    movlt r0, #0         @ Clamp to min
    cmp r0, #255
    movgt r0, #255       @ Clamp to max
    uxtb r0, r0          @ Zero-extend result
    bx lr

# Using SSAT/USAT instructions (if available)
saturate_ssat:
    ssat r0, #8, r0      @ Saturate to signed 8-bit (-128 to 127)
    bx lr

saturate_usat:
    usat r0, #8, r0      @ Saturate to unsigned 8-bit (0 to 255)
    bx lr
```

**Pixel Format Conversion:**

```assembly
# Convert RGBA8888 to RGB565
# r0 = RGBA8888 value (0xRRGGBBAA)
rgba8888_to_rgb565:
    # Extract and scale red (take bits 7:3)
    ubfx r1, r0, #27, #5 @ Extract R[7:3]
    lsl r1, r1, #11      @ Shift to position
    
    # Extract and scale green (take bits 7:2)
    ubfx r2, r0, #18, #6 @ Extract G[7:2]
    lsl r2, r2, #5       @ Shift to position
    orr r1, r1, r2
    
    # Extract and scale blue (take bits 7:3)
    ubfx r2, r0, #11, #5 @ Extract B[7:3]
    orr r0, r1, r2       @ Combine all components
    
    bx lr

# Convert RGB565 to RGBA8888 (with full alpha)
rgb565_to_rgba8888:
    # Extract red and scale to 8-bit
    ubfx r1, r0, #11, #5
    lsl r1, r1, #3       @ Scale: 5-bit -> 8-bit
    orr r1, r1, r1, LSR #5  @ Replicate bits for better accuracy
    lsl r1, r1, #24      @ Position in result
    
    # Extract green and scale to 8-bit
    ubfx r2, r0, #5, #6
    lsl r2, r2, #2       @ Scale: 6-bit -> 8-bit
    orr r2, r2, r2, LSR #6  @ Replicate bits
    lsl r2, r2, #16      @ Position in result
    orr r1, r1, r2
    
    # Extract blue and scale to 8-bit
    ubfx r2, r0, #0, #5
    lsl r2, r2, #3       @ Scale: 5-bit -> 8-bit
    orr r2, r2, r2, LSR #5  @ Replicate bits
    lsl r2, r2, #8       @ Position in result
    orr r1, r1, r2
    
    # Add full alpha
    orr r0, r1, #0xFF    @ Alpha = 255
    
    bx lr
```

**Key Points:**

- LDM/STM instructions transfer multiple registers in a single operation, reducing instruction count and improving performance for bulk data operations
- Register lists are processed in ascending numerical order regardless of specification order, with lower registers accessing lower memory addresses
- Stack operations use specific LDM/STM variants: STMDB/LDMIA for full descending stacks (ARM standard)
- Block transfers enable efficient memory copying, structure operations, and context switching with up to 32 bytes per instruction
- Byte operations (LDRB/STRB) and halfword operations (LDRH/STRH) provide direct access to sub-word data types
- Zero extension (UXTB/UXTH) preserves unsigned values by filling upper bits with zeros; sign extension (SXTB/SXTH) preserves signed values by replicating the sign bit
- Bit field operations (UBFX/SBFX/BFI/BFC) enable efficient extraction and insertion of arbitrary bit ranges without masking and shifting sequences
- Proper extension is critical for correct arithmetic on mixed-size signed/unsigned values and prevents incorrect value interpretation

---

## Bit Field Manipulation

Bit field manipulation involves extracting, inserting, and modifying contiguous groups of bits within registers. ARM architectures provide specialized instructions and techniques for efficient bit field operations, essential for hardware register access, data packing, and protocol handling.

### Bit Field Extract Instructions

ARMv6T2 and later architectures include dedicated bit field extract instructions that isolate specific bit ranges from registers in a single operation.

**UBFX - Unsigned Bit Field Extract**

UBFX extracts a bit field from a source register, zero-extends it, and stores the result in a destination register. The syntax `UBFX rd, rn, #lsb, #width` extracts `width` bits starting at bit position `lsb` from register `rn`, placing the zero-extended result in `rd`.

The extracted bits shift to the least significant position in the destination register. Upper bits are filled with zeros. This operation efficiently isolates bit fields without requiring separate mask and shift operations.

**Example:**

```assembly
@ Extract bits 8-15 from r0 (8 bits starting at position 8)
UBFX r1, r0, #8, #8         @ r1 = (r0 >> 8) & 0xFF

@ Extract a 4-bit field at position 12
MOV r0, #0x12345678
UBFX r1, r0, #12, #4        @ r1 = 0x4 (extracts bits 12-15)

@ Extracting RGB components from a 16-bit color (RGB565)
@ Format: RRRRRGGGGGGBBBBB
MOV r0, #0xF800             @ Sample color value
UBFX r1, r0, #11, #5        @ r1 = red (5 bits at position 11)
UBFX r2, r0, #5, #6         @ r2 = green (6 bits at position 5)
UBFX r3, r0, #0, #5         @ r3 = blue (5 bits at position 0)
```

**SBFX - Signed Bit Field Extract**

SBFX operates identically to UBFX but sign-extends the extracted bit field instead of zero-extending. If the most significant bit of the extracted field is 1, upper bits are filled with 1s; if 0, upper bits are filled with 0s. This preserves the sign of two's complement values stored in bit fields.

**Example:**

```assembly
@ Extract signed 8-bit value from bits 8-15
MOV r0, #0x0000FF00         @ Bit field = 0xFF (-1 if signed)
SBFX r1, r0, #8, #8         @ r1 = 0xFFFFFFFF (sign-extended -1)

@ Extract signed 4-bit value
MOV r0, #0x00007000         @ Bit field = 0x7 (positive)
SBFX r1, r0, #12, #4        @ r1 = 0x00000007 (sign-extended +7)

MOV r0, #0x00008000         @ Bit field = 0x8 (-8 in 4-bit two's complement)
SBFX r1, r0, #12, #4        @ r1 = 0xFFFFFFF8 (sign-extended -8)
```

### Bit Field Insert Instructions

Bit field insertion modifies specific bit ranges within a register while preserving surrounding bits.

**BFI - Bit Field Insert**

BFI copies the least significant bits from a source register into a specified bit field of a destination register, leaving other destination bits unchanged. The syntax `BFI rd, rn, #lsb, #width` takes the lower `width` bits from `rn` and inserts them into `rd` starting at position `lsb`.

This instruction enables efficient bit field updates without reading, masking, shifting, and writing back separately. It's particularly useful for modifying hardware control registers or packed data structures.

**Example:**

```assembly
@ Insert 4 bits into position 8-11
MOV r0, #0x12345678         @ Destination value
MOV r1, #0x0000000A         @ Source value (lower 4 bits = 0xA)
BFI r0, r1, #8, #4          @ r0 = 0x12345A78 (bits 8-11 replaced)

@ Update status flags in a packed structure
MOV r0, #0x00000000         @ Empty structure
MOV r1, #3                  @ Status code
BFI r0, r1, #16, #3         @ Insert 3-bit status at position 16
MOV r1, #7                  @ Priority level
BFI r0, r1, #20, #3         @ Insert 3-bit priority at position 20
@ r0 now contains both fields without separate masking

@ Construct RGB565 color value from components
MOV r0, #0                  @ Start with zero
MOV r1, #31                 @ Red component (5 bits, max value)
BFI r0, r1, #11, #5         @ Insert red at bits 11-15
MOV r1, #63                 @ Green component (6 bits, max value)
BFI r0, r1, #5, #6          @ Insert green at bits 5-10
MOV r1, #31                 @ Blue component (5 bits, max value)
BFI r0, r1, #0, #5          @ Insert blue at bits 0-4
@ r0 = 0xFFFF (white in RGB565)
```

**BFC - Bit Field Clear**

BFC clears (sets to zero) a specified bit field within a register. The syntax `BFC rd, #lsb, #width` zeros `width` bits starting at position `lsb` in register `rd`, preserving all other bits.

**Example:**

```assembly
@ Clear bits 8-15
MOV r0, #0xFFFFFFFF         @ All bits set
BFC r0, #8, #8              @ r0 = 0xFFFF00FF (bits 8-15 cleared)

@ Clear status field in packed structure
MOV r0, #0x12345678
BFC r0, #16, #8             @ Clear 8-bit status field at position 16
                            @ r0 = 0x12005678
```

### Traditional Bit Manipulation Techniques

On architectures without dedicated bit field instructions (pre-ARMv6T2), bit manipulation uses combinations of logical operations and shifts.

**Extracting Bit Fields**

Extraction requires shifting the desired field to the least significant position, then masking to isolate it. For unsigned extraction, shift right logically (LSR) then AND with a mask. For signed extraction, shift right arithmetically (ASR) to sign-extend.

**Example:**

```assembly
@ Extract bits 8-15 without UBFX (unsigned)
MOV r0, #0x12345678
LSR r1, r0, #8              @ Shift field to LSB: r1 = 0x00123456
AND r1, r1, #0xFF           @ Mask to isolate: r1 = 0x00000056

@ Extract signed field (bits 8-15) without SBFX
MOV r0, #0x0000FF00         @ Field value is 0xFF
LSL r1, r0, #16             @ Shift field to MSB: r1 = 0xFF000000
ASR r1, r1, #24             @ Arithmetic shift back: r1 = 0xFFFFFFFF (sign-extended)
```

**Inserting Bit Fields**

Insertion requires clearing the target bit field in the destination, preparing the source value by shifting it to the correct position, then combining with OR.

**Example:**

```assembly
@ Insert 4 bits at position 8 without BFI
MOV r0, #0x12345678         @ Destination
BIC r0, r0, #0x0F00         @ Clear bits 8-11: r0 = 0x12345078
MOV r1, #0x0000000A         @ Source value
LSL r1, r1, #8              @ Shift to position: r1 = 0x00000A00
ORR r0, r0, r1              @ Combine: r0 = 0x12345A78
```

### Bit Manipulation Patterns

**Setting Bits**

Individual or multiple bits are set using ORR with appropriate masks. Each set bit in the mask sets the corresponding bit in the register.

**Example:**

```assembly
ORR r0, r0, #0x80           @ Set bit 7
ORR r0, r0, #0x0F           @ Set bits 0-3
```

**Clearing Bits**

BIC (bit clear) clears bits indicated by the mask. Each set bit in the mask clears the corresponding bit in the register.

**Example:**

```assembly
BIC r0, r0, #0x10           @ Clear bit 4
BIC r0, r0, #0xF0           @ Clear bits 4-7
```

**Toggling Bits**

EOR (exclusive OR) toggles bits indicated by the mask. Bits corresponding to set mask bits flip between 0 and 1.

**Example:**

```assembly
EOR r0, r0, #0x20           @ Toggle bit 5
EOR r0, r0, #0xFF           @ Toggle bits 0-7
```

**Testing Bits**

TST performs bitwise AND without storing the result, only updating flags. The Z flag indicates whether specified bits are clear (Z=1) or any are set (Z=0).

**Example:**

```assembly
TST r0, #0x01               @ Test bit 0
BEQ bit_clear               @ Branch if bit 0 is clear (Z=1)
BNE bit_set                 @ Branch if bit 0 is set (Z=0)

TST r0, #0x0F               @ Test if any of bits 0-3 are set
```

**Counting Set Bits**

[Inference: ARMv5T and later include the CLZ (count leading zeros) instruction that counts consecutive zeros from bit 31 downward, useful for bit scanning algorithms. Population count (counting total set bits) may require software implementation on architectures without dedicated instructions].

**Example:**

```assembly
@ Count leading zeros
MOV r0, #0x00100000         @ Bit 20 is highest set bit
CLZ r1, r0                  @ r1 = 11 (bits 31-21 are zero)

@ Find highest set bit position
MOV r0, #0x00100000
CLZ r1, r0                  @ r1 = 11 leading zeros
RSB r1, r1, #31             @ r1 = 20 (bit position of highest set bit)
```

## Saturation Arithmetic

Saturation arithmetic clamps results to representable ranges instead of wrapping around on overflow. When an operation would exceed maximum or minimum representable values, saturation sets the result to the boundary value. This behavior is crucial for signal processing, multimedia applications, and control systems where wrapping could cause instability.

### Saturation Concepts

**Wrapping vs Saturation**

Standard arithmetic wraps on overflow: adding 1 to 0x7FFFFFFF (max positive 32-bit signed) produces 0x80000000 (max negative). Saturation instead produces 0x7FFFFFFF, clamping to the maximum representable value. Similarly, subtracting 1 from 0x80000000 produces 0x7FFFFFFF with wrapping, or 0x80000000 with saturation.

**Applications**

Audio processing uses saturation to prevent distortion when mixing signals - clipping is preferable to wraparound which creates severe artifacts. Image processing clamps pixel values to valid ranges (0-255 for 8-bit). Control systems avoid instability from integer overflow in feedback loops.

### Saturating Instructions

ARMv6 and later architectures include dedicated saturation instructions in the DSP extensions.

**SSAT - Signed Saturate**

SSAT saturates a signed value to a specified bit width. The syntax `SSAT rd, #n, rn` saturates the value in `rn` to an n-bit signed range (-2^(n-1) to 2^(n-1)-1) and stores the result in `rd`. The Q (saturation) flag in CPSR is set if saturation occurs.

**Example:**

```assembly
@ Saturate to 8-bit signed range (-128 to 127)
MOV r0, #200                @ Value exceeds 8-bit signed max
SSAT r1, #8, r0             @ r1 = 127 (saturated to max)
                            @ Q flag set

MOV r0, #-200               @ Value below 8-bit signed min
SSAT r1, #8, r0             @ r1 = -128 (saturated to min)
                            @ Q flag set

MOV r0, #50                 @ Value within range
SSAT r1, #8, r0             @ r1 = 50 (no saturation)
                            @ Q flag unchanged

@ Saturate result of arithmetic to 16-bit signed
MOV r0, #30000
MOV r1, #10000
ADD r2, r0, r1              @ r2 = 40000 (exceeds 16-bit signed)
SSAT r2, #16, r2            @ r2 = 32767 (saturated)
```

**USAT - Unsigned Saturate**

USAT saturates an unsigned value to a specified bit width. The syntax `USAT rd, #n, rn` saturates the value in `rn` to an n-bit unsigned range (0 to 2^n-1). Negative input values saturate to 0.

**Example:**

```assembly
@ Saturate to 8-bit unsigned range (0 to 255)
MOV r0, #300                @ Value exceeds 8-bit unsigned max
USAT r1, #8, r0             @ r1 = 255 (saturated to max)

MOV r0, #-50                @ Negative value
USAT r1, #8, r0             @ r1 = 0 (saturated to min)

MOV r0, #100                @ Value within range
USAT r1, #8, r0             @ r1 = 100 (no saturation)
```

**Saturation with Shift**

Both SSAT and USAT accept optional shift amounts applied before saturation. This enables scaling combined with range limiting in a single instruction.

**Example:**

```assembly
@ Saturate with left shift: SSAT rd, #n, rm, LSL #shift
MOV r0, #100
SSAT r1, #8, r0, LSL #2     @ r1 = (100 << 2) saturated to 8-bit signed
                            @ = 400 saturated to 127

@ Saturate with right shift: SSAT rd, #n, rm, ASR #shift
MOV r0, #1000
SSAT r1, #8, r0, ASR #2     @ r1 = (1000 >> 2) saturated to 8-bit signed
                            @ = 250 saturated to 127
```

### Q Flag - Saturation Status

The Q (saturation/overflow) flag in CPSR bit 27 indicates whether saturation or overflow occurred in DSP instructions. Unlike NZCV flags, the Q flag is sticky: once set, it remains set until explicitly cleared by software.

**Checking Q Flag**

The Q flag cannot be tested directly with condition codes like NZCV flags. Instead, the MRS (move from special register) instruction reads CPSR into a general-purpose register, then bit testing determines Q flag state.

**Example:**

```assembly
@ Perform saturating operation
MOV r0, #300
USAT r1, #8, r0             @ Saturation occurs, Q flag set

@ Check if saturation occurred
MRS r2, CPSR                @ Read CPSR into r2
TST r2, #0x08000000         @ Test Q flag (bit 27)
BNE saturation_occurred     @ Branch if Q was set

@ Clear Q flag
MRS r2, CPSR
BIC r2, r2, #0x08000000     @ Clear Q flag bit
MSR CPSR_f, r2              @ Write flags back to CPSR
```

### Software Saturation Implementation

On architectures without hardware saturation instructions, software implements saturation using comparison and conditional moves.

**Example:**

```assembly
@ Software signed saturation to 8-bit range (-128 to 127)
@ Input in r0, output in r1
saturate_8bit_signed:
    MOV r1, r0                  @ Copy input
    CMP r1, #127                @ Compare with max
    MOVGT r1, #127              @ If greater, saturate to max
    CMN r1, #128                @ Compare with min (r1 + 128)
    MOVLT r1, #-128             @ If less, saturate to min
    BX lr

@ Software unsigned saturation to 8-bit range (0 to 255)
@ Input in r0, output in r1
saturate_8bit_unsigned:
    MOV r1, r0                  @ Copy input
    CMP r1, #0                  @ Compare with min
    MOVLT r1, #0                @ If negative, saturate to 0
    CMP r1, #255                @ Compare with max
    MOVGT r1, #255              @ If greater, saturate to max
    BX lr
```

## SIMD Instructions Basics

SIMD (Single Instruction Multiple Data) instructions perform the same operation on multiple data elements simultaneously within a single register. ARM architectures provide SIMD capabilities through NEON extensions (Advanced SIMD) and earlier SIMD instructions in ARMv6.

### SIMD Architecture Overview

**Register Organization**

NEON provides 32 64-bit registers (D0-D31) or 16 128-bit registers (Q0-Q15) for SIMD operations. Q registers are formed by pairs of D registers: Q0 = {D1, D0}, Q1 = {D3, D2}, etc. These registers are separate from the general-purpose r0-r15 registers.

**Data Type Support**

SIMD instructions operate on vectors of 8-bit, 16-bit, 32-bit, or 64-bit elements. A 128-bit Q register can hold sixteen 8-bit values, eight 16-bit values, four 32-bit values, or two 64-bit values. Instructions specify element size with suffixes: .8, .16, .32, .64.

**Signed and Unsigned Operations**

Most SIMD instructions have signed and unsigned variants, indicated by additional suffixes. For example, VADD.I16 adds signed or unsigned 16-bit integers (same operation), while VMAX.S16 finds maximum treating values as signed, and VMAX.U16 treats them as unsigned.

### Basic NEON Instructions

**VADD - Vector Addition**

VADD adds corresponding elements from two source vectors, storing results in a destination vector. All additions occur in parallel.

**Example:**

```assembly
@ Add eight 16-bit values in parallel
@ Q0 = {a0, a1, a2, a3, a4, a5, a6, a7}
@ Q1 = {b0, b1, b2, b3, b4, b5, b6, b7}
VADD.I16 Q2, Q0, Q1
@ Q2 = {a0+b0, a1+b1, a2+b2, a3+b3, a4+b4, a5+b5, a6+b6, a7+b7}

@ Add four 32-bit values in parallel
VADD.I32 Q3, Q4, Q5

@ Add sixteen 8-bit values in parallel
VADD.I8 Q6, Q7, Q8
```

**VSUB - Vector Subtraction**

VSUB subtracts corresponding elements of the second vector from the first vector.

**Example:**

```assembly
VSUB.I16 Q0, Q1, Q2         @ Q0 = Q1 - Q2 (eight 16-bit subtractions)
VSUB.I32 D0, D1, D2         @ D0 = D1 - D2 (two 32-bit subtractions)
```

**VMUL - Vector Multiplication**

VMUL multiplies corresponding elements from two vectors. [Inference: The result width typically matches input width, potentially losing high-order bits for large products].

**Example:**

```assembly
VMUL.I16 Q0, Q1, Q2         @ Q0 = Q1 * Q2 (eight 16-bit multiplications)
VMUL.I32 D0, D1, D2         @ D0 = D1 * D2 (two 32-bit multiplications)
```

**VMAX/VMIN - Vector Maximum/Minimum**

VMAX computes the maximum of corresponding elements, VMIN computes the minimum. Signed and unsigned variants compare values differently.

**Example:**

```assembly
@ Maximum of signed 16-bit values
VMAX.S16 Q0, Q1, Q2         @ Q0[i] = max(Q1[i], Q2[i])

@ Minimum of unsigned 8-bit values
VMIN.U8 Q3, Q4, Q5          @ Q3[i] = min(Q4[i], Q5[i])
```

**VLD1/VST1 - Vector Load/Store**

VLD1 loads multiple elements from memory into SIMD registers, VST1 stores SIMD registers to memory. These instructions support various configurations for different data layouts.

**Example:**

```assembly
@ Load 128 bits (16 bytes) from memory
VLD1.8 {Q0}, [r0]           @ Load 16 bytes from address in r0

@ Load 64 bits (8 bytes) into D register
VLD1.32 {D0}, [r1]          @ Load 2x32-bit values

@ Store 128 bits to memory
VST1.8 {Q1}, [r2]           @ Store 16 bytes to address in r2

@ Load with post-increment
VLD1.16 {Q2}, [r3]!         @ Load and increment r3 by 16
```

### SIMD Use Cases

**Parallel Arithmetic**

Processing arrays of values benefits from SIMD by operating on multiple elements per instruction. Adding two arrays of 16-bit integers processes eight elements per VADD.I16 instead of one per ADD.

**Example:**

```assembly
@ Add two arrays of 128 16-bit integers
@ r0 = pointer to array A
@ r1 = pointer to array B
@ r2 = pointer to result array
MOV r3, #128/8              @ Loop counter (128 elements / 8 per iteration)
add_arrays:
    VLD1.16 {Q0}, [r0]!     @ Load 8 elements from A, post-increment
    VLD1.16 {Q1}, [r1]!     @ Load 8 elements from B, post-increment
    VADD.I16 Q2, Q0, Q1     @ Add 8 elements in parallel
    VST1.16 {Q2}, [r2]!     @ Store 8 results, post-increment
    SUBS r3, r3, #1
    BNE add_arrays
```

**Color Conversion**

Image processing operations like RGB to grayscale conversion or color space transformations benefit from parallel processing of multiple pixels.

**Example:**

```assembly
@ Convert 8 RGB pixels to grayscale (simplified)
@ Grayscale = 0.3*R + 0.59*G + 0.11*B
@ Using integer approximation: Grayscale = (77*R + 150*G + 29*B) >> 8

@ Assuming RGB data is loaded with R, G, B components separated
VDUP.16 Q4, r4              @ Broadcast red coefficient (77)
VDUP.16 Q5, r5              @ Broadcast green coefficient (150)
VDUP.16 Q6, r6              @ Broadcast blue coefficient (29)

VMUL.I16 Q0, Q0, Q4         @ Multiply 8 red values
VMUL.I16 Q1, Q1, Q5         @ Multiply 8 green values
VMUL.I16 Q2, Q2, Q6         @ Multiply 8 blue values

VADD.I16 Q0, Q0, Q1         @ Sum red and green
VADD.I16 Q0, Q0, Q2         @ Add blue component

VSHR.U16 Q0, Q0, #8         @ Divide by 256 (shift right 8)
```

**Audio Processing**

Mixing multiple audio channels, applying filters, or performing FFT operations parallelize naturally with SIMD.

### SIMD Limitations

**Data Alignment**

[Inference: SIMD load and store operations may require aligned memory addresses for optimal performance or correctness, depending on the specific instruction and architecture. Unaligned accesses might be slower or unsupported].

**Code Complexity**

SIMD code is typically more complex than scalar equivalents, requiring careful data layout and loop restructuring. The benefit must justify the development and maintenance cost.

**Architecture Availability**

Not all ARM cores include NEON extensions. ARMv7-A typically includes NEON, but ARMv7-R and ARMv7-M implementations vary. Code must detect and adapt to SIMD availability or provide scalar fallbacks.

## Packed Data Operations

Packed data operations treat registers as holding multiple smaller values packed together, operating on these sub-register elements simultaneously. This provides SIMD-like parallelism using standard integer registers before dedicated SIMD extensions existed.

### ARMv6 SIMD Instructions

ARMv6 introduced SIMD operations on general-purpose registers, treating 32-bit registers as containing multiple 8-bit or 16-bit values. These instructions predated NEON and provided basic parallel operations for DSP and multimedia tasks.

**SADD16/UADD16 - Parallel 16-bit Addition**

SADD16 treats each register as containing two signed 16-bit values and adds them in parallel. UADD16 performs the same operation treating values as unsigned. The syntax `SADD16 rd, rn, rm` adds the two halfwords from `rn` to the corresponding halfwords from `rm`, storing both results in `rd`.

**Example:**

```assembly
@ r0 = 0x00050003 (containing 5 and 3 as 16-bit values)
@ r1 = 0x00020001 (containing 2 and 1 as 16-bit values)
SADD16 r2, r0, r1           @ r2 = 0x00070004 (5+2=7, 3+1=4)

@ Parallel addition wraps independently for each halfword
MOV r0, #0x7FFF0001         @ Max positive and 1
MOV r1, #0x00010001         @ 1 and 1
SADD16 r2, r0, r1           @ r2 = 0x80000002 (overflow in upper, normal in lower)
```

**SADD8/UADD8 - Parallel 8-bit Addition**

SADD8 and UADD8 treat registers as containing four 8-bit values, adding all four pairs in parallel.

**Example:**

```assembly
@ r0 = 0x04030201 (containing 4, 3, 2, 1 as bytes)
@ r1 = 0x01010101 (containing 1, 1, 1, 1 as bytes)
SADD8 r2, r0, r1            @ r2 = 0x05040302 (four parallel additions)
```

**SSUB16/USUB16, SSUB8/USUB8 - Parallel Subtraction**

Parallel subtraction instructions mirror the addition versions, subtracting packed values independently.

**Example:**

```assembly
MOV r0, #0x00050003
MOV r1, #0x00020001
SSUB16 r2, r0, r1           @ r2 = 0x00030002 (5-2=3, 3-1=2)

MOV r0, #0x04030201
MOV r1, #0x01010101
SSUB8 r2, r0, r1            @ r2 = 0x03020100 (four parallel subtractions)
```

**GE Flags**

Parallel operations set GE (Greater than or Equal) flags in CPSR bits 16-19. Each GE bit corresponds to one byte lane, indicating whether that lane's operation result was non-negative (for subtraction) or carried (for addition). [Inference: GE flags enable subsequent selection or masking operations based on parallel comparison results].

### Saturating Parallel Operations

ARMv6 includes saturating versions of parallel arithmetic that clamp individual lanes to representable ranges.

**QADD16/QSUB16 - Saturating Parallel 16-bit Operations**

QADD16 adds two halfwords in parallel with saturation, clamping each result independently to 16-bit signed range. QSUB16 performs saturating subtraction.

**Example:**

```assembly
@ Saturating parallel addition
MOV r0, #0x7FFF7FFF         @ Two max positive values
MOV r1, #0x00010001         @ Two 1s
QADD16 r2, r0, r1           @ r2 = 0x7FFF7FFF (both saturate to max)

MOV r0, #0x80008000         @ Two min negative values
MOV r1, #0xFFFFFFFF         @ Two -1s
QSUB16 r2, r0, r1           @ r2 = 0x80008000 (both saturate to min)
```

**QADD8/QSUB8 - Saturating Parallel 8-bit Operations**

QADD8 and QSUB8 provide saturating arithmetic on four packed bytes.

**Example:**

```assembly
MOV r0, #0x7F7F7F7F         @ Four max signed bytes
MOV r1, #0x01010101         @ Four 1s
QADD8 r2, r0, r1            @ r2 = 0x7F7F7F7F (all saturate)
```

### Packed Data Use Cases

**Pixel Processing**

Image pixels stored as packed bytes (RGBA, ARGB) can be processed in parallel. Four 8-bit color channels fit in one 32-bit register, enabling simultaneous operations on all channels.

**Example:**

```assembly
@ Brighten an RGBA pixel by adding to all channels
@ r0 contains packed pixel: 0xAABBGGRR
MOV r1, #0x10101010         @ Add 16 to each channel
UADD8 r2, r0, r1            @ Parallel addition
USAT16 r2, #8, r2           @ Saturate each byte (requires NEON or separate saturation)
```

**Audio Sample Processing**

Stereo 16-bit audio samples pack left and right channels into 32-bit words. Parallel operations process both channels simultaneously.

**Example:**

```assembly
@ Attenuate stereo sample (divide by 2)
@ r0 = 0xLLLLRRRR (left and right 16-bit samples)
SHADD16 r1, r0, r0          @ Halving add r0 + r0 = r0 (signed, with rounding)
ASR r1, r0, #1              @ Or simple arithmetic shift right by 1
```

**Protocol Header Processing**

Network packet headers containing multiple small fields benefit from parallel field extraction and manipulation.

### Packed vs Full SIMD Trade-offs

**Register Pressure**

Packed operations use general-purpose registers, competing with other uses. NEON uses separate register files, reducing pressure on r0-r15.

**Capability**

ARMv6 SIMD provides basic operations (add, subtract, some multiply variants). NEON offers comprehensive instruction sets including loads/stores with various patterns, wide multiplications, reductions, and permutations.

**Performance**

[Inference: NEON instructions typically provide better performance for sustained SIMD workloads due to wider registers (128-bit vs 32-bit), dedicated execution units, and more sophisticated operations. Packed operations in general-purpose registers are suitable for opportunistic parallelism where full SIMD setup overhead isn't justified].

**Availability**

ARMv6 SIMD is available on older architectures without NEON. Modern ARM cores typically include NEON, making it the preferred choice when available.

**Key Points:**

- UBFX and SBFX extract bit fields in single instructions on ARMv6T2+, with signed variant performing sign-extension
- BFI inserts bit fields while preserving surrounding bits; BFC clears specified bit fields without affecting other bits
- Saturation arithmetic clamps results to representable ranges instead of wrapping, with SSAT for signed and USAT for unsigned values on ARMv6+
- Q flag (CPSR bit 27) is sticky and indicates saturation/overflow in DSP instructions, requiring MRS to read and explicit clearing
- NEON provides 32 64-bit (D0-D31) or 16 128-bit (Q0-Q15) SIMD registers separate from general-purpose registers
- SIMD instructions operate on vectors of multiple elements (8-bit, 16-bit, 32-bit, 64-bit) with parallel execution
- ARMv6 SIMD instructions (SADD16, SADD8, etc.) provide basic packed operations using general-purpose registers before NEON
- GE flags (CPSR bits 16-19) track per-byte results in ARMv6 parallel operations for subsequent conditional selection

### Advanced Packed Operations

**SXTB/SXTH - Sign Extend Byte/Halfword**

Sign extension instructions convert smaller signed values to larger widths while preserving sign. SXTB extends an 8-bit signed value to 32 bits, SXTH extends 16-bit to 32-bit.

**Example:**

```assembly
@ Sign extend byte to word
MOV r0, #0x000000FF         @ Byte value 0xFF = -1 as signed byte
SXTB r1, r0                 @ r1 = 0xFFFFFFFF (sign-extended -1)

MOV r0, #0x0000007F         @ Byte value 0x7F = +127
SXTB r1, r0                 @ r1 = 0x0000007F (sign-extended +127)

@ Sign extend halfword to word
MOV r0, #0x0000FFFF         @ Halfword 0xFFFF = -1 as signed halfword
SXTH r1, r0                 @ r1 = 0xFFFFFFFF (sign-extended -1)

MOV r0, #0x00007FFF         @ Halfword 0x7FFF = +32767
SXTH r1, r0                 @ r1 = 0x00007FFF (sign-extended +32767)
```

**UXTB/UXTH - Zero Extend Byte/Halfword**

Zero extension converts smaller unsigned values to larger widths by filling upper bits with zeros.

**Example:**

```assembly
@ Zero extend byte to word
MOV r0, #0xFFFFFFFF         @ Word with all bits set
UXTB r1, r0                 @ r1 = 0x000000FF (lower byte extracted, zero-extended)

@ Zero extend halfword to word
MOV r0, #0xFFFFFFFF         
UXTH r1, r0                 @ r1 = 0x0000FFFF (lower halfword extracted)
```

**Rotation with Extension**

Sign and zero extend instructions accept rotation parameters to extract and extend bytes or halfwords from different positions within the source register.

**Example:**

```assembly
@ Extract and sign-extend byte from different positions
MOV r0, #0x12345678

SXTB r1, r0, ROR #0         @ Extract byte 0: r1 = 0x00000078 (positive)
SXTB r2, r0, ROR #8         @ Extract byte 1: r1 = 0x00000056 (positive)
SXTB r3, r0, ROR #16        @ Extract byte 2: r1 = 0x00000034 (positive)
SXTB r4, r0, ROR #24        @ Extract byte 3: r1 = 0x00000012 (positive)

@ With negative byte values
MOV r0, #0xFF7F0080         @ Contains mixed signed bytes
SXTB r1, r0                 @ r1 = 0xFFFFFF80 (-128 sign-extended)
SXTB r2, r0, ROR #8         @ r2 = 0x00000000 (0 sign-extended)
SXTB r3, r0, ROR #16        @ r3 = 0x0000007F (+127 sign-extended)
SXTB r4, r0, ROR #24        @ r4 = 0xFFFFFFFF (-1 sign-extended)
```

### Parallel Select Operations

**SEL - Select Bytes**

The SEL instruction selects bytes from two source registers based on GE flags set by previous parallel operations. For each byte position, if the corresponding GE bit is set, the byte from the first source is selected; otherwise, the byte from the second source is selected.

**Example:**

```assembly
@ Use GE flags to select bytes after comparison
MOV r0, #0x80604020         @ First values
MOV r1, #0x70503010         @ Second values

@ Parallel unsigned subtract sets GE flags
USUB8 r2, r0, r1            @ r2 = result, GE flags indicate which bytes of r0 >= r1
                            @ GE[3]=1 (0x80>=0x70), GE[2]=1 (0x60>=0x50)
                            @ GE[1]=1 (0x40>=0x30), GE[0]=1 (0x20>=0x10)

MOV r3, #0xAAAAAAAA         @ Value A
MOV r4, #0xBBBBBBBB         @ Value B
SEL r5, r3, r4              @ Select bytes: where GE=1 take from r3, where GE=0 take from r4
                            @ All GE bits are 1, so r5 = 0xAAAAAAAA

@ Example with mixed GE flags
MOV r0, #0x80402010
MOV r1, #0x70503010
USUB8 r2, r0, r1            @ GE[3]=1, GE[2]=0, GE[1]=1, GE[0]=0

MOV r3, #0xAAAAAAAA
MOV r4, #0xBBBBBBBB  
SEL r5, r3, r4              @ r5 = 0xAABBAABB (bytes selected based on GE flags)
```

**Absolute Value with SEL**

Combining parallel subtraction with SEL computes absolute values of packed differences efficiently.

**Example:**

```assembly
@ Compute absolute difference of packed bytes
@ |r0 - r1| for each byte
USUB8 r2, r0, r1            @ r2 = r0 - r1, sets GE where r0 >= r1
USUB8 r3, r1, r0            @ r3 = r1 - r0
SEL r4, r2, r3              @ Select positive result for each byte
                            @ r4 contains absolute differences
```

### Multiply-Accumulate Variants

**SMLAD/SMALD - Dual Multiply-Accumulate**

SMLAD performs two 16-bit multiplications on packed halfwords and accumulates both products with a third register. This implements dot product operations efficiently.

**Example:**

```assembly
@ Dual multiply-accumulate
@ r0 = 0x00050003 (two signed 16-bit values: 5, 3)
@ r1 = 0x00020004 (two signed 16-bit values: 2, 4)
@ r2 = accumulator value
SMLAD r3, r0, r1, r2        @ r3 = r2 + (5*2) + (3*4) = r2 + 10 + 12 = r2 + 22

@ Dot product of two 4-element 16-bit vectors
@ Vector A in r0 (lower 2 elements) and r1 (upper 2 elements)
@ Vector B in r2 (lower 2 elements) and r3 (upper 2 elements)
MOV r4, #0                  @ Initialize accumulator
SMLAD r4, r0, r2, r4        @ Accumulate first two products
SMLAD r4, r1, r3, r4        @ Accumulate next two products
@ r4 now contains dot product
```

**SMUAD - Dual Multiply-Add**

SMUAD performs dual multiplication without an accumulator input, returning the sum of two products.

**Example:**

```assembly
MOV r0, #0x00030002         @ Values 3, 2
MOV r1, #0x00050004         @ Values 5, 4
SMUAD r2, r0, r1            @ r2 = (3*5) + (2*4) = 15 + 8 = 23
```

**SMLSD - Signed Multiply Subtract-Accumulate**

SMLSD multiplies packed halfwords but subtracts the second product from the first before accumulating.

**Example:**

```assembly
MOV r0, #0x00050003         @ Values 5, 3
MOV r1, #0x00020004         @ Values 2, 4
MOV r2, #100                @ Accumulator
SMLSD r3, r0, r1, r2        @ r3 = r2 + (5*2) - (3*4) = 100 + 10 - 12 = 98
```

### Parallel Comparison and Clamping

**USAT16 - Unsigned Saturate Packed Halfwords**

USAT16 saturates two packed 16-bit signed values to unsigned n-bit ranges independently.

**Example:**

```assembly
@ Saturate two signed 16-bit values to 8-bit unsigned range
MOV r0, #0x00FF0100         @ Values 255, 256 (second exceeds 8-bit)
USAT16 r1, #8, r0           @ r1 = 0x00FF00FF (both saturated to 255)

MOV r0, #0xFFFF0001         @ Values -1 (0xFFFF), 1
USAT16 r1, #8, r0           @ r1 = 0x00000001 (negative saturates to 0, positive unchanged)
```

**SSAT16 - Signed Saturate Packed Halfwords**

SSAT16 saturates two packed signed 16-bit values to signed n-bit ranges independently.

**Example:**

```assembly
@ Saturate two 16-bit values to 8-bit signed range (-128 to 127)
MOV r0, #0x00800100         @ Values 128, 256
SSAT16 r1, #8, r0           @ r1 = 0x007F007F (both saturate to 127)

MOV r0, #0xFF000100         @ Values -256, 256  
SSAT16 r1, #8, r0           @ r1 = 0xFF80007F (-128, 127)
```

### Halfword Multiply Variants

**SMUL** variants provide various combinations of halfword multiplications from packed 32-bit registers.

**SMULBB/SMULBT/SMULTB/SMULTT - Signed Multiply Halfwords**

These instructions multiply selected halfwords from two source registers. The suffix indicates which halfword (Bottom or Top) to use from each source.

**Example:**

```assembly
@ r0 = 0x00050003 (top=5, bottom=3)
@ r1 = 0x00070002 (top=7, bottom=2)

SMULBB r2, r0, r1           @ r2 = bottom(r0) * bottom(r1) = 3 * 2 = 6
SMULBT r3, r0, r1           @ r3 = bottom(r0) * top(r1) = 3 * 7 = 21
SMULTB r4, r0, r1           @ r4 = top(r0) * bottom(r1) = 5 * 2 = 10
SMULTT r5, r0, r1           @ r5 = top(r0) * top(r1) = 5 * 7 = 35
```

**SMLA variants - Signed Multiply-Accumulate Halfwords**

SMLA variants combine halfword multiplication with accumulation.

**Example:**

```assembly
MOV r0, #0x00050003
MOV r1, #0x00070002  
MOV r2, #100                @ Accumulator

SMLABB r3, r0, r1, r2       @ r3 = 100 + (3*2) = 106
SMLABT r4, r0, r1, r2       @ r4 = 100 + (3*7) = 121
SMLATB r5, r0, r1, r2       @ r5 = 100 + (5*2) = 110
SMLATT r6, r0, r1, r2       @ r6 = 100 + (5*7) = 135
```

### Halving Operations

Halving arithmetic performs operations then divides results by 2, useful for averaging and preventing overflow.

**SHADD8/SHADD16 - Signed Halving Add**

SHADD adds packed values then arithmetically shifts right by 1 (divides by 2 with rounding toward negative infinity).

**Example:**

```assembly
@ Average two packed byte values
MOV r0, #0x64644B4B         @ Values 100, 100, 75, 75
MOV r1, #0x32321E1E         @ Values 50, 50, 30, 30
SHADD8 r2, r0, r1           @ r2 = 0x4B4B3434 (averages: 75, 75, 52, 52)
                            @ (100+50)/2=75, (75+30)/2=52

@ Prevent overflow when averaging large values
MOV r0, #0x7F7F7F7F         @ Four max signed bytes (127)
MOV r1, #0x7F7F7F7F         @ Four max signed bytes (127)
SHADD8 r2, r0, r1           @ r2 = 0x7F7F7F7F (127, no overflow)
                            @ Regular add would overflow: 127+127=254→-2
                            @ Halving: (127+127)/2=127
```

**UHADD8/UHADD16 - Unsigned Halving Add**

UHADD performs unsigned halving addition, logically shifting right by 1.

**Example:**

```assembly
MOV r0, #0xFFFFFFFF         @ Four max unsigned bytes (255)
MOV r1, #0xFFFFFFFF
UHADD8 r2, r0, r1           @ r2 = 0xFFFFFFFF ((255+255)/2=255)
                            @ With rounding up
```

**SHSUB8/SHSUB16 - Signed Halving Subtract**

SHSUB subtracts then divides by 2.

**Example:**

```assembly
MOV r0, #0x64006400         @ Values 100, 0, 100, 0
MOV r1, #0x32003200         @ Values 50, 0, 50, 0
SHSUB8 r2, r0, r1           @ r2 = 0x19001900 ((100-50)/2=25)
```

### Reversal Instructions

**REV - Byte-Reverse Word**

REV reverses the byte order within a 32-bit word, converting between big-endian and little-endian representations.

**Example:**

```assembly
MOV r0, #0x12345678
REV r1, r0                  @ r1 = 0x78563412
                            @ Bytes reversed: [12][34][56][78] → [78][56][34][12]
```

**REV16 - Byte-Reverse Halfwords**

REV16 reverses bytes within each halfword independently.

**Example:**

```assembly
MOV r0, #0x12345678
REV16 r1, r0                @ r1 = 0x34127856
                            @ [12][34][56][78] → [34][12][78][56]
```

**REVSH - Byte-Reverse Signed Halfword**

REVSH reverses the bytes of the lower halfword and sign-extends to 32 bits.

**Example:**

```assembly
MOV r0, #0x12345678         @ Lower halfword: 0x5678
REVSH r1, r0                @ Reverse: 0x7856, sign-extend: 0x00007856

MOV r0, #0x123480FF         @ Lower halfword: 0x80FF (negative when signed)
REVSH r1, r0                @ Reverse: 0xFF80, sign-extend: 0xFFFFFF80
```

**RBIT - Reverse Bits**

RBIT reverses all 32 bits within a register (available on ARMv6T2+).

**Example:**

```assembly
MOV r0, #0x80000000         @ Binary: 10000000...00000000
RBIT r1, r0                 @ r1 = 0x00000001 (reversed: 00000000...00000001)

MOV r0, #0x12345678         @ Binary pattern
RBIT r1, r0                 @ r1 = 0x1E6A2C48 (all 32 bits reversed)
```

### Packing and Unpacking

**PKHBT/PKHTB - Pack Halfwords**

PKHBT and PKHTB combine halfwords from two source registers into a destination register.

**PKHBT - Pack Halfword Bottom-Top**

PKHBT takes the bottom halfword from the first source and top halfword from the second source.

**Example:**

```assembly
MOV r0, #0x11112222         @ Bottom halfword: 0x2222
MOV r1, #0x33334444         @ Top halfword: 0x3333
PKHBT r2, r0, r1, LSL #16   @ r2 = 0x33332222
                            @ Take bottom of r0 (0x2222) and top of r1 (0x3333)
```

**PKHTB - Pack Halfword Top-Bottom**

PKHTB takes the top halfword from the first source and bottom halfword from the second source.

**Example:**

```assembly
MOV r0, #0x11112222         @ Top halfword: 0x1111
MOV r1, #0x33334444         @ Bottom halfword: 0x4444
PKHTB r2, r0, r1, ASR #16   @ r2 = 0x11114444
                            @ Take top of r0 (0x1111) and bottom of r1 (0x4444)
```

### SIMD Advanced Techniques

**Vector Reduction**

Reduction operations combine all elements of a vector into a single scalar result, such as summing all elements or finding the maximum.

**Example (using NEON):**

```assembly
@ Sum all 8 elements of a 16-bit vector
VLD1.16 {Q0}, [r0]          @ Load 8x16-bit values

@ Pairwise addition to reduce
VPADD.I16 D0, D0, D1        @ Add pairs: 8 values → 4 values in D0
VPADD.I16 D0, D0, D0        @ Add pairs: 4 values → 2 values
VPADD.I16 D0, D0, D0        @ Add pairs: 2 values → 1 value (sum)

VMOV.32 r1, D0[0]           @ Extract final sum to general register
```

**Vector Widening Operations**

Widening operations multiply or add narrower elements producing wider results, preventing overflow.

**VMULL - Vector Multiply Long**

VMULL multiplies elements from two 64-bit registers producing 128-bit results with doubled element width.

**Example:**

```assembly
@ Multiply 8-bit values producing 16-bit results
@ D0 contains four 8-bit values
@ D1 contains four 8-bit values
VMULL.S8 Q0, D0, D1         @ Q0 contains four 16-bit products
                            @ No overflow since result width doubled

@ Multiply 16-bit values producing 32-bit results
VMULL.S16 Q1, D2, D3        @ Four 16-bit * 16-bit → four 32-bit products
```

**VADDL - Vector Add Long**

VADDL adds narrow elements producing wider results.

**Example:**

```assembly
@ Add 8-bit values producing 16-bit results
VADDL.S8 Q0, D0, D1         @ Eight 8-bit additions → eight 16-bit results

@ Add 16-bit values producing 32-bit results
VADDL.U16 Q1, D2, D3        @ Four 16-bit additions → four 32-bit results
```

**VADDW - Vector Add Wide**

VADDW adds narrow elements to wide elements, useful for accumulation.

**Example:**

```assembly
@ Accumulate 8-bit values into 16-bit accumulator
@ Q0 contains eight 16-bit accumulated values
@ D2 contains eight 8-bit values to add
VADDW.S8 Q0, Q0, D2         @ Add 8-bit values to 16-bit accumulators
```

### Lane Operations

**VMOV - Move Scalar to/from SIMD Register**

VMOV transfers individual elements between SIMD and general-purpose registers.

**Example:**

```assembly
@ Move scalar from general register to SIMD lane
MOV r0, #42
VMOV.32 D0[0], r0           @ Set D0 lane 0 to 42
VMOV.16 D0[1], r1           @ Set D0 lane 1 (16-bit) from r1

@ Move scalar from SIMD lane to general register
VMOV.32 r2, D1[1]           @ Extract D1 lane 1 to r2
VMOV.16 r3, D2[3]           @ Extract D2 lane 3 (16-bit) to r3
```

**VDUP - Duplicate Scalar to Vector**

VDUP broadcasts a single value to all lanes of a vector.

**Example:**

```assembly
@ Broadcast from general register
MOV r0, #100
VDUP.8 Q0, r0               @ All 16 bytes in Q0 = 100
VDUP.16 D1, r1              @ All 4 halfwords in D1 = r1 value
VDUP.32 Q2, r2              @ All 4 words in Q2 = r2 value

@ Broadcast from SIMD lane
VDUP.16 Q3, D0[2]           @ Broadcast D0 lane 2 to all lanes of Q3
```

**VEXT - Extract Elements**

VEXT extracts elements from two concatenated vectors.

**Example:**

```assembly
@ Q0 = {0, 1, 2, 3, 4, 5, 6, 7} (eight 16-bit values)
@ Q1 = {8, 9, 10, 11, 12, 13, 14, 15}
VEXT.16 Q2, Q0, Q1, #3      @ Q2 = {3, 4, 5, 6, 7, 8, 9, 10}
                            @ Extract starting from element 3 of concatenated Q0|Q1
```

### Table Lookup

**VTBL/VTBX - Vector Table Lookup**

VTBL performs table lookups using indices in one vector to select elements from table vectors.

**Example:**

```assembly
@ Table in D0-D1 (16 bytes)
@ Indices in D2 (8 bytes)
VTBL.8 D3, {D0, D1}, D2     @ D3[i] = table[D2[i]] for each byte
                            @ Out-of-range indices produce 0

@ VTBX preserves destination where index out of range
VTBX.8 D3, {D0, D1}, D2     @ Like VTBL but keeps D3[i] if D2[i] out of range
```

### Interleaving and Deinterleaving

**VLD2/VST2 - Load/Store 2-way Interleaved**

VLD2 loads interleaved data and separates it into two registers. VST2 interleaves two registers when storing.

**Example:**

```assembly
@ Memory contains interleaved RGB data: RGBRGBRGB...
@ Load and deinterleave
VLD2.8 {D0, D1}, [r0]       @ D0 = R R R R R R R R, D1 = G G G G G G G G
                            @ (assuming only 2-channel for example)

@ Interleave and store
VST2.8 {D2, D3}, [r1]       @ Interleaves D2 and D3 when storing
```

**VLD3/VST3 and VLD4/VST4**

Similar operations for 3-way and 4-way interleaving.

**Example:**

```assembly
@ Load interleaved RGB (3-way)
VLD3.8 {D0, D1, D2}, [r0]   @ D0=R..., D1=G..., D2=B...

@ Load interleaved RGBA (4-way)
VLD4.8 {D0, D1, D2, D3}, [r1] @ D0=R..., D1=G..., D2=B..., D3=A...
```

### Transposition

**VTRN - Vector Transpose**

VTRN transposes elements between two vectors.

**Example:**

```assembly
@ D0 = {a0, a1, a2, a3} (32-bit elements)
@ D1 = {b0, b1, b2, b3}
VTRN.32 D0, D1              @ D0 = {a0, b0, a2, b2}
                            @ D1 = {a1, b1, a3, b3}

@ Useful for matrix operations and data restructuring
```

**VZIP - Vector Zip (Interleave)**

VZIP interleaves elements from two vectors.

**Example:**

```assembly
@ D0 = {a0, a1, a2, a3}
@ D1 = {b0, b1, b2, b3}
VZIP.16 D0, D1              @ D0 = {a0, b0, a1, b1}
                            @ D1 = {a2, b2, a3, b3}
```

**VUZP - Vector Unzip (Deinterleave)**

VUZP deinterleaves elements into two vectors.

**Example:**

```assembly
@ D0 = {a0, b0, a1, b1}
@ D1 = {a2, b2, a3, b3}
VUZP.16 D0, D1              @ D0 = {a0, a1, a2, a3}
                            @ D1 = {b0, b1, b2, b3}
```

### Optimization Considerations

**Memory Alignment**

SIMD operations typically require aligned memory accesses for optimal performance. [Inference: Unaligned accesses may incur significant performance penalties or cause faults depending on the instruction and architecture].

**Example:**

```assembly
@ Ensure 16-byte alignment for Q register loads
.align 4                    @ Align to 16-byte boundary
my_data:
    .word 0x00000000, 0x00000000, 0x00000000, 0x00000000

@ Aligned load (fast)
LDR r0, =my_data
VLD1.32 {Q0}, [r0:128]      @ :128 indicates 128-bit (16-byte) alignment

@ Unaligned load may be slower
VLD1.32 {Q1}, [r1]          @ No alignment specified
```

**Loop Unrolling with SIMD**

Processing multiple SIMD vectors per iteration reduces loop overhead and improves instruction-level parallelism.

**Example:**

```assembly
@ Process array with loop unrolling
@ r0 = source pointer, r1 = dest pointer, r2 = count/16
process_loop:
    VLD1.32 {Q0, Q1}, [r0]! @ Load 8 values (2 Q registers), post-increment
    VLD1.32 {Q2, Q3}, [r0]! @ Load 8 more values
    
    @ Process 16 values with SIMD operations
    VADD.I32 Q0, Q0, Q4
    VADD.I32 Q1, Q1, Q4
    VADD.I32 Q2, Q2, Q4
    VADD.I32 Q3, Q3, Q4
    
    VST1.32 {Q0, Q1}, [r1]! @ Store results
    VST1.32 {Q2, Q3}, [r1]!
    
    SUBS r2, r2, #1
    BNE process_loop
```

**Register Allocation**

NEON operations have separate register files, but moving data between general-purpose and SIMD registers incurs overhead. Minimize transfers by keeping data in SIMD registers throughout processing chains.

**Data Layout Transformation**

Array-of-structures (AoS) layouts may require conversion to structure-of-arrays (SoA) for efficient SIMD processing, as SIMD works best on contiguous identical data types.

**Example:**

```assembly
@ AoS: struct {x, y, z} points[N]
@ Memory: x0 y0 z0 x1 y1 z1 x2 y2 z2...

@ SoA: struct {x[N], y[N], z[N]}
@ Memory: x0 x1 x2 ... xN, y0 y1 y2 ... yN, z0 z1 z2 ... zN

@ SoA enables efficient SIMD:
VLD1.32 {Q0}, [r0]          @ Load 4 x-coordinates
VLD1.32 {Q1}, [r1]          @ Load 4 y-coordinates
VLD1.32 {Q2}, [r2]          @ Load 4 z-coordinates
@ Process all coordinates with SIMD operations

@ AoS requires deinterleaving first
VLD3.32 {D0, D1, D2}, [r0]  @ Load and deinterleave x, y, z
```

**Key Points:**

- Sign extension (SXTB/SXTH) and zero extension (UXTB/UXTH) convert smaller values to 32-bit with optional rotation for extracting from different byte positions
- SEL instruction uses GE flags from parallel operations to conditionally select bytes from two sources
- Dual multiply-accumulate instructions (SMLAD/SMUAD) compute sums of two products efficiently for dot product operations
- Halving arithmetic (SHADD/SHSUB) performs addition/subtraction then divides by 2, preventing overflow when averaging values
- Byte reversal instructions (REV/REV16/REVSH/RBIT) convert endianness and reverse bit patterns
- Pack/unpack instructions (PKHBT/PKHTB) combine or separate halfwords from registers
- NEON widening operations (VMULL/VADDL/VADDW) multiply or add narrow elements producing wider results without overflow
- Vector lane operations (VMOV/VDUP/VEXT) transfer individual elements or broadcast values across vectors
- Interleaving instructions (VLD2/VLD3/VLD4, VZIP/VUZP/VTRN) efficiently handle multi-channel data like RGB pixels
- Memory alignment significantly affects SIMD performance; aligned accesses are typically required for optimal execution
- Loop unrolling with multiple SIMD register operations per iteration reduces overhead and improves throughput

**Important related topics:** NEON floating-point operations (single and double precision), cryptographic extensions for AES/SHA acceleration, vector polynomial multiply for CRC/crypto, FP16 half-precision floating-point support in ARMv8.2+, SVE (Scalable Vector Extension) in ARMv8-A for vector-length-agnostic programming, auto-vectorization compiler optimizations and hints, cache effects on SIMD memory access patterns, runtime CPU feature detection for optimal code path selection, mixed-precision arithmetic for machine learning inference, intrinsics vs inline assembly trade-offs for SIMD programming.

## Conditional SIMD Operations

NEON provides conditional operations that select or blend results based on comparisons, enabling data-dependent processing within SIMD vectors.

### Vector Comparisons

**VCEQ - Vector Compare Equal**

VCEQ compares corresponding elements from two vectors, setting result lanes to all 1s where elements are equal, all 0s otherwise.

**Example:**

```assembly
@ Compare eight 16-bit values
@ Q0 = {10, 20, 30, 40, 50, 60, 70, 80}
@ Q1 = {10, 25, 30, 35, 50, 65, 70, 75}
VCEQ.I16 Q2, Q0, Q1         @ Q2 = {0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0}
                            @ Lanes 0, 2, 4, 6 match (all bits set)
                            @ Lanes 1, 3, 5, 7 differ (all bits clear)
```

**VCGE/VCGT - Vector Compare Greater Equal/Greater Than**

VCGE and VCGT perform magnitude comparisons with signed or unsigned variants.

**Example:**

```assembly
@ Compare greater than (signed)
@ Q0 = {-10, 20, 30, 40}
@ Q1 = {0, 15, 30, 50}
VCGT.S32 Q2, Q0, Q1         @ Q2 = {0, 0xFFFFFFFF, 0, 0}
                            @ Lane 1: 20 > 15 (true)
                            @ Lane 2: 30 > 30 (false)

@ Compare greater or equal (unsigned)
VCGE.U32 Q3, Q0, Q1         @ Treats values as unsigned
```

**VCLE/VCLT - Vector Compare Less Equal/Less Than**

Less-than comparisons are achieved by reversing operand order or using dedicated VCLE/VCLT instructions.

**Example:**

```assembly
@ Compare less than
VCLT.S16 Q2, Q0, Q1         @ Q2[i] = all 1s if Q0[i] < Q1[i]

@ Equivalent using reversed VCGT
VCGT.S16 Q2, Q1, Q0         @ Q2[i] = all 1s if Q1[i] > Q0[i] (same as Q0[i] < Q1[i])
```

**VTST - Vector Test Bits**

VTST tests whether corresponding elements have any bits in common (bitwise AND is non-zero).

**Example:**

```assembly
@ Test if bits overlap
@ Q0 = {0x0F, 0xF0, 0xFF, 0x00, ...}
@ Q1 = {0x10, 0x0F, 0xAA, 0xFF, ...}
VTST.8 Q2, Q0, Q1           @ Q2 = {0, 0, 0xFF, 0, ...}
                            @ Lane 0: 0x0F & 0x10 = 0 (false)
                            @ Lane 1: 0xF0 & 0x0F = 0 (false)
                            @ Lane 2: 0xFF & 0xAA ≠ 0 (true)
                            @ Lane 3: 0x00 & 0xFF = 0 (false)
```

### Vector Select/Blend

**VBSL - Vector Bitwise Select**

VBSL (Bit Select) selects bits from two sources based on a mask. For each bit position, if the mask bit is 1, select from the first source; if 0, select from the second source.

**Example:**

```assembly
@ Mask in Q0 (typically from comparison results)
@ Q0 = {0xFFFFFFFF, 0, 0xFFFFFFFF, 0} (from previous VCEQ/VCGT)
@ Q1 = {100, 200, 300, 400} (true values)
@ Q2 = {10, 20, 30, 40} (false values)
VBSL Q0, Q1, Q2             @ Q0 = {100, 20, 300, 40}
                            @ Select Q1 where mask is all 1s, Q2 where mask is 0
```

**VBIF/VBIT - Vector Bit Insert if False/True**

VBIF and VBIT conditionally update destination bits based on a mask.

**Example:**

```assembly
@ VBIF: Bit Insert if False - update where mask is 0
@ Q0 = destination
@ Q1 = source values
@ Q2 = mask
VBIF Q0, Q1, Q2             @ Q0[bit] = Q1[bit] if Q2[bit] == 0, else unchanged

@ VBIT: Bit Insert if True - update where mask is 1
VBIT Q0, Q1, Q2             @ Q0[bit] = Q1[bit] if Q2[bit] == 1, else unchanged
```

### Conditional Processing Patterns

**Clamping Values**

Use comparisons and selects to clamp values to ranges.

**Example:**

```assembly
@ Clamp values to range [min, max]
@ Q0 = input values
VDUP.32 Q1, r0              @ Q1 = all min value
VDUP.32 Q2, r1              @ Q2 = all max value

@ Clamp to minimum
VCGE.S32 Q3, Q0, Q1         @ Q3 = mask where Q0 >= min
VBSL Q3, Q0, Q1             @ Q3 = Q0 where >= min, else min

@ Clamp to maximum  
VCLE.S32 Q4, Q3, Q2         @ Q4 = mask where Q3 <= max
VBSL Q4, Q3, Q2             @ Q4 = Q3 where <= max, else max
@ Q4 now contains clamped values

@ Alternatively using VMIN/VMAX:
VMAX.S32 Q0, Q0, Q1         @ Q0 = max(Q0, min)
VMIN.S32 Q0, Q0, Q2         @ Q0 = min(Q0, max)
```

**Conditional Accumulation**

Accumulate only values meeting certain conditions.

**Example:**

```assembly
@ Sum only positive values from a vector
@ Q0 = input values (signed)
@ Q1 = accumulator (initially zero)
VDUP.32 Q2, #0              @ Zero vector for comparison

VCGT.S32 Q3, Q0, Q2         @ Q3 = mask where Q0 > 0
VAND Q4, Q0, Q3             @ Q4 = Q0 where positive, 0 elsewhere
VADD.I32 Q1, Q1, Q4         @ Accumulate only positive values
```

**Conditional Replacement**

Replace elements matching a condition.

**Example:**

```assembly
@ Replace all zeros with a default value
@ Q0 = input values
VDUP.32 Q1, r0              @ Q1 = default value (all lanes)
VDUP.32 Q2, #0              @ Q2 = zero

VCEQ.I32 Q3, Q0, Q2         @ Q3 = mask where Q0 == 0
VBSL Q3, Q1, Q0             @ Q3 = default where Q0==0, else original Q0
```

## Advanced Memory Access Patterns

NEON provides sophisticated memory access instructions for various data layouts and access patterns common in multimedia and signal processing.

### Structure Loads and Stores

**Single-Structure Loads**

VLD1 loads contiguous data into one or more registers.

**Example:**

```assembly
@ Load 64 bytes (four Q registers) contiguously
VLD1.8 {Q0, Q1, Q2, Q3}, [r0]

@ Load with post-increment
VLD1.32 {Q0}, [r0]!         @ Load 16 bytes, r0 += 16

@ Load with specific increment
MOV r1, #32
VLD1.16 {Q0}, [r0], r1      @ Load 16 bytes, r0 += 32
```

**Multi-Structure Loads**

VLD2/VLD3/VLD4 load interleaved data and separate into multiple registers.

**Example:**

```assembly
@ Stereo audio: LRLRLRLR... format
VLD2.16 {D0, D1}, [r0]!     @ D0 = LLLL (left channel)
                            @ D1 = RRRR (right channel)
                            @ Loads and deinterleaves 8 samples

@ RGB image data: RGBRGBRGB... format
VLD3.8 {D0, D1, D2}, [r0]!  @ D0 = RRRRRRRR (red channel)
                            @ D1 = GGGGGGGG (green channel)
                            @ D2 = BBBBBBBB (blue channel)

@ RGBA image data: RGBARGBARGBA... format
VLD4.8 {D0, D1, D2, D3}, [r0]! @ Separate R, G, B, A channels
```

**Lane-Specific Loads**

VLD1 with lane specifier loads a single element into a specific vector lane.

**Example:**

```assembly
@ Load single 32-bit value into specific lane
VLD1.32 {D0[0]}, [r0]       @ Load into D0 lane 0, preserve lane 1
VLD1.32 {D0[1]}, [r1]       @ Load into D0 lane 1, preserve lane 0

@ Useful for gathering non-contiguous data
```

**All-Lane Loads**

VLD1 with replication loads a single value into all lanes.

**Example:**

```assembly
@ Load and replicate across all lanes
VLD1.32 {D0[]}, [r0]        @ Load 32-bit value, broadcast to both lanes of D0
VLD1.16 {Q0[]}, [r1]        @ Load 16-bit value, broadcast to all 8 lanes of Q0
```

### Structure Stores

Store operations mirror load operations for interleaving during storage.

**Example:**

```assembly
@ Store interleaved stereo audio
@ D0 = left channel samples
@ D1 = right channel samples
VST2.16 {D0, D1}, [r0]!     @ Store as LRLRLRLR...

@ Store interleaved RGB
@ D0 = red, D1 = green, D2 = blue
VST3.8 {D0, D1, D2}, [r1]!  @ Store as RGBRGBRGB...

@ Store interleaved RGBA
VST4.8 {D0, D1, D2, D3}, [r2]! @ Store as RGBARGBARGBA...
```

### Alignment Specifications

Specifying alignment enables optimizations and may be required for certain instructions.

**Example:**

```assembly
@ Load with alignment specification
VLD1.32 {Q0}, [r0:128]      @ 128-bit (16-byte) aligned access
VLD1.64 {Q0}, [r0:256]      @ 256-bit (32-byte) aligned access (Q register pair)

@ Store with alignment
VST1.32 {Q0}, [r0:128]      @ 128-bit aligned store
```

### Gather Operations

[Inference: NEON lacks direct gather instructions (indexed loads from non-contiguous locations). Gathers must be implemented with scalar loads into individual lanes or computed indices with clever data arrangement].

**Example:**

```assembly
@ Manual gather using lane loads
@ r0 = base address, r1-r4 contain offsets
LDR r5, [r0, r1]            @ Load element at base + offset1
VMOV.32 D0[0], r5           @ Move to SIMD lane 0
LDR r5, [r0, r2]
VMOV.32 D0[1], r5           @ Move to SIMD lane 1
LDR r5, [r0, r3]
VMOV.32 D1[0], r5           @ Move to SIMD lane 2
LDR r5, [r0, r4]
VMOV.32 D1[1], r5           @ Move to SIMD lane 3
@ Q0 now contains gathered values
```

## Floating-Point SIMD Operations

NEON supports single-precision (32-bit) floating-point SIMD operations. ARMv8-A adds double-precision support.

### Basic Floating-Point Operations

**VADD.F32/VSUB.F32 - Vector Floating-Point Add/Subtract**

**Example:**

```assembly
@ Add four single-precision floats in parallel
VLD1.32 {Q0}, [r0]          @ Load 4 floats
VLD1.32 {Q1}, [r1]          @ Load 4 floats
VADD.F32 Q2, Q0, Q1         @ Q2 = Q0 + Q1 (four additions)

@ Subtract
VSUB.F32 Q3, Q0, Q1         @ Q3 = Q0 - Q1
```

**VMUL.F32/VDIV.F32 - Vector Floating-Point Multiply/Divide**

**Example:**

```assembly
@ Multiply four floats
VMUL.F32 Q0, Q1, Q2         @ Q0 = Q1 * Q2

@ Divide (available in ARMv7-A with VFPv4 and later)
VDIV.F32 S0, S1, S2         @ Scalar divide: S0 = S1 / S2
                            @ [Inference: Full vector divide may require
                            @ per-lane scalar operations depending on architecture]
```

**VMLA/VMLS - Vector Multiply-Accumulate/Subtract**

Fused multiply-accumulate operations improve precision and performance.

**Example:**

```assembly
@ Multiply-accumulate: Q0 = Q0 + (Q1 * Q2)
VMLA.F32 Q0, Q1, Q2         @ Four FMA operations

@ Multiply-subtract: Q0 = Q0 - (Q1 * Q2)
VMLS.F32 Q0, Q1, Q2         @ Four FMS operations
```

### Floating-Point Comparisons

**VCGE.F32/VCGT.F32 - Vector Floating-Point Compare**

Floating-point comparisons produce masks like integer comparisons.

**Example:**

```assembly
@ Compare greater than
VCGT.F32 Q0, Q1, Q2         @ Q0[i] = all 1s if Q1[i] > Q2[i]

@ Use for conditional selection
VCGE.F32 Q3, Q4, Q5         @ Q3 = mask where Q4 >= Q5
VBSL Q3, Q6, Q7             @ Select from Q6 or Q7 based on mask
```

**VCEQ.F32 - Vector Floating-Point Equal**

[Inference: Floating-point equality comparison should be used cautiously due to precision issues; consider tolerance-based comparisons for robust code].

**Example:**

```assembly
VCEQ.F32 Q0, Q1, Q2         @ Q0[i] = all 1s if Q1[i] == Q2[i]
```

### Floating-Point Math Functions

**VABS.F32/VNEG.F32 - Absolute Value/Negate**

**Example:**

```assembly
@ Absolute value of four floats
VABS.F32 Q0, Q1             @ Q0[i] = |Q1[i]|

@ Negate
VNEG.F32 Q0, Q1             @ Q0[i] = -Q1[i]
```

**VSQRT.F32 - Square Root**

**Example:**

```assembly
@ Square root (scalar in most implementations)
VSQRT.F32 S0, S1            @ S0 = sqrt(S1)

@ For vector, may require per-lane scalar operations
VSQRT.F32 S0, S4            @ sqrt lane 0
VSQRT.F32 S1, S5            @ sqrt lane 1
VSQRT.F32 S2, S6            @ sqrt lane 2
VSQRT.F32 S3, S7            @ sqrt lane 3
```

**VRECPE/VRSQRTE - Reciprocal/Reciprocal Square Root Estimate**

Fast approximations for reciprocal and reciprocal square root with Newton-Raphson refinement steps for full precision.

**Example:**

```assembly
@ Fast reciprocal estimate: Q0 ≈ 1/Q1
VRECPE.F32 Q0, Q1           @ Initial estimate (about 8-9 bits accuracy)

@ Newton-Raphson refinement: x_new = x * (2 - x * input)
VMUL.F32 Q2, Q0, Q1         @ Q2 = estimate * input
VRECPS.F32 Q2, Q2, #2.0     @ Q2 = 2 - (estimate * input)
VMUL.F32 Q0, Q0, Q2         @ Q0 = refined estimate

@ Reciprocal sqrt estimate: Q0 ≈ 1/sqrt(Q1)
VRSQRTE.F32 Q0, Q1          @ Initial estimate

@ Refinement
VMUL.F32 Q2, Q0, Q1         @ Q2 = estimate * input
VRSQRTS.F32 Q2, Q2, Q0      @ Refinement step
VMUL.F32 Q0, Q0, Q2         @ Refined estimate
```

**VMAX.F32/VMIN.F32 - Vector Floating-Point Max/Min**

**Example:**

```assembly
@ Maximum of corresponding elements
VMAX.F32 Q0, Q1, Q2         @ Q0[i] = max(Q1[i], Q2[i])

@ Minimum
VMIN.F32 Q0, Q1, Q2         @ Q0[i] = min(Q1[i], Q2[i])
```

### Floating-Point Conversions

**VCVT - Vector Convert**

Conversions between integer and floating-point, and between precisions.

**Example:**

```assembly
@ Convert signed 32-bit integer to float
VCVT.F32.S32 Q0, Q1         @ Q0 (float) = Q1 (signed int)

@ Convert unsigned integer to float
VCVT.F32.U32 Q0, Q1         @ Q0 (float) = Q1 (unsigned int)

@ Convert float to signed integer (round toward zero)
VCVT.S32.F32 Q0, Q1         @ Q0 (signed int) = Q1 (float)

@ Convert float to unsigned integer
VCVT.U32.F32 Q0, Q1         @ Q0 (unsigned int) = Q1 (float)

@ Fixed-point conversions with fractional bits
VCVT.F32.S32 Q0, Q1, #8     @ Convert with 8 fractional bits
                            @ float = int / 256
VCVT.S32.F32 Q0, Q1, #8     @ Convert to fixed-point
                            @ int = float * 256
```

### Rounding Modes

**VRINT - Vector Round Floating-Point**

Various rounding modes for floating-point to integer conversion.

**Example:**

```assembly
@ Round to nearest (ties to even)
VRINTN.F32 Q0, Q1           @ Q0 = round(Q1)

@ Round toward zero (truncate)
VRINTZ.F32 Q0, Q1           @ Q0 = trunc(Q1)

@ Round toward positive infinity (ceil)
VRINTP.F32 Q0, Q1           @ Q0 = ceil(Q1)

@ Round toward negative infinity (floor)
VRINTM.F32 Q0, Q1           @ Q0 = floor(Q1)

@ Round to nearest away from zero
VRINTA.F32 Q0, Q1           @ Q0 = round(Q1), ties away from zero
```

## Performance Optimization Techniques

### Avoiding Pipeline Stalls

**Data Dependencies**

Minimize dependencies between consecutive SIMD instructions to allow parallel execution.

**Example:**

```assembly
@ Poor: sequential dependencies
VADD.I32 Q0, Q1, Q2         @ Q0 depends on Q1, Q2
VADD.I32 Q3, Q0, Q4         @ Q3 depends on Q0 (just computed)
VADD.I32 Q5, Q3, Q6         @ Q5 depends on Q3 (just computed)
@ Each instruction waits for previous result

@ Better: independent operations
VADD.I32 Q0, Q1, Q2         @ Independent
VADD.I32 Q3, Q7, Q8         @ Independent (doesn't need Q0)
VADD.I32 Q5, Q9, Q10        @ Independent (doesn't need Q0 or Q3)
VADD.I32 Q11, Q0, Q4        @ Now uses Q0 (more time has passed)
```

**Load/Use Latency**

Insert other work between loads and uses of loaded data.

**Example:**

```assembly
@ Poor: immediate use after load
VLD1.32 {Q0}, [r0]!
VADD.I32 Q1, Q0, Q2         @ Stalls waiting for load

@ Better: interleave other work
VLD1.32 {Q0}, [r0]!
VLD1.32 {Q3}, [r1]!         @ Another independent load
VMUL.I32 Q4, Q5, Q6         @ Work not depending on loads
VADD.I32 Q1, Q0, Q2         @ Now use loaded Q0
VADD.I32 Q7, Q3, Q8         @ Use loaded Q3
```

### Prefetching

**PLD/PLDW - Preload Data**

PLD hints to the memory system to fetch data before it's needed.

**Example:**

```assembly
@ Prefetch ahead in array processing
MOV r2, #64                 @ Prefetch distance
process_loop:
    PLD [r0, r2]            @ Prefetch 64 bytes ahead
    VLD1.32 {Q0}, [r0]!     @ Load current data
    @ Process Q0
    VADD.I32 Q0, Q0, Q1
    VST1.32 {Q0}, [r1]!
    SUBS r3, r3, #1
    BNE process_loop
```

### Register Blocking

Group operations on related data to maximize register reuse and minimize memory traffic.

**Example:**

```assembly
@ Matrix multiplication with register blocking
@ Process 4x4 block, keeping intermediate results in registers
VLD1.32 {Q0, Q1}, [r0]!     @ Load 2 rows of A matrix (8 values)
VLD1.32 {Q2, Q3}, [r1]      @ Load 2 rows of B matrix

@ Accumulate products into Q8-Q15 (result block)
VMLA.F32 Q8, Q0, Q2[0]      @ Multiply-accumulate
VMLA.F32 Q9, Q0, Q2[1]
VMLA.F32 Q10, Q0, Q2[2]
VMLA.F32 Q11, Q0, Q2[3]
VMLA.F32 Q12, Q1, Q3[0]
VMLA.F32 Q13, Q1, Q3[1]
VMLA.F32 Q14, Q1, Q3[2]
VMLA.F32 Q15, Q1, Q3[3]
@ Continue for full block...
```

### Cache-Friendly Access Patterns

**Sequential vs Strided Access**

Sequential memory access patterns utilize cache lines efficiently.

**Example:**

```assembly
@ Good: sequential access (cache-friendly)
VLD1.32 {Q0}, [r0]!         @ Load 16 consecutive bytes
VLD1.32 {Q1}, [r0]!         @ Next 16 bytes
@ Cache lines filled efficiently

@ Poor: large strides (cache-unfriendly)
VLD1.32 {Q0[0]}, [r0]       @ Load from address
ADD r0, r0, #1024
VLD1.32 {Q0[1]}, [r0]       @ Load 1KB away
@ May miss cache, load unnecessary data
```

**Tiling**

Process data in cache-sized tiles rather than full dimensions.

**Example:**

```assembly
@ Instead of processing entire row at once (may exceed cache):
@ for (i = 0; i < N; i++)
@     process_entire_row(i)

@ Use tiling (better cache utilization):
@ for (tile_i = 0; tile_i < N; tile_i += TILE)
@     for (tile_j = 0; tile_j < M; tile_j += TILE)
@         process_tile(tile_i, tile_j, TILE, TILE)
```

### Mixed Scalar-SIMD Code

Balance SIMD overhead with processing benefit. Short operations may not justify SIMD setup costs.

**Example:**

```assembly
@ For small counts, scalar might be faster
CMP r2, #8                  @ Check count
BLT scalar_path             @ Use scalar for < 8 elements

simd_path:
    @ SIMD processing for bulk of data
    LSR r3, r2, #2          @ Count / 4 (process 4 at a time)
simd_loop:
    VLD1.32 {D0}, [r0]!
    VADD.I32 D0, D0, D1
    VST1.32 {D0}, [r1]!
    SUBS r3, r3, #1
    BNE simd_loop
    
    @ Handle remainder with scalar
    AND r2, r2, #3          @ Remainder elements
    B scalar_path

scalar_path:
    @ Process remaining elements with scalar instructions
    CMP r2, #0
    BEQ done
scalar_loop:
    LDR r3, [r0], #4
    ADD r3, r3, r4
    STR r3, [r1], #4
    SUBS r2, r2, #1
    BNE scalar_loop
done:
```

**Key Points:**

- Vector comparisons (VCEQ/VCGT/VCGE) generate all-1s or all-0s masks for true/false results per lane
- VBSL performs bitwise selection using comparison masks to blend values from two sources
- Multi-structure loads/stores (VLD2/VLD3/VLD4) efficiently handle interleaved multi-channel data like audio or images
- Lane-specific loads enable gathering non-contiguous data into SIMD registers though less efficiently than direct gather instructions
- NEON supports single-precision floating-point operations with vector math functions including multiply-accumulate and fast reciprocal estimates
- VCVT handles conversions between integer and floating-point formats with optional fixed-point fractional bit specifications
- Pipeline optimization requires separating dependent operations and interleaving loads with computation to hide latency
- PLD prefetch instructions hint memory system to fetch data ahead of actual use, reducing load stalls
- Cache-friendly sequential access patterns significantly outperform strided access by maximizing cache line utilization
- Mixed scalar-SIMD code paths handle small data sizes with scalar operations to avoid SIMD setup overhead

**Important related topics:** ARM Mali GPU integration with CPU SIMD for heterogeneous computing, NEON code generation from intrinsics and auto-vectorization quality, thermal throttling effects on sustained SIMD workloads, power consumption trade-offs between scalar and SIMD implementations for battery-constrained devices, memory bandwidth limitations as SIMD bottleneck, ARMv8 crypto extensions for AES/SHA acceleration using SIMD registers, precision vs performance trade-offs in floating-point SIMD (FP16 vs FP32), SIMD debugging techniques and visualization tools.

---

# Optimization Techniques

Optimization in ARM assembly involves strategically organizing instructions, managing registers, and structuring code to maximize processor efficiency. These techniques exploit CPU architecture features like pipelines, register files, and execution units to reduce cycles and improve throughput.

## Instruction Scheduling

Instruction scheduling reorders instructions to minimize pipeline stalls and maximize instruction-level parallelism without changing program semantics.

### Pipeline Hazards and Dependencies

ARM processors use multi-stage pipelines (fetch, decode, execute, memory, writeback). Hazards occur when consecutive instructions create dependencies:

**Data Hazards**: RAW (Read After Write), WAR (Write After Read), WAW (Write After Write)

**Example** - Poor scheduling with RAW hazard:

```assembly
ADD r0, r1, r2    ; r0 = r1 + r2
SUB r3, r0, r4    ; Stalls waiting for r0
```

**Example** - Optimized scheduling:

```assembly
ADD r0, r1, r2    ; r0 = r1 + r2
LDR r5, [r6]      ; Independent instruction (fills bubble)
MUL r7, r8, r9    ; Independent instruction
SUB r3, r0, r4    ; r0 now ready
```

### Load/Store Latency Management

Memory operations have higher latency than register operations. Schedule independent instructions between load and use:

```assembly
; Unoptimized
LDR r0, [r1]
ADD r2, r0, r3    ; Stalls 2-3 cycles

; Optimized
LDR r0, [r1]
ADD r4, r5, r6    ; Independent work
MUL r7, r8, r9    ; More independent work
ADD r2, r0, r3    ; Load completed
```

### Branch Delay Considerations

ARM uses branch prediction, but mispredictions cost cycles. On older ARM architectures without branch prediction, filling branch delay slots was critical:

```assembly
CMP r0, #10
BNE loop
ADD r1, r1, #1    ; May execute before branch completes
```

Modern ARM processors benefit from keeping branches predictable and minimizing branch-dependent chains.

### Dual-Issue and Superscalar Scheduling

Modern ARM cores (Cortex-A series) can issue multiple instructions per cycle. Scheduling should enable parallel execution:

```assembly
; Can dual-issue on some ARM cores
ADD r0, r1, r2    ; ALU operation
LDR r3, [r4]      ; Load operation (different execution unit)
```

## Loop Unrolling

Loop unrolling reduces loop overhead by executing multiple iterations' worth of work per loop iteration, decreasing branch instructions and enabling better instruction scheduling.

### Basic Loop Unrolling

**Example** - Original loop:

```assembly
    MOV r0, #0        ; sum = 0
    MOV r1, #0        ; i = 0
    LDR r2, =array
    MOV r3, #100      ; count

loop:
    LDR r4, [r2, r1, LSL #2]
    ADD r0, r0, r4
    ADD r1, r1, #1
    CMP r1, r3
    BLT loop
```

**Example** - 4x unrolled:

```assembly
    MOV r0, #0
    MOV r1, #0
    LDR r2, =array
    MOV r3, #100

loop_unrolled:
    LDR r4, [r2, r1, LSL #2]
    LDR r5, [r2, r1, LSL #2 + 4]
    LDR r6, [r2, r1, LSL #2 + 8]
    LDR r7, [r2, r1, LSL #2 + 12]
    
    ADD r0, r0, r4
    ADD r0, r0, r5
    ADD r0, r0, r6
    ADD r0, r0, r7
    
    ADD r1, r1, #4
    CMP r1, r3
    BLT loop_unrolled
```

### Benefits

- Reduced branch overhead (75% fewer branches in 4x unroll)
- Better instruction-level parallelism (loads can overlap)
- More opportunities for instruction scheduling
- Reduced loop counter updates

### Handling Remainder Iterations

When loop count doesn't divide evenly by unroll factor:

```assembly
    MOV r3, #100
    AND r8, r3, #3      ; remainder = count & 3
    SUB r3, r3, r8      ; adjusted count for main loop

main_loop:
    ; 4x unrolled body
    ADD r1, r1, #4
    CMP r1, r3
    BLT main_loop

cleanup_loop:
    CMP r8, #0
    BEQ done
    LDR r4, [r2, r1, LSL #2]
    ADD r0, r0, r4
    ADD r1, r1, #1
    SUB r8, r8, #1
    B cleanup_loop

done:
```

### Optimal Unroll Factor

[Inference] Unroll factor depends on:

- Available registers (unrolling consumes more registers)
- Code cache size (excessive unrolling increases code size)
- Loop body complexity
- Typical loop iteration counts

Common unroll factors: 2x, 4x, 8x. Beyond 8x often shows diminishing returns due to code size and register pressure.

## Software Pipelining

Software pipelining overlaps multiple loop iterations by interleaving instructions from different iterations, maximizing resource utilization in loops with dependencies.

### Concept

Traditional loop execution completes iteration N before starting iteration N+1. Software pipelining starts iteration N+1 before N completes, overlapping independent operations.

### Stages: Prologue, Kernel, Epilogue

**Example** - Loop with load-compute-store pattern:

Original sequential:

```assembly
loop:
    LDR r0, [r1], #4     ; Load iteration N
    ADD r0, r0, #10      ; Compute iteration N
    STR r0, [r2], #4     ; Store iteration N
    SUBS r3, r3, #1
    BGT loop
```

Software pipelined:

```assembly
; Prologue: Prime the pipeline
    LDR r0, [r1], #4     ; Start iteration 0 load
    SUBS r3, r3, #1
    BLE epilogue
    
    LDR r4, [r1], #4     ; Start iteration 1 load
    ADD r0, r0, #10      ; Finish iteration 0 compute
    SUBS r3, r3, #1
    BLE epilogue_1

; Kernel: Steady state (overlapped iterations)
kernel:
    LDR r5, [r1], #4     ; Load iteration N+2
    ADD r4, r4, #10      ; Compute iteration N+1
    STR r0, [r2], #4     ; Store iteration N
    
    MOV r0, r4           ; Rotate registers
    MOV r4, r5
    
    SUBS r3, r3, #1
    BGT kernel

; Epilogue: Drain the pipeline
epilogue_1:
    ADD r4, r4, #10
    STR r0, [r2], #4
    
epilogue:
    STR r4, [r2], #4
```

### Modulo Scheduling

Modulo scheduling is a systematic software pipelining technique that schedules instructions into time slots modulo the initiation interval (II).

**Key Points:**

- Initiation Interval (II): Cycles between starting successive iterations
- Minimum II determined by resource constraints and recurrence cycles
- Instructions from different iterations occupy different time slots within II

**Example** - 3-stage pipeline with II=1:

```assembly
; Each cycle starts new iteration while completing previous ones
cycle_0:
    LDR r0, [r1], #4     ; Iteration N: stage 1
    ADD r4, r4, #10      ; Iteration N-1: stage 2  
    STR r8, [r2], #4     ; Iteration N-2: stage 3
```

### Register Rotation

Software pipelining requires multiple register versions for overlapped iterations:

```assembly
; Without rotation - needs many registers
LDR r0, [r1]    ; iter 0
LDR r1, [r1]    ; iter 1
LDR r2, [r1]    ; iter 2
; ...

; With rotation - reuse register names
; (some ARM cores support register renaming in hardware)
```

### Trade-offs

**Benefits:**

- Hides memory latency
- Increases instruction throughput
- Exploits instruction-level parallelism across iterations

**Costs:**

- Code size increase (prologue + epilogue)
- Register pressure (multiple live values)
- Complexity (difficult to hand-code, usually compiler-generated)

## Register Allocation Strategies

Register allocation assigns variables to limited physical registers to minimize memory accesses and maximize performance.

### ARM Register File

ARM provides 16 general-purpose registers (r0-r15):

- r0-r3: Argument passing and return values (caller-saved)
- r4-r11: Local variables (callee-saved)
- r12 (IP): Intra-procedure call scratch register
- r13 (SP): Stack pointer
- r14 (LR): Link register
- r15 (PC): Program counter

Effective registers for allocation: r0-r11 (12 registers)

### Local vs Global Allocation

**Local Allocation**: Within a basic block (straight-line code)

```assembly
; Simple assignment r0-r3 for temporaries
ADD r0, r1, r2
MUL r0, r0, r3
SUB r0, r0, r4
```

**Global Allocation**: Across basic blocks and function boundaries

```assembly
function:
    PUSH {r4-r7, lr}    ; Save callee-saved registers
    ; Use r4-r7 for important variables across calls
    BL other_function   ; r0-r3 may be clobbered
    ; r4-r7 still valid
    POP {r4-r7, pc}
```

### Live Range Analysis

A variable's live range spans from definition to last use. Overlapping live ranges require different registers:

**Example**:

```assembly
; Live ranges
ADD r0, r1, r2    ; r0 live starts
MUL r3, r4, r5    ; r3 live starts (r0 still live)
ADD r6, r0, r3    ; r0 last use (dies), r3 last use (dies), r6 live starts
SUB r7, r6, r8    ; r6 last use (dies)

; r0 and r3 need different registers (live ranges overlap)
; r0/r3 can reuse same registers as r6/r7 (ranges don't overlap)
```

### Graph Coloring Allocation

Register allocation modeled as graph coloring:

1. Build interference graph (nodes = variables, edges = overlapping live ranges)
2. Color graph with K colors (K = available registers)
3. Spill variables if graph not K-colorable

**Example** - Interference graph:

```
Variables: a, b, c, d
Interferences: a-b, b-c, c-d, a-c

Graph coloring with 3 registers (r0, r1, r2):
a -> r0
b -> r1  
c -> r2
d -> r0 (doesn't interfere with a)
```

### Spilling Strategies

When insufficient registers, spill variables to memory:

```assembly
; Register pressure - need 5 values, only 4 registers available
; Spill least-frequently-used variable (d)

    LDR r0, =a
    LDR r1, =b  
    LDR r2, =c
    ; d spilled to stack
    
    ADD r3, r0, r1
    MUL r3, r3, r2
    
    ; Reload d when needed
    LDR r4, [sp, #offset_d]
    SUB r3, r3, r4
```

**Spill Cost Metrics:**

- Usage frequency (spill infrequently-used variables)
- Loop nesting depth (avoid spilling in inner loops)
- Live range length (prefer spilling short-lived variables)

### Caller-Saved vs Callee-Saved Strategy

**Caller-Saved (r0-r3):**

- Fast for leaf functions (no preservation needed)
- Use for temporary values
- Assumed clobbered across function calls

**Callee-Saved (r4-r11):**

- Preserved across function calls
- Use for important variables in calling functions
- Require PUSH/POP overhead

**Example** - Register usage pattern:

```assembly
outer_function:
    PUSH {r4-r6, lr}
    MOV r4, r0        ; Save important arg in r4 (survives calls)
    MOV r5, #0        ; Counter in r5
    
loop:
    MOV r0, r4        ; Pass arg in r0 (caller-saved)
    BL inner_function ; r0-r3 clobbered
    ADD r5, r5, r0    ; r4-r5 still valid
    CMP r5, #100
    BLT loop
    
    MOV r0, r5        ; Return in r0
    POP {r4-r6, pc}
```

### Register Pressure Reduction

Techniques to reduce register demand:

**Value Recomputation:**

```assembly
; High pressure
LDR r0, =base
ADD r1, r0, #offset1
ADD r2, r0, #offset2
ADD r3, r0, #offset3

; Reduced pressure - recompute base
LDR r0, =base
ADD r1, r0, #offset1
; ... use r1 ...
LDR r0, =base      ; Recompute instead of keeping in register
ADD r2, r0, #offset2
```

**Memory Access Scheduling:**

```assembly
; Load values just-in-time rather than all upfront
LDR r0, [r1]
ADD r0, r0, #1
STR r0, [r1]       ; Store immediately to free r0

LDR r0, [r2]       ; Reuse r0 for next value
ADD r0, r0, #2
STR r0, [r2]
```

### Function-Specific Allocation

**Leaf Functions:** Can use caller-saved registers freely (no calls to preserve)

```assembly
leaf_function:
    ; Use r0-r3 aggressively, no PUSH/POP needed
    ADD r0, r0, r1
    MUL r0, r0, r2
    BX lr
```

**Non-Leaf Functions:** Strategically use callee-saved for persistence

```assembly
non_leaf:
    PUSH {r4-r7, lr}
    ; r4-r7 for values that survive multiple calls
    MOV r4, r0
    BL func1
    ADD r4, r4, r0
    BL func2  
    ADD r0, r4, r0
    POP {r4-r7, pc}
```

## Combined Optimization Example

**Example** - Optimized vector addition combining techniques:

```assembly
; Optimized: unrolling + scheduling + register allocation
vector_add_optimized:
    PUSH {r4-r11, lr}
    
    ; r0 = src1, r1 = src2, r2 = dst, r3 = count
    ; Unroll 4x, software pipeline loads
    
    SUBS r3, r3, #4
    BLT cleanup
    
    ; Prologue: Start pipeline
    LDR r4, [r0], #4     ; Load src1[0]
    LDR r5, [r1], #4     ; Load src2[0]
    
main_loop:
    LDR r6, [r0], #4     ; Load src1[1] (overlapped)
    LDR r7, [r1], #4     ; Load src2[1] (overlapped)
    ADD r4, r4, r5       ; Compute [0]
    
    LDR r8, [r0], #4     ; Load src1[2]
    LDR r9, [r1], #4     ; Load src2[2]
    ADD r6, r6, r7       ; Compute [1]
    STR r4, [r2], #4     ; Store [0]
    
    LDR r4, [r0], #4     ; Load src1[3] (reuse r4)
    LDR r5, [r1], #4     ; Load src2[3] (reuse r5)
    ADD r8, r8, r9       ; Compute [2]
    STR r6, [r2], #4     ; Store [1]
    
    ADD r4, r4, r5       ; Compute [3] (prepare for next iter)
    STR r8, [r2], #4     ; Store [2]
    
    SUBS r3, r3, #4
    BGE main_loop
    
    ; Epilogue: Drain pipeline
    STR r4, [r2], #4     ; Store [3]
    
cleanup:
    ADDS r3, r3, #4
    BEQ done
    
cleanup_loop:
    LDR r4, [r0], #4
    LDR r5, [r1], #4
    ADD r4, r4, r5
    STR r4, [r2], #4
    SUBS r3, r3, #1
    BGT cleanup_loop
    
done:
    POP {r4-r11, pc}
```

**Key Points:**

- 4x loop unrolling reduces branch overhead
- Software pipelining overlaps loads from next iteration with computation
- Register allocation uses r4-r9 for pipelined values
- Instruction scheduling intersperses loads, computes, stores
- Prologue/epilogue handle pipeline startup/shutdown
- Cleanup loop handles non-multiple-of-4 counts

**Important related topics:** NEON SIMD optimization, cache-aware algorithms, link-time optimization, profile-guided optimization, instruction fusion on modern ARM cores

---

## Strength Reduction

Strength reduction replaces expensive operations with cheaper equivalent operations to improve performance. In ARM assembly, this primarily involves replacing multiplication and division with shifts, additions, and subtractions.

**Multiplication by constants:**

- Multiplying by powers of 2: Replace `MUL` with logical shift left (`LSL`)
- Multiplying by (2^n ± 1): Use shift and add/subtract combinations
- Multiplying by small constants: Decompose into shifts and adds

**Division by constants:**

- Dividing by powers of 2: Replace with arithmetic shift right (`ASR`) for signed, logical shift right (`LSR`) for unsigned
- Dividing by other constants: Use multiplication by reciprocal (fixed-point arithmetic) when division hardware is unavailable or slow

**Common transformations:**

- `x * 2` → `x << 1` or `LSL x, x, #1`
- `x * 4` → `x << 2` or `LSL x, x, #2`
- `x * 3` → `x + (x << 1)` or `ADD x, x, x, LSL #1`
- `x * 5` → `x + (x << 2)` or `ADD x, x, x, LSL #2`
- `x * 7` → `x - (x << 3)` (negated) or `RSB x, x, x, LSL #3`
- `x / 8` → `x >> 3` or `ASR x, x, #3` (signed)

**Example:**

```asm
; Inefficient - multiplication
MOV r1, #12
MUL r0, r1, r0          ; r0 = r0 * 12

; Optimized - strength reduction
; 12 = 8 + 4 = (x << 3) + (x << 2)
ADD r0, r0, r0, LSL #2  ; r0 = r0 + (r0 * 4) = r0 * 5
ADD r0, r0, r0, LSL #1  ; r0 = r0 + (r0 * 2) = r0 * 3
; Alternative: 12 = 3 * 4
ADD r0, r0, r0, LSL #1  ; r0 = r0 * 3
LSL r0, r0, #2          ; r0 = r0 * 4
```

**Address calculation optimization:** Array indexing can be optimized using shifted register operands instead of separate multiplication:

```asm
; Accessing array[i] where each element is 4 bytes
; Inefficient
MOV r2, #4
MUL r2, r1, r2          ; offset = i * 4
LDR r0, [r0, r2]        ; load array[i]

; Optimized - use shifted register addressing
LDR r0, [r0, r1, LSL #2] ; load array[i], offset = i << 2
```

## Common Subexpression Elimination

Common subexpression elimination (CSE) identifies repeated calculations and stores the result for reuse, reducing redundant computation and register pressure.

**Manual CSE in assembly:** When the same calculation appears multiple times, compute once and store in a register:

**Example:**

```asm
; Without CSE - redundant calculations
LDR r0, [r4, #8]
ADD r1, r0, #10
MUL r1, r1, r2
STR r1, [r5]

LDR r0, [r4, #8]        ; Redundant load
ADD r3, r0, #10         ; Redundant calculation
MUL r3, r3, r6
STR r3, [r7]

; With CSE - calculate once, reuse
LDR r0, [r4, #8]
ADD r1, r0, #10         ; Common subexpression computed once
MUL r2, r1, r2          ; Use r1
STR r2, [r5]
MUL r3, r1, r6          ; Reuse r1
STR r3, [r7]
```

**Loop invariant code motion:** Move calculations that don't change between loop iterations outside the loop:

```asm
; Inefficient - calculation inside loop
loop:
    LDR r0, [r4], #4
    LDR r1, =constant
    ADD r1, r1, r5      ; Loop invariant - doesn't change
    MUL r0, r0, r1
    STR r0, [r6], #4
    SUBS r3, r3, #1
    BNE loop

; Optimized - move invariant outside
LDR r1, =constant
ADD r1, r1, r5          ; Computed once before loop
loop:
    LDR r0, [r4], #4
    MUL r0, r0, r1      ; Use precomputed value
    STR r0, [r6], #4
    SUBS r3, r3, #1
    BNE loop
```

**Address calculation reuse:**

```asm
; Calculate base address once
LDR r0, =array_base
ADD r0, r0, r1, LSL #2  ; base + offset

; Reuse for multiple accesses
LDR r2, [r0]            ; array[i]
LDR r3, [r0, #4]        ; array[i+1]
LDR r4, [r0, #8]        ; array[i+2]
```

## Branch Prediction Considerations

Modern ARM processors use branch prediction to maintain pipeline efficiency. Writing branch-friendly code improves performance by reducing misprediction penalties.

**Branch prediction basics:**

- Static prediction: Backward branches (loops) predicted taken, forward branches predicted not taken
- Dynamic prediction: Processor learns branch behavior patterns
- Misprediction penalty: 10-20+ cycles depending on pipeline depth

**Optimization strategies:**

**Eliminate branches with conditional execution (ARMv7 and earlier):**

```asm
; With branch - may cause pipeline stall
CMP r0, r1
BEQ skip
ADD r2, r2, #1
skip:
    ; continue

; Without branch - conditional execution
CMP r0, r1
ADDNE r2, r2, #1        ; Execute only if not equal
; continue immediately
```

**Use conditional select (ARMv8/AArch64):**

```asm
; Branch version
CMP x0, x1
BGT greater
MOV x2, x3
B done
greater:
    MOV x2, x4
done:

; Branchless version
CMP x0, x1
CSEL x2, x4, x3, GT     ; x2 = (x0 > x1) ? x4 : x3
```

**Arrange branches for predictability:**

```asm
; Loop branches - naturally predictable (backward, taken)
loop:
    ; loop body
    SUBS r0, r0, #1
    BNE loop            ; Highly predictable - taken most iterations

; Error handling - place unlikely path forward
CMP r0, #0
BEQ error_handler       ; Forward branch, predicted not taken
; Normal path continues
; ...
B continue
error_handler:
    ; Rarely executed code
continue:
```

**Avoid branches in tight loops:**

```asm
; Inefficient - branch inside loop
loop:
    LDR r0, [r4], #4
    CMP r0, #0
    BLE skip            ; Unpredictable branch
    ADD r1, r1, r0
skip:
    SUBS r2, r2, #1
    BNE loop

; Better - use conditional execution or predication
loop:
    LDR r0, [r4], #4
    CMP r0, #0
    ADDGT r1, r1, r0    ; No branch needed
    SUBS r2, r2, #1
    BNE loop
```

**Loop unrolling to reduce branch frequency:**

```asm
; Original - branch every iteration
loop:
    LDR r0, [r4], #4
    ADD r1, r1, r0
    SUBS r2, r2, #1
    BNE loop

; Unrolled - branch every 4 iterations
loop:
    LDR r0, [r4], #4
    ADD r1, r1, r0
    LDR r0, [r4], #4
    ADD r1, r1, r0
    LDR r0, [r4], #4
    ADD r1, r1, r0
    LDR r0, [r4], #4
    ADD r1, r1, r0
    SUBS r2, r2, #4
    BNE loop
```

**Branch hints (architecture-specific):** Some ARM implementations support branch hint instructions or encoding bits to guide prediction, though effectiveness varies [Inference - based on ARM architecture specifications, but prediction behavior is implementation-dependent].

## Cache-Aware Programming

Cache optimization focuses on maximizing data locality and minimizing cache misses to improve memory access performance.

**Cache hierarchy understanding:**

- L1 cache: 16-64KB, 1-4 cycle latency
- L2 cache: 256KB-2MB, 10-20 cycle latency
- L3 cache: 2-32MB, 30-50 cycle latency (if present)
- Main memory: 100-300+ cycle latency
- Cache line size: typically 64 bytes on ARM

**Spatial locality optimization:**

**Structure of arrays (SoA) vs Array of structures (AoS):**

```asm
; Array of Structures - poor cache utilization
; struct { int x, y, z, padding; } points[N];
; Processing only x coordinates loads unnecessary data
loop:
    LDR r0, [r4]        ; Load x (also brings y, z into cache)
    ADD r0, r0, #1
    STR r0, [r4], #16   ; Skip to next structure (wastes cache line)
    SUBS r1, r1, #1
    BNE loop

; Structure of Arrays - better cache utilization
; int x[N], y[N], z[N];
; Processing x coordinates uses cache efficiently
loop:
    LDR r0, [r4]        ; Load x (next elements in same cache line)
    ADD r0, r0, #1
    STR r0, [r4], #4    ; Sequential access
    SUBS r1, r1, #1
    BNE loop
```

**Sequential memory access patterns:**

```asm
; Good - sequential access (cache-friendly)
MOV r2, #0
loop:
    LDR r0, [r4, r2]    ; Sequential loads
    ; process r0
    ADD r2, r2, #4
    CMP r2, #1024
    BLT loop

; Poor - strided access with large stride
MOV r2, #0
loop:
    LDR r0, [r4, r2]    ; Stride of 256 bytes may cross cache lines
    ; process r0
    ADD r2, r2, #256
    CMP r2, #4096
    BLT loop
```

**Temporal locality optimization:**

**Data reuse within registers:**

```asm
; Poor - repeated loads from memory
loop:
    LDR r0, [r4]
    ADD r0, r0, #1
    STR r0, [r4]
    LDR r0, [r4]        ; Redundant load
    MUL r0, r0, r1
    STR r0, [r4]
    SUBS r2, r2, #1
    BNE loop

; Good - keep data in registers
loop:
    LDR r0, [r4]
    ADD r0, r0, #1
    MUL r0, r0, r1      ; Reuse r0 from register
    STR r0, [r4]
    SUBS r2, r2, #1
    BNE loop
```

**Cache line alignment:**

```asm
; Align frequently accessed data to cache line boundaries
.align 6                ; 64-byte alignment (2^6)
hot_data:
    .word 0, 0, 0, 0    ; Ensure critical data starts on cache line
    ; ...

; Align loop entry points
.align 4                ; 16-byte alignment for code
hot_loop:
    ; frequently executed loop body
    SUBS r0, r0, #1
    BNE hot_loop
```

**Prefetching (explicit):**

```asm
; ARM has PLD (Preload Data) instruction
; [Inference] PLD effectiveness depends on processor implementation
loop:
    PLD [r4, #64]       ; Prefetch data 64 bytes ahead
    LDR r0, [r4]
    ; process r0
    STR r0, [r5], #4
    ADD r4, r4, #4
    SUBS r1, r1, #1
    BNE loop
```

**Loop blocking/tiling for cache:**

```asm
; Matrix multiplication with blocking
; Instead of processing entire rows/columns,
; process small blocks that fit in cache

; Outer loops over blocks
outer_i:
    ; ...
outer_j:
    ; Inner loops process block elements
    ; Block size chosen to fit in L1 cache
    MOV r8, #0          ; block_i
block_loop_i:
    MOV r9, #0          ; block_j
block_loop_j:
        ; Actual computation on cache-resident data
        LDR r0, [r4]
        LDR r1, [r5]
        MLA r2, r0, r1, r2
        ; ...
        ADD r9, r9, #1
        CMP r9, #BLOCK_SIZE
        BLT block_loop_j
    ADD r8, r8, #1
    CMP r8, #BLOCK_SIZE
    BLT block_loop_i
```

**Avoid cache thrashing:**

```asm
; Problem: Multiple arrays with same cache set mapping
; If array_a[i] and array_b[i] map to same cache sets,
; they evict each other (conflict misses)

; Solution: Pad arrays or use different access patterns
.align 6
array_a:
    .space 4096
    .space 64           ; Padding to change cache mapping
array_b:
    .space 4096
```

**Write combining and memory barriers:**

```asm
; Buffered writes improve performance
STR r0, [r4]            ; May be buffered
STR r1, [r4, #4]
STR r2, [r4, #8]
DMB                     ; Memory barrier when ordering required

; Use appropriate barriers
DMB SY                  ; Full system barrier
DMB ISH                 ; Inner shareable domain
DSB                     ; Data synchronization barrier
ISB                     ; Instruction synchronization barrier
```

**Key Points:**

- Strength reduction saves cycles by replacing expensive operations with cheaper equivalents
- Common subexpression elimination reduces redundant calculations
- Branch prediction optimization minimizes pipeline stalls through predictable patterns and branchless code
- Cache-aware programming maximizes data locality and minimizes memory latency through sequential access, blocking, and alignment

**Important considerations:** For specific performance characteristics, actual measurements on target hardware are necessary as cache behavior, branch prediction accuracy, and instruction timing vary significantly across ARM processor implementations [Inference - based on documented architectural differences between ARM Cortex-A series, Cortex-M series, and custom implementations].

---

# ARM Thumb and Thumb-2

## Thumb Instruction Set Overview

Thumb is a compressed instruction set architecture designed by ARM to improve code density while maintaining reasonable performance. Introduced with the ARMv4T architecture, Thumb represents a subset of the full 32-bit ARM instruction set encoded into 16-bit instructions.

The primary motivation for Thumb was to address memory constraints and cost considerations in embedded systems. By reducing instruction size from 32 bits to 16 bits, Thumb achieves approximately 65-70% of the code size of equivalent ARM code while delivering roughly 85-90% of the performance. This trade-off made ARM processors more viable for applications with limited memory bandwidth or storage capacity.

Thumb instructions operate on a restricted register set, primarily using registers R0-R7 (low registers), though some instructions can access high registers R8-R15. The instruction set includes common operations such as data processing, load/store, branches, and basic arithmetic, but with reduced flexibility compared to full ARM instructions.

## 16-bit Instruction Encoding

Thumb's 16-bit encoding scheme packs essential instruction information into a compact format. The encoding uses various bit field arrangements depending on the instruction type:

**Format 1 - Move shifted register:**

```
15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0
|  Op  |   Offset5    |  Rs  |  Rd  |
```

**Format 2 - Add/subtract:**

```
15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0
| 0  0  0  1  1|I|Op| Rn/imm3|Rs|Rd|
```

**Format 3 - Move/compare/add/subtract immediate:**

```
15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0
| 0  0  1 | Op |  Rd  |   Offset8    |
```

The encoding constraints impose several limitations:

**Register Access:** Most instructions can only access R0-R7. Special instructions like ADD, CMP, and MOV can access high registers R8-R15, but with reduced functionality.

**Immediate Values:** Limited to small constants. Add/subtract operations support 3-bit immediates (0-7), while move/compare operations support 8-bit immediates (0-255).

**Conditional Execution:** Unlike ARM instructions where almost every instruction can be conditionally executed, only branch instructions support conditions in Thumb. This requires additional branch instructions for conditional code sequences.

**Shift Operations:** Cannot combine shifts with data processing in a single instruction. Shifts must be performed as separate instructions.

**Load/Store Offsets:** Restricted offset ranges. Word loads use 5-bit offsets (0-124 in steps of 4), halfword loads use 5-bit offsets (0-62 in steps of 2), and byte loads use 5-bit offsets (0-31).

**Example** of encoding density comparison:

ARM code (32-bit instructions):

```assembly
ADD  R0, R1, R2      ; 32 bits
SUB  R3, R4, #100    ; 32 bits
LDR  R5, [R6, #16]   ; 32 bits
```

Thumb code (16-bit instructions):

```assembly
ADD  R0, R1, R2      ; 16 bits
MOV  R0, #100        ; 16 bits (immediate split)
SUB  R3, R4, R0      ; 16 bits
LDR  R5, [R6, #16]   ; 16 bits
```

## Thumb-2 Mixed 16/32-bit Instructions

Thumb-2, introduced with ARMv6T2 and refined in ARMv7, extends the Thumb instruction set with 32-bit encodings to eliminate most performance gaps between Thumb and ARM states. This hybrid approach combines the code density benefits of 16-bit instructions with the functionality of 32-bit instructions.

**Architecture:** Thumb-2 is not a separate processor state. It operates in Thumb state but supports both 16-bit and 32-bit instruction encodings intermixed freely within the instruction stream.

**32-bit Thumb Instructions:** Encoded as two consecutive 16-bit halfwords. The first halfword (bits 15-11 = 11101, 11110, or 11111) indicates a 32-bit instruction follows. The processor fetches both halfwords before decoding.

**32-bit instruction format:**

```
First halfword (15-0):  |1 1 1|op1|op2|...
Second halfword (15-0): |op3|...
```

**Capability Restoration:** Thumb-2 32-bit instructions restore capabilities absent in 16-bit Thumb:

**Full Register Access:** All 32-bit instructions can access all registers R0-R15 without restrictions.

**IT Block (If-Then):** Replaces ARM's conditional execution. The IT instruction creates a conditional execution block for up to four following instructions:

```assembly
IT EQ           ; If equal
ADDEQ R0, R1, R2   ; Execute if Z flag set
```

The IT instruction specifies a condition and pattern for subsequent instructions:

```assembly
ITTTE NE        ; If-Then-Then-Else, Not Equal
ADDNE R0, R1, R2   ; Execute if NE
SUBNE R3, R4, R5   ; Execute if NE  
MOVNE R6, R7       ; Execute if NE
MOVEQ R8, R9       ; Execute if EQ (inverse)
```

**Complex Addressing Modes:** 32-bit loads/stores support pre-indexed, post-indexed, and offset addressing with larger offsets (±4095 bytes for LDR/STR).

**Modified Immediate Constants:** Thumb-2 uses a 12-bit encoding for immediate values that can generate a wider range of constants through rotation and replication patterns.

**Additional Instructions:** Includes DSP extensions (SIMD operations), saturating arithmetic, bit field manipulation (BFI, UBFX), divide instructions (SDIV, UDIV), and multiply-accumulate operations.

**Example** of Thumb-2 capability:

16-bit Thumb (limited):

```assembly
MOV  R0, #100       ; 16-bit: Limited immediate
ADD  R0, R0, R1     ; 16-bit: Simple add
LDR  R2, [R3, #32]  ; 16-bit: Small offset
```

Thumb-2 (enhanced):

```assembly
MOVW R0, #45000     ; 32-bit: Move 16-bit immediate
MLA  R0, R1, R2, R3 ; 32-bit: Multiply-accumulate
LDR  R2, [R3, #2048]!  ; 32-bit: Pre-indexed, large offset
UDIV R4, R5, R6     ; 32-bit: Unsigned divide
```

**Code Density Analysis:** [Inference based on architectural design] Thumb-2 typically achieves 95-98% of ARM code performance while maintaining code density close to original Thumb (approximately 70-75% of ARM code size). The processor dynamically uses 16-bit encodings where possible and 32-bit encodings where necessary.

## Mode Switching (ARM/Thumb Interworking)

ARM and Thumb interworking refers to the mechanism for transitioning between ARM state (32-bit instructions) and Thumb state (16-bit/mixed instructions). This capability is essential for hybrid systems where some code benefits from ARM's full instruction set while other code prioritizes Thumb's density.

**State Indication:** The processor's current state is determined by the T bit (bit 5) in the Current Program Status Register (CPSR):

- T=0: ARM state (32-bit instructions)
- T=1: Thumb state (16-bit/Thumb-2 instructions)

**Branch and Exchange (BX):** The primary instruction for mode switching. BX transfers control to an address and switches state based on bit 0 of the target address:

```assembly
BX   Rm    ; Branch to address in Rm, switch state based on Rm[0]
```

- If Rm[0] = 1: Switch to Thumb state, branch to Rm & 0xFFFFFFFE
- If Rm[0] = 0: Switch to ARM state, branch to Rm & 0xFFFFFFFC

**Example** of ARM to Thumb transition:

```assembly
; In ARM state
LDR  R0, =thumb_function + 1  ; Load Thumb function address with bit 0 set
BX   R0                         ; Branch and switch to Thumb

; In Thumb state now
thumb_function:
    PUSH {R4-R7, LR}
    ; Thumb code here
    POP  {R4-R7, PC}  ; Return switches back if LR bit 0 indicates ARM
```

**Branch with Link and Exchange (BLX):** Combines function call with state switching:

```assembly
BLX  Rm    ; Call function at Rm, switch state, save return address in LR
```

The return address saved in LR has bit 0 set to indicate the state to return to. When returning via `BX LR` or `POP {PC}`, the processor automatically switches back to the correct state.

**Example** of Thumb to ARM call:

```assembly
; In Thumb state
LDR  R0, =arm_function    ; Load ARM function address (bit 0 = 0)
BLX  R0                   ; Call ARM function, save return address

; In ARM state now
arm_function:
    STMFD SP!, {R4-R11, LR}
    ; ARM code here
    LDMFD SP!, {R4-R11, PC}  ; Return to Thumb (LR bit 0 = 1)
```

**Exception Handling:** When an exception occurs, the processor always enters the exception handler in ARM state (T bit cleared). Exception vectors reside in ARM code. To return to Thumb code after exception handling:

```assembly
; Exception handler (ARM state)
handler:
    ; Save context
    ; Handle exception
    ; Restore context
    SUBS PC, LR, #4   ; Return, automatically restore T bit from SPSR
```

The SUBS instruction with PC as destination restores the CPSR from SPSR, which includes the T bit reflecting the state before the exception.

**Interworking with Function Pointers:** Compilers store function pointers with bit 0 encoding the target state. This allows indirect calls to work correctly:

```assembly
LDR  R0, [R1]    ; Load function pointer
BLX  R0          ; Call function, automatically switch state if needed
```

**ARMv7 and Thumb-2:** In ARMv7 profiles with Thumb-2 support, many implementations deprecate or remove ARM state entirely (particularly Cortex-M series). These processors execute only Thumb/Thumb-2 instructions, eliminating mode switching overhead. The T bit remains set, and BX/BLX ignore bit 0 of the target address.

**Performance Considerations:** [Inference] Mode switching via BX/BLX typically incurs minimal overhead (1-2 cycles for pipeline flush on older cores). However, frequent switching can degrade performance due to instruction cache thrashing if ARM and Thumb code occupy different cache lines. Optimal designs minimize mode transitions by grouping ARM-state code and Thumb-state code into separate compilation units.

**Linker Support:** The linker must generate interworking veneers (small code sequences) when a call crosses state boundaries and the distance requires a long branch. These veneers handle the state switch:

```assembly
; Veneer for ARM to Thumb long call
arm_to_thumb_veneer:
    LDR  PC, =thumb_function + 1

; Veneer for Thumb to ARM long call  
thumb_to_arm_veneer:
    LDR  R12, =arm_function
    BX   R12
```

**Key Points:**

- Thumb provides 16-bit encoding for reduced memory footprint
- Thumb-2 adds 32-bit instructions to Thumb state, combining density with performance
- Mode switching uses BX/BLX instructions with bit 0 of target address indicating state
- Modern ARMv7-M processors execute only Thumb-2, eliminating ARM state entirely
- Interworking requires compiler and linker support for correct state transitions

---

## Code Density Benefits

Code density refers to the amount of functionality achieved per byte of instruction memory. Thumb instruction sets reduce code size significantly compared to the standard 32-bit ARM instruction set.

### Instruction Encoding Comparison

**ARM (32-bit):** Every instruction is 4 bytes

```assembly
; ARM mode - each instruction is 32 bits
ADD r0, r1, r2      ; 4 bytes
MOV r3, #100        ; 4 bytes
LDR r4, [r5]        ; 4 bytes
B target            ; 4 bytes
; Total: 16 bytes
```

**Thumb (16-bit):** Most instructions are 2 bytes

```assembly
; Thumb mode - most instructions are 16 bits
ADD r0, r1, r2      ; 2 bytes (low registers)
MOV r3, #100        ; 2 bytes
LDR r4, [r5]        ; 2 bytes
B target            ; 2 bytes (short range)
; Total: 8 bytes (50% reduction)
```

**Thumb-2 (mixed):** 16-bit and 32-bit instructions

```assembly
; Thumb-2 mode - intelligent mix
ADD r0, r1, r2      ; 2 bytes (common case)
MOVW r3, #1000      ; 4 bytes (wide immediate)
LDR r4, [r5, #128]  ; 2 bytes (offset in range)
B target            ; 2 or 4 bytes depending on distance
; Total: 10 bytes (37.5% reduction, more flexible)
```

### Code Size Statistics

[Inference] Typical code size reductions compared to ARM:

- **Thumb:** 65-70% of ARM code size (30-35% reduction)
- **Thumb-2:** 74-78% of ARM code size (22-26% reduction)
- Thumb-2 approaches Thumb density while maintaining near-ARM functionality

### Register Access Limitations in Thumb

Original Thumb (ARMv4T-ARMv6) restricts most operations to low registers (r0-r7):

```assembly
; Thumb - low register operations (16-bit)
ADD r0, r1, r2      ; Valid - all low registers
MOV r3, r4          ; Valid - low registers

; High register access requires special encoding
MOV r8, r0          ; Special encoding for high registers
ADD r0, r8, r9      ; Not available in original Thumb
```

Thumb-2 removes most register restrictions:

```assembly
; Thumb-2 - full register access
ADD r0, r1, r2      ; 16-bit encoding
ADD r8, r9, r10     ; 32-bit encoding (high registers)
ADD.W r0, r1, r2    ; 32-bit encoding (explicit wide)
```

### Memory Footprint Impact

**Example** - Function code size comparison:

```assembly
; ARM mode (32-bit instructions)
function_arm:
    PUSH {r4-r7, lr}        ; 4 bytes
    MOV r4, r0              ; 4 bytes
    MOV r5, r1              ; 4 bytes
    LDR r6, =constant       ; 4 bytes
    ADD r4, r4, r5          ; 4 bytes
    MUL r4, r4, r6          ; 4 bytes
    MOV r0, r4              ; 4 bytes
    POP {r4-r7, pc}         ; 4 bytes
; Total: 32 bytes

; Thumb-2 mode (mixed encoding)
function_thumb2:
    PUSH {r4-r7, lr}        ; 2 bytes
    MOV r4, r0              ; 2 bytes
    MOV r5, r1              ; 2 bytes
    LDR r6, =constant       ; 4 bytes (PC-relative load)
    ADD r4, r4, r5          ; 2 bytes
    MUL r4, r4, r6          ; 2 bytes
    MOV r0, r4              ; 2 bytes
    POP {r4-r7, pc}         ; 2 bytes
; Total: 18 bytes (43.75% reduction)
```

### Cache and Memory Benefits

**Instruction Cache Efficiency:**

- Smaller code fits more functions in cache
- Reduced cache misses improve performance
- [Inference] 30% code size reduction can increase effective cache capacity by ~43%

**Memory Bandwidth:**

- Fetching 16-bit instructions uses half the bus bandwidth
- Particularly beneficial for systems with narrow memory buses
- Flash memory systems benefit from reduced storage requirements

**Example** - Cache line utilization (32-byte cache line):

```
ARM mode (32-bit instructions):
- Fits 8 instructions per cache line
- Function needing 12 instructions = 2 cache lines

Thumb-2 mode (mostly 16-bit):
- Fits ~14-16 instructions per cache line  
- Same 12-instruction function = 1 cache line
- Reduced cache pollution
```

## Performance Tradeoffs

While Thumb provides code density, it involves performance compromises. Thumb-2 significantly reduces these penalties.

### Original Thumb Limitations

**Restricted Immediate Values:**

```assembly
; ARM - flexible immediates (12-bit encoded, rotated 8-bit values)
MOV r0, #0x12000000     ; Single instruction

; Thumb - 8-bit immediates only
MOV r0, #0xFF           ; Max 8-bit immediate
; For larger values, need multiple instructions or literal pool
LDR r0, =0x12000000     ; Literal pool access (slower)
```

**Limited Shift Operations:**

```assembly
; ARM - shift as part of instruction
ADD r0, r1, r2, LSL #3  ; Single instruction

; Thumb - separate shift required
LSL r2, r2, #3          ; Separate instruction
ADD r0, r1, r2          ; Then add
```

**Conditional Execution:**

```assembly
; ARM - predicated instructions
CMP r0, #10
ADDGT r1, r1, #1        ; Only executes if r0 > 10
ADDLE r2, r2, #1        ; Only executes if r0 <= 10

; Thumb - requires branches
CMP r0, #10
BLE skip
ADD r1, r1, #1
skip:
ADD r2, r2, #1
```

**Register Restrictions Impact:**

```assembly
; Need high registers but only low registers accessible
; Requires register shuffling

; ARM - direct access
ADD r8, r9, r10         ; Single instruction

; Thumb - register shuffling needed
MOV r0, r8              ; Move to low register
MOV r1, r9              ; Move to low register
ADD r0, r0, r1          ; Perform operation
MOV r8, r0              ; Move back
; 4 instructions instead of 1
```

### Thumb-2 Performance Improvements

Thumb-2 eliminates most Thumb limitations:

**Wide Immediates:**

```assembly
; Thumb-2 MOVW/MOVT for 32-bit immediates
MOVW r0, #0x1234        ; Lower 16 bits (2 bytes)
MOVT r0, #0x5678        ; Upper 16 bits (2 bytes)
; Result: r0 = 0x56781234

; Alternative for smaller values
MOV.W r0, #0xFF00       ; 32-bit encoding with flexible immediate
```

**Flexible Operands:**

```assembly
; Thumb-2 - shifted operands available
ADD.W r0, r1, r2, LSL #3    ; 32-bit instruction, same as ARM
```

**IT Blocks for Conditional Execution:**

```assembly
; Thumb-2 IT blocks (covered in detail below)
CMP r0, #10
ITE GT                  ; If-Then-Else
ADDGT r1, r1, #1        ; Executes if GT (Then)
ADDLE r2, r2, #1        ; Executes if LE (Else)
```

### Performance Metrics

**Execution Speed:**

- Thumb instructions may execute in same cycles as ARM equivalents
- Register shuffling and extra instructions add overhead in original Thumb
- Thumb-2 typically within 95-98% of ARM performance
- [Inference] Performance gap primarily from increased instruction count, not slower instruction execution

**Example** - Cycle count comparison (simplified model):

```assembly
; ARM version - 3 instructions, ~3 cycles
ADD r0, r1, r2, LSL #2    ; 1 cycle
LDR r3, [r0, #16]         ; 2 cycles (load latency)
ADD r4, r3, #100          ; 1 cycle

; Original Thumb - 5 instructions, ~5 cycles  
LSL r2, r2, #2            ; 1 cycle
ADD r0, r1, r2            ; 1 cycle
LDR r3, [r0, #16]         ; 2 cycles
MOV r4, r3                ; 1 cycle (if needed)
ADD r4, r4, #100          ; 1 cycle

; Thumb-2 - 3 instructions, ~3 cycles (matches ARM)
ADD.W r0, r1, r2, LSL #2  ; 1 cycle
LDR r3, [r0, #16]         ; 2 cycles
ADD r4, r3, #100          ; 1 cycle
```

### Branch Range Limitations

**Branch Distance:**

```assembly
; ARM - 24-bit signed offset (±32MB range)
B far_target            ; Can reach ±32MB

; Thumb - 11-bit signed offset (±2KB range)
B near_target           ; Limited to ±2KB
; For far branches, use:
BL far_target           ; Branch with link (longer range)
; Or veneer code

; Thumb-2 - 24-bit signed offset in BL/B.W
B.W far_target          ; Wide branch, ±16MB
```

**Veneer Code for Out-of-Range Branches:**

```assembly
; Original target out of Thumb branch range
B far_away              ; Won't reach

; Linker inserts veneer (trampoline)
B veneer                ; Branch to nearby veneer
; ...
veneer:
    LDR pc, =far_away   ; Indirect branch through veneer
```

### Mode Switching Overhead

Switching between ARM and Thumb modes incurs small overhead:

```assembly
; BX - Branch and Exchange (changes instruction set)
.arm                    ; ARM mode
    BX r0               ; Branch to address in r0, switch mode based on bit 0

.thumb                  ; Thumb mode
    BX r1               ; Branch to address in r1, switch mode

; Bit 0 of target address determines mode:
; 0 = ARM mode, 1 = Thumb mode
```

**Example** - Interworking between modes:

```assembly
.thumb
thumb_function:
    PUSH {lr}
    LDR r0, =arm_function
    ORR r0, r0, #0      ; Clear bit 0 for ARM mode
    BLX r0              ; Branch with link and exchange to ARM
    POP {pc}

.arm  
arm_function:
    ; ARM code here
    BX lr               ; Return (LR has correct mode bit)
```

[Inference] Mode switching overhead is typically 0-1 cycles on modern processors with mode prediction, but legacy processors may experience pipeline flush.

## IT Blocks (If-Then)

IT (If-Then) blocks enable conditional execution in Thumb-2, replacing ARM's predicated instruction capability within the Thumb instruction set.

### IT Instruction Syntax

```assembly
IT{x{y{z}}} cond

; x, y, z can be:
; T = Then (same condition as cond)
; E = Else (opposite condition)
; cond = condition code (EQ, NE, GT, LT, etc.)
```

### Basic IT Block Structure

**Example** - Simple conditional execution:

```assembly
; ARM predicated instructions
CMP r0, #10
ADDGT r1, r1, #1        ; Add if greater
MOVGT r2, #5            ; Move if greater

; Thumb-2 equivalent with IT
CMP r0, #10
ITT GT                  ; If-Then-Then (2 instructions, both GT)
ADDGT r1, r1, #1        ; Executes if GT
MOVGT r2, #5            ; Executes if GT
```

### IT Block Patterns

Up to 4 conditional instructions can follow IT:

```assembly
; IT - 1 instruction
CMP r0, #0
IT EQ
MOVEQ r1, #1

; ITT - 2 instructions (Then-Then)
CMP r0, #0
ITT NE
ADDNE r1, r1, #1
SUBNE r2, r2, #1

; ITE - 2 instructions (Then-Else)
CMP r0, #0  
ITE EQ
MOVEQ r1, #1            ; If equal
MOVNE r1, #0            ; If not equal

; ITTE - 3 instructions (Then-Then-Else)
CMP r0, #10
ITTE GT
ADDGT r1, r1, #1        ; If greater
MOVGT r2, #5            ; If greater
MOVLE r2, #0            ; If less or equal

; ITTEE - 4 instructions (Then-Then-Else-Else)
CMP r0, r1
ITTEE EQ
ADDEQ r2, r2, #1        ; If equal
MOVEQ r3, #1            ; If equal
ADDNE r2, r2, #2        ; If not equal
MOVNE r3, #0            ; If not equal
```

### Condition Codes in IT Blocks

All standard ARM condition codes work with IT:

```assembly
; EQ/NE - Equal/Not Equal
CMP r0, r1
ITE EQ
MOVEQ r2, #1
MOVNE r2, #0

; GT/LE - Greater Than/Less or Equal
CMP r0, #100
ITT GT
ADDGT r1, r1, #1
STRGT r1, [r2]

; GE/LT - Greater or Equal/Less Than
CMP r0, #0
ITE GE
MOVGE r3, r0
RSBLT r3, r0, #0        ; Negate if negative

; HI/LS - Higher/Lower or Same (unsigned)
CMP r0, #255
IT HI
MOVHI r0, #255          ; Clamp to maximum

; CS/CC - Carry Set/Carry Clear
ADDS r0, r1, r2
IT CS
MOVCS r3, #1            ; Set overflow flag
```

### IT Block Restrictions

**Critical restrictions:**

- All instructions in IT block must match pattern (T or E as specified)
- Cannot branch into middle of IT block
- Cannot use IT inside another IT block
- IT blocks cannot contain other IT instructions
- Some instructions prohibited in IT blocks (explained below)

**Example** - Invalid IT usage:

```assembly
; INVALID - condition mismatch
CMP r0, #10
ITT GT
ADDGT r1, r1, #1        ; OK - matches GT
ADDLE r2, r2, #1        ; ERROR - should be GT, not LE

; INVALID - branch into IT block
CMP r0, #10
ITT GT
target:                 ; ERROR - cannot branch here
    ADDGT r1, r1, #1
    MOVGT r2, #5

; INVALID - nested IT
CMP r0, #10
ITT GT
    IT NE               ; ERROR - IT inside IT block
    ADDNE r1, r1, #1
    MOVGT r2, r5
```

### Instructions Prohibited in IT Blocks

Certain instructions cannot appear in IT blocks on ARMv8-A and later (Thumb-2 only):

```assembly
; These instructions CANNOT be in IT blocks (ARMv8-A):
IT EQ
PUSHEQ {r0-r3}          ; ERROR - PUSH not allowed
POPEQ {r0-r3}           ; ERROR - POP not allowed
LDM/STM                 ; ERROR - Load/Store Multiple not allowed
CB(N)Z                  ; ERROR - Compare and Branch not allowed

; Must use branches instead:
CMP r0, #0
BNE skip
PUSH {r0-r3}
skip:
```

### Combining IT with Barrel Shifter

**Example** - Conditional operations with shifts:

```assembly
CMP r0, #0
ITTE GT
ADDGT.W r1, r2, r3, LSL #2      ; 32-bit encoding for shift
MOVGT r4, #100
MOVLE r4, #0
```

### Performance Considerations

**Benefits:**

- Eliminates branch misprediction penalties for simple conditionals
- Reduces code size compared to branch-based conditionals
- Maintains straight-line code flow

**Limitations:**

- IT blocks limited to 4 instructions (longer conditionals need branches)
- [Inference] Some processors may have lower throughput for IT blocks
- Overuse can reduce code readability

**Example** - Performance comparison:

```assembly
; Branch-based (may mispredict)
CMP r0, #10
BLE skip
ADD r1, r1, #1
MOV r2, #5
skip:
; 2-3 instructions if taken, potential branch penalty

; IT block (no branch)
CMP r0, #10
ITT GT
ADDGT r1, r1, #1
MOVGT r2, #5
; Always 3 instructions, no branch penalty
```

### Practical IT Block Examples

**Example** - Absolute value:

```assembly
; Compute absolute value of r0
CMP r0, #0
IT LT
RSBLT r0, r0, #0        ; Negate if negative
```

**Example** - Min/Max operations:

```assembly
; r0 = max(r0, r1)
CMP r0, r1
IT LT
MOVLT r0, r1            ; If r0 < r1, r0 = r1

; r0 = min(r0, r1)  
CMP r0, r1
IT GT
MOVGT r0, r1            ; If r0 > r1, r0 = r1
```

**Example** - Saturating arithmetic:

```assembly
; Saturate r0 to range [0, 255]
CMP r0, #0
ITE LT
MOVLT r0, #0            ; Clamp to 0 if negative
CMPGE r0, #255          ; Compare if >= 0
IT GT
MOVGT r0, #255          ; Clamp to 255 if > 255
```

**Example** - Conditional update pattern:

```assembly
; Update counter if condition met
LDR r0, [r1]            ; Load value
CMP r0, #100
ITTE LT
ADDLT r0, r0, #1        ; Increment if < 100
STRLT r0, [r1]          ; Store if < 100
MOVGE r0, #100          ; Cap at 100
```

## Thumb-2 Exclusive Instructions

Thumb-2 introduces instructions not available in standard ARM or original Thumb, optimized for specific operations.

### Bit Field Instructions

**BFI - Bit Field Insert:**

```assembly
; BFI Rd, Rn, #lsb, #width
; Insert bits from Rn into Rd at position lsb, width bits

; Example: Insert bits [7:0] of r1 into bits [15:8] of r0
BFI r0, r1, #8, #8

; Before: r0 = 0xXXXXYYYY, r1 = 0xZZZZWWAA
; After:  r0 = 0xXXXXAAYY (AA inserted at bit 8)
```

**BFC - Bit Field Clear:**

```assembly
; BFC Rd, #lsb, #width
; Clear width bits starting at lsb position

; Example: Clear bits [11:4]
BFC r0, #4, #8

; Before: r0 = 0xABCDEF12
; After:  r0 = 0xABCD0002 (bits [11:4] cleared)
```

**UBFX - Unsigned Bit Field Extract:**

```assembly
; UBFX Rd, Rn, #lsb, #width
; Extract width bits from Rn starting at lsb, zero-extended

; Example: Extract bits [19:12] from r0 to r1
UBFX r1, r0, #12, #8

; Before: r0 = 0xABCDEF12
; After:  r1 = 0x000000DE (extracted and zero-extended)
```

**SBFX - Signed Bit Field Extract:**

```assembly
; SBFX Rd, Rn, #lsb, #width  
; Extract width bits from Rn starting at lsb, sign-extended

; Example: Extract signed 8-bit value from bits [15:8]
SBFX r1, r0, #8, #8

; Before: r0 = 0x0000FF00
; After:  r1 = 0xFFFFFFFF (extracted 0xFF, sign-extended)
```

**Example** - Practical bit field usage:

```assembly
; Pack RGB values into single 32-bit word
; r0 = red (8-bit), r1 = green (8-bit), r2 = blue (8-bit)
; Result: [31:24]=alpha, [23:16]=red, [15:8]=green, [7:0]=blue

MOV r3, #0xFF           ; Alpha = 255
BFI r3, r0, #16, #8     ; Insert red at bit 16
BFI r3, r1, #8, #8      ; Insert green at bit 8  
BFI r3, r2, #0, #8      ; Insert blue at bit 0
; r3 now contains 0xFFRRGGBB

; Unpack RGB
UBFX r0, r3, #16, #8    ; Extract red
UBFX r1, r3, #8, #8     ; Extract green
UBFX r2, r3, #0, #8     ; Extract blue
```

### Divide Instructions

Thumb-2 includes hardware divide instructions (on cores with divide support):

**UDIV - Unsigned Divide:**

```assembly
; UDIV Rd, Rn, Rm
; Rd = Rn / Rm (unsigned)

UDIV r0, r1, r2         ; r0 = r1 / r2 (unsigned)

; Example: Divide 100 by 3
MOV r1, #100
MOV r2, #3
UDIV r0, r1, r2         ; r0 = 33
```

**SDIV - Signed Divide:**

```assembly
; SDIV Rd, Rn, Rm
; Rd = Rn / Rm (signed)

SDIV r0, r1, r2         ; r0 = r1 / r2 (signed)

; Example: Divide -100 by 3
MOV r1, #-100
MOV r2, #3
SDIV r0, r1, r2         ; r0 = -33
```

**Computing Remainder:**

```assembly
; Hardware doesn't provide remainder directly
; Compute manually: remainder = dividend - (quotient * divisor)

; r0 = r1 % r2 (unsigned modulo)
UDIV r3, r1, r2         ; r3 = r1 / r2
MLS r0, r3, r2, r1      ; r0 = r1 - (r3 * r2)

; Example: 17 % 5 = 2
MOV r1, #17
MOV r2, #5
UDIV r3, r1, r2         ; r3 = 3
MLS r0, r3, r2, r1      ; r0 = 17 - (3 * 5) = 2
```

**Example** - Division with error checking:

```assembly
; Safe divide with zero check
CMP r2, #0
ITE NE
UDIVNE r0, r1, r2       ; Divide if denominator != 0
MOVNE r0, #0            ; Return 0 if divide by zero
```

**Performance:** [Inference] Hardware divide typically takes 2-12 cycles depending on core, vastly faster than software division routines which can take 50+ cycles.

### Multiply and Accumulate Enhancements

**MLS - Multiply and Subtract:**

```assembly
; MLS Rd, Rn, Rm, Ra
; Rd = Ra - (Rn * Rm)

MLS r0, r1, r2, r3      ; r0 = r3 - (r1 * r2)

; Example: Compute remainder in single instruction
; remainder = dividend - (quotient * divisor)
MOV r1, #17             ; dividend
MOV r2, #5              ; divisor
UDIV r3, r1, r2         ; quotient = 3
MLS r0, r3, r2, r1      ; r0 = 17 - (3 * 5) = 2
```

**SMMUL/SMMLA/SMMLS - Signed Most Significant Word Multiply:**

```assembly
; SMMUL Rd, Rn, Rm
; Rd = (Rn * Rm) >> 32 (top 32 bits of 64-bit signed product)

SMMUL r0, r1, r2        ; High 32 bits of r1 * r2

; Useful for fixed-point arithmetic
; Example: Multiply two Q31 fixed-point numbers
; Result in Q31 format (1 sign bit, 31 fraction bits)
SMMUL r0, r1, r2        ; High word = Q31 * Q31 result
```

**SMMLA - Signed Most Significant Word Multiply Accumulate:**

```assembly
; SMMLA Rd, Rn, Rm, Ra
; Rd = Ra + ((Rn * Rm) >> 32)

SMMLA r0, r1, r2, r3    ; r0 = r3 + high32(r1 * r2)
```

### Compare and Branch Instructions

**CBZ/CBNZ - Compare and Branch on Zero/Non-Zero:**

```assembly
; CBZ Rn, label
; Branch to label if Rn == 0 (does not affect flags)

CBZ r0, target          ; Branch if r0 == 0
; ... code if r0 != 0 ...
target:

; CBNZ Rn, label  
; Branch to label if Rn != 0

CBNZ r1, loop           ; Branch if r1 != 0
```

**Benefits:**

- Single instruction (compare + branch combined)
- Doesn't modify condition flags
- Smaller code size than CMP + Bcc
- Limited range (±126 bytes forward/backward)

**Example** - Loop countdown:

```assembly
; Traditional
loop:
    ; ... loop body ...
    SUBS r0, r0, #1
    BNE loop

; With CBNZ (doesn't affect flags if needed elsewhere)
loop:
    ; ... loop body ...
    SUB r0, r0, #1      ; Don't update flags
    CBNZ r0, loop       ; Branch if non-zero
```

**Example** - Null pointer check:

```assembly
; Traditional
CMP r0, #0
BEQ handle_null
; ... process pointer in r0 ...
handle_null:

; With CBZ (more compact)
CBZ r0, handle_null
; ... process pointer in r0 ...
handle_null:
```

### Table Branch Instructions

**TBB/TBH - Table Branch Byte/Halfword:**

```assembly
; TBB [Rn, Rm]
; Branch forward by byte offset in table at [Rn + Rm]

; TBH [Rn, Rm, LSL #1]
; Branch forward by halfword offset in table

; Example: Jump table for switch statement
; r0 = case value (0-3)

    TBB [pc, r0]            ; Branch using byte table
branch_table:
    .byte (case0 - branch_table) / 2
    .byte (case1 - branch_table) / 2
    .byte (case2 - branch_table) / 2
    .byte (case3 - branch_table) / 2

case0:
    ; Handle case 0
    B end_switch
case1:
    ; Handle case 1
    B end_switch
case2:
    ; Handle case 2
    B end_switch
case3:
    ; Handle case 3
end_switch:
```

**Example** - Halfword table for larger ranges:

```assembly
; TBH for switch with distant targets
    CMP r0, #4
    BHS default_case        ; Bounds check
    TBH [pc, r0, LSL #1]    ; r0 * 2 for halfword indexing

branch_table:
    .hword (case0 - branch_table) / 2
    .hword (case1 - branch_table) / 2
    .hword (case2 - branch_table) / 2
    .hword (case3 - branch_table) / 2

; Cases can be up to 128KB away with TBH
```

**Benefits:**

- Compact jump table representation
- Single cycle dispatch (table already in cache)
- Efficient for dense switch statements

### Load/Store Dual

**LDRD/STRD - Load/Store Register Dual:**

```assembly
; LDRD Rt, Rt2, [Rn, #imm]
; Load two consecutive registers from memory

LDRD r0, r1, [r2]       ; r0 = [r2], r1 = [r2+4]
LDRD r0, r1, [r2, #16]  ; r0 = [r2+16], r1 = [r2+20]

; STRD Rt, Rt2, [Rn, #imm]
; Store two consecutive registers to memory

STRD r0, r1, [r2]       ; [r2] = r0, [r2+4] = r1
STRD r0, r1, [r2, #16]  ; [r2+16] = r0, [r2+20] = r1
```

**Example** - 64-bit value handling:

```assembly
; Load 64-bit timestamp
LDR r0, =timestamp_addr
LDRD r2, r3, [r0]       ; r2 = lower 32 bits, r3 = upper 32 bits

; Manipulate 64-bit value
ADDS r2, r2, #1000      ; Add to lower word
ADC r3, r3, #0          ; Add carry to upper word

; Store back
STRD r2, r3, [r0]
```

**Benefits:**

- Single instruction for two word transfers
- [Inference] May complete in single memory cycle on 64-bit buses
- Useful for structure copying and 64-bit operations

### Move Wide Instructions

**MOVW - Move 16-bit immediate:**

```assembly
; MOVW Rd, #imm16
; Move 16-bit immediate to register, zero upper bits

MOVW r0, #0x1234        ; r0 = 0x00001234
MOVW r1, #0xABCD        ; r1 = 0x0000ABCD
```

**MOVT - Move Top:**

```assembly
; MOVT Rd, #imm16
; Move 16-bit immediate to upper half of register, preserve lower bits

MOVW r0, #0x1234        ; r0 = 0x00001234
MOVT r0, #0x5678        ; r0 = 0x56781234
```

### Push/Pop Enhancements

Thumb-2 extends push/pop with more flexible register lists:

**Example** - Extended register lists:

```assembly
; Original Thumb - limited register ranges
PUSH {r0-r3}            ; Low registers only
PUSH {lr}               ; Can include LR

; Thumb-2 - flexible combinations
PUSH {r0, r2, r4, r6}   ; Non-contiguous registers
PUSH {r4-r11, lr}       ; Large range with LR
POP {r4-r11, pc}        ; Pop and return

; Save/restore scratch and preserved registers
function:
    PUSH {r0-r3, r4-r7, lr}     ; Save arguments and locals
    ; ... function body ...
    POP {r0-r3, r4-r7, pc}      ; Restore and return
```

**Example** - Preserving specific registers:

```assembly
; Only save registers that will be modified
function_efficient:
    PUSH {r4, r6, lr}       ; Only save what's needed
    ; Use r4, r6 in function
    ; r5, r7-r11 not used, don't save
    POP {r4, r6, pc}
```

### Reverse Byte Order Instructions

**REV - Reverse Byte Order (32-bit):**

```assembly
; REV Rd, Rn
; Reverse byte order in 32-bit word

MOV r0, #0x12345678
REV r1, r0              ; r1 = 0x78563412

; Endianness conversion
LDR r0, [r1]            ; Load big-endian value
REV r0, r0              ; Convert to little-endian
```

**REV16 - Reverse Bytes in Halfwords:**

```assembly
; REV16 Rd, Rn
; Reverse bytes within each halfword independently

MOV r0, #0x12345678
REV16 r1, r0            ; r1 = 0x34127856
; [31:24] ↔ [23:16], [15:8] ↔ [7:0]
```

**REVSH - Reverse Bytes in Signed Halfword:**

```assembly
; REVSH Rd, Rn
; Reverse bytes in lower halfword, sign-extend to 32 bits

MOV r0, #0x0000FF80
REVSH r1, r0            ; r1 = 0xFFFF80FF (reversed and sign-extended)
```

**Example** - Network byte order conversion:

```assembly
; Convert network (big-endian) to host (little-endian)
LDR r0, [r1]            ; Load 32-bit network order
REV r0, r0              ; Convert to host order

; Convert 16-bit port number
LDRH r0, [r1]           ; Load 16-bit value
REV16 r0, r0            ; Swap bytes
UXTH r0, r0             ; Zero-extend to 32 bits
```

### Select Bytes Instruction

**SEL - Select Bytes:**

```assembly
; SEL Rd, Rn, Rm
; Select bytes from Rn or Rm based on GE flags

; GE flags set by SIMD instructions or explicitly:
; If GE[3] set: Rd[31:24] = Rn[31:24], else Rm[31:24]
; If GE[2] set: Rd[23:16] = Rn[23:16], else Rm[23:16]
; If GE[1] set: Rd[15:8] = Rn[15:8], else Rm[15:8]
; If GE[0] set: Rd[7:0] = Rn[7:0], else Rm[7:0]

; Set GE flags
SADD8 r4, r0, r1        ; Parallel add, sets GE flags

; Select bytes based on GE flags
SEL r5, r2, r3          ; Select from r2 or r3 per byte
```

**Example** - Byte-wise max operation:

```assembly
; Compute max of each byte independently
; r0 = {a3, a2, a1, a0}, r1 = {b3, b2, b1, b0}

USUB8 r2, r0, r1        ; Sets GE where r0[i] >= r1[i]
SEL r3, r0, r1          ; r3[i] = max(r0[i], r1[i])
```

### Saturating Instructions

**SSAT/USAT - Signed/Unsigned Saturate:**

```assembly
; SSAT Rd, #imm, Rn {,shift}
; Saturate Rn to signed range [-(2^(imm-1)), 2^(imm-1)-1]

MOV r0, #1000
SSAT r1, #8, r0         ; Saturate to 8-bit signed [-128, 127]
                        ; r1 = 127 (clamped)

MOV r0, #-200
SSAT r1, #8, r0         ; r1 = -128 (clamped)

; USAT Rd, #imm, Rn {,shift}
; Saturate Rn to unsigned range [0, 2^imm-1]

MOV r0, #1000
USAT r1, #8, r0         ; Saturate to 8-bit unsigned [0, 255]
                        ; r1 = 255 (clamped)

MOV r0, #-10
USAT r1, #8, r0         ; r1 = 0 (negative clamped to 0)
```

**Example** - Audio sample clamping:

```assembly
; Clamp audio sample to 16-bit signed range
; r0 = processed sample (may overflow)

SSAT r0, #16, r0        ; Clamp to [-32768, 32767]
```

**Example** - RGB color clamping:

```assembly
; Clamp color component to [0, 255]
ADD r0, r0, r1          ; May exceed 255
USAT r0, #8, r0         ; Clamp to unsigned 8-bit
```

### Parallel Add/Subtract Instructions

**SADD8/USADD8 - Parallel Byte Add:**

```assembly
; SADD8 Rd, Rn, Rm
; Add four bytes in parallel (signed, sets GE flags)

; r0 = {10, 20, 30, 40}
; r1 = { 5, 10, 15, 20}
SADD8 r2, r0, r1        ; r2 = {15, 30, 45, 60}
                        ; GE flags set for each byte

; USADD8 Rd, Rn, Rm  
; Add four bytes in parallel (unsigned, saturating)

MOV r0, #0xFFFFFFFF     ; All bytes = 255
MOV r1, #0x01010101     ; All bytes = 1
USADD8 r2, r0, r1       ; r2 = 0xFFFFFFFF (saturated)
```

**SADD16/USADD16 - Parallel Halfword Add:**

```assembly
; SADD16 Rd, Rn, Rm
; Add two halfwords in parallel

; r0 = {1000, 2000} (16-bit values)
; r1 = { 500, 1500}
SADD16 r2, r0, r1       ; r2 = {1500, 3500}
```

**SSUB8/USUB8 - Parallel Byte Subtract:**

```assembly
; SSUB8 Rd, Rn, Rm
; Subtract four bytes in parallel (sets GE flags)

SSUB8 r2, r0, r1        ; r2[i] = r0[i] - r1[i] for each byte
                        ; GE[i] set if r0[i] >= r1[i]
```

**Example** - SIMD-style operations without NEON:

```assembly
; Process 4 pixels simultaneously (8-bit grayscale)
; Add brightness to each pixel

LDR r0, [r1]            ; Load 4 pixels: {p3, p2, p1, p0}
MOV r2, #0x10101010     ; Brightness delta: {16, 16, 16, 16}
UQADD8 r0, r0, r2       ; Saturating add to each pixel
STR r0, [r1]            ; Store back
```

### Packing and Unpacking Instructions

**PKHBT - Pack Halfword Bottom Top:**

```assembly
; PKHBT Rd, Rn, Rm {,LSL #imm}
; Rd[15:0] = Rn[15:0], Rd[31:16] = Rm[31:16] << imm

MOV r0, #0x12345678     ; r0 = 0x12345678
MOV r1, #0xABCDEF00     ; r1 = 0xABCDEF00
PKHBT r2, r0, r1        ; r2 = 0xABCD5678
                        ; Takes bottom of r0, top of r1
```

**PKHTB - Pack Halfword Top Bottom:**

```assembly
; PKHTB Rd, Rn, Rm {,ASR #imm}
; Rd[31:16] = Rn[31:16], Rd[15:0] = (Rm >> imm)[15:0]

MOV r0, #0x12345678     ; r0 = 0x12345678
MOV r1, #0xABCDEF00     ; r1 = 0xABCDEF00
PKHTB r2, r0, r1        ; r2 = 0x1234EF00
                        ; Takes top of r0, bottom of r1
```

**Example** - Combine two 16-bit values:

```assembly
; r0 = x coordinate (16-bit)
; r1 = y coordinate (16-bit)
; Combine into single 32-bit value

PKHBT r2, r0, r1, LSL #16   ; r2 = (y << 16) | x
```

**Example** - Extract and combine color components:

```assembly
; Extract green/blue, combine with new red/alpha
UBFX r0, r3, #0, #16    ; Extract GB (bits 15:0)
PKHBT r4, r0, r1, LSL #16   ; Combine with new RA
```

### Saturating Add/Subtract Instructions

**QADD/QSUB - Saturating Add/Subtract (32-bit):**

```assembly
; QADD Rd, Rn, Rm
; Rd = saturate(Rn + Rm) to 32-bit signed range

MOV r0, #0x7FFFFFFF     ; Max positive 32-bit
MOV r1, #100
QADD r2, r0, r1         ; r2 = 0x7FFFFFFF (saturated)
                        ; Sets Q flag in CPSR if saturated

; QSUB Rd, Rn, Rm
; Rd = saturate(Rn - Rm)

MOV r0, #0x80000000     ; Max negative 32-bit
MOV r1, #100
QSUB r2, r0, r1         ; r2 = 0x80000000 (saturated)
```

**QDADD/QDSUB - Saturating Double and Add/Subtract:**

```assembly
; QDADD Rd, Rn, Rm
; Rd = saturate(Rn + saturate(Rm * 2))

; QDSUB Rd, Rn, Rm
; Rd = saturate(Rn - saturate(Rm * 2))

; Useful for DSP algorithms
```

**QADD16/QSUB16 - Saturating Parallel Halfword Operations:**

```assembly
; QADD16 Rd, Rn, Rm
; Saturating add of two packed halfwords

; r0 = {32000, 10000}
; r1 = { 1000, 30000}
QADD16 r2, r0, r1       ; r2 = {32767, 32767} (both saturated)
```

**QADD8/QSUB8 - Saturating Parallel Byte Operations:**

```assembly
; QADD8 Rd, Rn, Rm
; Saturating add of four packed bytes

MOV r0, #0x7F7F7F7F     ; All bytes = 127
MOV r1, #0x01010101     ; All bytes = 1
QADD8 r2, r0, r1        ; r2 = 0x7F7F7F7F (all saturated at 127)
```

### Leading Zero Count

**CLZ - Count Leading Zeros:**

```assembly
; CLZ Rd, Rn
; Count number of leading zero bits in Rn

MOV r0, #0x00001000     ; Binary: 0000...0001000000000000
CLZ r1, r0              ; r1 = 19 (19 leading zeros)

MOV r0, #0x80000000     ; Binary: 1000...0000
CLZ r1, r0              ; r1 = 0 (no leading zeros)

MOV r0, #0
CLZ r1, r0              ; r1 = 32 (all zeros)
```

**Example** - Computing log2 (integer):**

```assembly
; Compute floor(log2(x)) for x > 0
; log2(x) = 31 - CLZ(x)

MOV r0, #1000           ; Input value
CLZ r1, r0              ; Count leading zeros
RSB r2, r1, #31         ; r2 = 31 - r1 = floor(log2(1000)) = 9
                        ; 2^9 = 512, 2^10 = 1024
```

**Example** - Normalizing values:**

```assembly
; Normalize 32-bit value (shift left until MSB is 1)
CLZ r1, r0              ; Count leading zeros
LSL r0, r0, r1          ; Shift to normalize
; r1 contains normalization shift amount
```

**Example** - Priority encoder:**

```assembly
; Find highest set bit position
; bit_position = 31 - CLZ(value)

MOV r0, #0x00040000     ; Bit 18 set
CLZ r1, r0              ; r1 = 13
RSB r2, r1, #31         ; r2 = 18 (position of highest bit)
```

### Sum of Absolute Differences

**USAD8 - Unsigned Sum of Absolute Differences:**

```assembly
; USAD8 Rd, Rn, Rm
; Rd = |Rn[31:24] - Rm[31:24]| + |Rn[23:16] - Rm[23:16]| +
;      |Rn[15:8] - Rm[15:8]| + |Rn[7:0] - Rm[7:0]|

; Example: Compare pixel similarity (4 bytes)
LDR r0, [r1]            ; Load 4 pixels from image A
LDR r2, [r3]            ; Load 4 pixels from image B
USAD8 r4, r0, r2        ; Sum of absolute differences
                        ; Lower value = more similar
```

**USADA8 - Unsigned Sum of Absolute Differences and Accumulate:**

```assembly
; USADA8 Rd, Rn, Rm, Ra
; Rd = Ra + USAD8(Rn, Rm)

MOV r5, #0              ; Accumulator
USADA8 r5, r0, r2, r5   ; Accumulate differences
```

**Example** - Block matching in video encoding:**

```assembly
; Compare 4x4 block (4 pixels per iteration)
MOV r4, #0              ; Total SAD
MOV r5, #4              ; Row counter

block_loop:
    LDR r0, [r1], #4    ; Load 4 pixels from reference
    LDR r2, [r3], #4    ; Load 4 pixels from candidate
    USADA8 r4, r0, r2, r4   ; Accumulate SAD
    SUBS r5, r5, #1
    BNE block_loop
    
; r4 now contains total SAD for 4x4 block (16 pixels)
```

### DSP-Oriented Multiply Instructions

**SMMUL/SMMLA/SMMLS** (covered earlier but expanded):

**Example** - Fixed-point Q15 multiplication:**

```assembly
; Q15 format: 1 sign bit, 15 fraction bits
; Range: [-1.0, 0.999969482421875]

; Multiply two Q15 numbers
; Result needs to be in Q15 format

SMULBB r0, r1, r2       ; r0 = (r1[15:0] * r2[15:0])
SSAT r0, #16, r0, ASR #15   ; Shift and saturate to Q15

; Or using SMMUL for Q31 (higher precision)
; Q31: 1 sign bit, 31 fraction bits

SMMUL r0, r1, r2        ; High 32 bits of 64-bit product
LSL r0, r0, #1          ; Adjust for Q31 format
```

**SMUL/SMLA - Signed Multiply with halfword operands:**

```assembly
; SMULBB - Multiply bottom halfwords
SMULBB r0, r1, r2       ; r0 = r1[15:0] * r2[15:0]

; SMULBT - Multiply bottom of first, top of second
SMULBT r0, r1, r2       ; r0 = r1[15:0] * r2[31:16]

; SMULTB - Multiply top of first, bottom of second  
SMULTB r0, r1, r2       ; r0 = r1[31:16] * r2[15:0]

; SMULTT - Multiply top halfwords
SMULTT r0, r1, r2       ; r0 = r1[31:16] * r2[31:16]
```

**Example** - Complex number multiplication (DSP):**

```assembly
; Complex multiply: (a + bi) * (c + di) = (ac - bd) + (ad + bc)i
; r0 = {a[31:16], b[15:0]} (real, imag)
; r1 = {c[31:16], d[15:0]} (real, imag)

SMULBB r2, r0, r1       ; r2 = b * d
SMULTT r3, r0, r1       ; r3 = a * c
SMLABB r4, r0, r1, r0   ; r4 = temp (for imaginary part)
SMLABT r5, r0, r1, r0   ; r5 = temp (for imaginary part)

SUB r6, r3, r2          ; Real part: ac - bd
ADD r7, r4, r5          ; Imaginary part: ad + bc (simplified)
PKHBT r8, r7, r6, LSL #16   ; Pack result
```

### Exclusive Load/Store (for synchronization)

**LDREX/STREX - Exclusive Load/Store:**

```assembly
; LDREX Rt, [Rn]
; Load exclusive - marks memory for exclusive access

; STREX Rd, Rt, [Rn]  
; Store exclusive - stores only if exclusive access maintained
; Rd = 0 if successful, 1 if failed

; Atomic increment example
retry:
    LDREX r1, [r0]          ; Load value with exclusive monitor
    ADD r1, r1, #1          ; Increment
    STREX r2, r1, [r0]      ; Attempt exclusive store
    CMP r2, #0              ; Check if succeeded
    BNE retry               ; Retry if failed

; Alternative with IT block
retry:
    LDREX r1, [r0]
    ADD r1, r1, #1
    STREX r2, r1, [r0]
    CBNZ r2, retry          ; Retry if failed (r2 != 0)
```

**LDREXB/STREXB - Byte exclusive:**

```assembly
; Atomic byte operations
retry:
    LDREXB r1, [r0]
    ADD r1, r1, #1
    STREXB r2, r1, [r0]
    CBNZ r2, retry
```

**LDREXH/STREXH - Halfword exclusive:**

```assembly
; Atomic halfword operations
retry:
    LDREXH r1, [r0]
    ADD r1, r1, #1
    STREXH r2, r1, [r0]
    CBNZ r2, retry
```

**Example** - Compare-and-swap (CAS):**

```assembly
; Atomic compare and swap
; r0 = address, r1 = expected value, r2 = new value
; Returns: r0 = 1 if successful, 0 if failed

cas:
    LDREX r3, [r0]          ; Load current value
    CMP r3, r1              ; Compare with expected
    ITT EQ
    STREXEQ r4, r2, [r0]    ; Store new value if match
    RSBEQ r0, r4, #1        ; Return success (1) or fail (0)
    
    IT NE
    MOVNE r0, #0            ; Return 0 if comparison failed
    
    BX lr
```

**Example** - Spinlock implementation:**

```assembly
; Acquire spinlock (value at [r0])
acquire_lock:
    MOV r1, #1              ; Lock value
try_lock:
    LDREX r2, [r0]          ; Load lock status
    CMP r2, #0              ; Check if unlocked
    ITT EQ
    STREXEQ r3, r1, [r0]    ; Try to acquire
    CMPEQ r3, #0            ; Check if succeeded
    BNE try_lock            ; Retry if failed
    DMB                     ; Data memory barrier
    BX lr

; Release spinlock
release_lock:
    DMB                     ; Data memory barrier
    MOV r1, #0
    STR r1, [r0]            ; Release lock
    BX lr
```

### Memory Barrier Instructions

**DMB - Data Memory Barrier:**

```assembly
; DMB {option}
; Ensures memory operations before DMB complete before operations after

DMB                     ; Full system DMB
DMB SY                  ; Full system (explicit)
DMB ISH                 ; Inner shareable domain
DMB OSH                 ; Outer shareable domain
DMB NSH                 ; Non-shareable
```

**DSB - Data Synchronization Barrier:**

```assembly
; DSB {option}
; Stronger than DMB - waits for all operations to complete

DSB                     ; Full system DSB
DSB SY                  ; Full system (explicit)
```

**ISB - Instruction Synchronization Barrier:**

```assembly
; ISB
; Flushes pipeline, ensures subsequent instructions see memory/context changes

ISB                     ; Flush instruction pipeline
```

**Example** - Proper synchronization usage:**

```assembly
; Producer-consumer with memory barriers
producer:
    STR r0, [r1]            ; Write data
    DMB                     ; Ensure write completes
    MOV r2, #1
    STR r2, [r3]            ; Set flag
    BX lr

consumer:
wait_flag:
    LDR r0, [r3]            ; Read flag
    CMP r0, #1
    BNE wait_flag
    DMB                     ; Ensure flag read before data read
    LDR r1, [r1]            ; Read data
    ; Process data
```

## Thumb-2 Unified Assembly Language

Thumb-2 introduced Unified Assembly Language (UAL) syntax that works across ARM and Thumb modes:

### UAL Syntax Features

**Width Specifiers:**

```assembly
; Explicit width control
ADD r0, r1, r2          ; Assembler chooses (16-bit if possible)
ADD.N r0, r1, r2        ; Force narrow (16-bit) encoding
ADD.W r0, r1, r2        ; Force wide (32-bit) encoding

; Narrow encoding fails if requirements not met
ADD.N r8, r9, r10       ; Error: high registers need wide encoding
ADD.W r8, r9, r10       ; OK: explicit wide
```

**Conditional Suffix Consistency:**

```assembly
; UAL uses same syntax for ARM and Thumb-2
; In Thumb-2, needs IT block

.arm
ADDGT r0, r1, r2        ; Predicated in ARM

.thumb
CMP r3, #0
IT GT
ADDGT r0, r1, r2        ; Predicated via IT block in Thumb-2
```

**Example** - Code that assembles for both modes:**

```assembly
; UAL code works in both ARM and Thumb-2
function:
    PUSH {r4-r7, lr}
    MOV r4, r0
    ADD r5, r4, #100
    CMP r5, #1000
    IT LT
    MOVLT r0, r5
    POP {r4-r7, pc}

; Assembler generates appropriate encoding for target mode
```

## Performance Optimization with Thumb-2

### When to Use Narrow vs Wide Encodings

**Prefer narrow (16-bit) for:**

- Low register operations (r0-r7)
- Small immediate values
- Simple operations without shifts
- Common instruction sequences

**Require wide (32-bit) for:**

- High register operations (r8-r15)
- Large immediate values
- Shifted operands in data processing
- Extended addressing modes
- Complex operations (divide, bit fields)

**Example** - Optimizing for code density:**

```assembly
; Good: Maximum use of narrow encodings
function_compact:
    PUSH {r4-r7, lr}        ; 16-bit
    MOV r4, r0              ; 16-bit (low registers)
    MOV r5, r1              ; 16-bit
    ADD r4, r4, r5          ; 16-bit
    MOV r0, r4              ; 16-bit
    POP {r4-r7, pc}         ; 16-bit
; Total: 12 bytes

; Poor: Unnecessary wide encodings
function_bloated:
    PUSH.W {r4-r7, lr}      ; 32-bit (unnecessary .W)
    MOV.W r4, r0            ; 32-bit (unnecessary .W)
    MOV.W r5, r1            ; 32-bit
    ADD.W r4, r4, r5        ; 32-bit
    MOV.W r0, r4            ; 32-bit
    POP.W {r4-r7, pc}       ; 32-bit
; Total: 24 bytes (100% larger!)
```

### Register Allocation for Thumb-2

**Strategy:** Prioritize low registers (r0-r7) for hot code paths:

```assembly
; Optimized: Important variables in low registers
hot_loop:
    ; r0-r3: frequently accessed variables (narrow encodings)
    LDR r0, [r4]            ; 16-bit
    ADD r1, r1, r0          ; 16-bit
    STR r1, [r4], #4        ; 16-bit
    SUBS r3, r3, #1         ; 16-bit
    BNE hot_loop            ; 16-bit

; Unoptimized: Using high registers unnecessarily
hot_loop_bad:
    ; r8-r10: force 32-bit encodings
    LDR r8, [r4]            ; 32-bit
    ADD.W r9, r9, r8        ; 32-bit
    STR r8, [r4], #4        ; 32-bit
    SUBS r10, r10, #1       ; 32-bit
    BNE hot_loop_bad        ; 16-bit
```

### Mixing IT Blocks and Branches

**Example** - When to use IT vs branches:**

```assembly
; IT block: Good for 1-4 simple instructions
CMP r0, #10
ITE GT
ADDGT r1, r1, #1
MOVLE r1, #0

; Branch: Better for longer conditional blocks
CMP r0, #10
BLE else_case
    ; Multiple instructions when > 10
    ADD r1, r1, #1
    MOV r2, #5
    MUL r3, r1, r2
    STR r3, [r4]
    B endif
else_case:
    ; Multiple instructions when <= 10
    MOV r1, #0
    MOV r2, #0
    STR r2, [r4]
endif:
```

### Literal Pool Management

Thumb-2 PC-relative loads have limited range (±4KB):

```assembly
; Close literal (within ±4KB)
LDR r0, =value          ; Assembler generates PC-relative load
; ...
value: .word 0x12345678

; Far literal (beyond range)
LDR r0, =far_value      ; May need veneer or different approach

; Better: Use MOVW/MOVT for constants
MOVW r0, #0x5678
MOVT r0, #0x1234        ; r0 = 0x12345678 (no literal pool needed)
```

**Example** - Efficient constant loading:**

```assembly
; Poor: Excessive literal pool usage
LDR r0, =0x12345678     ; Literal pool entry
LDR r1, =0x87654321     ; Another entry
LDR r2, =0xABCDEF00     ; Another entry
; 3 literal pool entries = 12 bytes + load overhead

; Better: Use MOVW/MOVT
MOVW r0, #0x5678
MOVT r0, #0x1234        ; 8 bytes, no pool access
MOVW r1, #0x4321
MOVT r1, #0x8765        ; 8 bytes
MOVW r2, #0xEF00
MOVT r2, #0xABCD        ; 8 bytes
; Total: 24 bytes but faster (no memory access), no pool
```

## Mode Selection Strategy

### Thumb-2 as Default

[Inference] Modern ARM Cortex-M processors (M0, M3, M4, M7) only support Thumb-2, making it the only choice. Cortex-A processors support both ARM and Thumb-2 modes.

**General Recommendations:**

- **Default to Thumb-2** for most code (code density wins)
- **Consider ARM mode** for: compute-intensive kernels, tight inner loops requiring maximum performance, code with heavy use of high registers
- [Inference] Performance difference typically <5% for well-optimized Thumb-2

**Example** - Hybrid approach:**

```assembly
.thumb
main_function:
    ; Thumb-2 for most code
    PUSH {r4-r7, lr}
    ; ... setup ...
    
    ; Call ARM mode for performance-critical kernel
    LDR r0, =dsp_kernel
    BLX r0              ; Call ARM function
    
    ; Resume Thumb-2
    POP {r4-r7, pc}

.arm
dsp_kernel:
    ; Performance-critical DSP code in ARM mode
    ; Uses conditional execution heavily
    ; ... intensive computation ...
    BX lr               ; Return to Thumb-2
```

## Thumb-2 Interworking Mechanisms

### BX and BLX Instructions

**BX - Branch and Exchange:**

```assembly
; BX Rm
; Branch to address in Rm, switch mode based on Rm[0]
; Rm[0] = 0: Switch to ARM mode
; Rm[0] = 1: Switch to Thumb mode

.thumb
    LDR r0, =arm_function
    BIC r0, r0, #1          ; Clear bit 0 for ARM mode
    BX r0                   ; Branch and switch to ARM

.arm
arm_function:
    ; ARM code executes here
    ; To return to Thumb caller:
    BX lr                   ; lr has bit 0 set by caller
```

**BLX - Branch with Link and Exchange:**

```assembly
; BLX Rm
; Branch with link, exchange modes based on Rm[0]

.thumb
    LDR r0, =arm_helper
    BLX r0                  ; Call and switch to ARM
                            ; lr = PC | 1 (marks return to Thumb)
    ; Returns here in Thumb mode

; BLX label (immediate form)
; Branch to label and exchange modes

.thumb
    BLX arm_function        ; Direct call, automatic mode switch

.arm
arm_function:
    ; ARM code
    BX lr                   ; Return to Thumb
```

**Example** - Complete interworking pattern:

```assembly
.thumb
.global thumb_entry
thumb_entry:
    PUSH {r4-r7, lr}
    
    ; Call ARM function
    BL arm_helper           ; If arm_helper is ARM, assembler generates BLX
    
    ; Process result
    ADD r0, r0, #10
    
    POP {r4-r7, pc}

.arm
.global arm_helper
arm_helper:
    ; ARM mode function
    STMFD sp!, {r4-r7, lr}
    
    ; Can call back to Thumb
    BL thumb_utility
    
    ; Return to Thumb caller
    LDMFD sp!, {r4-r7, lr}
    BX lr                   ; Mode switch on return

.thumb
thumb_utility:
    ; Thumb utility function
    ADD r0, r0, #5
    BX lr
```

### Veneer Generation

When branches exceed Thumb range, linkers generate veneers (trampolines):

**Example** - Automatic veneer insertion:

```assembly
.thumb
far_call:
    ; Target is >16MB away (beyond B.W range)
    B far_target            ; Linker converts to veneer

; Linker generates veneer automatically:
__far_target_veneer:
    LDR pc, =far_target     ; Long-range indirect branch

; ... many sections later ...
far_target:
    ; Actual target code
```

**Manual veneer for cross-mode calls:**

```assembly
.thumb
thumb_function:
    B arm_function_veneer

.section .veneers
arm_function_veneer:
    PUSH {lr}
    LDR r12, =arm_function
    BIC r12, r12, #1        ; Ensure ARM mode (bit 0 = 0)
    BLX r12
    POP {pc}

.arm
.section .text.arm
arm_function:
    ; ARM code here
    BX lr
```

## Advanced Thumb-2 Optimization Techniques

### Instruction Fusion and Pairing

Modern ARM cores can fuse or dual-issue certain instruction pairs:

**Example** - Fusible patterns (core-dependent):

```assembly
; Some cores fuse CMP + conditional branch
CMP r0, #10
BLT target              ; May fuse into single micro-op

; Some cores fuse MOVW + MOVT
MOVW r0, #0x1234
MOVT r0, #0x5678        ; May execute in parallel

; Dual-issue friendly patterns
ADD r0, r1, r2          ; ALU operation
LDR r3, [r4]            ; Memory operation (different unit)
                        ; May execute simultaneously
```

**Example** - Optimization for dual-issue:

```assembly
; Unoptimized: Sequential ALU operations
ADD r0, r1, r2
ADD r3, r4, r5
ADD r6, r7, r8

; Optimized: Interleave with memory operations
ADD r0, r1, r2
LDR r9, [r10]           ; Can dual-issue with ADD
ADD r3, r4, r5
LDR r11, [r12]          ; Can dual-issue with ADD
ADD r6, r7, r8
```

### Loop Optimization with Thumb-2

**Example** - Optimized memory copy loop:

```assembly
; Thumb-2 optimized memcpy (aligned, multiple of 16 bytes)
; r0 = dest, r1 = src, r2 = count (in bytes)

memcpy_opt:
    PUSH {r4-r7}
    
    ; Check if count is multiple of 16
    LSRS r3, r2, #4         ; count / 16
    BEQ cleanup             ; Skip if less than 16 bytes

copy_loop:
    ; Load 16 bytes (4 words) - narrow encodings
    LDM r1!, {r4-r7}        ; 16-bit instruction
    
    ; Store 16 bytes - narrow encoding
    STM r0!, {r4-r7}        ; 16-bit instruction
    
    ; Decrement counter
    SUBS r3, r3, #1         ; 16-bit instruction
    BNE copy_loop           ; 16-bit instruction

cleanup:
    ; Handle remaining bytes (if needed)
    ANDS r2, r2, #15        ; Get remainder
    BEQ done
    
byte_loop:
    LDRB r4, [r1], #1       ; 16-bit
    STRB r4, [r0], #1       ; 16-bit
    SUBS r2, r2, #1         ; 16-bit
    BNE byte_loop           ; 16-bit

done:
    POP {r4-r7}
    BX lr

; Entire loop body uses 16-bit instructions for maximum density
```

**Example** - Unrolled loop with Thumb-2:

```assembly
; Sum array elements (unrolled 4x)
; r0 = array, r1 = count, returns sum in r0

array_sum:
    MOV r2, #0              ; sum = 0
    LSRS r3, r1, #2         ; count / 4
    BEQ remainder

unrolled_loop:
    LDM r0!, {r4-r7}        ; Load 4 elements
    ADD r2, r2, r4          ; Accumulate
    ADD r2, r2, r5
    ADD r2, r2, r6
    ADD r2, r2, r7
    SUBS r3, r3, #1
    BNE unrolled_loop

remainder:
    ANDS r1, r1, #3         ; Remaining elements
    BEQ done

rem_loop:
    LDR r4, [r0], #4
    ADD r2, r2, r4
    SUBS r1, r1, #1
    BNE rem_loop

done:
    MOV r0, r2              ; Return sum
    BX lr
```

### Combining CBZ/CBNZ with Loop Optimization

**Example** - Countdown loop with CBZ:

```assembly
; Traditional approach
loop:
    ; ... loop body ...
    SUBS r0, r0, #1
    BNE loop

; Thumb-2 with CBNZ (saves cycles if flags needed elsewhere)
loop:
    ; ... loop body ...
    SUB r0, r0, #1          ; Don't update flags
    CBNZ r0, loop           ; Branch if non-zero
```

**Example** - Early exit pattern:

```assembly
; Search for zero in array
; r0 = array, r1 = count

search_loop:
    LDR r2, [r0], #4        ; Load element
    CBZ r2, found           ; Early exit if zero
    SUBS r1, r1, #1
    BNE search_loop
    
    MOV r0, #0              ; Not found
    BX lr
    
found:
    MOV r0, #1              ; Found
    BX lr
```

### Thumb-2 Specific Idioms

**Example** - Setting/clearing individual bits:

```assembly
; Set bit using BFI
MOV r1, #1
BFI r0, r1, #5, #1      ; Set bit 5

; Clear bit using BFC
BFC r0, #5, #1          ; Clear bit 5

; Traditional approach (more instructions)
ORR r0, r0, #(1<<5)     ; Set bit 5
BIC r0, r0, #(1<<5)     ; Clear bit 5
```

**Example** - Extracting bit fields:

```assembly
; Extract status bits [7:4] from hardware register
LDR r0, [r1]            ; Read register
UBFX r2, r0, #4, #4     ; Extract bits [7:4]

; Traditional approach
LDR r0, [r1]
LSR r2, r0, #4          ; Shift down
AND r2, r2, #0xF        ; Mask
```

**Example** - Byte swapping in halfwords:

```assembly
; Swap bytes within halfwords of 32-bit value
LDR r0, [r1]            ; r0 = 0xAABBCCDD
REV16 r0, r0            ; r0 = 0xBBAADDCC (bytes swapped per halfword)

; Network to host order (16-bit values)
LDRH r0, [r1]           ; Load 16-bit big-endian
REV16 r0, r0            ; Swap bytes
UXTH r0, r0             ; Ensure upper bits clear
```

**Example** - Saturating pixel operations:

```assembly
; Add brightness to pixel, clamp to [0, 255]
LDRB r0, [r1]           ; Load pixel
ADD r0, r0, #50         ; Add brightness
USAT r0, #8, r0         ; Saturate to 8-bit
STRB r0, [r1]           ; Store back
```

### Stack Frame Optimization

**Example** - Minimal stack frame:

```assembly
; Traditional approach (separate operations)
.thumb
function_old:
    PUSH {r4-r7, lr}        ; 2 bytes
    SUB sp, sp, #16         ; Allocate locals (2 bytes)
    ; ... function body ...
    ADD sp, sp, #16         ; Deallocate (2 bytes)
    POP {r4-r7, pc}         ; 2 bytes

; Optimized approach (combined allocation)
function_new:
    PUSH {r4-r7, lr}        ; 2 bytes
    SUB sp, #16             ; 2 bytes (shorter encoding)
    ; ... function body ...
    ADD sp, #16             ; 2 bytes
    POP {r4-r7, pc}         ; 2 bytes
    
; Even better: adjust pop if possible
function_better:
    PUSH {r4-r7, lr}
    ; Use stack directly without separate allocation
    STR r0, [sp, #-16]!     ; Allocate and store
    ; ... use [sp, #0], [sp, #4], etc. ...
    ADD sp, #16             ; Deallocate
    POP {r4-r7, pc}
```

### Table-Based Dispatch Optimization

**Example** - Efficient switch statement with TBB:

```assembly
; Switch with dense cases (0-7)
; r0 = case value

switch_dispatch:
    CMP r0, #8              ; Bounds check
    BHS default_case        ; Out of range
    
    TBB [pc, r0]            ; Table branch byte
    
jump_table:
    .byte (case0 - jump_table) / 2
    .byte (case1 - jump_table) / 2
    .byte (case2 - jump_table) / 2
    .byte (case3 - jump_table) / 2
    .byte (case4 - jump_table) / 2
    .byte (case5 - jump_table) / 2
    .byte (case6 - jump_table) / 2
    .byte (case7 - jump_table) / 2
    .align 2                ; Align after table

case0:
    MOV r0, #100
    B end_switch
case1:
    MOV r0, #200
    B end_switch
; ... more cases ...
case7:
    MOV r0, #800
    B end_switch
    
default_case:
    MOV r0, #0
    
end_switch:
    BX lr

; Table dispatch is: 1 CMP + 1 BHS + 1 TBB = 3 instructions
; Traditional: 8 CMP + 8 branch instructions
```

**Example** - Sparse switch with hybrid approach:

```assembly
; Switch with sparse cases
switch_sparse:
    ; Quick check for common cases
    CMP r0, #1
    BEQ case1
    CMP r0, #5
    BEQ case5
    
    ; Fall through to table for other cases
    CMP r0, #20
    BLO table_dispatch
    B default_case

table_dispatch:
    ; Use table for cases 2-19
    SUB r1, r0, #2          ; Normalize to 0-based
    TBB [pc, r1]
    ; ...table...
```

## Real-World Thumb-2 Code Examples

### String Length (strlen)

**Example** - Optimized strlen implementation:

```assembly
; Compute string length
; r0 = string pointer
; Returns length in r0

.thumb
strlen:
    MOV r1, r0              ; Save original pointer
    
    ; Align to word boundary first
    ANDS r2, r0, #3         ; Get alignment
    BEQ aligned             ; Already aligned
    
byte_align:
    LDRB r3, [r0], #1       ; Load byte
    CBZ r3, found_end       ; Check for null
    SUBS r2, r2, #1
    BNE byte_align

aligned:
    ; Process 4 bytes at a time
    LDR r2, =0x01010101     ; Magic constant
    
word_loop:
    LDR r3, [r0], #4        ; Load 4 bytes
    
    ; Check if any byte is zero using USUB8
    USUB8 r4, r3, r2        ; Subtract 1 from each byte
    SEL r4, r4, r2          ; Select based on GE flags
    TST r4, r4              ; Check if any byte was zero
    BEQ word_loop           ; All non-zero, continue
    
    ; Found zero in this word, find which byte
    SUB r0, r0, #4          ; Back up
    
byte_scan:
    LDRB r3, [r0], #1
    CBZ r3, found_end
    B byte_scan

found_end:
    SUB r0, r0, r1          ; End - start
    SUB r0, r0, #1          ; Don't count null
    BX lr
```

### Memory Set (memset)

**Example** - Optimized memset:

```assembly
; Fill memory with byte value
; r0 = dest, r1 = value, r2 = count
; Returns dest in r0

.thumb
memset:
    CMP r2, #0
    IT EQ
    BXEQ lr                 ; Return if count == 0
    
    PUSH {r4-r5}
    MOV r3, r0              ; Save dest
    
    ; Replicate byte to all positions
    AND r1, r1, #0xFF       ; Ensure single byte
    ORR r1, r1, r1, LSL #8  ; Replicate to 16 bits
    ORR r1, r1, r1, LSL #16 ; Replicate to 32 bits
    
    ; Handle misalignment
    ANDS r4, r0, #3
    BEQ aligned_set
    
    RSB r4, r4, #4          ; Bytes until aligned
    CMP r2, r4
    IT LT
    MOVLT r4, r2            ; Don't exceed count
    
    SUB r2, r2, r4          ; Adjust count
    
align_loop:
    STRB r1, [r0], #1
    SUBS r4, r4, #1
    BNE align_loop

aligned_set:
    ; Fill 16 bytes at a time
    MOV r4, r1              ; Duplicate value
    MOV r5, r1
    
    LSRS r12, r2, #4        ; count / 16
    BEQ word_fill

block_loop:
    STM r0!, {r1, r4, r5, r12}  ; Not all cores allow r12 in STM
    ; Alternative:
    ; STR r1, [r0], #4
    ; STR r4, [r0], #4
    ; STR r5, [r0], #4
    ; STR r1, [r0], #4
    SUBS r12, r12, #1
    BNE block_loop

word_fill:
    ; Fill remaining words
    ANDS r12, r2, #12       ; Remaining words * 4
    BEQ byte_fill
    
word_loop:
    STR r1, [r0], #4
    SUBS r12, r12, #4
    BNE word_loop

byte_fill:
    ; Fill remaining bytes
    ANDS r2, r2, #3
    BEQ done_set
    
byte_loop:
    STRB r1, [r0], #1
    SUBS r2, r2, #1
    BNE byte_loop

done_set:
    MOV r0, r3              ; Restore dest
    POP {r4-r5}
    BX lr
```

### Fixed-Point Math Operations

**Example** - Q16.16 fixed-point multiply:

```assembly
; Multiply two Q16.16 fixed-point numbers
; r0 = first operand (Q16.16)
; r1 = second operand (Q16.16)
; Returns result in r0 (Q16.16)

.thumb
fp_multiply:
    SMULL r2, r3, r0, r1    ; 64-bit result: r3:r2 = r0 * r1
    
    ; Extract middle 32 bits (shift right 16)
    LSR r2, r2, #16         ; Lower part >> 16
    BFI r2, r3, #16, #16    ; Insert upper 16 bits from r3
    
    MOV r0, r2              ; Result
    BX lr

; Alternative using immediate shifts
fp_multiply_alt:
    SMULL r2, r3, r0, r1
    LSL r3, r3, #16         ; Shift high part left
    ORR r0, r3, r2, LSR #16 ; Combine parts
    BX lr
```

**Example** - Q31 fixed-point operations:

```assembly
; Q31 multiply (1 sign bit, 31 fraction bits)
; Range: [-1.0, 0.999999999767169356346130371093750]

q31_multiply:
    SMMUL r0, r0, r1        ; High 32 bits of 64-bit product
    LSL r0, r0, #1          ; Adjust for Q31 format
    BX lr

; Q31 multiply-accumulate
; r0 = accumulator, r1 = multiplicand, r2 = multiplier
q31_mac:
    SMMLA r0, r1, r2, r0    ; acc += (a * b) >> 32
    LSL r0, r0, #1          ; Adjust for Q31
    BX lr
```

### Bit Manipulation Utilities

**Example** - Population count (count set bits):

```assembly
; Count number of set bits in r0
; Returns count in r0

.thumb
popcount:
    MOVW r1, #0x5555
    MOVT r1, #0x5555        ; r1 = 0x55555555
    
    AND r2, r0, r1          ; Isolate odd bits
    LSR r0, r0, #1          ; Shift even bits
    AND r0, r0, r1          ; Isolate (shifted) even bits
    ADD r0, r0, r2          ; Add pairs
    
    MOVW r1, #0x3333
    MOVT r1, #0x3333        ; r1 = 0x33333333
    
    AND r2, r0, r1
    LSR r0, r0, #2
    AND r0, r0, r1
    ADD r0, r0, r2          ; Add 4-bit groups
    
    MOVW r1, #0x0F0F
    MOVT r1, #0x0F0F        ; r1 = 0x0F0F0F0F
    
    ADD r0, r0, r0, LSR #4  ; Add 8-bit groups
    AND r0, r0, r1          ; Mask result
    
    ADD r0, r0, r0, LSR #8  ; Add 16-bit groups
    ADD r0, r0, r0, LSR #16 ; Add 32-bit groups
    AND r0, r0, #0xFF       ; Final count
    
    BX lr

; Alternative using multiply trick
popcount_fast:
    ; Parallel count in groups
    MOVW r1, #0x5555
    MOVT r1, #0x5555
    LSR r2, r0, #1
    AND r2, r2, r1
    SUB r0, r0, r2
    
    MOVW r1, #0x3333
    MOVT r1, #0x3333
    AND r2, r0, r1
    LSR r0, r0, #2
    AND r0, r0, r1
    ADD r0, r0, r2
    
    ADD r0, r0, r0, LSR #4
    MOVW r1, #0x0F0F
    MOVT r1, #0x0F0F
    AND r0, r0, r1
    
    ; Multiply to sum all bytes
    MOVW r1, #0x0101
    MOVT r1, #0x0101        ; r1 = 0x01010101
    MUL r0, r0, r1
    LSR r0, r0, #24         ; Extract result
    
    BX lr
```

**Example** - Find first set bit (ffs):

```assembly
; Find position of first set bit (LSB = 0)
; Returns position in r0, or -1 if no bits set

.thumb
find_first_set:
    CBZ r0, no_bits         ; Return -1 if zero
    
    RBIT r0, r0             ; Reverse bits
    CLZ r0, r0              ; Count leading zeros
    BX lr
    
no_bits:
    MOV r0, #-1
    BX lr

; Alternative without RBIT (for cores without it)
ffs_portable:
    CBZ r0, no_bits
    
    RSB r1, r0, #0          ; -r0
    AND r0, r0, r1          ; Isolate lowest set bit
    CLZ r0, r0              ; Count leading zeros
    RSB r0, r0, #31         ; Position = 31 - CLZ
    BX lr
    
no_bits:
    MOV r0, #-1
    BX lr
```

## Debugging and Profiling Thumb-2 Code

### Instruction Size Analysis

**Example** - Analyzing code size:

```assembly
.thumb
.global function_start
function_start:
    PUSH {r4-r7, lr}        ; 2 bytes
    MOV r4, r0              ; 2 bytes
    ADD r5, r4, #100        ; 2 bytes
    CMP r5, #1000           ; 2 bytes
    IT LT                   ; 2 bytes
    MOVLT r0, r5            ; 2 bytes
    POP {r4-r7, pc}         ; 2 bytes
function_end:

; Total: 14 bytes (7 instructions × 2 bytes each)

; To get size in assembly:
.set function_size, function_end - function_start
```

### Mixed Encoding Example

**Example** - Identifying wide instructions:

```assembly
.thumb
mixed_function:
    PUSH {r4-r7, lr}        ; 16-bit
    MOVW r4, #0x1234        ; 32-bit (wide immediate)
    MOVT r4, #0x5678        ; 32-bit
    ADD.W r5, r8, r9        ; 32-bit (high registers)
    LDR r6, [r4, #200]      ; 32-bit (large offset)
    BFI r6, r5, #8, #8      ; 32-bit (bit field operation)
    ADD r0, r4, r6          ; 16-bit (result)
    POP {r4-r7, pc}         ; 16-bit

; 6 instructions: 3×16-bit + 3×32-bit = 18 bytes
```

## Performance Considerations Summary

[Inference] Based on typical ARM Cortex-A and Cortex-M processor characteristics:

**Code Density:**

- Thumb-2: ~25-30% smaller than ARM
- Mostly 16-bit instructions for common operations
- 32-bit instructions for complex operations

**Performance:**

- Thumb-2: Typically 95-98% of ARM performance
- Identical execution speed per instruction
- Slight overhead from additional instructions needed for complex operations
- Better instruction cache utilization can offset minor inefficiencies

**When Thumb-2 Matches ARM:**

- Simple data processing (low registers, small immediates)
- Load/store operations
- Branch operations within range
- Most common code patterns

**When Thumb-2 Requires Extra Instructions:**

- High register operations (pre-Thumb-2)
- Complex immediates (pre-MOVW/MOVT)
- Heavily predicated code (requires IT blocks)
- Long-range branches (may need veneers)

**Key Takeaway:** Thumb-2 provides excellent code density with minimal performance penalty, making it the preferred mode for most applications on modern ARM processors.

**Important related topics:** NEON vectorization in Thumb-2 mode, Cortex-M processor-specific Thumb-2 extensions, mixed ARM/Thumb-2 optimization strategies, profile-guided optimization for instruction set selection, cache-aware code placement strategies

---

# Floating Point (VFP/NEON)

## VFP Architecture and Registers

The Vector Floating Point (VFP) architecture provides hardware support for IEEE 754 floating-point arithmetic on ARM processors. NEON is ARM's SIMD (Single Instruction Multiple Data) extension that extends VFP capabilities.

**VFP versions:**

- VFPv1: Initial implementation (rare)
- VFPv2: ARMv5TE and ARMv6
- VFPv3: ARMv7-A, added 32 double-precision registers
- VFPv4: ARMv7-A, added fused multiply-add
- VFPv5: ARMv8-A integration with NEON

**Register organization (VFPv3/VFPv4):**

**Single-precision registers (S registers):**

- S0-S31: 32 single-precision (32-bit) registers
- Each holds one single-precision floating-point value

**Double-precision registers (D registers):**

- D0-D31: 32 double-precision (64-bit) registers in VFPv3-D32
- D0-D15: 16 double-precision registers in VFPv3-D16
- Each holds one double-precision floating-point value

**Quad-word registers (Q registers - NEON):**

- Q0-Q15: 16 quad-word (128-bit) registers
- Used for SIMD operations

**Register aliasing:** The S and D registers share the same physical register file:

- D0 overlaps with S0 (low 32 bits) and S1 (high 32 bits)
- D1 overlaps with S2 and S3
- D16-D31 have no S register aliases (VFPv3-D32 only)
- Q0 overlaps with D0 and D1 (or S0-S3)

```
Q0:  [    D0    |    D1    ]
     [S0 |S1    |S2 |S3    ]

Q1:  [    D2    |    D3    ]
     [S4 |S5    |S6 |S7    ]
```

**FPSCR (Floating-Point Status and Control Register):** Controls floating-point behavior and stores status flags:

- Rounding modes: Round to nearest, toward +∞, toward -∞, toward zero
- Exception flags: Invalid operation, Division by zero, Overflow, Underflow, Inexact
- Flush-to-zero mode: Denormalized numbers treated as zero
- Default NaN mode: All NaN operations produce default NaN

**Enabling VFP/NEON:**

```asm
; Enable VFP/NEON coprocessor access (typically done by OS)
MRC p15, 0, r0, c1, c0, 2   ; Read CP Access Control Register
ORR r0, r0, #0xF00000       ; Enable CP10 and CP11 (VFP/NEON)
MCR p15, 0, r0, c1, c0, 2   ; Write CP Access Control Register
ISB

; Enable VFP
MOV r0, #0x40000000
VMSR FPEXC, r0              ; Set EN bit in FPEXC
```

**Register naming conventions:**

- ARMv7 (32-bit): S, D, Q registers
- ARMv8 AArch64: V registers (V0-V31) replace S/D/Q, accessed with type suffixes

## Single and Double Precision Operations

**Single-precision operations use S registers:**

**Basic arithmetic:**

```asm
VADD.F32 S0, S1, S2         ; S0 = S1 + S2
VSUB.F32 S0, S1, S2         ; S0 = S1 - S2
VMUL.F32 S0, S1, S2         ; S0 = S1 * S2
VDIV.F32 S0, S1, S2         ; S0 = S1 / S2
VNEG.F32 S0, S1             ; S0 = -S1
VABS.F32 S0, S1             ; S0 = |S1|
VSQRT.F32 S0, S1            ; S0 = √S1
```

**Fused multiply-add/subtract (VFPv4+):**

```asm
VFMA.F32 S0, S1, S2         ; S0 = S0 + (S1 * S2) - single rounding
VFMS.F32 S0, S1, S2         ; S0 = S0 - (S1 * S2) - single rounding
VFNMA.F32 S0, S1, S2        ; S0 = -S0 - (S1 * S2)
VFNMS.F32 S0, S1, S2        ; S0 = -S0 + (S1 * S2)
```

**Multiply-accumulate (older VFP):**

```asm
VMLA.F32 S0, S1, S2         ; S0 = S0 + (S1 * S2) - two roundings
VMLS.F32 S0, S1, S2         ; S0 = S0 - (S1 * S2)
VNMLA.F32 S0, S1, S2        ; S0 = -S0 - (S1 * S2)
VNMLS.F32 S0, S1, S2        ; S0 = -S0 + (S1 * S2)
VNMUL.F32 S0, S1, S2        ; S0 = -(S1 * S2)
```

**Comparison:**

```asm
VCMP.F32 S0, S1             ; Compare S0 with S1, set FPSCR flags
VCMP.F32 S0, #0.0           ; Compare S0 with zero
VMRS APSR_nzcv, FPSCR       ; Transfer FPSCR flags to APSR
BEQ equal                   ; Branch based on comparison
BGT greater
BLT less
```

**Move and load immediate:**

```asm
VMOV.F32 S0, S1             ; Copy S1 to S0
VMOV.F32 S0, #1.0           ; Load immediate (limited values)
VMOV.F32 S0, #-2.0
VMOV.F32 S0, #0.5

; Immediate values must fit encoding constraints
; Can represent values like: ±n/16, ±n/8, ±n/4, ±n/2, ±n, ±2n, ±4n, etc.
; where n is power of 2 from 0.5 to 31
```

**Load and store:**

```asm
VLDR.F32 S0, [R0]           ; Load from memory address in R0
VSTR.F32 S0, [R0]           ; Store to memory address in R0
VLDR.F32 S0, [R0, #4]       ; Load with offset
VSTR.F32 S0, [R0, #-8]      ; Store with offset

; Load/store multiple (stack operations)
VPUSH {S0-S3}               ; Push S0, S1, S2, S3 to stack
VPOP {S0-S3}                ; Pop from stack to S0-S3
VSTM R0, {S0-S7}            ; Store multiple to memory
VLDM R0, {S0-S7}            ; Load multiple from memory
```

**Double-precision operations use D registers:**

**Basic arithmetic:**

```asm
VADD.F64 D0, D1, D2         ; D0 = D1 + D2
VSUB.F64 D0, D1, D2         ; D0 = D1 - D2
VMUL.F64 D0, D1, D2         ; D0 = D1 * D2
VDIV.F64 D0, D1, D2         ; D0 = D1 / D2
VNEG.F64 D0, D1             ; D0 = -D1
VABS.F64 D0, D1             ; D0 = |D1|
VSQRT.F64 D0, D1            ; D0 = √D1
```

**Fused multiply-add/subtract:**

```asm
VFMA.F64 D0, D1, D2         ; D0 = D0 + (D1 * D2)
VFMS.F64 D0, D1, D2         ; D0 = D0 - (D1 * D2)
VFNMA.F64 D0, D1, D2        ; D0 = -D0 - (D1 * D2)
VFNMS.F64 D0, D1, D2        ; D0 = -D0 + (D1 * D2)
```

**Comparison:**

```asm
VCMP.F64 D0, D1             ; Compare D0 with D1
VCMP.F64 D0, #0.0           ; Compare D0 with zero
VMRS APSR_nzcv, FPSCR       ; Transfer flags to condition codes
```

**Move and load immediate:**

```asm
VMOV.F64 D0, D1             ; Copy D1 to D0
VMOV.F64 D0, #1.0           ; Load immediate
VMOV.F64 D0, #3.14159       ; Not possible - use memory load instead
```

**Load and store:**

```asm
VLDR.F64 D0, [R0]           ; Load from memory
VSTR.F64 D0, [R0]           ; Store to memory
VLDR.F64 D0, [R0, #8]       ; Load with offset
VPUSH {D0-D7}               ; Push to stack
VPOP {D0-D7}                ; Pop from stack
```

**Example:**

```asm
; Calculate: result = (a + b) * (c - d)
; Single precision
VLDR.F32 S0, [R0]           ; Load a
VLDR.F32 S1, [R0, #4]       ; Load b
VLDR.F32 S2, [R0, #8]       ; Load c
VLDR.F32 S3, [R0, #12]      ; Load d

VADD.F32 S4, S0, S1         ; S4 = a + b
VSUB.F32 S5, S2, S3         ; S5 = c - d
VMUL.F32 S6, S4, S5         ; S6 = (a + b) * (c - d)
VSTR.F32 S6, [R1]           ; Store result

; Double precision version
VLDR.F64 D0, [R0]           ; Load a
VLDR.F64 D1, [R0, #8]       ; Load b
VLDR.F64 D2, [R0, #16]      ; Load c
VLDR.F64 D3, [R0, #24]      ; Load d

VADD.F64 D4, D0, D1         ; D4 = a + b
VSUB.F64 D5, D2, D3         ; D5 = c - d
VMUL.F64 D6, D4, D5         ; D6 = (a + b) * (c - d)
VSTR.F64 D6, [R1]           ; Store result
```

## Floating Point Arithmetic

**IEEE 754 representation:**

**Single precision (32-bit):**

- Sign: 1 bit
- Exponent: 8 bits (biased by 127)
- Mantissa: 23 bits (implicit leading 1)
- Range: approximately ±1.4 × 10^-45 to ±3.4 × 10^38
- Precision: about 7 decimal digits

**Double precision (64-bit):**

- Sign: 1 bit
- Exponent: 11 bits (biased by 1023)
- Mantissa: 52 bits (implicit leading 1)
- Range: approximately ±4.9 × 10^-324 to ±1.8 × 10^308
- Precision: about 15-17 decimal digits

**Special values:**

**Infinity:**

```asm
; Positive infinity: exponent all 1s, mantissa all 0s
; Negative infinity: sign bit 1, exponent all 1s, mantissa all 0s
VMOV.F32 S0, #1.0
VMOV.F32 S1, #0.0
VDIV.F32 S2, S0, S1         ; S2 = +infinity

VNEG.F32 S3, S2             ; S3 = -infinity
```

**NaN (Not a Number):**

```asm
; NaN: exponent all 1s, mantissa non-zero
; Generated by invalid operations
VMOV.F32 S0, #0.0
VDIV.F32 S1, S0, S0         ; S1 = NaN (0/0)

VSQRT.F32 S2, S3            ; NaN if S3 is negative
```

**Zero:**

```asm
; +0.0 and -0.0 are distinct but compare equal
VMOV.F32 S0, #0.0           ; +0.0
VNEG.F32 S1, S0             ; -0.0

VCMP.F32 S0, S1
VMRS APSR_nzcv, FPSCR
BEQ equal                   ; Branch taken: +0.0 == -0.0
```

**Denormalized numbers:** Numbers with exponent = 0 and non-zero mantissa represent values smaller than the minimum normalized number. VFP can operate in flush-to-zero mode where denormals are treated as zero for performance.

**Rounding modes:**

```asm
; Read FPSCR
VMRS R0, FPSCR

; Set rounding mode (bits 23-22)
; 00 = Round to Nearest (default)
; 01 = Round toward Plus Infinity
; 10 = Round toward Minus Infinity
; 11 = Round toward Zero

; Set round toward zero
BIC R0, R0, #0x00C00000     ; Clear rounding bits
ORR R0, R0, #0x00C00000     ; Set both bits (11)
VMSR FPSCR, R0

; Restore round to nearest
BIC R0, R0, #0x00C00000
VMSR FPSCR, R0
```

**Exception handling:**

```asm
; Enable exception traps (typically left disabled)
VMRS R0, FPSCR
ORR R0, R0, #0x00000F00     ; Enable all exception traps
VMSR FPSCR, R0

; Check exception flags
VMRS R0, FPSCR
TST R0, #0x00000001         ; Test Invalid Operation flag
TST R0, #0x00000002         ; Test Division by Zero flag
TST R0, #0x00000004         ; Test Overflow flag
TST R0, #0x00000008         ; Test Underflow flag
TST R0, #0x00000010         ; Test Inexact flag

; Clear exception flags
BIC R0, R0, #0x0000001F
VMSR FPSCR, R0
```

**Mathematical operations example:**

```asm
; Vector dot product: result = a·b = a.x*b.x + a.y*b.y + a.z*b.z
dot_product:
    ; Load vector a
    VLDM R0, {S0-S2}        ; S0=a.x, S1=a.y, S2=a.z
    ; Load vector b
    VLDM R1, {S3-S5}        ; S3=b.x, S4=b.y, S5=b.z
    
    ; Multiply components
    VMUL.F32 S6, S0, S3     ; S6 = a.x * b.x
    VMUL.F32 S7, S1, S4     ; S7 = a.y * b.y
    VMUL.F32 S8, S2, S5     ; S8 = a.z * b.z
    
    ; Sum
    VADD.F32 S9, S6, S7     ; S9 = a.x*b.x + a.y*b.y
    VADD.F32 S9, S9, S8     ; S9 = a.x*b.x + a.y*b.y + a.z*b.z
    
    ; Store result
    VSTR.F32 S9, [R2]
    BX LR

; Using fused multiply-add (more efficient, more accurate)
dot_product_fma:
    VLDM R0, {S0-S2}
    VLDM R1, {S3-S5}
    
    VMUL.F32 S9, S0, S3     ; S9 = a.x * b.x
    VFMA.F32 S9, S1, S4     ; S9 += a.y * b.y
    VFMA.F32 S9, S2, S5     ; S9 += a.z * b.z
    
    VSTR.F32 S9, [R2]
    BX LR
```

**Comparison and selection:**

```asm
; Find maximum of two floats
vfp_max:
    VLDR.F32 S0, [R0]       ; Load first value
    VLDR.F32 S1, [R1]       ; Load second value
    
    VCMP.F32 S0, S1         ; Compare
    VMRS APSR_nzcv, FPSCR   ; Transfer flags
    
    BLE second_larger       ; Branch if S0 <= S1
    VMOV.F32 S2, S0         ; S0 is larger
    B done
second_larger:
    VMOV.F32 S2, S1         ; S1 is larger
done:
    VSTR.F32 S2, [R2]       ; Store result
    BX LR

; Or using VMAXNM (ARMv8)
; VMAXNM.F32 S2, S0, S1    ; S2 = max(S0, S1)
```

## Conversions Between Integer and Float

**Integer to floating point:**

**Signed integer to float:**

```asm
; S register to single precision
VCVT.F32.S32 S0, S1         ; S0 = (float)S1 (signed)

; Core register to single precision
VMOV S0, R0                 ; Move integer from core register
VCVT.F32.S32 S0, S0         ; Convert to float

; S register to double precision
VCVT.F64.S32 D0, S1         ; D0 = (double)S1

; Core register to double precision
VMOV S0, R0
VCVT.F64.S32 D0, S0
```

**Unsigned integer to float:**

```asm
; Unsigned conversion
VCVT.F32.U32 S0, S1         ; S0 = (float)S1 (unsigned)
VCVT.F64.U32 D0, S1         ; D0 = (double)S1 (unsigned)

; From core register
VMOV S0, R0
VCVT.F32.U32 S0, S0
```

**Floating point to integer:**

**Float to signed integer (round to nearest):**

```asm
; Single to signed integer
VCVT.S32.F32 S0, S1         ; S0 = (int)S1 (round to nearest)

; Double to signed integer
VCVT.S32.F64 S0, D1         ; S0 = (int)D1

; Move to core register
VCVT.S32.F32 S0, S1
VMOV R0, S0                 ; R0 = integer result
```

**Float to unsigned integer:**

```asm
VCVT.U32.F32 S0, S1         ; S0 = (unsigned)S1
VCVT.U32.F64 S0, D1         ; S0 = (unsigned)D1
```

**Rounding variants:**

```asm
; Round toward zero (truncate)
VCVT.S32.F32 S0, S1         ; Default uses current FPSCR rounding
VCVTR.S32.F32 S0, S1        ; Explicit round toward zero
VCVTR.U32.F32 S0, S1        ; Unsigned, round toward zero

; Round toward minus infinity (floor)
VCVTM.S32.F32 S0, S1        ; Floor (ARMv8)

; Round toward plus infinity (ceiling)
VCVTP.S32.F32 S0, S1        ; Ceiling (ARMv8)

; Round to nearest
VCVTN.S32.F32 S0, S1        ; Round to nearest (ARMv8)

; Round using current mode
VCVTA.S32.F32 S0, S1        ; Round using FPSCR mode (ARMv8)
```

**Fixed-point conversions:**

VFP supports conversion between floating point and fixed-point representations:

```asm
; Float to fixed point
; VCVT.S32.F32 Sd, Sm, #fbits
; Converts float to fixed point with specified fractional bits
VCVT.S32.F32 S0, S1, #16    ; S0 = S1 * 2^16 (16 fractional bits)
VCVT.U32.F32 S0, S1, #8     ; S0 = S1 * 2^8 (8 fractional bits)

; Fixed point to float
VCVT.F32.S32 S0, S1, #16    ; S0 = S1 / 2^16
VCVT.F32.U32 S0, S1, #8     ; S0 = S1 / 2^8

; Range: #fbits can be 1-32
```

**Precision conversions:**

```asm
; Single to double precision
VCVT.F64.F32 D0, S1         ; D0 = (double)S1 (widening)

; Double to single precision
VCVT.F32.F64 S0, D1         ; S0 = (float)D1 (narrowing, may lose precision)
```

**Transfer between core and VFP registers:**

```asm
; Core register to VFP single precision
VMOV S0, R0                 ; S0 = R0 (bitwise copy, no conversion)

; VFP single precision to core register
VMOV R0, S0                 ; R0 = S0 (bitwise copy)

; Core registers to VFP double precision
VMOV D0, R0, R1             ; D0 = {R1, R0} (R0=low, R1=high)

; VFP double precision to core registers
VMOV R0, R1, D0             ; {R1, R0} = D0

; Direct transfer with conversion
VMOV S0, R0                 ; Bitwise move
VCVT.F32.S32 S0, S0         ; Then convert

; Or use combined operation (ARMv8)
; FMOV (AArch64)
```

**Example:**

```asm
; Convert array of integers to floats
; R0 = source array (int32_t*)
; R1 = destination array (float*)
; R2 = count

int_to_float_array:
    PUSH {R4-R5, LR}
    MOV R4, R0
    MOV R5, R1
    
loop:
    LDR R3, [R4], #4        ; Load integer
    VMOV S0, R3             ; Move to VFP
    VCVT.F32.S32 S0, S0     ; Convert to float
    VSTR.F32 S0, [R5], #4   ; Store float
    
    SUBS R2, R2, #1
    BNE loop
    
    POP {R4-R5, PC}

; Convert float array to integers with rounding
float_to_int_array:
    PUSH {R4-R5, LR}
    MOV R4, R0
    MOV R5, R1
    
loop2:
    VLDR.F32 S0, [R4], #4   ; Load float
    VCVTR.S32.F32 S1, S0    ; Convert (round toward zero)
    VMOV R3, S1             ; Move to core register
    STR R3, [R5], #4        ; Store integer
    
    SUBS R2, R2, #1
    BNE loop2
    
    POP {R4-R5, PC}
```

**Handling conversion overflow:**

```asm
; Safe float-to-int with bounds checking
safe_float_to_int:
    VLDR.F32 S0, [R0]       ; Load float value
    
    ; Check if value is in valid range for int32
    VMOV.F32 S1, #2147483648.0  ; INT_MAX + 1 (not exactly representable)
    VNEG.F32 S2, S1         ; -INT_MAX - 1
    
    VCMP.F32 S0, S1
    VMRS APSR_nzcv, FPSCR
    BGE overflow_pos        ; Too large
    
    VCMP.F32 S0, S2
    VMRS APSR_nzcv, FPSCR
    BLE overflow_neg        ; Too small
    
    ; Value is in range
    VCVTR.S32.F32 S3, S0
    VMOV R0, S3
    BX LR
    
overflow_pos:
    MOV R0, #0x7FFFFFFF     ; INT_MAX
    BX LR
    
overflow_neg:
    MOV R0, #0x80000000     ; INT_MIN
    BX LR
```

**Key Points:**

- VFP provides S registers (32-bit single precision) and D registers (64-bit double precision) with register aliasing
- Single and double precision operations follow IEEE 754 standard with support for special values (infinity, NaN, denormals)
- Floating-point arithmetic includes basic operations, fused multiply-add, comparisons, and mathematical functions
- Conversions support integer↔float, float↔fixed-point, and single↔double precision with various rounding modes
- FPSCR controls rounding behavior, exception handling, and flush-to-zero mode

[Inference] Performance characteristics vary across ARM implementations - actual instruction latencies and throughput depend on specific processor microarchitecture (Cortex-A series vs Cortex-M series vs custom designs).

---

## NEON SIMD Architecture

NEON is ARM's Advanced SIMD (Single Instruction, Multiple Data) architecture extension that enables parallel processing of multiple data elements with a single instruction. Introduced with ARMv7-A and available in Cortex-A series processors, NEON operates alongside the Vector Floating Point (VFP) unit, sharing the same register bank but providing distinct instruction sets.

**Architecture Overview:** NEON implements a 128-bit wide datapath capable of processing vectors of 8, 16, 32, or 64-bit elements simultaneously. The architecture supports both integer and floating-point operations, making it suitable for multimedia processing, signal processing, graphics, and computational workloads requiring data parallelism.

**VFP vs NEON Distinction:**

**VFP (Vector Floating Point):** Provides scalar floating-point arithmetic compliant with IEEE 754. VFPv3 and VFPv4 support single-precision (32-bit) and double-precision (64-bit) operations. VFP executes one operation per instruction on scalar values.

**NEON:** Provides SIMD operations on packed integer and floating-point data. NEON can perform multiple operations simultaneously on vector elements. NEON floating-point is not fully IEEE 754 compliant—it lacks support for denormalized numbers, NaN propagation differs, and rounding modes are limited.

**Datapath Configuration:** NEON operates on multiple data types within vectors:

- **8-bit elements:** 8×8 (64-bit vector) or 16×8 (128-bit vector) - bytes
- **16-bit elements:** 4×16 (64-bit) or 8×16 (128-bit) - halfwords/16-bit integers/half-floats
- **32-bit elements:** 2×32 (64-bit) or 4×32 (128-bit) - words/32-bit integers/single-precision floats
- **64-bit elements:** 1×64 (64-bit) or 2×64 (128-bit) - doublewords/64-bit integers

**Instruction Categories:**

**Arithmetic:** Add, subtract, multiply, multiply-accumulate, absolute value, negate operations on vectors.

**Logical:** Bitwise AND, OR, XOR, NOT, bit clear operations.

**Comparison:** Element-wise comparisons generating mask results.

**Shifts and Rotates:** Vector shifts (logical/arithmetic), rotates, narrow/widen operations.

**Load/Store:** Multiple-element structure loads/stores for interleaved data, single-element access, alignment-specific operations.

**Permutation:** Vector extract, zip (interleave), unzip (deinterleave), transpose, reverse, table lookup operations.

**Conversion:** Type conversions between integer/float, widening/narrowing conversions, fixed-point conversions.

**Execution Pipeline:** [Inference based on typical ARM Cortex-A implementations] NEON instructions execute through dedicated SIMD execution units separate from the integer pipeline. The NEON pipeline depth varies by processor (9-10 stages in Cortex-A8/A9, shorter in newer cores). Load/store operations access memory through dedicated NEON load/store units with independent queues.

**IEEE 754 Non-Compliance in NEON:** NEON floating-point deviates from IEEE 754 in specific ways:

- **Denormalized numbers:** Flushed to zero on input and output
- **NaN handling:** Only quiet NaNs supported; operations may not propagate NaNs as IEEE 754 specifies
- **Rounding modes:** Only round-to-nearest supported (no round-toward-zero, toward-infinity modes)
- **Exception handling:** Does not generate floating-point exceptions; sets flags in FPSCR instead

These deviations improve performance and reduce hardware complexity but make NEON unsuitable for applications requiring strict IEEE 754 compliance.

## Vector Registers

NEON and VFP share a unified register bank with dual addressing modes. The register file consists of 32 single-precision (32-bit) registers or 16 double-precision (64-bit) registers in VFP mode, and 32 64-bit registers or 16 128-bit registers in NEON mode.

**Register Naming Conventions:**

**VFP Registers:**

- S0-S31: 32 single-precision (32-bit) registers
- D0-D15: 16 double-precision (64-bit) registers
- Overlapping mapping: D0 = {S1, S0}, D1 = {S3, S2}, etc.

**NEON Registers:**

- D0-D31: 32 doubleword (64-bit) registers
- Q0-Q15: 16 quadword (128-bit) registers
- Overlapping mapping: Q0 = {D1, D0}, Q1 = {D3, D2}, etc.

**Register Bank Structure:**

```
128-bit Quadword Registers (Q):
Q0:  |    D1    |    D0    |
Q1:  |    D3    |    D2    |
Q2:  |    D5    |    D4    |
...
Q15: |   D31    |   D30    |

64-bit Doubleword Registers (D):
D0, D1, D2, D3, ..., D31

32-bit Single Registers (S):
S0, S1, S2, S3, ..., S31 (map to D0-D15)
```

**Physical Organization:** [Inference based on ARM documentation] The register file physically contains 32×64-bit storage elements. Q registers access pairs of consecutive D registers. Writes to a Q register modify both constituent D registers; writes to a D register affect the corresponding half of the Q register.

**Register Overlap Examples:**

```assembly
; Writing to Q0 affects D0 and D1
VMOV.I32 Q0, #0          ; Clear Q0 (D0 and D1 become zero)

; Writing to D1 affects Q0
VMOV.I32 D1, #0xFF       ; Modify D1 (upper half of Q0)

; VFP S registers map to NEON D0-D15 only
VMOV.F32 S0, #1.0        ; S0 is lower 32 bits of D0
```

**Register Allocation:** Compilers typically allocate registers following calling conventions:

**ARM Procedure Call Standard (AAPCS):**

- D0-D7 (Q0-Q3, S0-S15): Argument passing and return values, caller-saved
- D8-D15 (Q4-Q7): Callee-saved (must be preserved across function calls)
- D16-D31 (Q8-Q15): Caller-saved, not used for parameter passing

Functions must preserve D8-D15 if modified, but D0-D7 and D16-D31 can be freely used without preservation.

**Register Usage Patterns:**

**Scalar VFP operations:**

```assembly
VLDR.F32  S0, [R0]       ; Load single float into S0
VLDR.F32  S1, [R1]       ; Load single float into S1
VADD.F32  S2, S0, S1     ; S2 = S0 + S1 (scalar addition)
VSTR.F32  S2, [R2]       ; Store result
```

**Vector NEON operations:**

```assembly
VLD1.32   {D0, D1}, [R0] ; Load 4×32-bit values into Q0
VLD1.32   {D2, D3}, [R1] ; Load 4×32-bit values into Q1
VADD.I32  Q2, Q0, Q1     ; Q2 = Q0 + Q1 (4 additions in parallel)
VST1.32   {D4, D5}, [R2] ; Store 4×32-bit results
```

**Lane Extraction:** Individual elements within vector registers can be accessed:

```assembly
VMOV.32   R0, D0[0]      ; Extract lane 0 of D0 to R0
VMOV.32   D1[1], R1      ; Insert R1 into lane 1 of D1
```

**Register Pressure Management:** With only 32 D registers (16 Q registers), register pressure becomes significant in complex NEON code. Strategies include:

- Reusing registers after values are consumed
- Spilling to memory when necessary
- Loop unrolling limited by available registers
- Careful ordering to minimize live ranges

**FPSCR (Floating-Point Status and Control Register):** Controls floating-point and NEON behavior:

- Rounding modes (VFP only)
- Flush-to-zero mode
- Default NaN mode
- Exception flags (invalid operation, divide by zero, overflow, underflow, inexact)
- Vector length and stride (deprecated in NEON)

```assembly
VMRS  R0, FPSCR          ; Read FPSCR to R0
VMSR  FPSCR, R1          ; Write R1 to FPSCR
```

## Parallel Data Processing

NEON's parallel processing capabilities enable significant performance improvements for data-parallel workloads. Single instructions operate on multiple data elements simultaneously, achieving throughput multiplication proportional to vector width.

**Data Parallelism Models:**

**Element-wise Operations:** Independent operations on corresponding elements across vectors:

```assembly
; Add 4 pairs of 32-bit integers in parallel
VADD.I32  Q0, Q1, Q2     ; Q0[i] = Q1[i] + Q2[i] for i=0,1,2,3
```

**Reduction Operations:** Accumulating results across vector elements:

```assembly
; Sum all elements in a vector
VPADD.I32 D0, D1, D2     ; Pairwise add: D0[0]=D1[0]+D1[1], D0[1]=D2[0]+D2[1]
```

**Broadcasting:** Replicating scalar values across vector lanes:

```assembly
VDUP.32   Q0, R0         ; Duplicate R0 to all 4 lanes of Q0
```

**Common Parallel Processing Patterns:**

**Vector Addition Example:**

```assembly
; Add two arrays of 16 32-bit integers
; R0 = array A base address
; R1 = array B base address  
; R2 = result array base address

    MOV     R3, #4               ; Loop counter (16/4 = 4 iterations)
loop:
    VLD1.32 {Q0}, [R0]!          ; Load 4 elements from A, post-increment
    VLD1.32 {Q1}, [R1]!          ; Load 4 elements from B, post-increment
    VADD.I32 Q2, Q0, Q1          ; Parallel add 4 elements
    VST1.32 {Q2}, [R2]!          ; Store 4 results, post-increment
    SUBS    R3, R3, #1           ; Decrement counter
    BNE     loop                 ; Loop if not zero
```

This processes 16 elements with 4 loop iterations, achieving 4× throughput compared to scalar code.

**Multiply-Accumulate Pattern:**

```assembly
; Dot product: sum(A[i] * B[i])
; Input: Q0 = A vector, Q1 = B vector, Q2 = accumulator

    VMLA.I32  Q2, Q0, Q1         ; Q2 += Q0 * Q1 (multiply-accumulate)
```

VMLA performs element-wise multiplication and adds results to the accumulator in a single instruction, crucial for matrix operations and signal processing.

**Widening and Narrowing Operations:** Handle mixed precision:

```assembly
; Widen 8-bit to 16-bit, multiply, narrow back
VMOVL.U8  Q0, D0               ; Widen 8 bytes to 8 halfwords
VMOVL.U8  Q1, D1               ; Widen 8 bytes to 8 halfwords
VMUL.I16  Q2, Q0, Q1           ; Multiply 8 halfwords
VSHRN.I16 D4, Q2, #8           ; Narrow and shift 8 halfwords to 8 bytes
```

**Load/Store Interleaving:** Efficiently handle structure-of-arrays (SoA) and array-of-structures (AoS) conversions:

**Interleaved load (AoS to SoA):**

```assembly
; Load RGB pixels: RGBRGBRGBRGB -> separate R, G, B vectors
VLD3.8  {D0, D1, D2}, [R0]     ; D0=RRRR, D1=GGGG, D2=BBBB
```

**Interleaved store (SoA to AoS):**

```assembly
; Store separate R, G, B vectors as RGB pixels
VST3.8  {D0, D1, D2}, [R0]     ; Interleave: RGBRGBRGBRGB
```

VLD3/VST3 automatically deinterleave/interleave 3-element structures. VLD2/VST2 and VLD4/VST4 handle 2 and 4-element structures respectively.

**Permutation and Rearrangement:**

**Vector zip (interleave):**

```assembly
VZIP.32  D0, D1                ; Interleave elements: 
                               ; D0={a0,a1}, D1={b0,b1} -> D0={a0,b0}, D1={a1,b1}
```

**Vector unzip (deinterleave):**

```assembly
VUZP.32  D0, D1                ; Deinterleave elements:
                               ; D0={a0,b0}, D1={a1,b1} -> D0={a0,a1}, D1={b0,b1}
```

**Vector extract:**

```assembly
VEXT.8   D0, D1, D2, #4        ; Extract: concatenate D1:D2, extract 8 bytes at offset 4
```

**Table lookup:**

```assembly
VTBL.8   D0, {D1, D2}, D3      ; Use D3 as indices into table {D1, D2}, store in D0
```

These operations enable complex data reorganization without scalar intervention.

**Saturating Arithmetic:** Prevents overflow by clamping results to representable range:

```assembly
VQADD.S16  Q0, Q1, Q2          ; Saturating add: clamp to [-32768, 32767]
```

Useful for signal processing and image operations where overflow should saturate rather than wrap.

**Comparison and Selection:**

```assembly
VCGT.S32  Q0, Q1, Q2           ; Compare: Q0[i] = (Q1[i] > Q2[i]) ? 0xFFFFFFFF : 0
VBSL      Q3, Q0, Q1, Q2       ; Bitwise select: Q3 = (Q0 & Q1) | (~Q0 & Q2)
```

VCGT generates a mask; VBSL (bit select) uses the mask to choose elements from two vectors, implementing conditional operations without branching.

**Performance Characteristics:** [Inference based on typical Cortex-A series specifications]

- Most NEON arithmetic instructions: 1 cycle throughput, 3-4 cycle latency
- Multiply operations: 1-2 cycle throughput, 4-5 cycle latency
- Load/store: 1 cycle throughput for aligned accesses, penalties for misalignment
- Complex operations (sqrt, divide): Multi-cycle execution

Optimal code maintains instruction-level parallelism by scheduling independent operations to hide latency.

**Example: RGB to Grayscale Conversion**

Scalar code processes one pixel per iteration:

```assembly
; R0 = source RGB, R1 = destination Gray, R2 = count
scalar_loop:
    LDRB    R3, [R0, #0]         ; Load R
    LDRB    R4, [R0, #1]         ; Load G
    LDRB    R5, [R0, #2]         ; Load B
    MOV     R6, R3               ; Gray = R * 0.299
    MUL     R6, R6, #77          ; Approximate 0.299 * 256
    MOV     R7, R4               ; + G * 0.587
    MUL     R7, R7, #150         ; Approximate 0.587 * 256
    ADD     R6, R6, R7
    MOV     R7, R5               ; + B * 0.114
    MUL     R7, R7, #29          ; Approximate 0.114 * 256
    ADD     R6, R6, R7
    LSR     R6, R6, #8           ; Divide by 256
    STRB    R6, [R1], #1         ; Store result
    ADD     R0, R0, #3           ; Next pixel
    SUBS    R2, R2, #1
    BNE     scalar_loop
```

NEON code processes 8 pixels per iteration:

```assembly
; R0 = source RGB, R1 = destination Gray, R2 = count (multiple of 8)
    VMOV.I8  D5, #77             ; R coefficient
    VMOV.I8  D6, #150            ; G coefficient
    VMOV.I8  D7, #29             ; B coefficient
neon_loop:
    VLD3.8   {D0, D1, D2}, [R0]! ; Load 8 RGB pixels (24 bytes)
                                 ; D0=RRRRRRRR, D1=GGGGGGGG, D2=BBBBBBBB
    VMULL.U8 Q2, D0, D5          ; R * 77 (widen to 16-bit)
    VMLAL.U8 Q2, D1, D6          ; + G * 150 (multiply-accumulate)
    VMLAL.U8 Q2, D2, D7          ; + B * 29
    VSHRN.U16 D3, Q2, #8         ; Shift right 8, narrow to 8-bit
    VST1.8   D3, [R1]!           ; Store 8 grayscale pixels
    SUBS     R2, R2, #8
    BNE      neon_loop
```

The NEON version achieves approximately 8× throughput, processing 8 pixels with similar instruction count as scalar processes 1.

## NEON Intrinsics vs Assembly

Developers can use NEON through hand-written assembly or compiler intrinsics. Each approach offers distinct trade-offs in portability, maintainability, and control.

**NEON Intrinsics:** C/C++ functions provided by ARM's ACLE (ARM C Language Extensions) specification that map directly to NEON instructions. Intrinsics have prefix `v` or `vq` and use types like `uint8x8_t`, `float32x4_t`, etc.

**Intrinsic Type System:**

**Vector types:**

- `int8x8_t`, `int8x16_t`: 8-bit signed integer vectors (64-bit/128-bit)
- `uint8x8_t`, `uint8x16_t`: 8-bit unsigned integer vectors
- `int16x4_t`, `int16x8_t`: 16-bit signed integer vectors
- `uint16x4_t`, `uint16x8_t`: 16-bit unsigned integer vectors
- `int32x2_t`, `int32x4_t`: 32-bit signed integer vectors
- `uint32x2_t`, `uint32x4_t`: 32-bit unsigned integer vectors
- `int64x1_t`, `int64x2_t`: 64-bit signed integer vectors
- `uint64x1_t`, `uint64x2_t`: 64-bit unsigned integer vectors
- `float32x2_t`, `float32x4_t`: 32-bit floating-point vectors
- `float16x4_t`, `float16x8_t`: 16-bit floating-point vectors (ARMv8.2+)

**Structure types:**

- `int32x4x2_t`: Structure of 2 vectors (for interleaved loads)
- `int32x4x3_t`: Structure of 3 vectors
- `int32x4x4_t`: Structure of 4 vectors

**Intrinsic Naming Convention:**

```
v[q][operation][_lane][_type]
```

- `v`: Vector prefix
- `q`: Quadword (128-bit) operation (absent for 64-bit)
- `operation`: Instruction mnemonic (add, mul, ld1, etc.)
- `_lane`: Lane-specific operation (optional)
- `_type`: Element type suffix (s8, u16, f32, etc.)

**Example** intrinsic names:

- `vaddq_s32`: Add quadword (128-bit) signed 32-bit integers
- `vmul_f32`: Multiply doubleword (64-bit) 32-bit floats
- `vld1q_u8`: Load one quadword of unsigned 8-bit integers
- `vget_lane_s16`: Extract lane from signed 16-bit vector

**Intrinsics Example: Vector Addition**

```c
#include <arm_neon.h>

void vector_add_intrinsics(const int32_t *a, const int32_t *b, int32_t *result, int count) {
    for (int i = 0; i < count; i += 4) {
        int32x4_t va = vld1q_s32(&a[i]);        // Load 4 int32 from a
        int32x4_t vb = vld1q_s32(&b[i]);        // Load 4 int32 from b
        int32x4_t vr = vaddq_s32(va, vb);       // Add vectors
        vst1q_s32(&result[i], vr);              // Store 4 int32 to result
    }
}
```

Equivalent assembly:

```assembly
vector_add_asm:
    ; R0=a, R1=b, R2=result, R3=count
loop:
    VLD1.32 {Q0}, [R0]!
    VLD1.32 {Q1}, [R1]!
    VADD.I32 Q2, Q0, Q1
    VST1.32 {Q2}, [R2]!
    SUBS    R3, R3, #4
    BNE     loop
    BX      LR
```

**Intrinsics Example: Dot Product**

```c
#include <arm_neon.h>

float dot_product_intrinsics(const float *a, const float *b, int count) {
    float32x4_t sum = vdupq_n_f32(0.0f);     // Initialize accumulator to zero
    
    for (int i = 0; i < count; i += 4) {
        float32x4_t va = vld1q_f32(&a[i]);   // Load 4 floats from a
        float32x4_t vb = vld1q_f32(&b[i]);   // Load 4 floats from b
        sum = vmlaq_f32(sum, va, vb);        // sum += va * vb
    }
    
    // Horizontal sum: sum all 4 lanes
    float32x2_t sum_low = vget_low_f32(sum);
    float32x2_t sum_high = vget_high_f32(sum);
    float32x2_t sum_pair = vadd_f32(sum_low, sum_high);
    float32x2_t sum_pairwise = vpadd_f32(sum_pair, sum_pair);
    
    return vget_lane_f32(sum_pairwise, 0);
}
```

**Intrinsics Example: Interleaved Load/Store**

```c
#include <arm_neon.h>

void separate_rgb_intrinsics(const uint8_t *rgb, uint8_t *r, uint8_t *g, uint8_t *b, int pixels) {
    for (int i = 0; i < pixels; i += 8) {
        uint8x8x3_t rgb_pixels = vld3_u8(&rgb[i * 3]);  // Load 8 RGB triplets
        vst1_u8(&r[i], rgb_pixels.val[0]);              // Store R channel
        vst1_u8(&g[i], rgb_pixels.val[1]);              // Store G channel
        vst1_u8(&b[i], rgb_pixels.val[2]);              // Store B channel
    }
}
```

The `vld3_u8` intrinsic maps directly to VLD3.8 instruction, automatically deinterleaving RGB data.

**Assembly Advantages:**

**Precise Control:** Direct register allocation, instruction scheduling, and pipeline management. Critical for squeezing maximum performance from tight loops.

**Register Reuse:** Explicit control over register lifetime enables optimal reuse patterns not always achieved by compilers.

**Instruction Selection:** Access to all NEON instructions including less common operations that may lack intrinsic mappings.

**Optimization Techniques:** Manual loop unrolling, software pipelining, prefetching control.

**Example** assembly optimization with manual scheduling:

```assembly
; Optimized with instruction reordering to hide latency
loop:
    VLD1.32  {Q0}, [R0]!         ; Load A (1 cycle)
    VLD1.32  {Q1}, [R1]!         ; Load B (1 cycle)
    PLD      [R0, #64]           ; Prefetch next A
    VADD.I32 Q2, Q0, Q1          ; Add (latency 4 cycles)
    PLD      [R1, #64]           ; Prefetch next B
    SUBS     R3, R3, #4          ; Decrement counter (fills delay slots)
    VST1.32  {Q2}, [R2]!         ; Store result
    BNE      loop
```

**Intrinsics Advantages:**

**Portability:** Code compiles for multiple architectures. Compiler generates appropriate instructions for target (NEON, SVE, x86 SSE/AVX with translation).

**Maintainability:** C/C++ syntax easier to read, modify, and debug than assembly. Integration with existing codebases straightforward.

**Compiler Optimizations:** Modern compilers apply register allocation, instruction scheduling, loop optimizations. [Inference] Compilers like GCC and Clang have sophisticated NEON optimization passes.

**Type Safety:** Compiler enforces type checking, preventing common assembly errors like register size mismatches.

**ABI Compliance:** Intrinsics automatically follow calling conventions for register preservation and parameter passing.

**Example** intrinsics with compiler auto-vectorization:

```c
#include <arm_neon.h>

// Compiler may further optimize this
void matrix_multiply_4x4(const float *A, const float *B, float *C) {
    for (int i = 0; i < 4; i++) {
        float32x4_t row = vld1q_f32(&A[i * 4]);
        for (int j = 0; j < 4; j++) {
            float32x4_t col = {B[j], B[j+4], B[j+8], B[j+12]};
            float32x4_t prod = vmulq_f32(row, col);
            float32x2_t sum = vadd_f32(vget_low_f32(prod), vget_high_f32(prod));
            sum = vpadd_f32(sum, sum);
            C[i * 4 + j] = vget_lane_f32(sum, 0);
        }
    }
}
```

**Performance Comparison:** [Inference based on observed compiler behavior]

**Well-written intrinsics:** Typically achieve 85-95% of hand-optimized assembly performance. Compilers have improved significantly; the gap has narrowed.

**Hand-optimized assembly:** Achieves maximum performance (100% baseline) but requires expert knowledge and extensive tuning.

**Auto-vectorized C code:** Without intrinsics, compiler auto-vectorization achieves 60-80% of optimal performance for regular patterns, less for complex code.

**Mixed Approach:** Practical development often combines approaches:

```c
#include <arm_neon.h>

void process_data(const float *input, float *output, int count) {
    // Use intrinsics for readability
    int i = 0;
    for (; i < count - 4; i += 4) {
        float32x4_t v = vld1q_f32(&input[i]);
        v = vmulq_f32(v, vdupq_n_f32(2.0f));
        vst1q_f32(&output[i], v);
    }
    
    // Critical section in assembly if needed
    if (i < count) {
        __asm__ volatile (
            "vld1.32 {d0}, [%0]\n"
            "vmul.f32 d0, d0, %1\n"
            "vst1.32 {d0}, [%2]\n"
            : 
            : "r"(&input[i]), "w"(2.0f), "r"(&output[i])
            : "d0", "memory"
        );
    }
}
```

**Debugging Considerations:**

**Intrinsics:** Standard C debuggers work normally. Inspect variables, single-step through code, examine vector contents.

**Assembly:** Requires assembly-level debugging. View register contents through debugger register windows. More difficult to correlate with source.

**Build System Integration:**

Intrinsics require compiler flags to enable NEON:

```bash
# GCC/Clang
gcc -mfpu=neon -mfloat-abi=hard -O3 -o program program.c

# ARMv8 (NEON mandatory)
gcc -march=armv8-a -O3 -o program program.c
```

Assembly modules integrate via inline assembly or separate `.s` files linked into the build.

**Key Points:**

- NEON provides SIMD parallelism for 8, 16, 32, 64-bit data types
- 32 D registers (64-bit) or 16 Q registers (128-bit) shared with VFP
- Parallel processing achieves throughput multiplication (4×, 8×, 16× depending on element size)
- Intrinsics offer portability and maintainability with near-optimal performance
- Hand-coded assembly provides maximum control for performance-critical sections
- Modern approach favors intrinsics with selective assembly optimization

---

# ARM System Programming

## Processor Modes (User, FIQ, IRQ, Supervisor, etc.)

ARM processors operate in different privilege modes that control access to system resources and provide isolation between operating system code and user applications.

**ARMv7 (32-bit) Processor Modes:**

**User Mode (USR):**

- Unprivileged mode for normal application execution
- Cannot access protected system resources
- Cannot change CPSR mode bits
- Cannot execute privileged instructions
- No access to banked registers of other modes

**System Mode (SYS):**

- Privileged mode with same register set as User mode
- Can access all system resources
- Used by operating system tasks that need privileges
- Shares R0-R15 with User mode (no banking)

**Fast Interrupt Mode (FIQ):**

- Entered on FIQ exception
- Priority over IRQ
- Has banked registers: R8_fiq through R14_fiq, SPSR_fiq
- Seven banked registers allow faster interrupt handling (less context saving)
- Vector address: 0x0000001C or 0xFFFF001C

**Interrupt Mode (IRQ):**

- Entered on IRQ exception
- Normal interrupt processing
- Has banked registers: R13_irq, R14_irq, SPSR_irq
- Vector address: 0x00000018 or 0xFFFF0018

**Supervisor Mode (SVC):**

- Entered on reset or SVC (software interrupt) instruction
- Used by operating system kernel
- Has banked registers: R13_svc, R14_svc, SPSR_svc
- Vector address: 0x00000008 or 0xFFFF0008

**Abort Mode (ABT):**

- Entered on memory access violations
- Data abort or prefetch abort
- Has banked registers: R13_abt, R14_abt, SPSR_abt
- Data abort vector: 0x00000010 or 0xFFFF0010
- Prefetch abort vector: 0x0000000C or 0xFFFF000C

**Undefined Mode (UND):**

- Entered when undefined instruction executed
- Used for software emulation of instructions
- Has banked registers: R13_und, R14_und, SPSR_und
- Vector address: 0x00000004 or 0xFFFF0004

**Monitor Mode (MON) - Security Extensions:**

- Secure state management
- TrustZone context switching
- Has banked registers: R13_mon, R14_mon, SPSR_mon

**Hyp Mode (HYP) - Virtualization Extensions:**

- Hypervisor mode for virtualization
- Most privileged mode in ARMv7
- Has banked registers: R13_hyp, SPSR_hyp, ELR_hyp

**Register banking:**

Each mode has access to:

- R0-R7: Shared across all modes (not banked)
- R8-R12: Banked only in FIQ mode (R8_fiq-R12_fiq)
- R13 (SP): Banked in all privileged modes
- R14 (LR): Banked in all privileged modes
- R15 (PC): Shared across all modes
- CPSR: Current Program Status Register (shared, but modified by mode)
- SPSR: Saved Program Status Register (banked in privileged modes)

```
Mode    | R0-R7 | R8-R12 | R13(SP) | R14(LR) | R15(PC) | CPSR | SPSR
--------|-------|--------|---------|---------|---------|------|------
User    | ✓     | ✓      | ✓       | ✓       | ✓       | ✓    | -
System  | ✓     | ✓      | ✓       | ✓       | ✓       | ✓    | -
FIQ     | ✓     | Banked | Banked  | Banked  | ✓       | ✓    | ✓
IRQ     | ✓     | ✓      | Banked  | Banked  | ✓       | ✓    | ✓
SVC     | ✓     | ✓      | Banked  | Banked  | ✓       | ✓    | ✓
ABT     | ✓     | ✓      | Banked  | Banked  | ✓       | ✓    | ✓
UND     | ✓     | ✓      | Banked  | Banked  | ✓       | ✓    | ✓
```

**CPSR Mode bits (bits 4-0):**

```
10000 (0x10) - User mode
10001 (0x11) - FIQ mode
10010 (0x12) - IRQ mode
10011 (0x13) - Supervisor mode
10111 (0x17) - Abort mode
11011 (0x1B) - Undefined mode
11111 (0x1F) - System mode
11010 (0x1A) - Hyp mode (if supported)
10110 (0x16) - Monitor mode (if supported)
```

**CPSR Control bits:**

- N (bit 31): Negative flag
- Z (bit 30): Zero flag
- C (bit 29): Carry flag
- V (bit 28): Overflow flag
- Q (bit 27): Saturation flag
- IT[1:0] (bits 26-25): If-Then execution state (Thumb)
- J (bit 24): Jazelle state
- GE[3:0] (bits 19-16): Greater-than-or-Equal flags (SIMD)
- IT[7:2] (bits 15-10): If-Then execution state
- E (bit 9): Endianness (0=little, 1=big)
- A (bit 8): Asynchronous abort disable
- I (bit 7): IRQ disable
- F (bit 6): FIQ disable
- T (bit 5): Thumb state
- M[4:0] (bits 4-0): Mode bits

**ARMv8 Exception Levels (AArch64):**

ARMv8 uses Exception Levels instead of modes:

- EL0: User applications (unprivileged)
- EL1: Operating system kernel (privileged)
- EL2: Hypervisor (virtualization)
- EL3: Secure monitor (TrustZone)

## Mode Switching

Mode switching occurs through exceptions or explicit privileged instructions.

**Automatic mode switching (exceptions):**

When an exception occurs:

1. Current CPSR saved to SPSR of exception mode
2. Mode bits in CPSR changed to exception mode
3. PC saved to LR of exception mode (adjusted for exception type)
4. Interrupt disable bits set (I and/or F)
5. PC loaded with exception vector address

**Software-initiated mode switching:**

**From User mode to privileged mode:**

```asm
; User mode cannot directly change to privileged mode
; Must use SVC instruction to trap to Supervisor mode

user_code:
    ; Running in User mode
    MOV R0, #syscall_number
    SVC #0              ; Generate supervisor call exception
    ; Returns here after SVC handler completes

; In exception vector table
svc_handler:
    ; Now in Supervisor mode
    ; Process system call
    ; Return to User mode with MOVS PC, LR
```

**Privileged mode switching (explicit):**

```asm
; Save current mode and switch to different privileged mode
; Must be in privileged mode to execute this

switch_to_irq_mode:
    MRS R0, CPSR        ; Read current CPSR
    BIC R1, R0, #0x1F   ; Clear mode bits
    ORR R1, R1, #0x12   ; Set IRQ mode bits
    MSR CPSR_c, R1      ; Write CPSR (control field)
    
    ; Now in IRQ mode
    ; Can access IRQ mode banked registers
    MOV R13, #irq_stack_top  ; Setup IRQ stack
    
    ; Restore original mode
    MSR CPSR_c, R0      ; Restore saved CPSR
    BX LR

; Switch to FIQ mode
switch_to_fiq:
    MRS R0, CPSR
    BIC R1, R0, #0x1F
    ORR R1, R1, #0x11   ; FIQ mode
    MSR CPSR_c, R1
    ; Setup FIQ stack and registers
    MSR CPSR_c, R0
    BX LR
```

**Setting up mode stacks:**

```asm
; Initialize stack pointers for all modes
; Must be called in privileged mode (typically Supervisor)

setup_stacks:
    ; Save return address
    MOV R2, LR
    
    ; Setup IRQ stack
    MRS R0, CPSR
    BIC R1, R0, #0x1F
    ORR R1, R1, #0x12   ; IRQ mode
    MSR CPSR_c, R1
    LDR SP, =irq_stack_top
    
    ; Setup FIQ stack
    BIC R1, R0, #0x1F
    ORR R1, R1, #0x11   ; FIQ mode
    MSR CPSR_c, R1
    LDR SP, =fiq_stack_top
    
    ; Setup Abort stack
    BIC R1, R0, #0x1F
    ORR R1, R1, #0x17   ; Abort mode
    MSR CPSR_c, R1
    LDR SP, =abt_stack_top
    
    ; Setup Undefined stack
    BIC R1, R0, #0x1F
    ORR R1, R1, #0x1B   ; Undefined mode
    MSR CPSR_c, R1
    LDR SP, =und_stack_top
    
    ; Setup System/User stack
    BIC R1, R0, #0x1F
    ORR R1, R1, #0x1F   ; System mode (shares User SP)
    MSR CPSR_c, R1
    LDR SP, =user_stack_top
    
    ; Return to Supervisor mode
    MSR CPSR_c, R0
    MOV PC, R2
```

**Returning from exception:**

```asm
; Return from exception to previous mode
; Restores CPSR from SPSR and adjusts PC

irq_handler:
    ; Exception handler code
    ; ...
    
    ; Return using special instruction
    SUBS PC, LR, #4     ; Adjust LR and restore CPSR from SPSR
    ; The 'S' suffix causes CPSR := SPSR

; Alternative explicit method
return_from_exception:
    MSR CPSR_c, SPSR    ; Restore CPSR from SPSR
    MOV PC, LR          ; Return to saved PC
    
; For nested exceptions, must preserve SPSR
nested_exception_return:
    ; Save SPSR before it gets overwritten
    MRS R0, SPSR
    PUSH {R0}
    
    ; Process exception...
    
    ; Restore and return
    POP {R0}
    MSR SPSR_cxsf, R0
    SUBS PC, LR, #4
```

**LR adjustment values for different exceptions:**

```asm
; Different exceptions require different LR adjustments
; on return due to pipeline effects

; SVC (Software Interrupt): no adjustment
MOVS PC, LR             ; or SUBS PC, LR, #0

; Undefined instruction: no adjustment
MOVS PC, LR

; Prefetch abort: adjust by 4
SUBS PC, LR, #4

; Data abort: adjust by 8
SUBS PC, LR, #8

; IRQ: adjust by 4
SUBS PC, LR, #4

; FIQ: adjust by 4
SUBS PC, LR, #4
```

## Exception Handling

Exception handling is the mechanism by which the processor responds to exceptional conditions including interrupts, software interrupts, and errors.

**Exception Vector Table:**

The exception vector table contains branch instructions to exception handlers:

```asm
.section .vectors
.org 0x00000000         ; or 0xFFFF0000 with high vectors

vector_table:
    LDR PC, reset_addr      ; 0x00: Reset
    LDR PC, undef_addr      ; 0x04: Undefined instruction
    LDR PC, svc_addr        ; 0x08: Software interrupt (SVC)
    LDR PC, pabt_addr       ; 0x0C: Prefetch abort
    LDR PC, dabt_addr       ; 0x10: Data abort
    NOP                     ; 0x14: Reserved
    LDR PC, irq_addr        ; 0x18: IRQ
    LDR PC, fiq_addr        ; 0x1C: FIQ

; Handler addresses
reset_addr:     .word reset_handler
undef_addr:     .word undef_handler
svc_addr:       .word svc_handler
pabt_addr:      .word pabt_handler
dabt_addr:      .word dabt_handler
irq_addr:       .word irq_handler
fiq_addr:       .word fiq_handler

; Alternative: Direct branches (if handlers within 32MB)
vector_table_direct:
    B reset_handler         ; 0x00
    B undef_handler         ; 0x04
    B svc_handler           ; 0x08
    B pabt_handler          ; 0x0C
    B dabt_handler          ; 0x10
    B .                     ; 0x14: Infinite loop for reserved
    B irq_handler           ; 0x18
    B fiq_handler           ; 0x1C
```

**Setting vector base address:**

```asm
; Configure VBAR (Vector Base Address Register)
; ARMv7 Security Extensions or later

set_vector_base:
    LDR R0, =vector_table
    MCR p15, 0, R0, c12, c0, 0  ; Write VBAR
    ISB                          ; Instruction Synchronization Barrier
    BX LR

; Check if high vectors enabled
check_high_vectors:
    MRC p15, 0, R0, c1, c0, 0   ; Read SCTLR
    TST R0, #0x2000              ; Test V bit (bit 13)
    BX LR                        ; Z flag set if low vectors
```

**Reset Exception Handler:**

```asm
reset_handler:
    ; Disable interrupts
    CPSID if                    ; Disable IRQ and FIQ
    
    ; Setup vector table
    LDR R0, =vector_table
    MCR p15, 0, R0, c12, c0, 0  ; Set VBAR
    
    ; Setup stacks for all modes
    BL setup_stacks
    
    ; Initialize MMU, caches (if needed)
    ; BL mmu_init
    ; BL cache_init
    
    ; Copy data section from ROM to RAM
    LDR R0, =_data_load
    LDR R1, =_data_start
    LDR R2, =_data_end
copy_data:
    CMP R1, R2
    LDRLT R3, [R0], #4
    STRLT R3, [R1], #4
    BLT copy_data
    
    ; Zero BSS section
    LDR R0, =_bss_start
    LDR R1, =_bss_end
    MOV R2, #0
zero_bss:
    CMP R0, R1
    STRLT R2, [R0], #4
    BLT zero_bss
    
    ; Switch to System mode for main execution
    CPS #0x1F               ; System mode
    
    ; Enable interrupts
    CPSIE if
    
    ; Branch to main program
    BL main
    
    ; If main returns, infinite loop
halt:
    B halt
```

**Undefined Instruction Handler:**

```asm
undef_handler:
    ; Save context
    STMFD SP!, {R0-R12, LR}
    
    ; Get undefined instruction address
    SUB R0, LR, #4          ; LR points to next instruction
    
    ; Get the undefined instruction
    LDR R1, [R0]
    
    ; Check if this is a coprocessor instruction we can emulate
    ; Or if it's an intentional undefined instruction for software
    
    ; Example: Emulate a custom instruction
    LDR R2, =0xE7F000F0     ; Undefined instruction pattern
    BIC R3, R1, #0x0F       ; Mask off immediate
    CMP R3, R2
    BEQ emulate_custom
    
    ; Real undefined instruction - report error
    MOV R0, R1              ; Pass instruction to error handler
    BL undefined_instruction_error
    
    ; Restore context and return
restore_und:
    LDMFD SP!, {R0-R12, PC}^ ; ^ restores CPSR from SPSR
    
emulate_custom:
    ; Emulate the instruction
    ; ...
    B restore_und
```

**Prefetch Abort Handler:**

```asm
pabt_handler:
    ; Save context
    SUB LR, LR, #4          ; Adjust return address
    STMFD SP!, {R0-R12, LR}
    
    ; Get abort address
    MRS R0, SPSR
    PUSH {R0}               ; Save SPSR
    
    ; Read IFSR (Instruction Fault Status Register)
    MRC p15, 0, R1, c5, c0, 1
    
    ; Read IFAR (Instruction Fault Address Register)
    MRC p15, 0, R2, c6, c0, 2
    
    ; Determine fault type
    AND R3, R1, #0x0F       ; Extract fault status
    
    ; Handle different fault types
    CMP R3, #0x05           ; Translation fault?
    BEQ handle_translation_fault
    
    CMP R3, #0x07           ; Permission fault?
    BEQ handle_permission_fault
    
    ; Unknown fault
    B abort_error
    
handle_translation_fault:
    ; Page fault handling
    MOV R0, R2              ; Fault address
    BL page_fault_handler
    B pabt_return
    
handle_permission_fault:
    ; Permission violation
    MOV R0, R2
    BL permission_fault_handler
    B pabt_return
    
pabt_return:
    ; Restore context
    POP {R0}
    MSR SPSR_cxsf, R0
    LDMFD SP!, {R0-R12, PC}^

abort_error:
    ; Fatal error
    B .
```

**Data Abort Handler:**

```asm
dabt_handler:
    ; Save context
    SUB LR, LR, #8          ; Adjust return address (data abort)
    STMFD SP!, {R0-R12, LR}
    
    MRS R0, SPSR
    PUSH {R0}
    
    ; Read DFSR (Data Fault Status Register)
    MRC p15, 0, R1, c5, c0, 0
    
    ; Read DFAR (Data Fault Address Register)
    MRC p15, 0, R2, c6, c0, 0
    
    ; Determine if read or write
    TST R1, #0x800          ; WnR bit
    BNE write_abort
    
read_abort:
    ; Handle read abort
    MOV R0, R2              ; Fault address
    MOV R1, #0              ; Read flag
    BL data_abort_handler
    B dabt_return
    
write_abort:
    ; Handle write abort
    MOV R0, R2
    MOV R1, #1              ; Write flag
    BL data_abort_handler
    
dabt_return:
    POP {R0}
    MSR SPSR_cxsf, R0
    LDMFD SP!, {R0-R12, PC}^
```

**Software Interrupt (SVC) Handler:**

```asm
svc_handler:
    ; Save context
    STMFD SP!, {R0-R12, LR}
    
    ; Get SVC number
    MRS R2, SPSR            ; Get caller's CPSR
    TST R2, #0x20           ; Check Thumb bit
    LDRNEH R0, [LR, #-2]    ; Thumb: 8-bit immediate
    BICNE R0, R0, #0xFF00   ; Extract lower byte
    LDREQ R0, [LR, #-4]     ; ARM: 24-bit immediate
    BICEQ R0, R0, #0xFF000000
    
    ; R0 now contains SVC number
    ; R1-R7 contain parameters from user
    
    ; Dispatch to system call handler
    CMP R0, #MAX_SYSCALL
    BXGE invalid_syscall
    
    LDR R12, =syscall_table
    LDR PC, [R12, R0, LSL #2]  ; Jump to handler
    
svc_return:
    ; R0 contains return value
    LDMFD SP!, {R0-R12, PC}^   ; Return to user mode

invalid_syscall:
    MOV R0, #-1             ; Error code
    B svc_return

; System call table
syscall_table:
    .word sys_read          ; 0
    .word sys_write         ; 1
    .word sys_open          ; 2
    .word sys_close         ; 3
    ; ...
```

## Interrupt Processing

Interrupt processing handles asynchronous external events (IRQ) and fast interrupts (FIQ).

**Interrupt Controller (GIC - Generic Interrupt Controller):**

ARM systems use an interrupt controller to manage multiple interrupt sources:

```asm
; GIC registers (example addresses - platform specific)
.equ GICD_BASE,     0x08000000  ; Distributor base
.equ GICC_BASE,     0x08010000  ; CPU interface base

; Distributor registers
.equ GICD_CTLR,     0x000       ; Control
.equ GICD_ISENABLER, 0x100      ; Interrupt Set-Enable
.equ GICD_ICENABLER, 0x180      ; Interrupt Clear-Enable
.equ GICD_IPRIORITYR, 0x400     ; Interrupt Priority
.equ GICD_ITARGETSR, 0x800      ; Interrupt Processor Targets

; CPU interface registers
.equ GICC_CTLR,     0x000       ; CPU Interface Control
.equ GICC_PMR,      0x004       ; Priority Mask
.equ GICC_IAR,      0x00C       ; Interrupt Acknowledge
.equ GICC_EOIR,     0x010       ; End of Interrupt
```

**GIC Initialization:**

```asm
gic_init:
    PUSH {LR}
    
    ; Disable GIC distributor
    LDR R0, =GICD_BASE
    MOV R1, #0
    STR R1, [R0, #GICD_CTLR]
    
    ; Disable all interrupts
    LDR R0, =GICD_BASE
    ADD R0, R0, #GICD_ICENABLER
    MOV R1, #0xFFFFFFFF
    MOV R2, #0
disable_loop:
    STR R1, [R0, R2, LSL #2]
    ADD R2, R2, #1
    CMP R2, #32                 ; 32 registers for 1020 interrupts
    BLT disable_loop
    
    ; Set all priorities to lowest
    LDR R0, =GICD_BASE
    ADD R0, R0, #GICD_IPRIORITYR
    LDR R1, =0xA0A0A0A0         ; Priority 0xA0 for all
    MOV R2, #0
priority_loop:
    STR R1, [R0, R2, LSL #2]
    ADD R2, R2, #1
    CMP R2, #255                ; 255 registers for 1020 interrupts
    BLT priority_loop
    
    ; Set all interrupts to target CPU0
    LDR R0, =GICD_BASE
    ADD R0, R0, #GICD_ITARGETSR
    MOV R1, #0x01010101         ; CPU0
    MOV R2, #0
target_loop:
    STR R1, [R0, R2, LSL #2]
    ADD R2, R2, #1
    CMP R2, #255
    BLT target_loop
    
    ; Enable GIC distributor
    LDR R0, =GICD_BASE
    MOV R1, #1
    STR R1, [R0, #GICD_CTLR]
    
    ; Configure CPU interface
    LDR R0, =GICC_BASE
    
    ; Set priority mask to lowest (allow all)
    MOV R1, #0xFF
    STR R1, [R0, #GICC_PMR]
    
    ; Enable CPU interface
    MOV R1, #1
    STR R1, [R0, #GICC_CTLR]
    
    POP {PC}
```

**Enable specific interrupt:**

```asm
; Enable interrupt number in R0
enable_interrupt:
    PUSH {R4, LR}
    
    ; Calculate register and bit position
    MOV R1, R0, LSR #5          ; Register index = IRQ / 32
    AND R2, R0, #0x1F           ; Bit position = IRQ % 32
    MOV R3, #1
    LSL R3, R3, R2              ; Create bit mask
    
    ; Set enable bit
    LDR R4, =GICD_BASE
    ADD R4, R4, #GICD_ISENABLER
    STR R3, [R4, R1, LSL #2]
    
    POP {R4, PC}

; Disable interrupt
disable_interrupt:
    PUSH {R4, LR}
    
    MOV R1, R0, LSR #5
    AND R2, R0, #0x1F
    MOV R3, #1
    LSL R3, R3, R2
    
    LDR R4, =GICD_BASE
    ADD R4, R4, #GICD_ICENABLER
    STR R3, [R4, R1, LSL #2]
    
    POP {R4, PC}
```

**IRQ Handler:**

```asm
irq_handler:
    ; Save context on IRQ stack
    SUB LR, LR, #4              ; Adjust return address
    STMFD SP!, {R0-R3, R12, LR} ; Save working registers
    
    ; Read interrupt ID from GIC
    LDR R0, =GICC_BASE
    LDR R1, [R0, #GICC_IAR]     ; Read IAR (acknowledges interrupt)
    
    ; Save interrupt ID
    MOV R2, R1
    
    ; Check for spurious interrupt
    LDR R3, =1023
    CMP R2, R3
    BEQ irq_spurious
    
    ; Save remaining context if needed for nested interrupts
    MRS R3, SPSR
    STMFD SP!, {R3, R4-R11}     ; Save SPSR and remaining registers
    
    ; Enable IRQ for nested interrupts (optional)
    ; CPSIE i
    
    ; Call C handler with interrupt ID
    MOV R0, R2                  ; Interrupt ID parameter
    BL irq_dispatch             ; Call C function
    
    ; Disable IRQ again if was enabled
    ; CPSID i
    
    ; Restore remaining context
    LDMFD SP!, {R3, R4-R11}
    MSR SPSR_cxsf, R3
    
    ; Signal end of interrupt to GIC
    LDR R0, =GICC_BASE
    STR R2, [R0, #GICC_EOIR]    ; Write interrupt ID to EOIR
    
irq_spurious:
    ; Restore working registers and return
    LDMFD SP!, {R0-R3, R12, PC}^  ; ^ restores CPSR from SPSR

; C-callable interrupt dispatcher
irq_dispatch:
    PUSH {LR}
    
    ; Look up handler in table
    LDR R1, =irq_handler_table
    LDR R2, [R1, R0, LSL #2]    ; Get handler address
    
    ; Check if handler is valid
    CMP R2, #0
    BEQ no_handler
    
    ; Call handler
    BLX R2
    
no_handler:
    POP {PC}
```

**FIQ Handler:**

FIQ is optimized for single, time-critical interrupt source:

```asm
fiq_handler:
    ; No need to save R8-R12 - they're banked in FIQ mode
    ; Only save R0-R7 if needed
    
    SUB LR, LR, #4              ; Adjust return address
    
    ; Minimal context save
    STMFD SP!, {R0-R3, LR}
    
    ; FIQ-specific code - usually handles single device
    ; Direct device register access for minimum latency
    
    ; Example: Service UART FIQ
    LDR R0, =UART_BASE
    LDR R1, [R0, #UART_STATUS]
    TST R1, #RX_READY
    LDRNE R2, [R0, #UART_DATA]
    
    ; Store received data
    LDR R3, =uart_rx_buffer
    STRB R2, [R3], #1
    
    ; Acknowledge FIQ at device level
    MOV R1, #FIQ_CLEAR
    STR R1, [R0, #UART_CLEAR]
    
    ; Restore and return
    LDMFD SP!, {R0-R3, PC}^     ; Return and restore CPSR
```

**Interrupt service routine registration:**

```asm
; Register IRQ handler
; R0 = interrupt number, R1 = handler address
register_irq_handler:
    PUSH {R4, LR}
    
    ; Store handler in table
    LDR R2, =irq_handler_table
    STR R1, [R2, R0, LSL #2]
    
    ; Enable the interrupt
    BL enable_interrupt
    
    POP {R4, PC}

; Unregister handler
unregister_irq_handler:
    PUSH {R4, LR}
    
    ; Disable interrupt
    BL disable_interrupt
    
    ; Clear handler
    LDR R2, =irq_handler_table
    MOV R1, #0
    STR R1, [R2, R0, LSL #2]
    
    POP {R4, PC}

.section .bss
irq_handler_table:
    .space 4 * 1020             ; Table for 1020 interrupts
```

**Critical sections and interrupt masking:**

```asm
; Disable interrupts and save state
disable_interrupts:
    MRS R0, CPSR
    CPSID if                    ; Disable IRQ and FIQ
    BX LR

; Restore interrupt state
restore_interrupts:
    MSR CPSR_c, R0
    BX LR

; Critical section wrapper
critical_section_enter:
    MRS R0, CPSR
    PUSH {R0}                   ; Save CPSR to stack
    CPSID if                    ; Disable IRQ and FIQ
    BX LR

critical_section_exit:
    POP {R0}                    ; Restore saved CPSR
    MSR CPSR_c, R0
    BX LR

; Example usage in C-callable function
atomic_increment:
    ; R0 = pointer to variable
    PUSH {R4, LR}
    
    BL critical_section_enter
    
    ; Critical section - no interrupts
    LDR R1, [R0]
    ADD R1, R1, #1
    STR R1, [R0]
    
    BL critical_section_exit
    
    POP {R4, PC}

; Disable only IRQ (leave FIQ enabled)
disable_irq:
    MRS R0, CPSR
    CPSID i
    BX LR

; Disable only FIQ (leave IRQ enabled)
disable_fiq:
    MRS R0, CPSR
    CPSID f
    BX LR

; Enable interrupts
enable_interrupts:
    CPSIE if
    BX LR
```

**Nested interrupt handling:**

```asm
; IRQ handler with nested interrupt support
irq_handler_nested:
    ; Save minimal context
    SUB LR, LR, #4
    STMFD SP!, {R0-R3, R12, LR}
    
    ; Acknowledge interrupt and get ID
    LDR R0, =GICC_BASE
    LDR R1, [R0, #GICC_IAR]
    MOV R2, R1                  ; Save interrupt ID
    
    ; Check for spurious
    LDR R3, =1023
    CMP R2, R3
    BEQ nested_spurious
    
    ; Save full context for nesting
    MRS R3, SPSR
    STMFD SP!, {R3, R4-R11}
    
    ; Switch to System mode to use user stack
    MRS R3, CPSR
    BIC R4, R3, #0x1F
    ORR R4, R4, #0x1F           ; System mode
    MSR CPSR_c, R4
    
    ; Now on system/user stack - safe to enable interrupts
    CPSIE i                     ; Enable IRQ for nesting
    
    ; Call handler (may be interrupted)
    MOV R0, R2
    BL irq_dispatch
    
    ; Disable interrupts before returning
    CPSID i
    
    ; Switch back to IRQ mode
    MSR CPSR_c, R3
    
    ; Signal EOI
    LDR R0, =GICC_BASE
    STR R2, [R0, #GICC_EOIR]
    
    ; Restore context
    LDMFD SP!, {R3, R4-R11}
    MSR SPSR_cxsf, R3
    
nested_spurious:
    LDMFD SP!, {R0-R3, R12, PC}^
```

**Interrupt latency optimization:**

```asm
; Fast interrupt acknowledge for minimal latency
fast_irq_entry:
    ; Use fastest possible instruction sequence
    SUB LR, LR, #4
    STMFD SP!, {R0-R1}          ; Save only what's needed
    
    ; Inline GIC access (no function call overhead)
    LDR R0, =(GICC_BASE + GICC_IAR)
    LDR R1, [R0]                ; Read and acknowledge
    
    ; Quick spurious check
    CMP R1, #1023
    BEQ fast_spurious
    
    ; Inline handler for critical interrupt
    CMP R1, #CRITICAL_IRQ_ID
    BEQ critical_handler_inline
    
    ; For other interrupts, use normal path
    STMFD SP!, {R2-R3, R12, LR}
    B normal_irq_path
    
critical_handler_inline:
    ; Handle critical interrupt with minimal overhead
    LDR R0, =CRITICAL_DEVICE_BASE
    LDR R2, [R0, #STATUS_REG]
    ; ... handle interrupt directly ...
    STR R2, [R0, #CLEAR_REG]
    
    ; Signal EOI inline
    LDR R0, =(GICC_BASE + GICC_EOIR)
    STR R1, [R0]
    
fast_spurious:
    LDMFD SP!, {R0-R1}
    MOVS PC, LR                 ; Fast return

normal_irq_path:
    ; Continue with normal interrupt processing
    ; ...
```

**Software-triggered interrupts (SGI):**

```asm
; Generate Software Generated Interrupt
; R0 = target CPU mask (bits 0-7 for CPUs 0-7)
; R1 = SGI number (0-15)
trigger_sgi:
    PUSH {R4, LR}
    
    ; Build GICD_SGIR value
    ; [31:26] reserved
    ; [25:24] target list filter (00 = use CPU list)
    ; [23:16] CPU target list
    ; [15] NSATT (Non-Secure)
    ; [3:0] SGI number
    
    LSL R2, R0, #16             ; CPU mask to bits [23:16]
    AND R3, R1, #0x0F           ; SGI number to bits [3:0]
    ORR R2, R2, R3
    
    ; Write to GICD_SGIR
    LDR R4, =GICD_BASE
    STR R2, [R4, #0xF00]        ; GICD_SGIR offset
    
    ; Memory barrier to ensure write completes
    DSB
    
    POP {R4, PC}

; Send SGI to specific CPU
; R0 = CPU number, R1 = SGI number
send_sgi_to_cpu:
    MOV R2, #1
    LSL R0, R2, R0              ; Convert CPU number to mask
    B trigger_sgi

; Send SGI to all CPUs except self
send_sgi_to_others:
    PUSH {LR}
    MOV R0, #0x01000000         ; Target list filter = 01 (all except self)
    AND R1, R1, #0x0F
    ORR R2, R0, R1
    
    LDR R3, =GICD_BASE
    STR R2, [R3, #0xF00]
    DSB
    
    POP {PC}
```

**Interrupt priorities and preemption:**

```asm
; Set interrupt priority
; R0 = interrupt number
; R1 = priority (0 = highest, 255 = lowest)
set_interrupt_priority:
    PUSH {R4-R5, LR}
    
    ; Calculate register offset
    ; Each register holds 4 priorities (8 bits each)
    LSR R2, R0, #2              ; Register index = IRQ / 4
    AND R3, R0, #0x03           ; Byte position = IRQ % 4
    LSL R3, R3, #3              ; Bit position = byte * 8
    
    ; Read current register value
    LDR R4, =GICD_BASE
    ADD R4, R4, #GICD_IPRIORITYR
    LDR R5, [R4, R2, LSL #2]
    
    ; Clear old priority
    MOV R0, #0xFF
    LSL R0, R0, R3
    BIC R5, R5, R0
    
    ; Set new priority
    LSL R1, R1, R3
    ORR R5, R5, R1
    
    ; Write back
    STR R5, [R4, R2, LSL #2]
    
    POP {R4-R5, PC}

; Set CPU priority mask
; R0 = priority mask (interrupts below this priority are masked)
set_priority_mask:
    LDR R1, =GICC_BASE
    STR R0, [R1, #GICC_PMR]
    BX LR

; Get current priority mask
get_priority_mask:
    LDR R1, =GICC_BASE
    LDR R0, [R1, #GICC_PMR]
    BX LR

; Temporarily raise priority to mask lower-priority interrupts
raise_priority:
    PUSH {R4, LR}
    
    ; Save current mask
    BL get_priority_mask
    MOV R4, R0
    
    ; Set new higher priority mask (lower value)
    MOV R0, #0x40               ; Example: priority 0x40
    BL set_priority_mask
    
    ; Return old mask in R0
    MOV R0, R4
    POP {R4, PC}

; Restore previous priority mask
restore_priority:
    ; R0 = saved priority mask
    B set_priority_mask
```

**Interrupt statistics and debugging:**

```asm
.section .data
irq_count:
    .space 4 * 1020             ; Counter for each interrupt

.section .text

; IRQ handler with statistics
irq_handler_stats:
    SUB LR, LR, #4
    STMFD SP!, {R0-R3, R12, LR}
    
    ; Get interrupt ID
    LDR R0, =GICC_BASE
    LDR R1, [R0, #GICC_IAR]
    MOV R2, R1
    
    ; Check for spurious
    LDR R3, =1023
    CMP R2, R3
    BEQ stats_spurious
    
    ; Increment counter for this interrupt
    LDR R3, =irq_count
    LDR R0, [R3, R2, LSL #2]
    ADD R0, R0, #1
    STR R0, [R3, R2, LSL #2]
    
    ; Save remaining context
    MRS R3, SPSR
    STMFD SP!, {R3, R4-R11}
    
    ; Call dispatcher
    MOV R0, R2
    BL irq_dispatch
    
    ; Signal EOI
    LDR R0, =GICC_BASE
    STR R2, [R0, #GICC_EOIR]
    
    ; Restore context
    LDMFD SP!, {R3, R4-R11}
    MSR SPSR_cxsf, R3
    
stats_spurious:
    LDMFD SP!, {R0-R3, R12, PC}^

; Get interrupt count
; R0 = interrupt number
; Returns: R0 = count
get_irq_count:
    LDR R1, =irq_count
    LDR R0, [R1, R0, LSL #2]
    BX LR

; Clear interrupt statistics
clear_irq_stats:
    PUSH {R4-R5, LR}
    
    LDR R4, =irq_count
    MOV R5, #0
    MOV R1, #0
clear_loop:
    STR R5, [R4, R1, LSL #2]
    ADD R1, R1, #1
    CMP R1, #1020
    BLT clear_loop
    
    POP {R4-R5, PC}
```

**Timer interrupts (common use case):**

```asm
; ARM Generic Timer setup
.equ CNTFRQ,    0               ; Counter Frequency
.equ CNTPCT,    0               ; Physical Count
.equ CNTP_TVAL, 0               ; Timer Value
.equ CNTP_CTL,  1               ; Timer Control

; Initialize system timer
timer_init:
    PUSH {LR}
    
    ; Read timer frequency
    MRC p15, 0, R0, c14, c0, 0  ; Read CNTFRQ
    LDR R1, =timer_frequency
    STR R0, [R1]
    
    ; Setup timer for 1ms tick
    LDR R2, =1000
    UDIV R0, R0, R2             ; Ticks per millisecond
    LDR R1, =timer_tick_value
    STR R0, [R1]
    
    ; Set timer value
    MCR p15, 0, R0, c14, c2, 0  ; Write CNTP_TVAL
    
    ; Enable timer
    MOV R0, #1
    MCR p15, 0, R0, c14, c2, 1  ; Write CNTP_CTL (enable)
    
    ; Enable timer interrupt in GIC
    MOV R0, #30                 ; Physical timer IRQ (platform specific)
    BL enable_interrupt
    
    ; Register handler
    LDR R1, =timer_irq_handler
    BL register_irq_handler
    
    POP {PC}

; Timer interrupt handler
timer_irq_handler:
    PUSH {R4, LR}
    
    ; Increment system tick count
    LDR R4, =system_ticks
    LDR R0, [R4]
    ADD R0, R0, #1
    STR R0, [R4]
    
    ; Reload timer for next tick
    LDR R0, =timer_tick_value
    LDR R0, [R0]
    MCR p15, 0, R0, c14, c2, 0  ; Write CNTP_TVAL
    
    ; Call scheduler or other tick handlers
    BL os_tick_handler
    
    POP {R4, PC}

.section .bss
timer_frequency:
    .word 0
timer_tick_value:
    .word 0
system_ticks:
    .word 0
```

**Interrupt controller setup for multicore:**

```asm
; Initialize GIC for specific CPU
; R0 = CPU number
gic_cpu_init:
    PUSH {R4-R5, LR}
    MOV R4, R0
    
    ; Configure CPU interface
    LDR R5, =GICC_BASE
    
    ; Set priority mask to lowest (allow all interrupts)
    MOV R0, #0xFF
    STR R0, [R5, #GICC_PMR]
    
    ; Set binary point (controls priority grouping)
    MOV R0, #0x03               ; Example: 4 bits for group, 4 for subpriority
    STR R0, [R5, #0x08]         ; GICC_BPR
    
    ; Enable CPU interface
    MOV R0, #1
    STR R0, [R5, #GICC_CTLR]
    
    ; Enable private peripheral interrupts (PPIs)
    LDR R0, =GICD_BASE
    ADD R0, R0, #GICD_ISENABLER
    LDR R1, =0xFFFF0000         ; Enable PPIs (16-31)
    STR R1, [R0]
    
    POP {R4-R5, PC}

; Secondary CPU startup with interrupt initialization
secondary_cpu_start:
    ; Disable interrupts
    CPSID if
    
    ; Setup vector table
    LDR R0, =vector_table
    MCR p15, 0, R0, c12, c0, 0
    
    ; Setup stack for this CPU
    MRC p15, 0, R0, c0, c0, 5   ; Read MPIDR
    AND R0, R0, #0x03           ; Extract CPU ID
    
    ; Setup stacks based on CPU ID
    ; ... (similar to setup_stacks but CPU-specific)
    
    ; Initialize GIC for this CPU
    BL gic_cpu_init
    
    ; Enable interrupts
    CPSIE if
    
    ; Enter secondary CPU main loop
    B secondary_cpu_main
```

**Deferred interrupt processing (bottom half):**

```asm
; Deferred work queue structure
.struct 0
work_next:      .word 0
work_handler:   .word 0
work_data:      .word 0
work_size = .

.section .bss
.align 4
work_queue_head:
    .word 0
work_queue_tail:
    .word 0
work_queue_lock:
    .word 0

.section .text

; Quick interrupt handler that defers work
quick_irq_handler:
    PUSH {R4, LR}
    
    ; Minimal processing - just queue work
    LDR R0, =work_item_1
    BL queue_work
    
    POP {R4, PC}

; Queue work for deferred processing
; R0 = work item address
queue_work:
    PUSH {R4-R6, LR}
    MOV R4, R0
    
    ; Disable interrupts for queue manipulation
    BL disable_interrupts
    MOV R5, R0                  ; Save interrupt state
    
    ; Add to tail of queue
    LDR R6, =work_queue_tail
    LDR R1, [R6]
    
    CMP R1, #0
    BEQ queue_empty
    
    ; Queue not empty - link to tail
    STR R4, [R1, #work_next]
    STR R4, [R6]                ; Update tail
    B queue_done
    
queue_empty:
    ; Queue empty - set both head and tail
    LDR R2, =work_queue_head
    STR R4, [R2]
    STR R4, [R6]
    
queue_done:
    ; Clear next pointer
    MOV R0, #0
    STR R0, [R4, #work_next]
    
    ; Restore interrupts
    MOV R0, R5
    BL restore_interrupts
    
    POP {R4-R6, PC}

; Process deferred work (called from main loop or worker thread)
process_deferred_work:
    PUSH {R4-R6, LR}
    
process_loop:
    ; Disable interrupts
    BL disable_interrupts
    MOV R5, R0
    
    ; Get head of queue
    LDR R6, =work_queue_head
    LDR R4, [R6]
    
    CMP R4, #0
    BEQ no_work
    
    ; Remove from queue
    LDR R0, [R4, #work_next]
    STR R0, [R6]
    
    ; Check if queue is now empty
    CMP R0, #0
    LDREQ R1, =work_queue_tail
    STREQ R0, [R1]
    
    ; Restore interrupts before calling handler
    MOV R0, R5
    BL restore_interrupts
    
    ; Call work handler
    LDR R0, [R4, #work_data]
    LDR R1, [R4, #work_handler]
    BLX R1
    
    ; Process next item
    B process_loop
    
no_work:
    MOV R0, R5
    BL restore_interrupts
    POP {R4-R6, PC}
```

**Key Points:**

- ARM processors support seven processor modes with different privilege levels and banked registers for isolation
- Mode switching occurs automatically during exceptions or explicitly via privileged instructions
- Exception handling uses a vector table with specific handlers for reset, interrupts, aborts, and software interrupts
- Interrupt processing involves GIC configuration, interrupt acknowledgment, handler dispatch, and EOI signaling
- FIQ provides lower latency through additional banked registers for time-critical interrupts
- Nested interrupts require careful context management and stack switching
- Critical sections use interrupt masking to protect shared data structures
- Deferred work queues separate fast interrupt acknowledgment from slower processing

[Inference] Specific GIC register addresses and interrupt numbers are platform-dependent and vary across ARM SoC implementations. The examples use typical values but actual systems require consulting platform documentation.

[Inference] Interrupt latency and handler performance characteristics depend on processor implementation, cache configuration, and memory system design - actual measurements on target hardware are necessary for real-time system design.

---

## System Calls

System calls provide the interface between user-space applications and the operating system kernel. On ARM architectures, system calls transition execution from unprivileged user mode to privileged kernel mode, allowing applications to request kernel services such as I/O operations, process management, memory allocation, and device access.

**Supervisor Call Instruction (SVC):** The primary mechanism for invoking system calls. Previously called SWI (Software Interrupt) in older ARM documentation, SVC generates a synchronous exception that vectors to the kernel's system call handler.

**SVC Instruction Format:**

ARM state (32-bit):

```assembly
SVC  #immed24    ; 24-bit immediate value
```

Thumb state (16-bit):

```assembly
SVC  #immed8     ; 8-bit immediate value
```

The immediate value is embedded in the instruction encoding but is typically not used for system call dispatch on modern ARM systems. Instead, the system call number is passed in a register (conventionally R7 on ARM Linux).

**Linux ARM System Call Convention (EABI):**

- **R7:** System call number
- **R0-R6:** System call arguments (up to 7 arguments)
- **R0:** Return value (or error code)
- **R1:** Secondary return value (for some calls like `pipe`)

**Example** - write() system call:

```assembly
; ssize_t write(int fd, const void *buf, size_t count);
; syscall number for write = 4

    MOV    R7, #4         ; System call number (write)
    MOV    R0, #1         ; File descriptor (stdout)
    LDR    R1, =message   ; Buffer pointer
    MOV    R2, #13        ; Count (message length)
    SVC    #0             ; Invoke system call
    ; R0 now contains return value (bytes written or -errno)

message:
    .ascii "Hello, World\n"
```

**System Call Execution Flow:**

1. User application loads system call number into R7 and arguments into R0-R6
2. SVC instruction triggers exception, switching to SVC mode
3. Processor saves return address to LR_svc and CPSR to SPSR_svc
4. PC jumps to SVC exception vector (typically 0x00000008 or 0xFFFF0008)
5. Kernel's SVC handler executes:
    - Saves user context (registers) to stack
    - Validates system call number
    - Dispatches to appropriate kernel function
    - Restores user context
    - Returns to user mode via exception return instruction
6. Execution resumes after SVC instruction with result in R0

**Kernel-side SVC Handler (simplified structure):**

```assembly
vector_svc:
    ; Save user registers
    STMFD   SP!, {R0-R12, LR}
    
    ; Get system call number from R7
    MOV     R10, R7
    
    ; Validate syscall number
    CMP     R10, #NR_syscalls
    BHS     syscall_invalid
    
    ; Load syscall table address
    LDR     R8, =sys_call_table
    
    ; Call handler: handler(R0, R1, R2, R3, R4, R5, R6)
    ; R0-R6 already contain arguments
    LDR     PC, [R8, R10, LSL #2]   ; Branch to handler
    
syscall_return:
    ; Restore registers
    LDMFD   SP!, {R0-R12, PC}^      ; ^ restores CPSR from SPSR_svc
    
syscall_invalid:
    MOV     R0, #-ENOSYS
    B       syscall_return
```

**System Call Table:** The kernel maintains a jump table indexed by system call number:

```c
const void *sys_call_table[NR_syscalls] = {
    [0] = sys_restart_syscall,
    [1] = sys_exit,
    [2] = sys_fork,
    [3] = sys_read,
    [4] = sys_write,
    [5] = sys_open,
    // ... hundreds more
};
```

**Error Handling:** System calls return negative errno values on error. User-space wrappers typically check for negative return values and set the `errno` global variable:

```assembly
; After SVC returns
    CMP     R0, #0
    BXGE    LR              ; Return if success (R0 >= 0)
    
    ; Handle error
    RSB     R0, R0, #0      ; Negate to positive errno
    LDR     R1, =errno
    STR     R0, [R1]        ; Store to errno
    MOV     R0, #-1         ; Return -1
    BX      LR
```

**Alternative System Call Mechanisms:**

**Fast System Calls (ARMv6+):** Some ARM implementations provide optimized system call paths that avoid full exception overhead. These use dedicated instructions or memory-mapped interfaces, though SVC remains the standard portable mechanism.

**vDSO (Virtual Dynamic Shared Object):** Linux kernel maps a page into user-space containing frequently-used system calls implemented without mode switches. Examples include `gettimeofday()` and `clock_gettime()` which read kernel-maintained data structures directly from user mode.

**System Call Example - open, read, close:**

```assembly
.global _start
.text

_start:
    ; open("file.txt", O_RDONLY)
    MOV     R7, #5          ; sys_open
    LDR     R0, =filename
    MOV     R1, #0          ; O_RDONLY
    MOV     R2, #0
    SVC     #0
    CMP     R0, #0
    BLT     error           ; Negative = error
    MOV     R4, R0          ; Save fd
    
    ; read(fd, buffer, 100)
    MOV     R7, #3          ; sys_read
    MOV     R0, R4          ; fd from open
    LDR     R1, =buffer
    MOV     R2, #100
    SVC     #0
    CMP     R0, #0
    BLT     error
    
    ; close(fd)
    MOV     R7, #6          ; sys_close
    MOV     R0, R4
    SVC     #0
    
    ; exit(0)
    MOV     R7, #1          ; sys_exit
    MOV     R0, #0
    SVC     #0

error:
    MOV     R7, #1
    MOV     R0, #1          ; Exit with code 1
    SVC     #0

.data
filename: .asciz "file.txt"
.bss
buffer:   .space 100
```

**ARMv8 System Calls:** ARMv8 AArch64 uses the `SVC` instruction similarly but with different register conventions:

- **X8:** System call number
- **X0-X5:** Arguments
- **X0:** Return value

The calling convention differs due to the 64-bit register set, but the conceptual model remains identical.

## Memory Management Unit (MMU) Basics

The Memory Management Unit provides virtual-to-physical address translation, memory protection, and cache control. The MMU enables features essential to modern operating systems: process isolation, demand paging, memory-mapped I/O, and efficient memory utilization.

**Virtual vs Physical Addressing:**

**Physical Address:** Actual hardware address on the memory bus. Physical memory is limited by installed RAM.

**Virtual Address:** Address space presented to software. Each process has its own virtual address space (typically 4GB on 32-bit ARM), isolated from other processes.

The MMU translates virtual addresses to physical addresses transparently during every memory access.

**Translation Process Overview:**

1. CPU generates virtual address during load/store/fetch
2. MMU consults translation tables (page tables) to map virtual → physical
3. MMU checks access permissions (read/write/execute, privilege level)
4. If translation succeeds, physical address sent to memory/cache
5. If translation fails, MMU raises abort exception (page fault)

**ARM MMU Architecture Components:**

**Translation Table Base Registers (TTBR0, TTBR1):** Hold physical addresses of page table base addresses. TTBR0 typically maps user space (0x00000000-0x7FFFFFFF), TTBR1 maps kernel space (0x80000000-0xFFFFFFFF).

**Translation Table Base Control Register (TTBCR):** Configures the split between TTBR0 and TTBR1 address spaces.

**Domain Access Control Register (DACR):** Provides coarse-grained access control through 16 domains. Each 2-bit field specifies domain access (no access, client, manager).

**System Control Register (SCCR):** Controls MMU enable/disable, cache enable, alignment checking, and other system features.

**Translation Lookaside Buffer (TLB):** Caches recent virtual-to-physical translations to avoid walking page tables on every access. TLB is transparent hardware but requires software maintenance (invalidation) when page tables change.

**Page Table Structure:**

ARM uses a two-level page table hierarchy (ARMv7 and earlier with Short-descriptor format):

**Level 1 (First-level descriptor table):** 4096 entries, each 4 bytes, covering 1MB sections of virtual address space. Total size = 16KB per process.

**Level 2 (Second-level page table):** 256 entries per table, each 4 bytes, covering 4KB pages. Multiple L2 tables exist, one for each 1MB section that uses fine-grained mapping.

**Virtual Address Breakdown (4KB pages):**

```
31           20 19        12 11          0
|   L1 Index   | L2 Index  | Page Offset |
|   (12 bits)  | (8 bits)  |  (12 bits)  |
```

**Translation Walk Example:**

```
Virtual Address: 0x12345678

Step 1: Extract L1 index
  L1_index = VA[31:20] = 0x123
  L1_descriptor = TTBR0 + (L1_index * 4)
  
Step 2: Read L1 descriptor
  If section (1MB page): descriptor contains physical base, done
  If page table: descriptor contains L2 table base address
  
Step 3: Extract L2 index (if L2 table)
  L2_index = VA[19:12] = 0x45
  L2_descriptor = L2_base + (L2_index * 4)
  
Step 4: Read L2 descriptor
  Physical_base = L2_descriptor[31:12]
  Page_offset = VA[11:0] = 0x678
  
Physical Address = Physical_base | Page_offset
```

**Descriptor Formats:**

**L1 Section Descriptor (1MB mapping):**

```
31          20 19 18 17 16 15 14 12 11 10 9 8  5 4 3 2 1 0
| Section Base |NS| 0|nG| S|AP2| TEX |AP1|  Domain  |XN|C|B|1|0|
```

**L1 Page Table Descriptor:**

```
31          10 9 8  5 4 3 2 1 0
| PT Base Addr | | Dom |NS| |0|1|
```

**L2 Small Page Descriptor (4KB mapping):**

```
31          12 11 10 9 8  6 5 4 3 2 1 0
| Page Base   |nG|S|AP2|TEX|AP1|C|B|1|
```

**Descriptor Field Meanings:**

- **Section/Page Base:** Physical address bits (4KB-aligned for pages, 1MB-aligned for sections)
- **AP (Access Permission):** Controls read/write access for privileged/unprivileged modes
- **Domain:** Security domain (0-15)
- **C (Cacheable), B (Bufferable):** Cache/buffer policy bits
- **TEX (Type Extension):** Extended memory type attributes
- **XN (Execute Never):** Prevents instruction execution
- **S (Shareable):** Memory shareable between cores
- **nG (not Global):** Process-specific translation (TLB management)
- **NS (Non-Secure):** TrustZone security attribute

**Access Permission Encoding:**

```
AP[2:0]  Privileged     Unprivileged
000      No access      No access
001      Read/Write     No access
010      Read/Write     Read-only
011      Read/Write     Read/Write
100      Reserved
101      Read-only      No access
110      Read-only      Read-only
111      Read-only      Read-only (deprecated)
```

**Memory Types and Cache Policy:**

The C, B, and TEX bits combine to specify memory type:

**Strongly-ordered:** No buffering, no caching. Used for device registers requiring strict ordering.

**Device:** Bufferable but not cacheable. Used for memory-mapped I/O.

**Normal:** Cacheable and bufferable with various policies (write-through, write-back, non-cacheable).

**Example** TEX, C, B encoding for Normal memory:

```
TEX C B  Memory Type
001 0 0  Normal, write-through, no allocate on write
001 1 1  Normal, write-back, no allocate on write
001 0 1  Normal, write-through, allocate on write
001 1 0  Normal, write-back, allocate on write
```

**MMU Programming Example:**

```assembly
; Enable MMU with identity mapping (virtual = physical)

    ; Disable MMU and caches
    MRC     p15, 0, R0, c1, c0, 0    ; Read SCCR
    BIC     R0, R0, #0x1             ; Clear M bit (MMU)
    BIC     R0, R0, #0x4             ; Clear C bit (D-cache)
    BIC     R0, R0, #0x1000          ; Clear I bit (I-cache)
    MCR     p15, 0, R0, c1, c0, 0    ; Write SCCR
    
    ; Invalidate TLB
    MOV     R0, #0
    MCR     p15, 0, R0, c8, c7, 0    ; TLBIALL
    
    ; Set domain access (domain 0 = manager)
    MVN     R0, #0                    ; All bits set
    MCR     p15, 0, R0, c3, c0, 0    ; Write DACR
    
    ; Create L1 page table (simplified identity map)
    LDR     R0, =page_table_base
    LDR     R1, =0x4096              ; 4096 entries
    MOV     R2, #0                   ; Physical base = 0
    MOV     R3, #0x0C02              ; Section descriptor bits
create_section:
    ORR     R4, R2, R3               ; Combine base + attributes
    STR     R4, [R0], #4             ; Store descriptor
    ADD     R2, R2, #0x100000        ; Next 1MB section
    SUBS    R1, R1, #1
    BNE     create_section
    
    ; Set TTBR0
    LDR     R0, =page_table_base
    MCR     p15, 0, R0, c2, c0, 0    ; Write TTBR0
    
    ; Set TTBCR (use only TTBR0)
    MOV     R0, #0
    MCR     p15, 0, R0, c2, c0, 2    ; Write TTBCR
    
    ; Enable MMU
    MRC     p15, 0, R0, c1, c0, 0    ; Read SCCR
    ORR     R0, R0, #0x1             ; Set M bit
    ORR     R0, R0, #0x4             ; Set C bit
    ORR     R0, R0, #0x1000          ; Set I bit
    MCR     p15, 0, R0, c1, c0, 0    ; Write SCCR
    
    ; MMU now active

.align 14  ; 16KB alignment
page_table_base:
    .space 16384
```

**TLB Management:** Software must explicitly invalidate TLB entries when modifying page tables:

```assembly
; Invalidate entire TLB
MOV     R0, #0
MCR     p15, 0, R0, c8, c7, 0      ; TLBIALL

; Invalidate TLB entry by virtual address
MCR     p15, 0, R0, c8, c7, 1      ; TLBIMVA, R0 = virtual address

; Invalidate TLB by ASID (Address Space ID)
MCR     p15, 0, R0, c8, c7, 2      ; TLBIASID, R0 = ASID
```

**Data/Instruction Synchronization Barriers:** Required after MMU/TLB operations to ensure changes are visible:

```assembly
DSB     ; Data Synchronization Barrier
ISB     ; Instruction Synchronization Barrier
```

**Page Fault Handling:** When translation fails, the MMU raises a data abort or prefetch abort exception. The kernel's fault handler:

1. Reads Fault Address Register (FAR) to get faulting virtual address
2. Reads Fault Status Register (FSR) to determine fault type
3. Decides action: load page from disk, allocate memory, terminate process
4. Updates page tables and invalidates TLB
5. Resumes execution (retry faulting instruction)

```assembly
; Read fault information
MRC     p15, 0, R0, c6, c0, 0      ; DFAR (Data Fault Address)
MRC     p15, 0, R1, c5, c0, 0      ; DFSR (Data Fault Status)
```

## Cache Control

ARM processors implement separate instruction and data caches (Harvard architecture) with various sizes and associativities depending on the implementation. Cache management is critical for correctness (maintaining coherency) and performance (optimizing hit rates).

**Cache Architecture:**

**L1 Cache:** Typically 16KB-64KB per cache (I-cache and D-cache), 2-way or 4-way set-associative, integrated into CPU core.

**L2 Cache:** Optional unified cache, 256KB-2MB, 8-way or 16-way set-associative, shared between cores or per-core.

**L3 Cache:** Present in high-end systems, several MB, shared across all cores.

**Cache Line Size:** Typically 32 or 64 bytes (8 or 16 words). Entire cache line is loaded/evicted as a unit.

**Cache Organization - Set Associative:**

```
Virtual Address (example: 32-bit, 32-byte lines, 4-way set-associative, 16KB cache):

31              10  9     5  4        0
|      Tag        | Index | Byte Offset |
|    (22 bits)    |(5 bits)|  (5 bits)  |

Number of sets = Cache size / (Ways × Line size)
               = 16384 / (4 × 32) = 128 sets (7 bits, but using 5 shown)

Each set contains 4 cache lines (ways)
```

**Cache Operations:** ARM provides coprocessor instructions for cache maintenance. These operations are essential when:

- DMA devices access memory (bypassing cache)
- Code is modified (self-modifying code, JIT compilation)
- Memory-mapped I/O requires uncached access
- Cache coherency must be maintained in multi-core systems

**Cache Maintenance Operations:**

**Clean:** Write dirty cache lines to memory, marking them clean but keeping them in cache.

**Invalidate:** Discard cache line contents without writing back. Use when external entity modified memory.

**Clean and Invalidate:** Write dirty lines to memory, then discard. Ensures memory is current and future reads fetch from memory.

**Cache Operation by Set/Way:**

```assembly
; Clean entire D-cache by set/way
    ; Determine cache geometry (omitted for brevity)
    ; Loop through all sets and ways
    
    MOV     R0, #0                   ; Set index
set_loop:
    MOV     R1, #0                   ; Way index
way_loop:
    ; Create set/way value
    ORR     R2, R0, R1, LSL #30      ; Combine set and way
    MCR     p15, 0, R2, c7, c10, 2   ; DCCSW (clean by set/way)
    ADD     R1, R1, #1
    CMP     R1, #4                   ; 4 ways
    BLT     way_loop
    ADD     R0, R0, #1
    CMP     R0, #128                 ; 128 sets
    BLT     set_loop
```

**Cache Operation by Virtual Address (MVA):**

```assembly
; Clean and invalidate D-cache line containing address in R0
MCR     p15, 0, R0, c7, c14, 1       ; DCCIMVAC

; Invalidate I-cache line containing address in R0
MCR     p15, 0, R0, c7, c5, 1        ; ICIMVAU
```

**Common Cache Maintenance Operations:**

```assembly
; Clean entire D-cache
MOV     R0, #0
MCR     p15, 0, R0, c7, c10, 0       ; DCCSW (implementation-specific)

; Invalidate entire I-cache
MOV     R0, #0
MCR     p15, 0, R0, c7, c5, 0        ; ICIALLU

; Invalidate entire D-cache
MOV     R0, #0
MCR     p15, 0, R0, c7, c6, 0        ; DCISW (use with caution)

; Clean D-cache range by MVA
; R0 = start address, R1 = end address, R2 = line size
clean_range:
    MCR     p15, 0, R0, c7, c10, 1   ; DCCMVAC
    ADD     R0, R0, R2               ; Next cache line
    CMP     R0, R1
    BLT     clean_range
    DSB                              ; Ensure completion
```

**Cache Maintenance Sequence for DMA:**

**Before DMA read (device → memory):**

```assembly
; Invalidate D-cache for buffer to discard stale data
; R0 = buffer address, R1 = buffer size, R2 = cache line size

    ADD     R1, R0, R1               ; End address
    BIC     R0, R0, #31              ; Align to cache line boundary
invalidate_loop:
    MCR     p15, 0, R0, c7, c6, 1    ; DCIMVAC
    ADD     R0, R0, R2
    CMP     R0, R1
    BLT     invalidate_loop
    DSB                              ; Wait for completion
    
    ; Now start DMA operation
```

**After DMA write (memory → device):**

```assembly
; Clean D-cache to ensure data is in memory
; R0 = buffer address, R1 = buffer size, R2 = cache line size

    ADD     R1, R0, R1               ; End address
clean_loop:
    MCR     p15, 0, R0, c7, c10, 1   ; DCCMVAC
    ADD     R0, R0, R2
    CMP     R0, R1
    BLT     clean_loop
    DSB                              ; Wait for completion
    
    ; Now start DMA operation
```

**Self-Modifying Code / JIT Compilation:**

```assembly
; After writing new code to memory
; R0 = code start, R1 = code end, R2 = cache line size

    ; Clean D-cache (write code to memory)
clean_code:
    MCR     p15, 0, R0, c7, c10, 1   ; DCCMVAC
    ADD     R0, R0, R2
    CMP     R0, R1
    BLT     clean_code
    DSB
    
    ; Invalidate I-cache (discard old instructions)
    LDR     R0, =code_start
invalidate_icache:
    MCR     p15, 0, R0, c7, c5, 1    ; ICIMVAU
    ADD     R0, R0, R2
    CMP     R0, R1
    BLT     invalidate_icache
    DSB
    ISB                              ; Synchronize pipeline
    
    ; Now safe to execute new code
```

**Cache Locking:** Some ARM implementations allow locking cache lines to guarantee they remain resident. Useful for deterministic real-time code:

```assembly
; Lock I-cache line (implementation-specific)
MCR     p15, 0, R0, c9, c0, 1        ; Lock I-cache line containing R0

; Lock D-cache line
MCR     p15, 0, R0, c9, c0, 0        ; Lock D-cache line containing R0
```

**Cache Performance Monitoring:** ARM Performance Monitor Unit (PMU) provides counters for cache events:

```assembly
; Enable PMU
MRC     p15, 0, R0, c9, c12, 0       ; Read PMCR
ORR     R0, R0, #1                   ; Enable all counters
MCR     p15, 0, R0, c9, c12, 0       ; Write PMCR

; Configure counter 0 for D-cache misses
MOV     R0, #0x03                    ; Event 0x03 = D-cache refill
MCR     p15, 0, R0, c9, c12, 5       ; Select counter 0
MCR     p15, 0, R0, c9, c13, 1       ; Set event type

; Read counter
MRC     p15, 0, R0, c9, c13, 2       ; Read counter 0 value
```

**Multi-core Cache Coherency:** Modern ARM implementations with multiple cores use hardware cache coherency protocols (MESI, MOESI) through interconnects like CoreLink. Software must still use appropriate barriers:

```assembly
; Ensure visibility across cores
DMB     ; Data Memory Barrier - ensures memory access ordering
DSB     ; Data Synchronization Barrier - waits for completion
ISB     ; Instruction Synchronization Barrier - flushes pipeline
```

**Barrier Variants:**

```assembly
DMB SY   ; System-wide barrier (all domains, all access types)
DMB ST   ; Store barrier only
DMB ISH  ; Inner Shareable domain
DMB OSH  ; Outer Shareable domain
```

**Cache Prefetching:** Explicit prefetch instructions help hide memory latency:

```assembly
PLD     [R0]           ; Prefetch for data load
PLDW    [R0]           ; Prefetch for data write (ARMv7)
PLI     [R0]           ; Prefetch for instruction fetch
```

## Coprocessor Instructions

ARM architecture supports up to 16 coprocessors (CP0-CP15), providing extensible interfaces for specialized hardware. CP15 is the system control coprocessor, mandatory in all implementations, providing access to configuration registers, MMU, caches, and performance counters. Other coprocessor numbers are used for optional features.

**Coprocessor Instruction Format:**

ARM provides four coprocessor instruction types:

**CDP (Coprocessor Data Processing):** Internal coprocessor operation.

```assembly
CDP  coproc, opcode1, CRd, CRn, CRm, opcode2
```

**MCR (Move to Coprocessor from ARM Register):** Transfer data from ARM register to coprocessor register.

```assembly
MCR  coproc, opcode1, Rt, CRn, CRm, opcode2
```

**MRC (Move to ARM Register from Coprocessor):** Transfer data from coprocessor register to ARM register.

```assembly
MRC  coproc, opcode1, Rt, CRn, CRm, opcode2
```

**LDC/STC (Load/Store Coprocessor):** Transfer data between memory and coprocessor.

```assembly
LDC  coproc, CRd, [Rn]
STC  coproc, CRd, [Rn]
```

**CP15 System Control Coprocessor:**

CP15 provides access to system control registers through a register addressing scheme using CRn (primary register), CRm (secondary register), and opcode values. The combination uniquely identifies each register.

**Common CP15 Registers:**

**c0 - Identification Registers:**

```assembly
; Main ID Register
MRC     p15, 0, R0, c0, c0, 0        ; Read MIDR
; Returns: implementer, variant, architecture, part number, revision

; Cache Type Register
MRC     p15, 0, R0, c0, c0, 1        ; Read CTR
; Returns: cache line sizes, cache type information

; Processor Feature Registers
MRC     p15, 0, R0, c0, c1, 0        ; Read ID_PFR0 (processor features)
MRC     p15, 0, R0, c0, c1, 1        ; Read ID_PFR1
```

**c1 - System Control Register (SCCR):**

```assembly
; Read SCCR
MRC     p15, 0, R0, c1, c0, 0

; SCCR bit fields:
; [0]    M  - MMU enable
; [1]    A  - Alignment check enable
; [2]    C  - D-cache enable
; [11]   Z  - Branch prediction enable
; [12]   I  - I-cache enable
; [13]   V  - High exception vectors (0xFFFF0000 vs 0x00000000)
; [14]   RR - Round-robin cache replacement
; [28]   TRE - TEX remap enable
; [29]   AFE - Access flag enable

; Enable MMU and caches
MRC     p15, 0, R0, c1, c0, 0
ORR     R0, R0, #0x1                 ; MMU
ORR     R0, R0, #0x4                 ; D-cache
ORR     R0, R0, #0x1000              ; I-cache
ORR     R0, R0, #0x800               ; Branch prediction
MCR     p15, 0, R0, c1, c0, 0
```

**c2 - Translation Table Base Registers:**

```assembly
; TTBR0 - User space translation table
MCR     p15, 0, R0, c2, c0, 0        ; Write TTBR0

; TTBR1 - Kernel space translation table
MCR     p15, 0, R0, c2, c0, 1        ; Write TTBR1

; TTBCR - Translation table base control
MCR     p15, 0, R0, c2, c0, 2        ; Write TTBCR
```

**c3 - Domain Access Control Register:**

```assembly
; Set domain 0 to manager, others no access
MOV     R0, #3                       ; Domain 0 = manager (11b)
MCR     p15, 0, R0, c3, c0, 0        ; Write DACR
```

**c5/c6 - Fault Status and Fault Address:**

```assembly
; Read fault information after abort
MRC     p15, 0, R0, c5, c0, 0        ; DFSR - Data Fault Status
MRC     p15, 0, R0, c5, c0, 1        ; IFSR - Instruction Fault Status
MRC     p15, 0, R0, c6, c0, 0        ; DFAR - Data Fault Address
MRC     p15, 0, R0, c6, c0, 2        ; IFAR - Instruction Fault Address
```

**c7 - Cache and Branch Predictor Maintenance:**

```assembly
; Invalidate entire I-cache
MOV     R0, #0
MCR     p15, 0, R0, c7, c5, 0        ; ICIALLU

; Invalidate I-cache line by MVA
MCR     p15, 0, R0, c7, c5, 1        ; ICIMVAU

; Clean D-cache line by MVA
MCR     p15, 0, R0, c7, c10, 1       ; DCCMVAC

; Clean and invalidate D-cache line by MVA
MCR     p15, 0, R0, c7, c14, 1       ; DCCIMVAC

; Invalidate branch predictor
MOV     R0, #0
MCR     p15, 0, R0, c7, c5, 6        ; BPIALL

````

**c8 - TLB Maintenance:**
```assembly
; Invalidate entire unified TLB
MOV     R0, #0
MCR     p15, 0, R0, c8, c7, 0        ; TLBIALL

; Invalidate TLB entry by MVA
MCR     p15, 0, R0, c8, c7, 1        ; TLBIMVA

; Invalidate TLB by ASID
MCR     p15, 0, R0, c8, c7, 2        ; TLBIASID

; Invalidate instruction TLB
MCR     p15, 0, R0, c8, c5, 0        ; ITLBIALL

; Invalidate data TLB
MCR     p15, 0, R0, c8, c6, 0        ; DTLBIALL
````

**c9 - Performance Monitors and Cache Lockdown:**

```assembly
; Performance Monitor Control Register
MRC     p15, 0, R0, c9, c12, 0       ; Read PMCR
ORR     R0, R0, #1                   ; Enable all counters
ORR     R0, R0, #2                   ; Reset event counters
ORR     R0, R0, #4                   ; Reset cycle counter
MCR     p15, 0, R0, c9, c12, 0       ; Write PMCR

; Count Enable Set Register
MOV     R0, #0x80000001              ; Enable cycle counter and counter 0
MCR     p15, 0, R0, c9, c12, 1       ; Write PMCNTENSET

; Event Counter Selection Register
MOV     R0, #0                       ; Select counter 0
MCR     p15, 0, R0, c9, c12, 5       ; Write PMSELR

; Event Type Select Register
MOV     R0, #0x04                    ; Event 0x04 = D-cache access
MCR     p15, 0, R0, c9, c13, 1       ; Write PMXEVTYPER

; Read cycle counter
MRC     p15, 0, R0, c9, c13, 0       ; Read PMCCNTR

; Read event counter
MRC     p15, 0, R0, c9, c13, 2       ; Read PMXEVCNTR (selected counter)
```

**c10 - Memory Remap and TLB Lockdown:**

```assembly
; Primary Region Remap Register
MRC     p15, 0, R0, c10, c2, 0       ; Read PRRR

; Normal Memory Remap Register
MRC     p15, 0, R0, c10, c2, 1       ; Read NMRR
```

**c13 - Context ID and Thread Registers:**

```assembly
; Context ID Register (ASID)
MOV     R0, #5                       ; Set ASID to 5
MCR     p15, 0, R0, c13, c0, 1       ; Write CONTEXTIDR

; Thread ID registers (for thread-local storage)
MCR     p15, 0, R0, c13, c0, 2       ; Write TPIDRURW (user read/write)
MCR     p15, 0, R0, c13, c0, 3       ; Write TPIDRURO (user read-only)
MCR     p15, 0, R0, c13, c0, 4       ; Write TPIDRPRW (privileged)

; Read thread ID
MRC     p15, 0, R0, c13, c0, 2       ; Read TPIDRURW
```

**c15 - Implementation-Defined Registers:**

CP15 c15 is reserved for implementation-specific features. Different ARM cores use this for various purposes:

```assembly
; Example: Cortex-A9 Auxiliary Control Register
MRC     p15, 0, R0, c1, c0, 1        ; Read ACTLR
ORR     R0, R0, #(1 << 6)            ; Enable SMP mode
MCR     p15, 0, R0, c1, c0, 1        ; Write ACTLR

; Implementation-specific cache operations might use c15
; (varies by processor)
```

**VFP/NEON Coprocessors (CP10/CP11):**

VFP uses coprocessors 10 and 11 for floating-point operations:

```assembly
; Enable VFP/NEON access
MRC     p15, 0, R0, c1, c0, 2        ; Read CPACR (Coprocessor Access Control)
ORR     R0, R0, #(0xF << 20)         ; Enable CP10 and CP11 (full access)
MCR     p15, 0, R0, c1, c0, 2        ; Write CPACR
ISB                                  ; Synchronize

; Enable VFP
MOV     R0, #0x40000000
VMSR    FPEXC, R0                    ; Enable VFP (EN bit in FPEXC)

; Access FPSCR (Floating-Point Status and Control Register)
VMRS    R0, FPSCR                    ; Move FPSCR to R0
BIC     R0, R0, #0x00370000          ; Clear exception enable bits
VMSR    FPSCR, R0                    ; Move R0 to FPSCR
```

**Coprocessor Access Control:**

The CPACR (Coprocessor Access Control Register) controls access to coprocessors from different privilege levels:

```assembly
; Read CPACR
MRC     p15, 0, R0, c1, c0, 2

; CPACR format: 2 bits per coprocessor
; 00 = Access denied
; 01 = Privileged access only
; 10 = Reserved
; 11 = Full access (privileged and unprivileged)

; Enable CP10 and CP11 for full access (VFP/NEON)
ORR     R0, R0, #(0xF << 20)         ; CP10/CP11 = 0b11 each
MCR     p15, 0, R0, c1, c0, 2
```

**Synchronization Requirements:**

Coprocessor register accesses affecting processor state require synchronization barriers:

```assembly
; After modifying MMU configuration
MCR     p15, 0, R0, c2, c0, 0        ; Write TTBR0
DSB                                  ; Ensure write completes
ISB                                  ; Flush pipeline

; After TLB invalidation
MCR     p15, 0, R0, c8, c7, 0        ; TLBIALL
DSB                                  ; Wait for completion
ISB                                  ; Synchronize context

; After cache maintenance
MCR     p15, 0, R0, c7, c5, 0        ; Invalidate I-cache
DSB                                  ; Data synchronization
ISB                                  ; Instruction synchronization
```

**Complete MMU Setup Example:**

```assembly
setup_mmu:
    ; Disable MMU and caches
    MRC     p15, 0, R0, c1, c0, 0
    BIC     R0, R0, #0x1             ; Disable MMU
    BIC     R0, R0, #0x4             ; Disable D-cache
    BIC     R0, R0, #0x1000          ; Disable I-cache
    BIC     R0, R0, #0x800           ; Disable branch prediction
    MCR     p15, 0, R0, c1, c0, 0
    DSB
    ISB
    
    ; Invalidate caches
    MOV     R0, #0
    MCR     p15, 0, R0, c7, c5, 0    ; Invalidate I-cache
    MCR     p15, 0, R0, c7, c6, 0    ; Invalidate D-cache
    
    ; Invalidate TLB
    MCR     p15, 0, R0, c8, c7, 0    ; TLBIALL
    DSB
    ISB
    
    ; Set domain access (domain 0 = client)
    MOV     R0, #1
    MCR     p15, 0, R0, c3, c0, 0    ; Write DACR
    
    ; Build page tables (implementation omitted)
    BL      build_page_tables
    
    ; Set TTBR0
    LDR     R0, =page_table_base
    MCR     p15, 0, R0, c2, c0, 0    ; Write TTBR0
    
    ; Set TTBCR (use only TTBR0, split at 0x00000000)
    MOV     R0, #0
    MCR     p15, 0, R0, c2, c0, 2    ; Write TTBCR
    
    DSB
    ISB
    
    ; Enable MMU, caches, branch prediction
    MRC     p15, 0, R0, c1, c0, 0
    ORR     R0, R0, #0x1             ; Enable MMU
    ORR     R0, R0, #0x4             ; Enable D-cache
    ORR     R0, R0, #0x1000          ; Enable I-cache
    ORR     R0, R0, #0x800           ; Enable branch prediction
    MCR     p15, 0, R0, c1, c0, 0
    DSB
    ISB
    
    BX      LR
```

**Debug and Trace Coprocessor (CP14):**

CP14 provides access to debug, breakpoint, and watchpoint registers:

```assembly
; Set breakpoint
; R0 = breakpoint address
MCR     p14, 0, R0, c0, c0, 4        ; Write DBGBVR0 (Breakpoint Value Register 0)

MOV     R1, #0x1E7                   ; Enable breakpoint, match address
MCR     p14, 0, R1, c0, c0, 5        ; Write DBGBCR0 (Breakpoint Control Register 0)

; Read Debug Status and Control Register
MRC     p14, 0, R0, c0, c1, 0        ; Read DBGDSCR
```

**ARMv8 AArch64 System Register Access:**

ARMv8 AArch64 state uses different syntax for system register access (MSR/MRS instructions instead of MCR/MRC):

```assembly
; ARMv8 AArch64 examples
MRS     X0, SCTLR_EL1                ; Read System Control Register
MSR     TTBR0_EL1, X0                ; Write Translation Table Base Register
MRS     X0, ID_AA64PFR0_EL1          ; Read Processor Feature Register

; Cache maintenance
DC      CIVAC, X0                    ; Clean and invalidate by VA
IC      IALLU                        ; Invalidate all I-cache

; TLB maintenance
TLBI    VMALLE1                      ; Invalidate all TLB entries
```

**Key Points:**

- System calls use SVC instruction to transition user mode → kernel mode
- MMU provides virtual memory through two-level page table translation
- TLB caches translations and requires explicit invalidation after page table modifications
- Cache maintenance operations essential for DMA, self-modifying code, and coherency
- CP15 provides system control through coprocessor registers accessed via MCR/MRC
- Synchronization barriers (DSB, ISB) required after configuration changes
- Different coprocessor numbers allocated for VFP (CP10/CP11), debug (CP14), and system control (CP15)

done

---

# ARM64 (AArch64)

AArch64 is ARM's 64-bit execution state, introduced with ARMv8-A architecture. It represents a clean-slate redesign rather than an extension of the 32-bit ARM architecture, featuring a fundamentally different instruction set, expanded registers, and modernized architectural choices. AArch64 coexists with AArch32 (the 32-bit execution state) but cannot intermix at the instruction level.

## 64-bit Architecture Differences

AArch64 introduces fundamental changes compared to the 32-bit ARM (AArch32) architecture, affecting registers, addressing, instruction encoding, and execution model.

### Execution States

**AArch64 vs AArch32:**

- Separate execution states, not instruction set modes
- Cannot switch between states at instruction level
- Exception level (EL) determines available states
- State change only via exception level transitions or processor reset

```assembly
; AArch32 (32-bit ARM/Thumb)
.arm
ADD r0, r1, r2          ; 32-bit registers

; AArch64 (completely different ISA)
.aarch64
ADD X0, X1, X2          ; 64-bit registers
ADD W0, W1, W2          ; 32-bit operations on 64-bit registers
```

**Exception Levels:**

- EL0: Application level (unprivileged)
- EL1: Operating system kernel
- EL2: Hypervisor
- EL3: Secure monitor

AArch64 available at all exception levels; AArch32 support optional and implementation-defined.

### Address Space

**64-bit Virtual Addressing:**

```assembly
; Full 64-bit address capability (implementation may use fewer bits)
; Typical: 48-bit virtual address space

LDR X0, [X1]                    ; Load from 64-bit address in X1
LDR X0, =0x0000FFFF80000000     ; Load 64-bit address
```

**Address Space Layout:**

- User space: 0x0000000000000000 - 0x0000FFFFFFFFFFFF (lower half)
- Kernel space: 0xFFFF000000000000 - 0xFFFFFFFFFFFFFFFF (upper half)
- Implementation-dependent bit width (commonly 48-bit, can be 52-bit)

**Large Physical Addressing:**

- Physical addresses up to 52 bits
- Supports systems with >4GB RAM without PAE complexity

```assembly
; Load/store with 64-bit pointers
LDR X0, [X1, X2]                ; Base + index, both 64-bit
LDR X0, [X1, #0x1000]           ; 64-bit base + 12-bit offset
```

### Data Types and Alignment

**Native Data Sizes:**

```assembly
; Byte operations (8-bit)
LDRB W0, [X1]                   ; Load byte, zero-extend
STRB W0, [X1]                   ; Store byte

; Halfword operations (16-bit)
LDRH W0, [X1]                   ; Load halfword, zero-extend
STRH W0, [X1]                   ; Store halfword

; Word operations (32-bit)
LDR W0, [X1]                    ; Load 32-bit word
STR W0, [X1]                    ; Store 32-bit word

; Doubleword operations (64-bit)
LDR X0, [X1]                    ; Load 64-bit doubleword
STR X0, [X1]                    ; Store 64-bit doubleword
```

**Alignment Requirements:** [Inference] Most AArch64 implementations allow unaligned access to normal memory with performance penalty. Aligned access preferred:

```assembly
; Aligned access (fastest)
LDR X0, [X1]                    ; X1 should be 8-byte aligned

; Unaligned access (may be slower)
LDR X0, [X1, #5]                ; Unaligned load (supported but slower)

; Device memory requires aligned access
LDR X0, [X1]                    ; Must be aligned for device memory
```

### Removed Features from AArch32

**No Predicated Execution:**

- AArch32 conditional execution removed
- Replaced with conditional select instructions
- Branch prediction and speculation preferred over predicates

```assembly
; AArch32 style (not available in AArch64)
; ADDGT r0, r1, r2              ; Conditional add

; AArch64 equivalent
CMP X1, #10
B.LE skip
ADD X0, X1, X2
skip:

; Or using conditional select
CMP X1, #10
ADD X3, X1, X2                  ; Compute speculatively
CSEL X0, X3, X0, GT             ; Select result if GT
```

**No IT Blocks:**

- Thumb-2 IT blocks don't exist in AArch64
- Conditional branches and CSEL instructions used instead

**No Load/Store Multiple (LDM/STM):**

- Replaced with Load/Store Pair (LDP/STP)
- More efficient with modern pipeline designs

```assembly
; AArch32 style
; PUSH {r4-r7, lr}

; AArch64 equivalent (using pairs)
STP X29, X30, [SP, #-16]!       ; Push frame pointer and link register
STP X19, X20, [SP, #-16]!       ; Push callee-saved registers
```

**No Barrel Shifter in Every Instruction:**

- Shift operations available but more restricted
- Separate shift instructions or limited immediate shifts

```assembly
; AArch32 style
; ADD r0, r1, r2, LSL #3

; AArch64 equivalent
LSL X3, X2, #3                  ; Separate shift
ADD X0, X1, X3                  ; Then add

; Or with immediate shift (limited to certain instructions)
ADD X0, X1, X2, LSL #3          ; Available in AArch64 too
```

**No Coprocessor Instructions:**

- CP15 system control replaced with system register instructions
- NEON/SIMD integrated, not separate coprocessor

### New Architectural Features

**Exception Model:**

- Simplified exception handling
- Dedicated exception level stack pointers
- Exception syndrome registers (ESR_ELx) for detailed exception info

**Memory Model:**

- Relaxed memory ordering by default
- Explicit barriers (DMB, DSB, ISB) with wider options
- Load-acquire/Store-release instructions for efficient synchronization

```assembly
; Acquire semantics (prevents reordering of subsequent loads/stores)
LDAR X0, [X1]                   ; Load-acquire

; Release semantics (prevents reordering of previous loads/stores)
STLR X0, [X1]                   ; Store-release

; Stronger than traditional barriers, lighter than full DMB
```

**Cryptographic Extensions:**

- Hardware acceleration for AES, SHA1, SHA256
- CRC32 instructions
- Integrated into instruction set

**PC Relative Addressing:**

- More flexible PC-relative loads
- Position-independent code easier to write

```assembly
; Load address relative to PC
ADRP X0, symbol                 ; Load page address
ADD X0, X0, :lo12:symbol        ; Add low 12 bits

; Load from PC-relative address
LDR X0, symbol                  ; PC-relative load (±1MB range)
```

## Extended Register Set (X0-X30)

AArch64 provides 31 general-purpose registers, each 64 bits wide, with separate 32-bit and 64-bit access modes.

### General Purpose Registers

**Register Naming:**

```assembly
; 64-bit access: X0-X30
MOV X0, #100                    ; 64-bit operation
ADD X1, X2, X3                  ; 64-bit addition

; 32-bit access: W0-W30 (lower 32 bits of X registers)
MOV W0, #100                    ; 32-bit operation (upper 32 bits zeroed)
ADD W1, W2, W3                  ; 32-bit addition

; Relationship: W0 is lower 32 bits of X0
MOV X0, #0xFFFFFFFFFFFFFFFF     ; X0 = 0xFFFFFFFFFFFFFFFF
MOV W0, #0                      ; X0 = 0x0000000000000000 (upper bits cleared)
```

**32-bit Operation Behavior:** [Inference] 32-bit operations (using W registers) zero-extend results to 64 bits:

```assembly
; Zero-extension behavior
MOV X0, #-1                     ; X0 = 0xFFFFFFFFFFFFFFFF
ADD W0, W0, #1                  ; W0 = 0x00000000 (overflow)
                                ; X0 = 0x0000000000000000 (upper 32 bits cleared)

; Sign extension requires explicit instruction
MOV W0, #-1                     ; X0 = 0x00000000FFFFFFFF (zero-extended)
SXTW X0, W0                     ; X0 = 0xFFFFFFFFFFFFFFFF (sign-extended)
```

**Register Count:**

- 31 general-purpose registers (X0-X30 / W0-W30)
- No dedicated flags register (flags in PSTATE)
- No dedicated PC register (PC not directly accessible)

### Special Register Purposes

**X0-X7: Argument and Return Registers**

```assembly
; Function call convention
; X0-X7: First 8 arguments
; X0-X1: Return values (X0 for single value, X0-X1 for 128-bit)

function:
    ; X0 = arg1, X1 = arg2, etc.
    ADD X0, X0, X1              ; Compute return value in X0
    RET                         ; Return
```

**X8: Indirect Result Register**

```assembly
; X8 used for returning large structures
; Caller passes pointer to result location in X8

function_returns_struct:
    ; X8 points to caller's result buffer
    STR X0, [X8]                ; Store result through X8
    STR X1, [X8, #8]
    RET
```

**X9-X15: Temporary Registers (Caller-saved)**

```assembly
; Scratch registers, not preserved across calls
function:
    MOV X9, #100                ; Can use freely
    BL other_function           ; X9 may be clobbered
    ; Don't assume X9 still contains 100
```

**X16-X17: Intra-Procedure-Call Registers (IP0, IP1)**

```assembly
; Used by linker veneers and PLT stubs
; Treated as caller-saved

; Linker veneer example (generated by linker)
veneer_to_far_function:
    ADRP X16, far_function
    ADD X16, X16, :lo12:far_function
    BR X16                      ; Indirect branch via X16
```

**X18: Platform Register**

```assembly
; Reserved for platform use
; Linux: Not used (available for applications)
; Windows: TEB (Thread Environment Block) pointer
; Some systems: Shadow call stack pointer

; Generally avoid using X18 in portable code
```

**X19-X28: Callee-Saved Registers**

```assembly
; Must be preserved across function calls
function:
    STP X19, X20, [SP, #-16]!   ; Save callee-saved registers
    
    MOV X19, X0                 ; Use X19 for persistent value
    BL other_function           ; X19 preserved
    
    MOV X0, X19                 ; X19 still valid
    
    LDP X19, X20, [SP], #16     ; Restore
    RET
```

**X29: Frame Pointer (FP)**

```assembly
; Points to current stack frame
; Used for stack unwinding and debugging

function:
    STP X29, X30, [SP, #-16]!   ; Save FP and LR
    MOV X29, SP                 ; Set up frame pointer
    
    ; Function body
    
    LDP X29, X30, [SP], #16     ; Restore FP and LR
    RET
```

**X30: Link Register (LR)**

```assembly
; Holds return address
BL function                     ; LR = PC + 4
; Inside function:
RET                             ; PC = LR (return)

; Nested calls require saving LR
outer_function:
    STP X29, X30, [SP, #-16]!   ; Save LR
    BL inner_function           ; LR overwritten
    LDP X29, X30, [SP], #16     ; Restore LR
    RET                         ; Return to original caller
```

**Stack Pointer (SP)**

```assembly
; Dedicated stack pointer register
; 16-byte alignment required

; Allocating stack space
SUB SP, SP, #32                 ; Allocate 32 bytes (must be multiple of 16)

; Accessing stack variables
STR X0, [SP]                    ; Store at SP
LDR X0, [SP, #8]                ; Load from SP + 8

; Deallocating
ADD SP, SP, #32                 ; Deallocate
```

**Zero Register (XZR / WZR)**

```assembly
; Reads as zero, writes discarded
; Useful for common operations

MOV X0, XZR                     ; X0 = 0
CMP X0, XZR                     ; Compare with zero
STR XZR, [X1]                   ; Store zero

; Discard result
ADD XZR, X0, X1                 ; Compute but discard (for flags)

; Test without destroying register
TST X0, X1                      ; Actually: ANDS XZR, X0, X1
```

### Register Usage Patterns

**Example** - Function using multiple register classes:

```assembly
.global compute_function
compute_function:
    ; X0 = input pointer
    ; X1 = count
    ; Returns sum in X0
    
    ; Save callee-saved registers
    STP X29, X30, [SP, #-48]!   ; Save FP, LR
    STP X19, X20, [SP, #16]     ; Save callee-saved
    STP X21, X22, [SP, #32]     ; Save more
    MOV X29, SP                 ; Set frame pointer
    
    ; Use callee-saved for persistent values
    MOV X19, X0                 ; Save input pointer (survives calls)
    MOV X20, X1                 ; Save count
    MOV X21, XZR                ; Sum = 0
    
loop:
    ; Use caller-saved for temporaries
    LDR X9, [X19], #8           ; Load value (X9 temporary)
    ADD X21, X21, X9            ; Accumulate
    
    ; Use X0-X7 for function arguments
    MOV X0, X9
    BL process_value            ; May clobber X9, but we're done with it
    
    SUBS X20, X20, #1           ; Decrement counter
    B.NE loop
    
    MOV X0, X21                 ; Return sum in X0
    
    ; Restore callee-saved registers
    LDP X21, X22, [SP, #32]
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #48     ; Restore FP, LR, deallocate
    RET
```

**Example** - Leaf function (no calls, minimal saves):

```assembly
.global leaf_function
leaf_function:
    ; X0 = a, X1 = b, X2 = c
    ; No function calls, no callee-saved needed
    
    ADD X0, X0, X1              ; a + b
    MUL X0, X0, X2              ; (a + b) * c
    RET                         ; No stack frame needed
```

### Register Aliasing and Subregs

**Byte and Halfword Access:**

```assembly
; Full register hierarchy
; X0:  [63:0]  - 64-bit
; W0:  [31:0]  - 32-bit (lower half of X0)
; Cannot directly access [15:0], [7:0] like AArch32

; Extracting bytes requires explicit instructions
LDRB W0, [X1]                   ; Load byte to W0 (zero-extended to 64-bit)
AND W0, W0, #0xFF               ; Mask to byte

; Extracting halfwords
LDRH W0, [X1]                   ; Load halfword (zero-extended)
AND W0, W0, #0xFFFF             ; Mask to halfword
```

**Sign Extension:**

```assembly
; Sign-extend byte to 64-bit
LDRSB X0, [X1]                  ; Load signed byte, extend to 64-bit
SXTB X0, W0                     ; Sign-extend byte from W0 to X0

; Sign-extend halfword
LDRSH X0, [X1]                  ; Load signed halfword, extend
SXTH X0, W0                     ; Sign-extend halfword from W0

; Sign-extend word
LDRSW X0, [X1]                  ; Load signed word, extend to 64-bit
SXTW X0, W0                     ; Sign-extend W0 to X0
```

**Example** - Working with different widths:

```assembly
; Processing mixed-width data
MOV X0, #0xFFFFFFFFFFFFFFFF     ; X0 = -1 (64-bit)

; 32-bit operation clears upper bits
ADD W0, W0, #1                  ; W0 = 0, X0 = 0x0000000000000000

; Sign-extend to get proper 64-bit value
MOV W1, #-1                     ; W1 = 0xFFFFFFFF, X1 = 0x00000000FFFFFFFF
SXTW X1, W1                     ; X1 = 0xFFFFFFFFFFFFFFFF

; Unsigned extension (automatic for W operations)
MOV W2, #0xFF                   ; X2 = 0x00000000000000FF (zero-extended)
```

### Vector and SIMD Registers

**NEON/SIMD Register Set:**

- 32 vector registers: V0-V31 (128-bit)
- Multiple access widths: B (8-bit), H (16-bit), S (32-bit), D (64-bit), Q (128-bit)

```assembly
; Scalar floating-point (lower 64 bits of V registers)
FMOV D0, #1.0                   ; D0 = 1.0 (64-bit float)
FADD D0, D1, D2                 ; Double-precision add

; Single-precision (lower 32 bits)
FMOV S0, #1.0                   ; S0 = 1.0 (32-bit float)
FADD S0, S1, S2                 ; Single-precision add

; SIMD operations (full 128-bit)
ADD V0.4S, V1.4S, V2.4S         ; Add 4 x 32-bit integers
FADD V0.2D, V1.2D, V2.2D        ; Add 2 x 64-bit floats
```

**Example** - Using vector registers:

```assembly
; Vector addition
vector_add:
    ; X0 = array1, X1 = array2, X2 = result, X3 = count
    
loop:
    LDP Q0, Q1, [X0], #32       ; Load 4 x 64-bit values (2 vectors)
    LDP Q2, Q3, [X1], #32       ; Load from second array
    
    ADD V0.2D, V0.2D, V2.2D     ; Add first pair
    ADD V1.2D, V1.2D, V3.2D     ; Add second pair
    
    STP Q0, Q1, [X2], #32       ; Store results
    
    SUBS X3, X3, #4             ; Decrement counter (4 elements processed)
    B.GT loop
    
    RET
```

## New Instruction Encodings

AArch64 uses fixed 32-bit instruction encoding with a more regular and orthogonal design compared to AArch32.

### Instruction Format

**All Instructions 32-bit:**

```assembly
; No 16-bit Thumb encoding
; Every instruction is exactly 4 bytes
; More regular encoding simplifies decode

ADD X0, X1, X2                  ; 4 bytes
MOV X0, #1                      ; 4 bytes
NOP                             ; 4 bytes
```

**Instruction Categories:**

- Data processing (immediate)
- Data processing (register)
- Load/store
- Branch, exception, system
- SIMD/floating-point

### Load/Store Instructions

**Basic Load/Store:**

```assembly
; Load register
LDR X0, [X1]                    ; Load from [X1] to X0
LDR X0, [X1, #8]                ; Load from [X1 + 8]
LDR W0, [X1]                    ; Load 32-bit word

; Store register
STR X0, [X1]                    ; Store X0 to [X1]
STR X0, [X1, #16]               ; Store to [X1 + 16]
STR W0, [X1]                    ; Store 32-bit word
```

**Addressing Modes:**

```assembly
; Base + offset (unsigned immediate, scaled by size)
LDR X0, [X1, #8]                ; Offset 0-32760, multiple of 8
LDR W0, [X1, #4]                ; Offset 0-16380, multiple of 4
LDRB W0, [X1, #1]               ; Offset 0-4095, byte-aligned

; Base + register offset
LDR X0, [X1, X2]                ; Address = X1 + X2
LDR X0, [X1, X2, LSL #3]        ; Address = X1 + (X2 << 3)
LDR X0, [X1, W2, SXTW #3]       ; Address = X1 + sign_extend(W2) << 3

; Pre-indexed (update base before access)
LDR X0, [X1, #8]!               ; X0 = [X1 + 8], then X1 = X1 + 8

; Post-indexed (update base after access)
LDR X0, [X1], #8                ; X0 = [X1], then X1 = X1 + 8

; PC-relative
LDR X0, label                   ; Load from PC-relative address (±1MB)
ADRP X0, label                  ; Load page address (4KB page)
```

**Example** - Different addressing modes:

```assembly
; Iterate through array
array_loop:
    LDR X0, [X1], #8            ; Post-index: load then increment
    ; Process X0
    SUBS X2, X2, #1
    B.NE array_loop

; Build stack frame
function:
    STP X29, X30, [SP, #-16]!   ; Pre-index: decrement then store
    ; Function body
    LDP X29, X30, [SP], #16     ; Post-index: load then increment
    RET
```

**Load/Store Pair:**

```assembly
; Load/store two registers
LDP X0, X1, [X2]                ; X0 = [X2], X1 = [X2 + 8]
STP X0, X1, [X2]                ; [X2] = X0, [X2 + 8] = X1

; With offset
LDP X0, X1, [X2, #16]           ; Load from X2 + 16, X2 + 24

; Pre/post-indexed
LDP X0, X1, [X2, #16]!          ; Pre-index
LDP X0, X1, [X2], #16           ; Post-index

; Signed offset (can be negative)
LDP X0, X1, [X2, #-16]          ; Load from X2 - 16, X2 - 8
```

**Example** - Efficient structure copy:

```assembly
; Copy structure using pairs
copy_struct:
    ; X0 = dest, X1 = src, X2 = size (multiple of 16)
loop:
    LDP X3, X4, [X1], #16       ; Load 16 bytes, increment src
    STP X3, X4, [X0], #16       ; Store 16 bytes, increment dest
    SUBS X2, X2, #16
    B.GT loop
    RET
```

**Load/Store Exclusive:**

```assembly
; Atomic operations
LDXR X0, [X1]                   ; Load exclusive
; Modify X0
STXR W2, X0, [X1]               ; Store exclusive, W2 = 0 if success

; Pair exclusive
LDXP X0, X1, [X2]               ; Load exclusive pair
STXP W3, X0, X1, [X2]           ; Store exclusive pair

; Acquire/Release variants
LDAXR X0, [X1]                  ; Load-acquire exclusive
STLXR W2, X0, [X1]              ; Store-release exclusive
```

### Data Processing Instructions

**Arithmetic:**

```assembly
; Add/subtract (immediate)
ADD X0, X1, #100                ; X0 = X1 + 100
SUB X0, X1, #50                 ; X0 = X1 - 50
ADDS X0, X1, #10                ; Add and set flags
SUBS X0, X1, #10                ; Subtract and set flags

; Add/subtract (register)
ADD X0, X1, X2                  ; X0 = X1 + X2
SUB X0, X1, X2                  ; X0 = X1 - X2
ADD X0, X1, X2, LSL #3          ; X0 = X1 + (X2 << 3)
SUB X0, X1, X2, LSR #2          ; X0 = X1 - (X2 >> 2)

; Add/subtract with carry
ADC X0, X1, X2                  ; X0 = X1 + X2 + carry
SBC X0, X1, X2                  ; X0 = X1 - X2 - !carry

; Negate
NEG X0, X1                      ; X0 = -X1 (alias for SUB X0, XZR, X1)
NGC X0, X1                      ; X0 = -X1 - !carry
```

**Logical:**

```assembly
; Bitwise operations
AND X0, X1, X2                  ; X0 = X1 & X2
ORR X0, X1, X2                  ; X0 = X1 | X2
EOR X0, X1, X2                  ; X0 = X1 ^ X2
BIC X0, X1, X2                  ; X0 = X1 & ~X2 (bit clear)

; With immediate (complex encoding)
AND X0, X1, #0xFF               ; X0 = X1 & 0xFF
ORR X0, X1, #0xF0F0F0F0F0F0F0F0 ; X0 = X1 | pattern

; Test (logical AND, discard result)
TST X0, X1                      ; Set flags based on X0 & X1
TST X0, #0xFF                   ; Test against immediate
```

**Shift and Rotate:**

```assembly
; Logical shifts
LSL X0, X1, #5                  ; X0 = X1 << 5
LSR X0, X1, #5                  ; X0 = X1 >> 5 (logical)

; Arithmetic shift
ASR X0, X1, #5                  ; X0 = X1 >> 5 (arithmetic, sign-extend)

; Rotate
ROR X0, X1, #5                  ; X0 = rotate_right(X1, 5)

; Variable shift
LSL X0, X1, X2                  ; X0 = X1 << (X2 & 63)
LSR X0, X1, X2                  ; X0 = X1 >> (X2 & 63)
ASR X0, X1, X2                  ; Arithmetic shift by register
ROR X0, X1, X2                  ; Rotate by register
```

**Multiply and Divide:**

```assembly
; Multiply (32-bit and 64-bit)
MUL X0, X1, X2                  ; X0 = X1 * X2 (64-bit)
MUL W0, W1, W2                  ; W0 = W1 * W2 (32-bit)

; Multiply-add/subtract
MADD X0, X1, X2, X3             ; X0 = X3 + (X1 * X2)
MSUB X0, X1, X2, X3             ; X0 = X3 - (X1 * X2)

; High multiply (upper 64 bits of 64x64 -> 128)
SMULH X0, X1, X2                ; X0 = (X1 * X2)[127:64] (signed)
UMULH X0, X1, X2                ; X0 = (X1 * X2)[127:64] (unsigned)

; Divide
SDIV X0, X1, X2                 ; X0 = X1 / X2 (signed)
UDIV X0, X1, X2                 ; X0 = X1 / X2 (unsigned)

; Compute remainder (no dedicated instruction)
UDIV X3, X1, X2                 ; quotient
MSUB X0, X3, X2, X1             ; remainder = dividend - quotient * divisor
```

**Example** - 64-bit multiplication with overflow detection:

```assembly
; Multiply X1 * X2, check for overflow
MUL X0, X1, X2                  ; Lower 64 bits
SMULH X3, X1, X2                ; Upper 64 bits

; Check overflow: upper 64 bits should be all 0 or all 1
CMP X3, X0, ASR #63             ; Compare with sign extension of lower
B.NE overflow                   ; Branch if overflow
```

### Bit Manipulation

**Bit Field Operations:**

```assembly
; Extract bit field (unsigned)
UBFX X0, X1, #8, #8             ; Extract bits [15:8] from X1

; Extract bit field (signed)
SBFX X0, X1, #8, #8             ; Extract bits [15:8], sign-extend

; Insert bit field
BFI X0, X1, #8, #8              ; Insert X1[7:0] into X0[15:8]

; Clear bit field
BFC X0, #8, #8                  ; Clear bits [15:8] of X0

; Example: Pack RGB values
UBFX X3, X0, #0, #8             ; Extract red
UBFX X4, X1, #0, #8             ; Extract green
UBFX X5, X2, #0, #8             ; Extract blue

MOV X6, X3                      ; Start with red
BFI X6, X4, #8, #8              ; Insert green at bits [15:8]
BFI X6, X5, #16, #8             ; Insert blue at bits [23:16]
; X6 = 0x00BBGGRR
```

**Bit Reverse and Count:**

```assembly
; Reverse bits
RBIT    X0, X1                      ; Reverse all 64 bits

; Reverse bytes
REV     X0, X1                      ; Reverse byte order (64-bit)
REV     W0, W1                      ; Reverse byte order (32-bit)
REV16   X0, X1                      ; Reverse bytes within halfwords
REV32   X0, X1                      ; Reverse bytes within words

; Count leading zeros
CLZ     X0, X1                      ; Count leading zeros (64-bit)
CLZ     W0, W1                      ; Count leading zeros (32-bit)

; Count leading sign bits
CLS     X0, X1                      ; Count leading sign bits

; Example: Find highest set bit
CLZ     X0, X1                      ; Count leading zeros
MOV     X2, #63
SUB     X0, X2, X0                  ; Position = 63 - CLZ
````

**Example** - Bit manipulation utilities:

```assembly
; Set bit N
set_bit:
    ; X0 = value, X1 = bit position
    MOV X2, #1
    LSL X2, X2, X1              ; Create mask
    ORR X0, X0, X2              ; Set bit
    RET

; Clear bit N
clear_bit:
    MOV X2, #1
    LSL X2, X2, X1
    BIC X0, X0, X2              ; Clear bit
    RET

; Toggle bit N
toggle_bit:
    MOV X2, #1
    LSL X2, X2, X1
    EOR X0, X0, X2              ; Toggle bit
    RET

; Test bit N (returns 0 or 1)
test_bit:
    LSR X0, X0, X1              ; Shift bit to position 0
    AND X0, X0, #1              ; Mask to single bit
    RET
````

### Move and Select Instructions

**Move Instructions:**

```assembly
; Move immediate (16-bit)
MOV X0, #0x1234                 ; X0 = 0x1234

; Move wide (16-bit immediate to any position)
MOVZ X0, #0x1234, LSL #0        ; X0 = 0x0000000000001234
MOVZ X0, #0x1234, LSL #16       ; X0 = 0x0000000012340000
MOVZ X0, #0x1234, LSL #32       ; X0 = 0x0000123400000000
MOVZ X0, #0x1234, LSL #48       ; X0 = 0x1234000000000000

; Move with keep (insert 16-bit immediate, keep other bits)
MOVK X0, #0x5678, LSL #16       ; Insert 0x5678 at bits [31:16]

; Move with NOT
MOVN X0, #0x1234, LSL #0        ; X0 = ~0x1234 = 0xFFFFFFFFFFFFEDCB

; Building 64-bit constant
MOVZ X0, #0x1234, LSL #0        ; X0 = 0x0000000000001234
MOVK X0, #0x5678, LSL #16       ; X0 = 0x0000000056781234
MOVK X0, #0x9ABC, LSL #32       ; X0 = 0x00009ABC56781234
MOVK X0, #0xDEF0, LSL #48       ; X0 = 0xDEF09ABC56781234
```

**Conditional Select:**

```assembly
; Select based on condition
CSEL X0, X1, X2, EQ             ; X0 = (condition == EQ) ? X1 : X2

; Example conditions:
CSEL X0, X1, X2, EQ             ; Equal
CSEL X0, X1, X2, NE             ; Not equal
CSEL X0, X1, X2, GT             ; Greater than (signed)
CSEL X0, X1, X2, LT             ; Less than (signed)
CSEL X0, X1, X2, GE             ; Greater or equal
CSEL X0, X1, X2, LE             ; Less or equal
CSEL X0, X1, X2, HI             ; Higher (unsigned)
CSEL X0, X1, X2, LS             ; Lower or same (unsigned)

; Conditional select increment
CSINC X0, X1, X2, EQ            ; X0 = (EQ) ? X1 : X2 + 1

; Conditional select invert
CSINV X0, X1, X2, EQ            ; X0 = (EQ) ? X1 : ~X2

; Conditional select negate
CSNEG X0, X1, X2, EQ            ; X0 = (EQ) ? X1 : -X2
```

**Example** - Branchless min/max:

```assembly
; Compute max(X0, X1) without branching
CMP X0, X1
CSEL X0, X0, X1, GT             ; X0 = (X0 > X1) ? X0 : X1

; Compute min(X0, X1)
CMP X0, X1
CSEL X0, X0, X1, LT             ; X0 = (X0 < X1) ? X0 : X1

; Absolute value
CMP X0, XZR                     ; Compare with zero
CSNEG X0, X0, X0, GE            ; X0 = (X0 >= 0) ? X0 : -X0

; Clamp to range [X1, X2]
CMP X0, X1
CSEL X0, X0, X1, GE             ; X0 = max(X0, X1)
CMP X0, X2
CSEL X0, X0, X2, LE             ; X0 = min(X0, X2)
```

**Conditional Compare:**

```assembly
; Conditional compare (register)
CCMP X0, X1, #0, EQ             ; If EQ: compare X0 with X1, else set flags to #0

; Conditional compare (immediate)
CCMP X0, #10, #0, NE            ; If NE: compare X0 with 10, else flags = 0

; Example: Complex condition without branches
; if (a == 10 && b < 20) ...
CMP X0, #10                     ; Compare a with 10
CCMP X1, #20, #0, EQ            ; If EQ, compare b with 20
B.LT then_block                 ; Branch if both conditions true
```

**Example** - Conditional operations in practice:

```assembly
; Traditional branching approach
compute_value:
    CMP X0, #100
    B.LT less_than
    MOV X0, #100                ; Clamp to 100
    B done
less_than:
    CMP X0, #0
    B.GE done
    MOV X0, #0                  ; Clamp to 0
done:
    RET

; Branchless approach with CSEL
compute_value_branchless:
    CMP X0, #100
    MOV X1, #100
    CSEL X0, X1, X0, GT         ; X0 = (X0 > 100) ? 100 : X0
    CMP X0, #0
    CSEL X0, XZR, X0, LT        ; X0 = (X0 < 0) ? 0 : X0
    RET
```

### Branch Instructions

**Unconditional Branch:**

```assembly
; Branch (±128MB range)
B label                         ; Branch to label

; Branch with link (function call)
BL function                     ; LR = PC + 4, PC = function

; Branch to register
BR X0                           ; PC = X0

; Branch with link to register
BLR X0                          ; LR = PC + 4, PC = X0

; Return
RET                             ; PC = LR (alias for BR X30)
RET X0                          ; PC = X0 (return to specific register)
```

**Conditional Branch:**

```assembly
; Branch on condition (±1MB range)
B.EQ label                      ; Branch if equal
B.NE label                      ; Branch if not equal
B.GT label                      ; Branch if greater (signed)
B.LT label                      ; Branch if less (signed)
B.GE label                      ; Branch if greater or equal
B.LE label                      ; Branch if less or equal
B.HI label                      ; Branch if higher (unsigned)
B.LS label                      ; Branch if lower or same
B.CS label                      ; Branch if carry set
B.CC label                      ; Branch if carry clear
B.MI label                      ; Branch if minus (negative)
B.PL label                      ; Branch if plus (positive)
B.VS label                      ; Branch if overflow set
B.VC label                      ; Branch if overflow clear

; Example: Multi-way branch
CMP X0, #10
B.LT case_less
B.EQ case_equal
B.GT case_greater
```

**Compare and Branch:**

```assembly
; Compare and branch on zero
CBZ X0, label                   ; Branch if X0 == 0 (±1MB range)
CBNZ X0, label                  ; Branch if X0 != 0

; Example: Loop with CBZ
loop:
    ; ... loop body ...
    SUBS X0, X0, #1             ; Decrement counter
    B.NE loop                   ; Traditional branch

; Alternative with CBZ (doesn't use flags)
loop2:
    ; ... loop body ...
    SUB X0, X0, #1              ; Decrement (no flags)
    CBNZ X0, loop2              ; Branch if non-zero
```

**Test Bit and Branch:**

```assembly
; Test bit and branch
TBZ X0, #5, label               ; Branch if bit 5 is zero (±32KB range)
TBNZ X0, #5, label              ; Branch if bit 5 is one

; Example: Check flag bit
TBZ X0, #31, not_set            ; Branch if bit 31 clear
    ; Bit is set
not_set:
    ; Bit is clear

; Example: Power-of-two check
; Check if X0 is power of 2 (only one bit set)
SUB X1, X0, #1
TST X0, X1                      ; Power of 2 if result is zero
B.EQ is_power_of_two
```

**Example** - Branch optimization:

```assembly
; Poor: Multiple conditional branches
check_range_poor:
    CMP X0, #0
    B.LT out_of_range
    CMP X0, #100
    B.GT out_of_range
    ; In range
    MOV X0, #1
    RET
out_of_range:
    MOV X0, #0
    RET

; Better: Minimize branches
check_range_better:
    CMP X0, #100
    CCMP X0, #0, #0, LS         ; If X0 <= 100, compare with 0
    CSINC X0, XZR, XZR, GE      ; X0 = (X0 >= 0) ? 1 : 0
    RET
```

### PC-Relative Addressing

**Address Loading:**

```assembly
; Load address of label (±1MB)
ADR X0, label                   ; X0 = address of label

; Load page address (±4GB)
ADRP X0, label                  ; X0 = page address (4KB aligned)
ADD X0, X0, :lo12:label         ; Add page offset (low 12 bits)

; Example: Load from data section
data_section:
    .quad 0x123456789ABCDEF0

code_section:
    ADRP X0, data_section       ; Load page address
    LDR X1, [X0, :lo12:data_section]    ; Load from page + offset
    
; Position-independent code
    ADRP X0, global_var
    ADD X0, X0, :lo12:global_var
    LDR X1, [X0]                ; Access global variable
```

**Example** - Position-independent function:

```assembly
.global pic_function
pic_function:
    ; Access global offset table
    ADRP X0, :got:external_symbol
    LDR X0, [X0, :got_lo12:external_symbol]
    
    ; Call through PLT
    ADRP X1, external_function
    ADD X1, X1, :lo12:external_function
    BLR X1
    
    RET
```

### System Instructions

**Memory Barriers:**

```assembly
; Data memory barrier
DMB SY                          ; Full system DMB
DMB ISH                         ; Inner shareable
DMB ISHST                       ; Inner shareable, stores only
DMB LD                          ; Load barrier
DMB ST                          ; Store barrier

; Data synchronization barrier
DSB SY                          ; Full system DSB
DSB ISH                         ; Inner shareable

; Instruction synchronization barrier
ISB                             ; Flush pipeline

; Example: Proper synchronization
    STR X0, [X1]                ; Write data
    DMB ISH                     ; Ensure write visible
    MOV X2, #1
    STR X2, [X3]                ; Write flag
```

**Cache Operations:**

```assembly
; Data cache operations
DC CIVAC, X0                    ; Clean and invalidate by VA to PoC
DC CVAC, X0                     ; Clean by VA to PoC
DC IVAC, X0                     ; Invalidate by VA to PoC
DC ZVA, X0                      ; Zero cache line

; Instruction cache operations
IC IVAU, X0                     ; Invalidate by VA to PoU
IC IALLU                        ; Invalidate all to PoU

; Example: Self-modifying code
    ; Write new instructions
    STR X0, [X1]
    DC CVAU, X1                 ; Clean data cache
    DSB ISH                     ; Ensure completion
    IC IVAU, X1                 ; Invalidate instruction cache
    DSB ISH
    ISB                         ; Synchronize pipeline
```

**System Register Access:**

```assembly
; Read system register
MRS X0, MPIDR_EL1               ; Read multiprocessor affinity register
MRS X0, CNTFRQ_EL0              ; Read counter frequency
MRS X0, CNTVCT_EL0              ; Read virtual counter

; Write system register
MSR TTBR0_EL1, X0               ; Write translation table base
MSR VBAR_EL1, X0                ; Write vector base address

; Example: Read CPU ID
MRS X0, MIDR_EL1                ; Main ID register
UBFX X1, X0, #4, #12            ; Extract part number
UBFX X2, X0, #16, #4            ; Extract variant
UBFX X3, X0, #20, #4            ; Extract architecture
```

**Hint Instructions:**

```assembly
; No operation
NOP                             ; No operation

; Yield (in spin-loops)
YIELD                           ; Hint to scheduler

; Wait for event/interrupt
WFE                             ; Wait for event
WFI                             ; Wait for interrupt
SEV                             ; Send event
SEVL                            ; Send event local

; Example: Spinlock with yield
spin_lock:
    LDAXR W1, [X0]              ; Load-acquire exclusive
    CBNZ W1, spin_lock_retry    ; If locked, retry
    MOV W1, #1
    STXR W2, W1, [X0]           ; Try to acquire
    CBNZ W2, spin_lock          ; Retry if failed
    RET

spin_lock_retry:
    YIELD                       ; Hint to other threads
    B spin_lock
```

## AArch64 Calling Conventions

The AArch64 Procedure Call Standard (AAPCS64) defines how functions pass arguments, return values, and manage the stack.

### Procedure Call Standard (AAPCS64)

**Register Roles:**

```
X0-X7:   Argument/result registers (caller-saved)
X8:      Indirect result location register
X9-X15:  Temporary registers (caller-saved)
X16-X17: Intra-procedure-call registers (IP0, IP1)
X18:     Platform register (reserved)
X19-X28: Callee-saved registers
X29:     Frame pointer (FP)
X30:     Link register (LR)
SP:      Stack pointer (must be 16-byte aligned)
```

**Parameter Passing:**

```assembly
; First 8 integer arguments in X0-X7
function_8_args:
    ; X0 = arg1, X1 = arg2, ..., X7 = arg8
    ADD X0, X0, X1              ; Use arguments
    ADD X0, X0, X2
    RET

; Additional arguments on stack
function_10_args:
    ; X0-X7 = first 8 arguments
    ; [SP] = arg9, [SP+8] = arg10
    LDR X8, [SP]                ; Load 9th argument
    LDR X9, [SP, #8]            ; Load 10th argument
    ADD X0, X0, X8
    ADD X0, X0, X9
    RET
```

**Return Values:**

```assembly
; Integer return in X0
function_returns_int:
    MOV X0, #42                 ; Return value in X0
    RET

; 128-bit return in X0-X1
function_returns_128bit:
    MOV X0, #0x1234567890ABCDEF ; Lower 64 bits
    MOV X1, #0xFEDCBA098765432  ; Upper 64 bits
    RET

; Large structure return via X8
function_returns_struct:
    ; X8 points to caller-allocated space
    ; Caller passes pointer in X8
    STR X0, [X8]                ; Store first field
    STR X1, [X8, #8]            ; Store second field
    STR X2, [X8, #16]           ; Store third field
    ; Don't modify X8
    RET

; Caller side
caller:
    SUB SP, SP, #32             ; Allocate space for result
    MOV X8, SP                  ; Pass pointer in X8
    BL function_returns_struct
    ; Result now in [SP] through [SP+23]
    LDR X0, [SP]                ; Access result
    ADD SP, SP, #32             ; Deallocate
```

**Floating-Point Arguments:**

```assembly
; First 8 FP arguments in V0-V7 (as S0-S7 or D0-D7)
function_fp_args:
    ; D0 = arg1, D1 = arg2, etc. (double precision)
    FADD D0, D0, D1             ; Add first two arguments
    RET

; Mixed integer and FP arguments
function_mixed:
    ; X0 = integer arg1
    ; D0 = double arg1
    ; X1 = integer arg2
    ; D1 = double arg2
    SCVTF D2, X0                ; Convert integer to double
    FADD D0, D0, D2             ; Add to FP argument
    RET
```

### Stack Frame Layout

**Standard Stack Frame:**

```assembly
; Function prologue
function:
    ; Save frame pointer and link register
    STP X29, X30, [SP, #-16]!   ; Pre-decrement SP by 16
    MOV X29, SP                 ; Set frame pointer
    
    ; Allocate local variables
    SUB SP, SP, #32             ; Allocate 32 bytes
    
    ; Save callee-saved registers if needed
    STP X19, X20, [SP, #16]
    
    ; Function body
    ; Local variables at [SP], [SP+8], etc.
    ; Saved registers at higher addresses
    
    ; Function epilogue
    LDP X19, X20, [SP, #16]     ; Restore callee-saved
    ADD SP, SP, #32             ; Deallocate locals
    LDP X29, X30, [SP], #16     ; Restore FP, LR
    RET
```

**Stack Frame Diagram:**

```
Higher addresses
+------------------+
| Previous frame   |
+------------------+
| Saved FP         | <- X29 (frame pointer)
+------------------+
| Saved LR         |
+------------------+
| Saved X19        |
+------------------+
| Saved X20        |
+------------------+
| Local var 1      |
+------------------+
| Local var 2      |
+------------------+
| ...              | <- SP (stack pointer)
+------------------+
Lower addresses
```

**Example** - Complete function with locals:

```assembly
.global compute_sum
compute_sum:
    ; Parameters: X0 = array, X1 = count
    ; Local variables: sum (X19), temp (X20)
    
    ; Prologue
    STP X29, X30, [SP, #-32]!   ; Save FP, LR, allocate frame
    MOV X29, SP                 ; Set frame pointer
    STP X19, X20, [SP, #16]     ; Save callee-saved registers
    
    ; Initialize locals
    MOV X19, XZR                ; sum = 0
    MOV X20, X0                 ; temp = array pointer
    
loop:
    CBZ X1, done                ; Check if count == 0
    
    LDR X0, [X20], #8           ; Load element, advance pointer
    ADD X19, X19, X0            ; sum += element
    
    SUB X1, X1, #1              ; count--
    B loop
    
done:
    MOV X0, X19                 ; Return sum
    
    ; Epilogue
    LDP X19, X20, [SP, #16]     ; Restore callee-saved
    LDP X29, X30, [SP], #32     ; Restore FP, LR, deallocate
    RET
```

### Caller-Saved vs Callee-Saved

**Caller-Saved Registers (X0-X18):**

```assembly
caller_function:
    ; Save any needed caller-saved registers before call
    MOV X9, #100                ; Use X9 (caller-saved)
    
    ; X9 might be clobbered by callee
    STR X9, [SP, #-16]!         ; Save X9 if needed after call
    BL callee_function
    LDR X9, [SP], #16           ; Restore X9
    
    ; Or don't save if not needed
    MOV X10, #200
    BL another_function         ; X10 will be lost (don't care)
```

**Callee-Saved Registers (X19-X28):**

```assembly
callee_function:
    ; Must save if using callee-saved registers
    STP X29, X30, [SP, #-32]!
    STP X19, X20, [SP, #16]
    
    ; Can use X19-X20 freely
    MOV X19, X0
    MOV X20, X1
    
    ; Call other functions
    BL helper
    ; X19-X20 preserved across call
    
    ADD X0, X19, X20
    
    ; Must restore before returning
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #32
    RET
```

**Example** - Register usage strategy:

```assembly
; Complex function with multiple calls
complex_function:
    STP X29, X30, [SP, #-64]!   ; Large frame
    MOV X29, SP
    STP X19, X20, [SP, #16]     ; Callee-saved
    STP X21, X22, [SP, #32]
    STP X23, X24, [SP, #48]
    
    ; Use callee-saved for persistent values
    MOV X19, X0                 ; Save arg1 persistently
    MOV X20, X1                 ; Save arg2
    
    ; Use caller-saved for temporaries
    MOV X9, #100
    ADD X10, X9, X19
    
    ; First call (X9-X15 may be clobbered)
    MOV X0, X10
    BL function1
    MOV X21, X0                 ; Save result in callee-saved
    
    ; Second call (X19-X20 still valid)
    MOV X0, X19
    MOV X1, X20
    BL function2
    MOV X22, X0                 ; Save result
    
    ; Combine results
    ADD X0, X21, X22
    
    ; Restore and return
    LDP X23, X24, [SP, #48]
    LDP X21, X22, [SP, #32]
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #64
    RET
```

### Variadic Functions

**Variable Argument Lists:**

```assembly
; va_list implementation
; Variadic function prototype: int sum(int count, ...)

.global sum
sum:
    ; X0 = count (number of additional arguments)
    ; X1-X7 = first 7 variadic arguments
    ; [SP] onwards = remaining arguments
    
    STP X29, X30, [SP, #-16]!
    MOV X29, SP
    
    MOV X9, XZR                 ; sum = 0
    MOV X10, #0                 ; processed count = 0
    
    ; Process register arguments (X1-X7)
loop_regs:
    CMP X10, X0                 ; Check if done
    B.GE done
    CMP X10, #7                 ; Check if still in registers
    B.GE loop_stack
    
    ; Load from appropriate register (simplified)
    ; In practice, would use computed addressing or table
    CMP X10, #0
    B.NE check1
    ADD X9, X9, X1
    B next_reg
check1:
    CMP X10, #1
    B.NE check2
    ADD X9, X9, X2
    B next_reg
check2:
    ; ... more cases ...
    
next_reg:
    ADD X10, X10, #1
    B loop_regs
    
loop_stack:
    ; Process stack arguments
    SUB X11, X10, #7            ; Stack argument index
    LDR X12, [X29, X11, LSL #3] ; Load from stack
    ADD X9, X9, X12
    ADD X10, X10, #1
    CMP X10, X0
    B.LT loop_stack
    
done:
    MOV X0, X9                  ; Return sum
    LDP X29, X30, [SP], #16
    RET

; Caller
caller:
    MOV X0, #5                  ; Count
    MOV X1, #10                 ; Arg 1
    MOV X2, #20                 ; Arg 2
    MOV X3, #30                 ; Arg 3
    MOV X4, #40                 ; Arg 4
    MOV X5, #50                 ; Arg 5
    BL sum                      ; Result in X0
```

### Structure Passing

**Small Structures (≤16 bytes):**

```assembly
; Structure in registers
; struct Point { long x, y; }

function_point:
    ; X0 = x, X1 = y (structure split into registers)
    ADD X0, X0, X1              ; x + y
    RET

; Caller
caller:
    MOV X0, #10                 ; point.x
    MOV X1, #20                 ; point.y
    BL function_point
```

**Large Structures (>16 bytes):**

```assembly
; Structure passed by reference
; struct LargeData { long a, b, c, d; }

function_large:
    ; X0 = pointer to structure
    LDR X1, [X0]                ; Load first field
    LDR X2, [X0, #8]            ; Load second field
    LDR X3, [X0, #16]           ; Load third field
    LDR X4, [X0, #24]           ; Load fourth field
    ADD X0, X1, X2
    ADD X0, X0, X3
    ADD X0, X0, X4
    RET

; Caller
caller:
    SUB SP, SP, #32             ; Allocate structure on stack
    MOV X1, #10
    STR X1, [SP]                ; field a
    MOV X1, #20
    STR X1, [SP, #8]            ; field b
    MOV X1, #30
    STR X1, [SP, #16]           ; field c
    MOV X1, #40
    STR X1, [SP, #24]           ; field d
    
    MOV X0, SP                  ; Pass pointer
    BL function_large
    
    ADD SP, SP, #32             ; Deallocate
```

### Floating-Point Calling Convention

**FP Parameter Passing:**

```assembly
; V0-V7 for first 8 FP arguments
.global fp_function
fp_function:
    ; D0 = double arg1
    ; S1 = float arg2
    ; D2 = double arg3
    
    FADD D0, D0, D2             ; Add double arguments
    FCVT D1, S1                 ; Convert float to double
    FADD D0, D0, D1             ; Add converted value
    RET                         ; Return in D0

; Mixed integer and FP
.global mixed_function
mixed_function:
    ; X0 = int64 arg
    ; D0 = double arg
    ; X1 = int64 arg
    ; D1 = double arg
    
    SCVTF D2, X0                ; Convert X0 to double
    FADD D0, D0, D2
    SCVTF D2, X1
    FADD D0, D0, D2
    FADD D0, D0, D1
    RET
```

**SIMD/Vector Parameters:**

```assembly
; V0-V7 for vector arguments (full 128-bit)
.global vector_add
vector_add:
    ; V0 = first vector (4x float32)
    ; V1 = second vector (4x float32)
    
    FADD V0.4S, V0.4S, V1.4S    ; Vector add
    RET                         ; Return in V0
```

### Stack Alignment

**16-Byte Alignment Requirement:**

```assembly
; SP must be 16-byte aligned at function entry

function:
    ; Allocate space (must be multiple of 16)
    SUB SP, SP, #32             ; OK: 32 is multiple of 16
    
    ; Store data
    STR X0, [SP]
    STR X1, [SP, #8]
    
    ; Call another function (SP still 16-byte aligned)
    BL other_function
    
    ; Deallocate
    ADD SP, SP, #32
    RET

; BAD: Misaligned allocation
bad_function:
    SUB SP, SP, #24             ; BAD: Not 16-byte aligned
    ; This violates ABI and may cause issues
```

**Variable-Size Allocation:**

```assembly
; Align dynamic allocation
alloca_example:
    ; X0 = size to allocate

    STP     X29, X30, [SP, #-16]!
    MOV     X29, SP

    ; Round up to 16-byte multiple
    ADD     X0, X0, #15             ; Add 15
    AND     X0, X0, #~15            ; Clear low 4 bits (align to 16)

    ; Allocate
    SUB     SP, SP, X0

    ; Use allocated space
    MOV     X1, SP                  ; Pointer to allocated space

    ; ... use space ...

    ; Deallocate: restore SP from frame pointer
    MOV     SP, X29
    LDP     X29, X30, [SP], #16
    RET
````

### Tail Call Optimization

**Tail Call Requirements:**
```assembly
; Tail call: function ends with call to another function
; Can reuse current stack frame if conditions met

.global tail_call_example
tail_call_example:
    ; Save nothing, no local variables
    ; Arguments already in correct registers
    
    ; Prepare arguments for target function
    MOV X0, X1                  ; Shuffle arguments if needed
    MOV X1, X2
    
    ; Jump directly instead of BL
    B target_function           ; Tail call (not BL)
    
; Instead of:
;   BL target_function
;   RET

; More complex example with cleanup
tail_call_with_frame:
    STP X29, X30, [SP, #-32]!
    MOV X29, SP
    STP X19, X20, [SP, #16]
    
    ; Setup for tail call
    MOV X19, X0
    
    ; Prepare arguments
    MOV X0, X19
    MOV X1, #100
    
    ; Cleanup before tail call
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #32
    
    ; Tail call
    B target_function           ; Jump, don't link
````

### Exception Handling and Unwinding

**Frame Layout for Unwinding:**

```assembly
.global unwind_example
unwind_example:
    ; Standard frame for unwinding
    STP X29, X30, [SP, #-16]!   ; FP and LR saved together
    MOV X29, SP                 ; FP points to saved FP
    
    ; This allows unwinder to:
    ; 1. Follow FP chain: previous FP at [X29]
    ; 2. Get return address: LR at [X29, #8]
    
    ; Allocate more space if needed
    SUB SP, SP, #32
    
    ; Function body
    BL might_throw_exception
    
    ; Normal cleanup
    ADD SP, SP, #32
    LDP X29, X30, [SP], #16
    RET
    
; Unwinder walks stack:
; 1. Current FP in X29
; 2. Previous FP = [X29]
; 3. Return address = [X29, #8]
; 4. Repeat until FP = 0
```

**Compact Unwinding Information:**

```assembly
; ARM64 uses compact unwind encoding
; Stored in __unwind_info section (Mach-O) or .eh_frame (ELF)

.cfi_startproc                  ; Start unwind info
function_with_cfi:
    STP X29, X30, [SP, #-16]!
    .cfi_def_cfa_offset 16      ; CFA offset is 16
    .cfi_offset 29, -16         ; X29 saved at CFA-16
    .cfi_offset 30, -8          ; X30 saved at CFA-8
    
    MOV X29, SP
    .cfi_def_cfa_register 29    ; CFA now relative to X29
    
    ; Function body
    
    LDP X29, X30, [SP], #16
    .cfi_restore 29
    .cfi_restore 30
    .cfi_def_cfa 31, 0          ; Back to SP
    RET
.cfi_endproc
```

### Real-World Examples

**Example** - String length (strlen):

```assembly
.global strlen
strlen:
    ; X0 = string pointer
    ; Returns length in X0
    
    MOV X1, X0                  ; Save original pointer
    
    ; Check alignment
    TST X0, #7                  ; Test if 8-byte aligned
    B.EQ aligned
    
byte_loop:
    LDRB W2, [X0], #1           ; Load byte
    CBZ W2, found_null          ; Check for null
    TST X0, #7                  ; Check alignment
    B.NE byte_loop
    
aligned:
    ; Process 8 bytes at a time
    MOV X3, #0x0101010101010101 ; Mask for null detection
    
word_loop:
    LDR X2, [X0], #8            ; Load 8 bytes
    
    ; Check for null byte using bit tricks
    SUB X4, X2, X3              ; Subtract 0x01 from each byte
    BIC X4, X4, X2              ; Clear with original
    TST X4, X3, LSL #7          ; Test for 0x80 in any byte
    B.EQ word_loop              ; Continue if no null
    
    ; Found null in this word
    SUB X0, X0, #8              ; Back up to start of word
    
byte_search:
    LDRB W2, [X0], #1
    CBZ W2, found_null
    B byte_search
    
found_null:
    SUB X0, X0, X1              ; Length = end - start
    SUB X0, X0, #1              ; Don't count null
    RET
```

**Example** - Memory copy (memcpy):

```assembly
.global memcpy
memcpy:
    ; X0 = dest, X1 = src, X2 = size
    ; Returns dest in X0
    
    MOV X3, X0                  ; Save dest
    CBZ X2, done                ; Handle zero size
    
    ; Check if we can use pairs (16-byte copies)
    CMP X2, #16
    B.LT byte_copy
    
    ; Check alignment
    ORR X4, X0, X1              ; Check both pointers
    TST X4, #7
    B.NE byte_copy              ; Use byte copy if misaligned
    
pair_copy:
    ; Copy 16 bytes at a time
    LDP X4, X5, [X1], #16       ; Load pair
    STP X4, X5, [X0], #16       ; Store pair
    SUBS X2, X2, #16
    B.GE pair_copy
    
    ; Handle remainder
    ADDS X2, X2, #16
    B.EQ done
    
byte_copy:
    LDRB W4, [X1], #1           ; Load byte
    STRB W4, [X0], #1           ; Store byte
    SUBS X2, X2, #1
    B.NE byte_copy
    
done:
    MOV X0, X3                  ; Restore dest
    RET
```

**Example** - Memory set (memset):

```assembly
.global memset
memset:
    ; X0 = dest, X1 = value (byte), X2 = size
    ; Returns dest in X0
    
    MOV X3, X0                  ; Save dest
    CBZ X2, done
    
    ; Replicate byte to all positions
    AND X1, X1, #0xFF
    ORR X1, X1, X1, LSL #8      ; 16-bit
    ORR X1, X1, X1, LSL #16     ; 32-bit
    ORR X1, X1, X1, LSL #32     ; 64-bit
    
    ; Check for large fills
    CMP X2, #16
    B.LT byte_fill
    
    ; Fill 16 bytes at a time
pair_fill:
    STP X1, X1, [X0], #16       ; Store pair
    SUBS X2, X2, #16
    B.GE pair_fill
    
    ; Handle remainder
    ADDS X2, X2, #16
    B.EQ done
    
byte_fill:
    STRB W1, [X0], #1
    SUBS X2, X2, #1
    B.NE byte_fill
    
done:
    MOV X0, X3
    RET
```

**Example** - Quick sort partition:

```assembly
.global partition
partition:
    ; X0 = array, X1 = low, X2 = high
    ; Returns pivot index in X0
    
    STP X29, X30, [SP, #-48]!
    MOV X29, SP
    STP X19, X20, [SP, #16]
    STP X21, X22, [SP, #32]
    
    ; Save parameters
    MOV X19, X0                 ; array
    MOV X20, X1                 ; low (i)
    MOV X21, X2                 ; high
    
    ; Load pivot = array[high]
    LDR X22, [X19, X21, LSL #3]
    
    SUB X20, X20, #1            ; i = low - 1
    
partition_loop:
    CMP X1, X21                 ; j < high?
    B.GE partition_done
    
    ; Load array[j]
    LDR X9, [X19, X1, LSL #3]
    
    ; if (array[j] <= pivot)
    CMP X9, X22
    B.GT skip_swap
    
    ; i++
    ADD X20, X20, #1
    
    ; Swap array[i] and array[j]
    LDR X10, [X19, X20, LSL #3]
    STR X9, [X19, X20, LSL #3]
    STR X10, [X19, X1, LSL #3]
    
skip_swap:
    ADD X1, X1, #1              ; j++
    B partition_loop
    
partition_done:
    ; Final swap: array[i+1] with array[high]
    ADD X20, X20, #1
    LDR X9, [X19, X20, LSL #3]
    LDR X10, [X19, X21, LSL #3]
    STR X10, [X19, X20, LSL #3]
    STR X9, [X19, X21, LSL #3]
    
    MOV X0, X20                 ; Return pivot index
    
    LDP X21, X22, [SP, #32]
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #48
    RET
```

**Example** - Binary search:

```assembly
.global binary_search
binary_search:
    ; X0 = array, X1 = size, X2 = target
    ; Returns index in X0, or -1 if not found
    
    MOV X3, #0                  ; low = 0
    SUB X4, X1, #1              ; high = size - 1
    
search_loop:
    CMP X3, X4                  ; low <= high?
    B.GT not_found
    
    ; mid = low + (high - low) / 2
    SUB X5, X4, X3              ; high - low
    LSR X5, X5, #1              ; Divide by 2
    ADD X5, X5, X3              ; mid = low + (high-low)/2
    
    ; Load array[mid]
    LDR X6, [X0, X5, LSL #3]
    
    ; Compare with target
    CMP X6, X2
    B.EQ found                  ; array[mid] == target
    B.GT search_left            ; array[mid] > target
    
    ; Search right: low = mid + 1
    ADD X3, X5, #1
    B search_loop
    
search_left:
    ; Search left: high = mid - 1
    SUB X4, X5, #1
    B search_loop
    
found:
    MOV X0, X5                  ; Return mid
    RET
    
not_found:
    MOV X0, #-1                 ; Return -1
    RET
```

### Performance Optimization Tips

**Register Allocation Strategy:**

```assembly
; Good: Use callee-saved for important values across calls
optimized_function:
    STP X29, X30, [SP, #-32]!
    STP X19, X20, [SP, #16]
    
    MOV X19, X0                 ; Important persistent value
    MOV X20, X1
    
    ; Multiple calls - X19, X20 preserved
    MOV X0, X19
    BL function1
    
    MOV X0, X20
    BL function2
    
    ; X19, X20 still valid
    ADD X0, X19, X20
    
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #32
    RET

; Poor: Unnecessary memory access
unoptimized_function:
    STP X29, X30, [SP, #-32]!
    
    STR X0, [SP, #16]           ; Spill to memory
    STR X1, [SP, #24]
    
    LDR X0, [SP, #16]           ; Reload for each call
    BL function1
    
    LDR X0, [SP, #24]
    BL function2
    
    LDR X0, [SP, #16]           ; Reload again
    LDR X1, [SP, #24]
    ADD X0, X0, X1
    
    LDP X29, X30, [SP], #32
    RET
```

**Minimize Stack Usage:**

```assembly
; Good: Minimal stack frame
leaf_function:
    ; No calls, no need to save LR
    ADD X0, X0, X1
    MUL X0, X0, X2
    RET                         ; No stack frame needed

; Good: Small frame
small_frame_function:
    STP X29, X30, [SP, #-16]!   ; Just FP and LR
    ; Use X9-X15 for temporaries (caller-saved, no save needed)
    MOV X9, #100
    ADD X0, X0, X9
    LDP X29, X30, [SP], #16
    RET

; Poor: Unnecessarily large frame
bloated_function:
    SUB SP, SP, #128            ; Huge allocation
    STP X29, X30, [SP]
    STP X19, X20, [SP, #16]
    ; ... only uses 32 bytes ...
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP]
    ADD SP, SP, #128
    RET
```

**Leverage CSEL for Branchless Code:**

```assembly
; Good: Branchless max
max_branchless:
    CMP X0, X1
    CSEL X0, X0, X1, GT         ; Single instruction
    RET

; Poor: Branch-based max
max_branching:
    CMP X0, X1
    B.GT done
    MOV X0, X1
done:
    RET

; Good: Branchless clamping
clamp:
    ; Clamp X0 to range [X1, X2]
    CMP X0, X1
    CSEL X0, X0, X1, GE         ; max(X0, X1)
    CMP X0, X2
    CSEL X0, X0, X2, LE         ; min(X0, X2)
    RET
```

**Optimize Load/Store Patterns:**

```assembly
; Good: Use load/store pairs
efficient_copy:
    LDP X3, X4, [X1], #16       ; Load 2 registers, 16 bytes
    STP X3, X4, [X0], #16       ; Store 2 registers, 16 bytes
    ; 2 instructions, 16 bytes transferred

; Poor: Individual loads/stores
inefficient_copy:
    LDR X3, [X1], #8            ; Load 1 register
    STR X3, [X0], #8            ; Store 1 register
    LDR X4, [X1], #8            ; Load another
    STR X4, [X0], #8            ; Store another
    ; 4 instructions, same 16 bytes

; Good: Prefetch for large copies
large_copy_with_prefetch:
    PRFM PLDL1KEEP, [X1, #64]   ; Prefetch source
    PRFM PSTL1KEEP, [X0, #64]   ; Prefetch destination
    LDP X3, X4, [X1], #16
    STP X3, X4, [X0], #16
    ; Continue...
```

**Alignment Awareness:**

```assembly
; Align hot code to cache line boundaries
.align 6                        ; 64-byte alignment (cache line)
hot_loop:
    ; Critical loop code
    LDR X0, [X1], #8
    ADD X2, X2, X0
    SUBS X3, X3, #1
    B.NE hot_loop
    RET

; Align data structures
.align 3                        ; 8-byte alignment
data_array:
    .quad 1, 2, 3, 4

.align 4                        ; 16-byte alignment for SIMD
vector_data:
    .quad 1, 2, 3, 4
```

## Advanced Features and Extensions

### Pointer Authentication (PAC)

**Pointer Authentication Codes:**

```assembly
; Sign return address
PACIASP                         ; Sign LR with SP (return address)
; Function body
AUTIASP                         ; Authenticate LR before return
RET

; Sign data pointer
PACIA X0, X1                    ; Sign X0 with modifier X1
; ... use signed pointer ...
AUTIA X0, X1                    ; Authenticate before use

; Check for authentication failure
AUTIA X0, X1
; If authentication fails, bits [63:56] set to error code
TST X0, #0xFF00000000000000
B.NE auth_failed
```

### Branch Target Identification (BTI)

**Indirect Branch Protection:**

```assembly
; Function entry with BTI
.global protected_function
protected_function:
    BTI C                       ; Branch target identification
    ; Only reachable via BL, BLR (calls)
    
    STP X29, X30, [SP, #-16]!
    ; Function body
    LDP X29, X30, [SP], #16
    RET

; Jump target
jump_target:
    BTI J                       ; Only reachable via BR (jumps)
    ; Code
    RET

; Call or jump target
dual_target:
    BTI JC                      ; Reachable via both
    ; Code
    RET
```

### Memory Tagging Extension (MTE)

**Tagged Memory Operations:**

```assembly
; Allocate with tag
IRG X0, X1                      ; Insert random tag into pointer
; X0 now has 4-bit tag in bits [59:56]

; Store with tag
STG X0, [X0]                    ; Store tag to memory

; Load and check tag
LDG X1, [X0]                    ; Load tag from memory
SUBPS X2, X1, X0                ; Compare tags
B.NE tag_mismatch

; Tagged load (checked automatically)
LDR X1, [X0]                    ; Hardware checks tag match
```

### Scalable Vector Extension (SVE)

**Variable-Length Vector Operations:**

```assembly
; SVE vector add (width determined at runtime)
FADD Z0.D, Z1.D, Z2.D           ; Add vectors (double precision)

; Predicated operations
FADD Z0.D, P0/M, Z0.D, Z1.D     ; Add where predicate true

; Get vector length
CNTD X0                         ; Count 64-bit elements in vector

; Example: Vector loop
sve_loop:
    LD1D Z0.D, P0/Z, [X1]       ; Load with predicate
    FADD Z0.D, P0/M, Z0.D, Z1.D ; Add
    ST1D Z0.D, P0, [X2]         ; Store
    INCB X1                     ; Increment by vector bytes
    INCB X2
    SUBS X3, X3, X4             ; Decrement counter
    B.GT sve_loop
```

**Important related topics:** NEON/SIMD optimization for AArch64, atomic operations and memory ordering, exception handling and signal frames, position-independent code (PIC) in AArch64, interaction between AArch64 and AArch32 code, ARMv8.x feature extensions, cache hierarchy and optimization strategies, debugging with hardware watchpoints and breakpoints

---

## Addressing Mode Changes

AArch64 introduces a simplified and more consistent addressing mode system compared to AArch32, removing some complex modes while adding new capabilities.

**Register changes:**

```asm
; AArch64 has 31 general-purpose registers (64-bit)
; X0-X30: 64-bit registers
; W0-W30: 32-bit access to lower half of X registers
; XZR/WZR: Zero register (reads as 0, writes discarded)
; SP: Stack pointer (no longer a general-purpose register)
; PC: Not directly accessible (no longer R15)

; Example register usage
MOV X0, #42                 ; 64-bit operation
MOV W0, #42                 ; 32-bit operation (upper 32 bits zeroed)
ADD X1, X2, X3              ; X1 = X2 + X3
ADD W1, W2, W3              ; W1 = W2 + W3 (32-bit)

; Zero register usage
MOV X0, XZR                 ; X0 = 0
CMP X1, XZR                 ; Compare with zero
STR XZR, [X0]               ; Store zero to memory
```

**Base addressing modes:**

**Offset addressing (base + offset):**

```asm
; Immediate offset (scaled or unscaled)
LDR X0, [X1]                ; Load from [X1]
LDR X0, [X1, #8]            ; Load from [X1 + 8]
LDR X0, [X1, #-16]          ; Load from [X1 - 16]
STR X0, [X1, #24]           ; Store to [X1 + 24]

; Scaled immediate (multiplied by operand size)
LDR X0, [X1, #8]            ; Offset 8 bytes (scaled for 64-bit)
LDR W0, [X1, #4]            ; Offset 4 bytes (scaled for 32-bit)
LDRH W0, [X1, #2]           ; Offset 2 bytes (scaled for 16-bit)
LDRB W0, [X1, #1]           ; Offset 1 byte

; Unscaled immediate offset
LDUR X0, [X1, #-5]          ; Unscaled offset (any value -256 to 255)
STUR X0, [X1, #-5]          ; Store with unscaled offset

; Register offset (extended or shifted)
LDR X0, [X1, X2]            ; Load from [X1 + X2]
LDR X0, [X1, X2, LSL #3]    ; Load from [X1 + (X2 << 3)]
LDR X0, [X1, W2, UXTW]      ; Load from [X1 + zero_extend(W2)]
LDR X0, [X1, W2, SXTW #3]   ; Load from [X1 + sign_extend(W2) << 3]
```

**Pre-indexed addressing (update base before access):**

```asm
; Pre-indexed with writeback
LDR X0, [X1, #16]!          ; X0 = *[X1 + 16], then X1 += 16
STR X0, [X1, #-8]!          ; X1 -= 8, then *[X1] = X0

; Useful for stack operations
STR X0, [SP, #-16]!         ; Push: SP -= 16, then store
LDR X0, [SP, #16]!          ; Pop: load, then SP += 16
```

**Post-indexed addressing (update base after access):**

```asm
; Post-indexed with writeback
LDR X0, [X1], #8            ; X0 = *[X1], then X1 += 8
STR X0, [X1], #16           ; *[X1] = X0, then X1 += 16

; Useful for array traversal
loop:
    LDR X0, [X1], #8        ; Load and advance pointer
    ; Process X0
    SUBS X2, X2, #1
    B.NE loop
```

**PC-relative addressing:**

```asm
; ADR: Calculate address relative to PC
ADR X0, label               ; X0 = address of label (±1MB range)
ADR X0, data_table          ; X0 = address of data_table

; ADRP: Calculate page address relative to PC
ADRP X0, label              ; X0 = page address of label (±4GB range)
ADD X0, X0, :lo12:label     ; Add low 12 bits for full address

; Load from PC-relative address
ADRP X0, my_data
LDR X1, [X0, :lo12:my_data]

; Example: Access global variable
ADRP X0, global_var
LDR W1, [X0, :lo12:global_var]
ADD W1, W1, #1
STR W1, [X0, :lo12:global_var]

label:
    .quad 0x123456789ABCDEF0
my_data:
    .word 42
global_var:
    .word 0
```

**Literal pool addressing (removed explicit literal pools):**

```asm
; AArch32 style (not available in AArch64)
; LDR R0, =0x12345678        ; Load from literal pool

; AArch64 alternatives:

; Method 1: MOV with immediate (for values that fit)
MOV X0, #0x1234             ; 16-bit immediate
MOVK X0, #0x5678, LSL #16   ; Insert 16 bits at position

; Method 2: Load from nearby memory
ADRP X0, constant_pool
LDR X1, [X0, :lo12:constant_value]

; Method 3: Build with multiple instructions
MOV X0, #0x5678
MOVK X0, #0x1234, LSL #16
MOVK X0, #0xABCD, LSL #32
MOVK X0, #0xEF00, LSL #48

constant_pool:
constant_value:
    .quad 0x123456789ABCDEF0
```

**Load/Store pair operations:**

```asm
; Load/store two registers simultaneously
LDP X0, X1, [X2]            ; X0 = [X2], X1 = [X2 + 8]
STP X0, X1, [X2]            ; [X2] = X0, [X2 + 8] = X1

; With offset
LDP X0, X1, [X2, #16]       ; Load from [X2 + 16] and [X2 + 24]
STP X0, X1, [X2, #32]       ; Store to [X2 + 32] and [X2 + 40]

; With pre-index
LDP X0, X1, [X2, #16]!      ; X2 += 16, then load
STP X0, X1, [X2, #-16]!     ; X2 -= 16, then store

; With post-index
LDP X0, X1, [X2], #16       ; Load, then X2 += 16
STP X0, X1, [X2], #16       ; Store, then X2 += 16

; Stack operations using pairs
STP X29, X30, [SP, #-16]!   ; Push frame pointer and link register
LDP X29, X30, [SP], #16     ; Pop frame pointer and link register

; Function prologue/epilogue
function_entry:
    STP X29, X30, [SP, #-32]!   ; Save FP, LR
    STP X19, X20, [SP, #16]     ; Save callee-saved registers
    MOV X29, SP                  ; Setup frame pointer
    ; Function body
    LDP X19, X20, [SP, #16]     ; Restore registers
    LDP X29, X30, [SP], #32     ; Restore FP, LR
    RET
```

**Exclusive load/store (atomic operations):**

```asm
; Load exclusive
LDXR X0, [X1]               ; Load exclusive (64-bit)
LDXR W0, [X1]               ; Load exclusive (32-bit)
LDXRH W0, [X1]              ; Load exclusive (16-bit)
LDXRB W0, [X1]              ; Load exclusive (8-bit)

; Store exclusive (returns 0 on success, 1 on failure)
STXR W2, X0, [X1]           ; Store exclusive (64-bit)
STXR W2, W0, [X1]           ; Store exclusive (32-bit)
STXRH W2, W0, [X1]          ; Store exclusive (16-bit)
STXRB W2, W0, [X1]          ; Store exclusive (8-bit)

; Atomic increment example
atomic_inc:
retry:
    LDXR X0, [X1]           ; Load exclusive
    ADD X0, X0, #1          ; Increment
    STXR W2, X0, [X1]       ; Store exclusive
    CBNZ W2, retry          ; Retry if failed
    RET

; Load/store acquire/release exclusive
LDAXR X0, [X1]              ; Load acquire exclusive
STLXR W2, X0, [X1]          ; Store release exclusive
```

**Comparison with AArch32:**

```asm
; AArch32: Complex flexible second operand
ADD R0, R1, R2, LSL #3      ; R0 = R1 + (R2 << 3)
ADD R0, R1, R2, ROR #8      ; R0 = R1 + rotate_right(R2, 8)

; AArch64: Simpler, separate shift instructions
LSL X2, X2, #3              ; X2 = X2 << 3
ADD X0, X1, X2              ; X0 = X1 + X2

; But register offset in memory access still supported
LDR X0, [X1, X2, LSL #3]    ; Load from [X1 + (X2 << 3)]

; AArch32: Auto-increment addressing with multiple registers
LDMIA R0!, {R1-R5}          ; Load R1-R5, increment R0

; AArch64: Must use pairs or individual loads
LDP X1, X2, [X0], #16       ; Load pair and increment
LDP X3, X4, [X0], #16
LDR X5, [X0], #8

; AArch32: Conditional execution
ADDNE R0, R1, R2            ; Execute if not equal

; AArch64: Conditional select instead
CSEL X0, X1, X2, NE         ; X0 = (NE) ? X1 : X2
```

## Removed Features from AArch32

AArch64 removes several AArch32 features to simplify the architecture and improve performance.

**Removed: Conditional execution (IT blocks)**

```asm
; AArch32: Conditional execution of almost any instruction
CMP R0, R1
ADDNE R2, R2, #1            ; Execute only if not equal
MOVEQ R3, #0                ; Execute only if equal

; AArch32: IT block in Thumb
IT EQ                       ; If equal, then...
ADDEQ R0, R0, #1            ; Conditionally executed

; AArch64: Must use branches or conditional select
CMP X0, X1
B.EQ skip
ADD X2, X2, #1              ; Executed if not equal
skip:

; Or use conditional select
CMP X0, X1
ADD X3, X2, #1              ; Calculate new value
CSEL X2, X3, X2, NE         ; Select based on condition

; Conditional increment macro
.macro CINC reg, cond
    ADD X9, \reg, #1
    CSEL \reg, X9, \reg, \cond
.endm

; Usage
CMP X0, X1
CINC X2, NE                 ; Increment X2 if not equal
```

**Conditional operations in AArch64:**

```asm
; Conditional select
CSEL Xd, Xn, Xm, cond       ; Xd = (cond) ? Xn : Xm
CSINC Xd, Xn, Xm, cond      ; Xd = (cond) ? Xn : Xm + 1
CSINV Xd, Xn, Xm, cond      ; Xd = (cond) ? Xn : ~Xm
CSNEG Xd, Xn, Xm, cond      ; Xd = (cond) ? Xn : -Xm

; Examples
CMP X0, #0
CSEL X1, X2, X3, GT         ; X1 = (X0 > 0) ? X2 : X3
CSINC X1, X2, XZR, EQ       ; X1 = (X0 == 0) ? X2 : 1
CSINV X1, X2, X2, NE        ; X1 = (X0 != 0) ? X2 : ~X2

; Conditional set (set to 0 or 1 based on condition)
CSET Xd, cond               ; Xd = (cond) ? 1 : 0
CSETM Xd, cond              ; Xd = (cond) ? -1 : 0

CMP X0, X1
CSET X2, GT                 ; X2 = (X0 > X1) ? 1 : 0
CSETM X3, EQ                ; X3 = (X0 == X1) ? -1 : 0

; Conditional increment/negate
CINC Xd, Xn, cond           ; Xd = (cond) ? Xn + 1 : Xn
CNEG Xd, Xn, cond           ; Xd = (cond) ? -Xn : Xn

CMP X0, #0
CINC X1, X1, EQ             ; Increment X1 if X0 == 0
CNEG X2, X2, LT             ; Negate X2 if X0 < 0

; Absolute value using conditional negate
ABS X0, X1:
    CMP X1, #0
    CNEG X0, X1, LT         ; X0 = (X1 < 0) ? -X1 : X1
```

**Removed: Predicated/conditional returns**

```asm
; AArch32: Conditional return
CMP R0, #0
BXEQ LR                     ; Return if equal

; AArch64: Must use branch
CMP X0, #0
B.NE continue
RET
continue:

; Or restructure code
CMP X0, #0
B.EQ early_return
; Normal path
; ...
RET

early_return:
RET
```

**Removed: Direct PC manipulation**

```asm
; AArch32: PC is R15, can be loaded/stored
LDR PC, [R0]                ; Jump to address in [R0]
ADD PC, PC, R1              ; Computed branch
MOV PC, LR                  ; Return

; AArch64: PC not accessible, must use dedicated instructions
LDR X0, [X1]
BR X0                       ; Branch to register

ADD X0, X0, X1
BR X0                       ; Computed branch

RET                         ; Return (uses X30/LR)

; Branch to register
BR Xn                       ; Branch to Xn
BLR Xn                      ; Branch with link to Xn
RET                         ; Return to X30 (LR)
RET Xn                      ; Return to Xn
```

**Removed: LDM/STM (Load/Store Multiple)**

```asm
; AArch32: Load/store multiple registers
PUSH {R4-R11, LR}           ; Save multiple registers
POP {R4-R11, PC}            ; Restore and return

LDMIA R0!, {R1-R8}          ; Load R1-R8 from [R0], increment R0
STMDB R13!, {R0-R3}         ; Store R0-R3, decrement R13 (push)

; AArch64: Must use pairs or individual operations
STP X19, X20, [SP, #-48]!   ; Save pairs
STP X21, X22, [SP, #16]
STP X23, X24, [SP, #32]

LDP X19, X20, [SP]          ; Restore pairs
LDP X21, X22, [SP, #16]
LDP X23, X24, [SP, #32]
ADD SP, SP, #48

; Helper macro for multiple push
.macro PUSH_REGS regs:vararg
    .irp reg, \regs
        SUB SP, SP, #8
        STR \reg, [SP]
    .endr
.endm

; Helper macro for multiple pop
.macro POP_REGS regs:vararg
    .irp reg, \regs
        LDR \reg, [SP]
        ADD SP, SP, #8
    .endr
.endm
```

**Removed: Coprocessor 15 (CP15) instructions**

```asm
; AArch32: Coprocessor instructions for system control
MRC p15, 0, R0, c1, c0, 0   ; Read SCTLR
MCR p15, 0, R0, c1, c0, 0   ; Write SCTLR

; AArch64: Dedicated system register instructions
MRS X0, SCTLR_EL1           ; Read system control register
MSR SCTLR_EL1, X0           ; Write system control register

MRS X0, MPIDR_EL1           ; Read multiprocessor ID
MRS X0, MIDR_EL1            ; Read main ID register
MRS X0, TPIDR_EL0           ; Read thread ID (EL0)
MSR TPIDR_EL0, X0           ; Write thread ID
```

**Removed: 26-bit addressing modes**

```asm
; AArch32 legacy: 26-bit PC with mode bits in upper 6 bits
; (Already removed in ARMv4 and later)

; AArch64: Always 64-bit addressing
; Virtual address space up to 52 bits (implementation dependent)
; Typically 48-bit virtual addressing (256 TB)
```

**Removed: Thumb state**

```asm
; AArch32: Switch between ARM (32-bit) and Thumb (16-bit) states
BX R0                       ; Branch and exchange (may switch state)
BLX R0                      ; Branch with link and exchange

; AArch64: Single instruction set (all 32-bit instructions)
; No state switching needed
BR X0                       ; Always 64-bit instruction set
BLR X0
```

**Removed: Rotate right with extend (RRX)**

```asm
; AArch32: Rotate through carry flag
MOV R0, R1, RRX             ; R0 = {C, R1[31:1]} (33-bit rotate)

; AArch64: Must implement manually
MRS X2, NZCV                ; Read flags
LSR X0, X1, #1              ; Shift right
AND X2, X2, #0x20000000     ; Extract carry flag
ORR X0, X0, X2, LSL #2      ; Insert carry at bit 31
```

**Removed: Separate CPSR/SPSR**

```asm
; AArch32: Current and Saved Program Status Registers
MRS R0, CPSR                ; Read CPSR
MSR CPSR_c, R0              ; Write CPSR control field
MRS R0, SPSR                ; Read SPSR (in exception modes)

; AArch64: PSTATE and system registers
MRS X0, NZCV                ; Read condition flags
MSR NZCV, X0                ; Write condition flags
MRS X0, DAIF                ; Read interrupt masks
MSR DAIF, X0                ; Write interrupt masks
MRS X0, SPSel               ; Read stack pointer select
MRS X0, CurrentEL           ; Read current exception level

; Exception return uses SPSR_ELx
MRS X0, SPSR_EL1            ; Read saved PSTATE for EL1
MSR SPSR_EL1, X0            ; Write saved PSTATE
ERET                        ; Exception return (restores PSTATE)
```

## Performance Improvements

AArch64 includes numerous architectural enhancements for improved performance.

**More registers:**

```asm
; AArch32: 13 general-purpose registers (R0-R12)
; R13=SP, R14=LR, R15=PC

; AArch64: 31 general-purpose registers (X0-X30)
; SP separate, LR is X30, PC not directly accessible

; Benefit: Reduced register pressure, fewer stack spills

; Example: Complex calculation without spilling
calculate_complex:
    ; AArch64 can keep more values in registers
    LDP X0, X1, [X10]
    LDP X2, X3, [X10, #16]
    LDP X4, X5, [X10, #32]
    LDP X6, X7, [X10, #48]
    
    MUL X8, X0, X1
    MUL X9, X2, X3
    MUL X11, X4, X5
    MUL X12, X6, X7
    
    ADD X13, X8, X9
    ADD X14, X11, X12
    ADD X0, X13, X14
    RET

; Same in AArch32 would require stack operations
; PUSH {R4-R11}
; ... operations with frequent spills ...
; POP {R4-R11}
```

**64-bit addressing and operations:**

```asm
; Native 64-bit arithmetic (no multi-word operations)
ADD X0, X1, X2              ; Single 64-bit add
MUL X0, X1, X2              ; Single 64-bit multiply

; AArch32 equivalent requires multiple instructions
; ADDS R0, R2, R4           ; Low 32 bits
; ADC R1, R3, R5            ; High 32 bits with carry

; Large array indexing
LDR X0, =large_array
MOV X1, #1000000000         ; Large index
LDR X2, [X0, X1, LSL #3]    ; Single instruction

; Pointer arithmetic simplified
ADD X0, X0, #0x100000000    ; Add large offset
```

**Improved immediate encoding:**

```asm
; Logical immediate bitmasks
MOV X0, #0xFFFF0000FFFF0000 ; Complex pattern in single instruction
AND X1, X1, #0x00FF00FF00FF00FF

; Shifted 16-bit immedials
MOV X0, #0x1234             ; Lower 16 bits
MOVK X0, #0x5678, LSL #16   ; Insert 16 bits
MOVK X0, #0x9ABC, LSL #32
MOVK X0, #0xDEF0, LSL #48   ; Build 64-bit constant

; Compare with AArch32
; MOV R0, #0x34              ; Build piecewise
; ORR R0, R0, #0x1200
; ... (more complex for 32-bit values)
```

**Load/Store pair instructions:**

```asm
; Load two registers in one operation (improves memory bandwidth)
LDP X0, X1, [X2]            ; Load 16 bytes
LDP W0, W1, [X2]            ; Load 8 bytes

; Store pairs
STP X0, X1, [X2]            ; Store 16 bytes

; Stack frame setup (one instruction vs multiple)
STP X29, X30, [SP, #-16]!   ; Save FP and LR atomically

; Copy memory using pairs (2x throughput)
copy_memory:
    ; X0 = dest, X1 = src, X2 = count (in 16-byte units)
loop:
    LDP X3, X4, [X1], #16   ; Load pair and increment
    STP X3, X4, [X0], #16   ; Store pair and increment
    SUBS X2, X2, #1
    B.NE loop
    RET
```

**Dedicated multiply-accumulate:**

```asm
; Fused multiply-add (single instruction, single rounding)
FMADD Dd, Dn, Dm, Da        ; Dd = Da + (Dn * Dm)
FMSUB Dd, Dn, Dm, Da        ; Dd = Da - (Dn * Dm)
FNMADD Dd, Dn, Dm, Da       ; Dd = -Da - (Dn * Dm)
FNMSUB Dd, Dn, Dm, Da       ; Dd = -Da + (Dn * Dm)

; Integer multiply-add/subtract
MADD Xd, Xn, Xm, Xa         ; Xd = Xa + (Xn * Xm)
MSUB Xd, Xn, Xm, Xa         ; Xd = Xa - (Xn * Xm)

; Example: Polynomial evaluation
; result = a + b*x + c*x^2 + d*x^3
eval_poly:
    FMUL D0, D1, D2         ; D0 = b * x
    FMADD D0, D3, D4, D0    ; D0 = D0 + c * x^2
    FMADD D0, D5, D6, D0    ; D0 = D0 + d * x^3
    FADD D0, D0, D7         ; D0 = D0 + a
    RET

; Integer dot product
dot_product:
    ; X0 = array1, X1 = array2, X2 = count
    MOV X3, XZR             ; accumulator
loop:
    LDR X4, [X0], #8
    LDR X5, [X1], #8
    MADD X3, X4, X5, X3     ; acc = acc + (a * b)
    SUBS X2, X2, #1
    B.NE loop
    MOV X0, X3
    RET
```

**Advanced SIMD (NEON) improvements:**

```asm
; AArch64 NEON: 32x 128-bit registers (V0-V31)
; AArch32 NEON: 16x 128-bit registers (Q0-Q15)

; Vector operations
FADD V0.4S, V1.4S, V2.4S    ; Add 4 floats in parallel
FMUL V0.2D, V1.2D, V2.2D    ; Multiply 2 doubles in parallel

; Vector load/store
LD1 {V0.16B}, [X0]          ; Load 16 bytes
ST1 {V0.4S, V1.4S}, [X0]    ; Store 8 floats

; Vector reduce operations
FADDP S0, V1.2S             ; Pairwise add (sum of 2 elements)
ADDV S0, V1.4S              ; Add across vector (sum of all elements)

; Example: Sum array of floats
sum_array_simd:
    ; X0 = array, X1 = count
    MOVI V0.4S, #0          ; Zero accumulator
loop:
    LD1 {V1.4S}, [X0], #16  ; Load 4 floats
    FADD V0.4S, V0.4S, V1.4S; Add to accumulator
    SUBS X1, X1, #4
    B.HI loop
    
    ; Reduce to scalar
    FADDP V0.4S, V0.4S, V0.4S   ; Pairwise add
    FADDP S0, V0.2S             ; Final pairwise add
    RET
```

**Atomic operations (ARMv8.1+):**

```asm
; Atomic memory operations (no load-exclusive loop)
LDADD X0, X1, [X2]          ; Atomic add: [X2] += X0, return old value in X1
LDCLR X0, X1, [X2]          ; Atomic clear bits
LDEOR X0, X1, [X2]          ; Atomic XOR
LDSET X0, X1, [X2]          ; Atomic set bits
LDSMAX X0, X1, [X2]         ; Atomic signed maximum
LDUMAX X0, X1, [X2]         ; Atomic unsigned maximum
LDSMIN X0, X1, [X2]         ; Atomic signed minimum
LDUMIN X0, X1, [X2]         ; Atomic unsigned minimum

; Atomic swap
SWP X0, X1, [X2]            ; Swap: X1 = [X2], [X2] = X0

; Compare and swap
CAS X0, X1, [X2]            ; If [X2] == X0, [X2] = X1, else X0 = [X2]
CASA X0, X1, [X2]           ; Acquire semantics
CASAL X0, X1, [X2]          ; Acquire + Release semantics

; Example: Lock-free increment (no retry loop needed)
atomic_increment:
    MOV X0, #1
    LDADD X0, X1, [X2]      ; Single instruction, no loop
    RET

; Compare with AArch32/AArch64 pre-v8.1
atomic_increment_old:
retry:
    LDXR X0, [X1]
    ADD X0, X0, #1
    STXR W2, X0, [X1]
    CBNZ W2, retry          ; Retry on failure
    RET
```

**Pointer authentication (ARMv8.3+):**

```asm
; Protect return addresses from ROP attacks
PACIASP                     ; Sign LR using SP as context
AUTIASP                     ; Authenticate LR using SP

; Function with pointer authentication
secure_function:
    PACIASP                 ; Sign return address
    STP X29, X30, [SP, #-16]!
    
    ; Function body
    ; ...
    
    LDP X29, X30, [SP], #16
    AUTIASP                 ; Authenticate return address
    RET                     ; Fault if authentication fails

; Generic pointer signing
PACIA X0, X1                ; Sign X0 using X1 as context
AUTIA X0, X1                ; Authenticate X0 using X1

; Data pointer signing
PACDA X0, X1                ; Sign data pointer
AUTDA X0, X1                ; Authenticate data pointer
```

**Branch target identification (ARMv8.5+):**

```asm
; Protect against JOP (Jump-Oriented Programming) attacks
BTI                         ; Branch target identification
BTI c                       ; BTI for call targets
BTI j                       ; BTI for jump targets
BTI jc                      ; BTI for both

; Function entry with BTI
protected_function:
    BTI c                   ; Must be first instruction
    STP X29, X30, [SP, #-16]!
    ; ...
    RET
```

**Cache maintenance improvements:**

```asm
; Data cache operations by set/way (for entire cache)
DC ISW, X0                  ; Invalidate by set/way
DC CSW, X0                  ; Clean by set/way
DC CISW, X0                 ; Clean and invalidate by set/way

; Data cache operations by VA (virtual address)
DC IVAC, X0                 ; Invalidate by VA to PoC
DC CVAC, X0                 ; Clean by VA to PoC
DC CVAU, X0                 ; Clean by VA to PoU
DC CIVAC, X0                ; Clean and invalidate by VA to PoC

; Zero data cache line (improves write performance)
DC ZVA, X0                  ; Zero cache line containing address in X0

; Example: Fast memory zero using DC ZVA
; X0 = address, X1 = size (must be cache-line aligned)
fast_memzero:
    ; Get cache line size
    MRS X2, CTR_EL0         ; Read Cache Type Register
    UBFX X2, X2, #16, #4    ; Extract DminLine
    MOV X3, #4
    LSL X3, X3, X2          ; Cache line size in bytes
    
zero_loop:
    DC ZVA, X0              ; Zero entire cache line (faster than stores)
    ADD X0, X0, X3
    SUBS X1, X1, X3
    B.GT zero_loop
    RET

; Instruction cache invalidation
IC IVAU, X0                 ; Invalidate by VA to PoU
IC IALLU                    ; Invalidate all to PoU
IC IALLUIS                  ; Invalidate all to PoU, Inner Shareable

; Example: Self-modifying code support
flush_icache:
    ; X0 = start address, X1 = end address
    MRS X2, CTR_EL0
    UBFX X2, X2, #16, #4    ; DminLine
    MOV X3, #4
    LSL X3, X3, X2          ; Cache line size
    
clean_loop:
    DC CVAU, X0             ; Clean D-cache
    ADD X0, X0, X3
    CMP X0, X1
    B.LO clean_loop
    
    DSB ISH                 ; Ensure visibility
    
    SUB X0, X1, X1          ; Reset to start
invalidate_loop:
    IC IVAU, X0             ; Invalidate I-cache
    ADD X0, X0, X3
    CMP X0, X1
    B.LO invalidate_loop
    
    DSB ISH
    ISB                     ; Synchronize context
    RET
```

**Memory ordering and barriers:**

```asm
; Load-Acquire/Store-Release for efficient synchronization
LDAR X0, [X1]               ; Load-Acquire (no loads/stores before can move after)
LDARH W0, [X1]              ; Load-Acquire halfword
LDARB W0, [X1]              ; Load-Acquire byte

STLR X0, [X1]               ; Store-Release (no loads/stores after can move before)
STLRH W0, [X1]              ; Store-Release halfword
STLRB W0, [X1]              ; Store-Release byte

; Compare with AArch32 (requires DMB)
; AArch32:
; LDR R0, [R1]
; DMB
; 
; AArch64:
; LDAR X0, [X1]             ; Single instruction, cheaper

; Memory barriers (more granular than AArch32)
DMB SY                      ; Full system data memory barrier
DMB ISH                     ; Inner shareable domain
DMB ISHST                   ; Store-only, inner shareable
DMB LD                      ; Load barrier
DMB ST                      ; Store barrier

DSB SY                      ; Full system data synchronization barrier
DSB ISH                     ; Inner shareable
DSB ISHST                   ; Store-only

ISB                         ; Instruction synchronization barrier

; Producer-consumer pattern (efficient with acquire/release)
producer:
    ; Produce data
    STR X0, [X1]            ; Store data
    MOV X2, #1
    STLR X2, [X3]           ; Release: flag = 1 (publish data)
    RET

consumer:
wait:
    LDAR X0, [X3]           ; Acquire: read flag
    CBZ X0, wait            ; Wait until flag set
    LDR X1, [X1]            ; Read data (guaranteed visible)
    RET
```

**Better branch prediction:**

```asm
; Hints for branch prediction (ARMv8.3+)
B.NE loop                   ; Backward branches predicted taken
B.NE forward                ; Forward branches predicted not taken

; Branch with hint (compiler/tools may use)
.ifdef HINT_LIKELY
    B.NE loop               ; Backward = likely
.endif

; Computed branch target cache (BTB) benefits from:
; - Simpler instruction encoding
; - No Thumb/ARM switching
; - Better indirect branch prediction

; Indirect branch with prediction
indirect_call:
    LDR X9, [X0, #offset]   ; Load function pointer
    BR X9                   ; Predicted if target consistent

; Switch statement optimization
switch_table:
    CMP X0, #10             ; Range check
    B.HS default_case
    ADR X1, jump_table
    LDR X2, [X1, X0, LSL #3]
    BR X2

.align 3
jump_table:
    .quad case_0
    .quad case_1
    .quad case_2
    ; ...
```

**Cryptography extensions (optional):**

```asm
; AES instructions
AESE V0.16B, V1.16B         ; AES single round encryption
AESD V0.16B, V1.16B         ; AES single round decryption
AESMC V0.16B, V1.16B        ; AES mix columns
AESIMC V0.16B, V1.16B       ; AES inverse mix columns

; SHA instructions
SHA1C Q0, S1, V2.4S         ; SHA1 hash update (choose)
SHA1H S0, S1                ; SHA1 fixed rotate
SHA256H Q0, Q1, V2.4S       ; SHA256 hash update

; CRC32 instructions
CRC32B W0, W1, W2           ; CRC32 byte
CRC32H W0, W1, W2           ; CRC32 halfword
CRC32W W0, W1, W2           ; CRC32 word
CRC32X W0, W1, X2           ; CRC32 doubleword

; Example: Fast CRC32 calculation
calculate_crc32:
    ; X0 = data pointer, X1 = length, W2 = initial CRC
    MOV W3, W2              ; Current CRC
loop:
    LDRB W4, [X0], #1       ; Load byte
    CRC32B W3, W3, W4       ; Update CRC
    SUBS X1, X1, #1
    B.NE loop
    MOV W0, W3              ; Return CRC
    RET

; Compare with software CRC (10-100x faster with hardware)
```

## Migration Considerations

Migrating from AArch32 to AArch64 requires code changes and architectural understanding.

**Register size changes:**

```asm
; AArch32: 32-bit registers
MOV R0, #42
LDR R1, [R2]

; AArch64: Must specify size
MOV W0, #42                 ; 32-bit operation
MOV X0, #42                 ; 64-bit operation
LDR W1, [X2]                ; 32-bit load
LDR X1, [X2]                ; 64-bit load

; Data type mapping
; AArch32 int       (32-bit) -> AArch64 int/W register
; AArch32 long      (32-bit) -> AArch64 long (64-bit)/X register
; AArch32 pointer   (32-bit) -> AArch64 pointer (64-bit)/X register
; AArch32 long long (64-bit) -> AArch64 long long/X register

; Structure alignment changes
; AArch32: 32-bit pointers
struct data32 {
    void *ptr;              // 4 bytes
    int value;              // 4 bytes
};                          // Total: 8 bytes

; AArch64: 64-bit pointers
struct data64 {
    void *ptr;              // 8 bytes
    int value;              // 4 bytes
    // padding               // 4 bytes (for 8-byte alignment)
};                          // Total: 16 bytes
```

**Calling convention differences:**

```asm
; AArch32 AAPCS (ARM Architecture Procedure Call Standard)
; R0-R3: Arguments and return value
; R4-R11: Callee-saved
; R12: IP (scratch)
; R13: SP
; R14: LR
; R15: PC

; AArch32 function
function_a32:
    PUSH {R4-R11, LR}       ; Save callee-saved registers
    ; R0-R3 contain arguments
    ; Function body
    ; Return value in R0
    POP {R4-R11, PC}        ; Restore and return

; AArch64 AAPCS64
; X0-X7: Arguments
; X8: Indirect result location
; X9-X15: Temporary registers
; X16-X17: Intra-procedure-call temporary (IP0, IP1)
; X18: Platform register (reserved)
; X19-X28: Callee-saved
; X29: Frame pointer (FP)
; X30: Link register (LR)
; SP: Stack pointer

; AArch64 function
function_a64:
    STP X29, X30, [SP, #-16]!   ; Save FP and LR
    MOV X29, SP                  ; Setup frame pointer
    STP X19, X20, [SP, #-16]!   ; Save callee-saved if needed
    
    ; X0-X7 contain arguments (8 registers vs 4)
    ; More arguments passed in registers (better performance)
    
    ; Return value in X0
    LDP X19, X20, [SP], #16     ; Restore callee-saved
    LDP X29, X30, [SP], #16     ; Restore FP and LR
    RET

; Variadic functions
; AArch32: Difficult, uses stack
; AArch64: Named registers for first 8 args, then stack

vararg_func:
    ; X0-X7 may contain arguments
    ; Must save X0-X7 to stack if needed
    STP X0, X1, [SP, #-80]!
    STP X2, X3, [SP, #16]
    STP X4, X5, [SP, #32]
    STP X6, X7, [SP, #48]
    ; Now can process variable arguments
    RET
```

**Stack alignment:**

```asm
; AArch32: 8-byte stack alignment (AAPCS)
; AArch64: 16-byte stack alignment (AAPCS64)

; AArch32 stack frame
function_a32_frame:
    PUSH {FP, LR}           ; 8 bytes
    SUB SP, SP, #16         ; Local variables
    ; ... (SP now 8-byte aligned)
    ADD SP, SP, #16
    POP {FP, PC}

; AArch64 stack frame (must maintain 16-byte alignment)
function_a64_frame:
    STP X29, X30, [SP, #-32]!   ; 16 bytes
    MOV X29, SP
    ; Local space (32 - 16 = 16 bytes available)
    ; SP always 16-byte aligned
    
    LDP X29, X30, [SP], #32
    RET

; Incorrect alignment (causes issues)
bad_alignment:
    SUB SP, SP, #8          ; Now misaligned!
    ; Some operations may fault or perform poorly
    
; Correct alignment
good_alignment:
    SUB SP, SP, #16         ; Maintains 16-byte alignment
    ; Or use pairs:
    STP X0, X1, [SP, #-16]! ; Atomic 16-byte operation
```

**Floating-point differences:**

```asm
; AArch32 VFP: S0-S31 (single), D0-D31 (double)
; AArch64: V0-V31 (128-bit), accessed as:
;   - B0-B31 (8-bit)
;   - H0-H31 (16-bit)
;   - S0-S31 (32-bit)
;   - D0-D31 (64-bit)
;   - Q0-Q31 (128-bit)

; AArch32 floating-point
vmov.f32 s0, #1.0
vadd.f32 s2, s0, s1

; AArch64 floating-point
FMOV S0, #1.0
FADD S2, S0, S1

; Calling convention:
; AArch32: S0-S15 for float args/return
; AArch64: V0-V7 for float args/return (can pass more)

float_function:
    ; V0-V7 contain float/double arguments
    ; V0 for return value
    FADD D0, D0, D1
    RET
```

**Exception level model:**

```asm
; AArch32: Privilege modes (User, FIQ, IRQ, SVC, etc.)
; AArch64: Exception Levels (EL0-EL3)

; EL0: Unprivileged (user applications)
; EL1: Privileged (OS kernel)
; EL2: Hypervisor (virtualization)
; EL3: Secure monitor (TrustZone)

; Check current exception level
get_current_el:
    MRS X0, CurrentEL
    LSR X0, X0, #2          ; Extract EL field
    AND X0, X0, #3          ; Mask to 0-3
    RET

; Exception handling differences
; AArch32: Mode-specific banked registers
; AArch64: EL-specific register banks

; AArch32 exception entry
irq_handler_a32:
    SUB LR, LR, #4          ; Adjust return address
    STMFD SP!, {R0-R3, R12, LR}  ; Save context
    ; Handle interrupt
    LDMFD SP!, {R0-R3, R12, PC}^ ; Return and restore CPSR

; AArch64 exception entry
.align 11                   ; Exception vector table alignment
exception_vectors:
    // Current EL with SP0
    .align 7
    B sync_current_el_sp0
    .align 7
    B irq_current_el_sp0
    .align 7
    B fiq_current_el_sp0
    .align 7
    B serror_current_el_sp0
    
    // Current EL with SPx
    .align 7
    B sync_current_el_spx
    .align 7
    B irq_current_el_spx
    .align 7
    B fiq_current_el_spx
    .align 7
    B serror_current_el_spx
    
    // Lower EL (AArch64)
    .align 7
    B sync_lower_el_a64
    .align 7
    B irq_lower_el_a64
    .align 7
    B fiq_lower_el_a64
    .align 7
    B serror_lower_el_a64
    
    // Lower EL (AArch32)
    .align 7
    B sync_lower_el_a32
    .align 7
    B irq_lower_el_a32
    .align 7
    B fiq_lower_el_a32
    .align 7
    B serror_lower_el_a32

irq_lower_el_a64:
    ; Save context
    STP X0, X1, [SP, #-16]!
    STP X2, X3, [SP, #-16]!
    ; ... save more registers
    
    ; Handle interrupt
    BL irq_handler
    
    ; Restore context
    LDP X2, X3, [SP], #16
    LDP X0, X1, [SP], #16
    ERET                    ; Exception return
```

**System register access:**

```asm
; AArch32: Coprocessor instructions
MRC p15, 0, R0, c0, c0, 0   ; Read MIDR
MCR p15, 0, R0, c1, c0, 0   ; Write SCTLR

; AArch64: Named system registers
MRS X0, MIDR_EL1            ; Read Main ID Register
MSR SCTLR_EL1, X0           ; Write System Control Register

; Common register mappings
; AArch32 SCTLR    -> AArch64 SCTLR_EL1
; AArch32 TTBR0    -> AArch64 TTBR0_EL1
; AArch32 TTBR1    -> AArch64 TTBR1_EL1
; AArch32 MPIDR    -> AArch64 MPIDR_EL1
; AArch32 VBAR     -> AArch64 VBAR_EL1

; Example: Get CPU ID
get_cpu_id_a32:
    MRC p15, 0, R0, c0, c0, 5   ; Read MPIDR
    AND R0, R0, #0x03           ; Extract CPU ID
    BX LR

get_cpu_id_a64:
    MRS X0, MPIDR_EL1
    AND X0, X0, #0xFF           ; Extract Aff0
    RET
```

**Memory management differences:**

```asm
; AArch32: 2-level page tables (1MB sections, 4KB pages)
; AArch64: Multi-level page tables (4KB, 16KB, or 64KB granule)

; AArch64 supports:
; - 4KB pages: 4-level translation (48-bit VA)
; - 16KB pages: 4-level translation (47-bit VA)
; - 64KB pages: 3-level translation (48-bit VA)
; - 52-bit VA with ARMv8.2+ (5-level with 4KB pages)

; Page table base setup
; AArch32
setup_mmu_a32:
    LDR R0, =page_table
    MCR p15, 0, R0, c2, c0, 0   ; Set TTBR0
    ; ...

; AArch64
setup_mmu_a64:
    LDR X0, =page_table
    MSR TTBR0_EL1, X0           ; Set TTBR0_EL1
    
    ; Configure TCR_EL1 (Translation Control Register)
    LDR X0, =0x0000000080803520
    ; TG0 = 4KB, T0SZ = 16 (48-bit VA), SH0 = Inner Shareable
    ; ORGN0/IRGN0 = Write-Back Cacheable
    MSR TCR_EL1, X0
    
    ; Configure MAIR_EL1 (Memory Attribute Indirection Register)
    LDR X0, =0x000000000044FF04
    ; Attr0 = Device-nGnRnE, Attr1 = Normal, etc.
    MSR MAIR_EL1, X0
    
    ; Enable MMU
    MRS X0, SCTLR_EL1
    ORR X0, X0, #0x1            ; M bit
    ORR X0, X0, #0x4            ; C bit (data cache)
    ORR X0, X0, #0x1000         ; I bit (instruction cache)
    MSR SCTLR_EL1, X0
    ISB
    RET
```

**Porting strategy:**

```asm
; 1. Identify architecture-specific code
#ifdef __aarch64__
    ; AArch64-specific code
    MOV X0, #42
    RET
#else
    ; AArch32-specific code
    MOV R0, #42
    BX LR
#endif

; 2. Create abstraction macros
.macro PUSH_REGS
#ifdef __aarch64__
    STP X29, X30, [SP, #-16]!
    STP X19, X20, [SP, #-16]!
#else
    PUSH {R4-R11, LR}
#endif
.endm

.macro POP_REGS
#ifdef __aarch64__
    LDP X19, X20, [SP], #16
    LDP X29, X30, [SP], #16
#else
    POP {R4-R11, PC}
#endif
.endm

; 3. Port critical sections first
; - Exception handlers
; - MMU setup
; - Cache maintenance
; - Synchronization primitives

; 4. Test incrementally
; - Start with boot code
; - Add interrupt handling
; - Port device drivers
; - Migrate application code

; 5. Performance validation
; - Benchmark critical paths
; - Profile cache behavior
; - Verify atomic operations
; - Check memory ordering
```

**Interworking (running 32-bit code on 64-bit kernel):**

```asm
; AArch64 kernel can run AArch32 applications (EL0)
; But mode transitions are explicit

; System call from AArch32 app to AArch64 kernel
; AArch32 app:
;   SVC #0
; 
; Kernel receives exception at EL1 in AArch64 mode
; Must handle AArch32 state:

syscall_handler_compat:
    ; Check if from AArch32
    MRS X0, SPSR_EL1
    TST X0, #0x10               ; Check execution state bit
    B.EQ from_aarch64
    
from_aarch32:
    ; Handle AArch32 syscall
    ; Read saved R0-R7 from exception context
    ; R7 contains syscall number (Linux convention)
    ; ...
    ERET                        ; Return to AArch32 mode

from_aarch64:
    ; Handle AArch64 syscall
    ; X8 contains syscall number
    ; ...
    ERET                        ; Return to AArch64 mode
```

**Key Points:**

- AArch64 simplifies addressing with consistent modes and removes complex ARM/Thumb interworking
- Removed features include conditional execution, PC manipulation, and LDM/STM instructions requiring code restructuring
- Performance improvements include more registers, native 64-bit operations, load/store pairs, and advanced atomics
- Migration requires changes to register usage, calling conventions, stack alignment, and exception handling
- Interworking allows 32-bit applications on 64-bit kernels but requires careful state management

[Inference] Specific performance improvements vary significantly across microarchitectures - actual performance gains depend on core design (Cortex-A53 vs Cortex-A76 vs Apple Silicon vs custom cores) and workload characteristics.

[Inference] Migration complexity depends on how much architecture-specific assembly exists in the codebase - high-level language code typically requires only recompilation while low-level system code needs substantial porting effort.

---

# Interfacing with High-Level Languages

## Calling C from Assembly

Calling C functions from assembly requires adherence to the Application Binary Interface (ABI) and Procedure Call Standard (PCS). The ARM Architecture Procedure Call Standard (AAPCS) defines how registers are used, how parameters are passed, and how the stack is managed across function calls.

**ARM Procedure Call Standard (AAPCS):**

The AAPCS defines register usage conventions that enable interoperability between separately compiled code modules:

**Register Usage:**

- **R0-R3 (a1-a4):** Argument registers, also scratch registers (caller-saved)
- **R4-R11 (v1-v8):** Variable registers (callee-saved, must be preserved)
- **R12 (IP):** Intra-procedure-call scratch register (caller-saved)
- **R13 (SP):** Stack pointer (callee-saved)
- **R14 (LR):** Link register (return address)
- **R15 (PC):** Program counter

**Floating-Point/NEON Registers:**

- **S0-S15 (D0-D7, Q0-Q3):** Argument/return values, scratch (caller-saved)
- **S16-S31 (D8-D15, Q4-Q7):** Callee-saved (must be preserved)
- **D16-D31 (Q8-Q15):** Scratch (caller-saved, if available)

**Parameter Passing Rules:**

**Integer/Pointer Arguments:**

- First four arguments: R0, R1, R2, R3
- Fifth and subsequent arguments: Stack (pushed right-to-left)
- 64-bit arguments: Use two consecutive registers (even-odd pairs like R0-R1, R2-R3)

**Floating-Point Arguments:**

- Single-precision: S0-S15
- Double-precision: D0-D7
- If floating-point registers exhausted, use integer registers or stack

**Return Values:**

- 32-bit or smaller: R0
- 64-bit: R0-R1
- Structures ≤32 bits: R0
- Structures ≤64 bits: R0-R1
- Larger structures: Caller allocates space, passes pointer in R0

**Stack Alignment:** AAPCS requires 8-byte (doubleword) stack alignment at public interfaces. Some systems require 16-byte alignment.

**Example** - Calling C function with integer arguments:

C function prototype:

```c
int add_three(int a, int b, int c);
```

Assembly calling code:

```assembly
.global _start
.text

_start:
    ; Call add_three(10, 20, 30)
    MOV     R0, #10              ; First argument
    MOV     R1, #20              ; Second argument
    MOV     R2, #30              ; Third argument
    BL      add_three            ; Call function
    ; R0 now contains return value (60)
    
    ; Exit program
    MOV     R7, #1               ; sys_exit
    SVC     #0
```

**Example** - Calling C function with stack arguments:

C function:

```c
int sum_six(int a, int b, int c, int d, int e, int f) {
    return a + b + c + d + e + f;
}
```

Assembly calling code:

```assembly
    ; Call sum_six(1, 2, 3, 4, 5, 6)
    ; Arguments 1-4 in registers, 5-6 on stack
    
    ; Prepare stack (must be 8-byte aligned)
    SUB     SP, SP, #8           ; Allocate 8 bytes
    MOV     R0, #6
    STR     R0, [SP, #4]         ; Sixth argument at SP+4
    MOV     R0, #5
    STR     R0, [SP, #0]         ; Fifth argument at SP+0
    
    ; Load register arguments
    MOV     R0, #1               ; First argument
    MOV     R1, #2               ; Second argument
    MOV     R2, #3               ; Third argument
    MOV     R3, #4               ; Fourth argument
    
    BL      sum_six              ; Call function
    ; R0 contains return value (21)
    
    ADD     SP, SP, #8           ; Clean up stack
```

**Example** - Calling C function with 64-bit argument:

C function:

```c
long long multiply(int a, long long b);
```

Assembly calling code:

```assembly
    ; Call multiply(5, 0x123456789ABCDEF0)
    MOV     R0, #5                   ; First argument (32-bit)
    LDR     R2, =0x9ABCDEF0          ; b low word (R2)
    LDR     R3, =0x12345678          ; b high word (R3)
    BL      multiply
    ; R0-R1 contains 64-bit return value
```

**Example** - Calling printf from assembly:

```assembly
.global main
.data
format: .asciz "Value: %d, String: %s\n"
msg:    .asciz "Hello"

.text
main:
    PUSH    {LR}                 ; Save return address
    
    ; printf(format, 42, "Hello")
    LDR     R0, =format          ; Format string (first argument)
    MOV     R1, #42              ; Integer value (second argument)
    LDR     R2, =msg             ; String pointer (third argument)
    BL      printf               ; Call printf
    
    MOV     R0, #0               ; Return 0
    POP     {PC}                 ; Return to caller
```

**Structure Return Example:**

C function:

```c
typedef struct {
    int x, y, z, w;
} Point4D;

Point4D make_point(int x, int y);
```

Assembly calling code:

```assembly
    ; Caller allocates space for return value
    SUB     SP, SP, #16          ; 16 bytes for structure
    MOV     R0, SP               ; Pass pointer to return space
    MOV     R1, #10              ; x argument (shifts to R1)
    MOV     R2, #20              ; y argument (shifts to R2)
    BL      make_point
    
    ; Structure now at [SP]
    LDR     R0, [SP, #0]         ; Load x
    LDR     R1, [SP, #4]         ; Load y
    LDR     R2, [SP, #8]         ; Load z
    LDR     R3, [SP, #12]        ; Load w
    ADD     SP, SP, #16          ; Clean up
```

**Preserving Callee-Saved Registers:**

When calling C functions that may use R4-R11, preserve them if your assembly code needs their values:

```assembly
my_function:
    PUSH    {R4-R11, LR}         ; Save callee-saved registers and LR
    
    ; Use R4-R11 freely
    MOV     R4, #100
    
    ; Call C function
    MOV     R0, #42
    BL      some_c_function
    
    ; R4-R11 preserved across call
    ADD     R0, R0, R4           ; Use preserved R4
    
    POP     {R4-R11, PC}         ; Restore and return
```

**Variable Argument Functions (varargs):**

Calling varargs functions like `printf` follows the same rules—fixed arguments in registers, variable arguments on stack:

```assembly
    ; printf("x=%d y=%d z=%d\n", x, y, z)
    LDR     R0, =format          ; Format string always first
    LDR     R1, =x_value         ; First vararg
    LDR     R1, [R1]
    LDR     R2, =y_value         ; Second vararg
    LDR     R2, [R2]
    LDR     R3, =z_value         ; Third vararg
    LDR     R3, [R3]
    BL      printf
```

**Soft Float vs Hard Float ABI:**

**Soft Float (EABI):** Floating-point arguments passed in integer registers (R0-R3). Software floating-point emulation or VFP instructions used for computation.

**Hard Float (EABIHF):** Floating-point arguments passed in VFP registers (S0-S15, D0-D7). More efficient for floating-point intensive code.

Hard float example:

```assembly
    ; Call float add(float a, float b)
    VMOV.F32 S0, #1.5            ; First argument
    VMOV.F32 S1, #2.5            ; Second argument
    BL       add_float
    ; S0 contains return value (4.0)
    VMOV     R0, S0              ; Move to R0 if needed
```

## Calling Assembly from C

Calling assembly functions from C requires the assembly function to follow AAPCS conventions. The assembly function must properly save/restore callee-saved registers and manage the stack.

**Basic Assembly Function Template:**

```assembly
.global my_asm_function
.type my_asm_function, %function

my_asm_function:
    ; Function prologue
    PUSH    {R4-R11, LR}         ; Save callee-saved regs and return address
    
    ; Function body
    ; R0-R3 contain arguments
    ; Use R4-R11 for local variables
    
    ; Prepare return value in R0
    
    ; Function epilogue
    POP     {R4-R11, PC}         ; Restore registers and return
    
.size my_asm_function, .-my_asm_function
```

**Example** - Simple integer function:

C declaration:

```c
int add(int a, int b);
```

Assembly implementation:

```assembly
.global add
.type add, %function

add:
    ADD     R0, R0, R1           ; R0 = R0 + R1
    BX      LR                   ; Return (no registers to save)
    
.size add, .-add
```

**Example** - Function using local variables:

C declaration:

```c
int compute(int a, int b, int c);
```

Assembly implementation:

```assembly
.global compute
.type compute, %function

compute:
    PUSH    {R4, LR}             ; Save R4 (callee-saved) and LR
    
    ; Use R4 for intermediate result
    MUL     R4, R0, R1           ; R4 = a * b
    ADD     R4, R4, R2           ; R4 += c
    LSL     R4, R4, #2           ; R4 <<= 2
    
    MOV     R0, R4               ; Return value in R0
    POP     {R4, PC}             ; Restore R4 and return
    
.size compute, .-compute
```

**Example** - Function with stack frame:

C declaration:

```c
int array_sum(int *array, int length);
```

Assembly implementation:

```assembly
.global array_sum
.type array_sum, %function

array_sum:
    PUSH    {R4, R5, LR}         ; Save callee-saved registers
    SUB     SP, SP, #4           ; Allocate local variable (maintain 8-byte alignment)
    
    ; R0 = array pointer, R1 = length
    MOV     R2, #0               ; sum = 0
    MOV     R3, #0               ; i = 0
    
loop:
    CMP     R3, R1               ; i < length?
    BGE     done
    
    LDR     R4, [R0, R3, LSL #2] ; Load array[i]
    ADD     R2, R2, R4           ; sum += array[i]
    ADD     R3, R3, #1           ; i++
    B       loop
    
done:
    MOV     R0, R2               ; Return sum
    ADD     SP, SP, #4           ; Deallocate local variable
    POP     {R4, R5, PC}         ; Restore and return
    
.size array_sum, .-array_sum
```

**Example** - Floating-point function (hard float):

C declaration:

```c
float vector_dot_product(float *a, float *b, int n);
```

Assembly implementation:

```assembly
.global vector_dot_product
.type vector_dot_product, %function

vector_dot_product:
    VPUSH   {D8}                 ; Save callee-saved D8 (overlaps S16-S17)
    
    VMOV.F32 S0, #0.0            ; sum = 0.0
    MOV      R3, #0              ; i = 0
    
loop:
    CMP      R3, R2              ; i < n?
    BGE      done
    
    VLDR.F32 S1, [R0, R3, LSL #2] ; Load a[i]
    VLDR.F32 S2, [R1, R3, LSL #2] ; Load b[i]
    VMLA.F32 S0, S1, S2          ; sum += a[i] * b[i]
    ADD      R3, R3, #1          ; i++
    B        loop
    
done:
    ; Return value already in S0
    VPOP     {D8}                ; Restore D8
    BX       LR
    
.size vector_dot_product, .-vector_dot_product
```

**Example** - Structure parameter and return:

C declarations:

```c
typedef struct {
    int x, y;
} Point;

Point translate_point(Point p, int dx, int dy);
```

Assembly implementation (structure ≤64 bits passed in registers):

```assembly
.global translate_point
.type translate_point, %function

translate_point:
    ; R0 = p.x, R1 = p.y, R2 = dx, R3 = dy
    ADD     R0, R0, R2           ; p.x += dx
    ADD     R1, R1, R3           ; p.y += dy
    ; Return in R0 (x) and R1 (y)
    BX      LR
    
.size translate_point, .-translate_point
```

**Example** - Large structure return:

C declarations:

```c
typedef struct {
    int data[10];
} LargeStruct;

LargeStruct make_struct(int value);
```

Assembly implementation:

```assembly
.global make_struct
.type make_struct, %function

make_struct:
    PUSH    {R4, R5, LR}
    
    ; R0 = pointer to return structure (passed by caller)
    ; R1 = value argument (original R0 shifted to R1)
    MOV     R2, R0               ; Save return pointer
    MOV     R3, #0               ; Counter
    
fill_loop:
    CMP     R3, #10
    BGE     done
    STR     R1, [R2, R3, LSL #2] ; Store value to data[i]
    ADD     R3, R3, #1
    B       fill_loop
    
done:
    ; Return value is pointer in R0 (already set)
    POP     {R4, R5, PC}
    
.size make_struct, .-make_struct
```

**Calling from C:**

```c
// C code calling assembly functions
#include <stdio.h>

// Declarations
int add(int a, int b);
int compute(int a, int b, int c);
int array_sum(int *array, int length);
float vector_dot_product(float *a, float *b, int n);

int main() {
    // Call simple function
    int result = add(10, 20);
    printf("add(10, 20) = %d\n", result);
    
    // Call complex function
    result = compute(2, 3, 4);
    printf("compute(2, 3, 4) = %d\n", result);
    
    // Call array function
    int arr[] = {1, 2, 3, 4, 5};
    result = array_sum(arr, 5);
    printf("array_sum = %d\n", result);
    
    // Call floating-point function
    float a[] = {1.0, 2.0, 3.0};
    float b[] = {4.0, 5.0, 6.0};
    float dot = vector_dot_product(a, b, 3);
    printf("dot product = %f\n", dot);
    
    return 0;
}
```

## Inline Assembly

Inline assembly allows embedding assembly instructions directly within C/C++ code. GCC uses extended asm syntax providing tight integration between C and assembly with compiler-managed register allocation.

**GCC Inline Assembly Syntax:**

```c
asm [volatile] (
    "assembly code"
    : output operands          /* optional */
    : input operands           /* optional */
    : clobbered registers      /* optional */
);
```

**Basic Example - No operands:**

```c
void enable_interrupts(void) {
    asm volatile ("CPSIE i");  // Clear PRIMASK (enable interrupts)
}

void disable_interrupts(void) {
    asm volatile ("CPSID i");  // Set PRIMASK (disable interrupts)
}
```

The `volatile` keyword prevents compiler optimization from removing or reordering the assembly.

**Example - Single input operand:**

```c
void delay(int count) {
    asm volatile (
        "1: SUBS %0, %0, #1\n"
        "   BNE 1b\n"
        : "+r" (count)         // +r means read-write register
    );
}
```

**Example - Input and output operands:**

```c
int add_asm(int a, int b) {
    int result;
    asm (
        "ADD %[res], %[x], %[y]"
        : [res] "=r" (result)  // Output: =r means write-only register
        : [x] "r" (a),         // Input: r means read-only register
          [y] "r" (b)
    );
    return result;
}
```

**Constraint Codes:**

**Register Constraints:**

- `r` - General-purpose register (R0-R15)
- `l` - Low register (R0-R7)
- `h` - High register (R8-R15)
- `w` - VFP/NEON register
- `t` - VFP single-precision register (S0-S31)

**Memory Constraints:**

- `m` - Memory operand
- `Q` - Memory with offset addressing
- `o` - Offsetable memory

**Immediate Constraints:**

- `I` - Immediate constant (0-255)
- `J` - Immediate constant (-4095 to 4095)
- `K` - Immediate constant (~I)
- `L` - Immediate constant (~J)
- `M` - Power of 2 or power of 2 minus 1

**Modifiers:**

- `=` - Write-only operand
- `+` - Read-write operand
- `&` - Early clobber (output modified before all inputs consumed)

**Example - Multiple instructions:**

```c
uint32_t read_cpsr(void) {
    uint32_t cpsr;
    asm volatile (
        "MRS %0, CPSR"
        : "=r" (cpsr)
    );
    return cpsr;
}

void write_cpsr(uint32_t cpsr) {
    asm volatile (
        "MSR CPSR_c, %0"
        :
        : "r" (cpsr)
    );
}
```

**Example - Clobber list:**

```c
int multiply_add(int a, int b, int c) {
    int result;
    asm (
        "MUL R12, %1, %2\n"    // Use R12 as temporary
        "ADD %0, R12, %3"
        : "=r" (result)
        : "r" (a), "r" (b), "r" (c)
        : "r12"                // R12 is clobbered
    );
    return result;
}
```

**Example - Memory operand:**

```c
void atomic_increment(int *ptr) {
    asm volatile (
        "LDREX R0, [%0]\n"     // Load exclusive
        "ADD R0, R0, #1\n"
        "STREX R1, R0, [%0]\n" // Store exclusive
        "CMP R1, #0\n"
        "BNE .-12"             // Retry if failed
        :
        : "r" (ptr)
        : "r0", "r1", "memory" // Memory clobber important
    );
}
```

The `"memory"` clobber tells the compiler that memory has been modified, preventing reordering of memory accesses across the asm block.

**Example - NEON inline assembly:**

```c
#include <arm_neon.h>

void vector_add_asm(float *a, float *b, float *result, int n) {
    for (int i = 0; i < n; i += 4) {
        asm (
            "VLD1.32 {D0, D1}, [%1]!\n"    // Load a
            "VLD1.32 {D2, D3}, [%2]!\n"    // Load b
            "VADD.F32 Q2, Q0, Q1\n"        // Add
            "VST1.32 {D4, D5}, [%0]!\n"    // Store result
            : "+r" (result), "+r" (a), "+r" (b)
            :
            : "q0", "q1", "q2", "memory"
        );
    }
}
```

**Example - Conditional execution:**

```c
int conditional_add(int a, int b, int condition) {
    int result = a;
    asm (
        "CMP %2, #0\n"
        "ADDNE %0, %0, %1"
        : "+r" (result)
        : "r" (b), "r" (condition)
        : "cc"                 // Condition codes clobbered
    );
    return result;
}
```

**Example - Barrier instructions:**

```c
void memory_barrier(void) {
    asm volatile ("DMB" ::: "memory");
}

void data_sync_barrier(void) {
    asm volatile ("DSB" ::: "memory");
}

void instruction_sync_barrier(void) {
    asm volatile ("ISB" ::: "memory");
}
```

**Example - Coprocessor access:**

```c
uint32_t get_control_register(void) {
    uint32_t value;
    asm volatile (
        "MRC p15, 0, %0, c1, c0, 0"
        : "=r" (value)
    );
    return value;
}

void set_control_register(uint32_t value) {
    asm volatile (
        "MCR p15, 0, %0, c1, c0, 0"
        :
        : "r" (value)
    );
}
```

**Example - Named register variables:**

```c
// Force specific register allocation
register int r4_val asm("r4");

void use_specific_register(void) {
    r4_val = 42;
    asm volatile (
        "ADD R5, R4, #10"  // R4 contains r4_val
        :
        :
        : "r5"
    );
}
```

**Example - Complex computation with temporary:**

```c
uint64_t multiply_64(uint32_t a, uint32_t b) {
    uint32_t low, high;
    asm (
        "UMULL %0, %1, %2, %3"
        : "=r" (low), "=r" (high)
        : "r" (a), "r" (b)
    );
    return ((uint64_t)high << 32) | low;
}
```

**Best Practices for Inline Assembly:**

[Inference based on compiler behavior and optimization considerations]

**Minimize usage:** Inline assembly prevents many compiler optimizations. Use only when necessary for hardware access or performance-critical sections.

**Use volatile judiciously:** Only mark as volatile if side effects matter (I/O, synchronization). Omit volatile for pure computations allowing compiler optimization.

**Specify clobbers accurately:** Tell compiler about all modified registers and memory. Missing clobbers cause subtle bugs.

**Prefer intrinsics:** For NEON/VFP, intrinsics often generate better code than inline assembly while remaining readable.

**Keep it simple:** Complex inline assembly is difficult to debug and maintain. Consider separate assembly functions for complex code.

## Mixed-Language Linking

Mixed-language projects combine C/C++ code with hand-written assembly modules. The linker resolves symbols and combines object files into final executables or libraries.

**Build Process Overview:**

1. **Compile C sources:** `gcc -c file.c -o file.o`
2. **Assemble assembly sources:** `as file.s -o file.o` or `gcc -c file.s -o file.o`
3. **Link object files:** `gcc file1.o file2.o -o program`

**Example Project Structure:**

```
project/
├── main.c           # C main function
├── utils.c          # C utility functions
├── asm_funcs.s      # Assembly functions
└── Makefile
```

**main.c:**

```c
#include <stdio.h>

// Declare assembly functions
extern int asm_add(int a, int b);
extern void asm_process_array(int *arr, int len);

int main() {
    int result = asm_add(10, 20);
    printf("asm_add(10, 20) = %d\n", result);
    
    int arr[] = {1, 2, 3, 4, 5};
    asm_process_array(arr, 5);
    
    for (int i = 0; i < 5; i++) {
        printf("arr[%d] = %d\n", i, arr[i]);
    }
    
    return 0;
}
```

**asm_funcs.s:**

```assembly
.global asm_add
.type asm_add, %function

asm_add:
    ADD     R0, R0, R1
    BX      LR
.size asm_add, .-asm_add

.global asm_process_array
.type asm_process_array, %function

asm_process_array:
    PUSH    {R4, LR}
    MOV     R2, #0               ; index
loop:
    CMP     R2, R1
    BGE     done
    LDR     R3, [R0, R2, LSL #2] ; Load arr[i]
    LSL     R3, R3, #1           ; Multiply by 2
    STR     R3, [R0, R2, LSL #2] ; Store back
    ADD     R2, R2, #1
    B       loop
done:
    POP     {R4, PC}
.size asm_process_array, .-asm_process_array
```

**Makefile:**

```makefile
CC = arm-linux-gnueabihf-gcc
AS = arm-linux-gnueabihf-as
CFLAGS = -O2 -Wall
ASFLAGS = -march=armv7-a

OBJS = main.o utils.o asm_funcs.o

program: $(OBJS)
	$(CC) $(OBJS) -o program

main.o: main.c
	$(CC) $(CFLAGS) -c main.c

utils.o: utils.c
	$(CC) $(CFLAGS) -c utils.c

asm_funcs.o: asm_funcs.s
	$(AS) $(ASFLAGS) asm_funcs.s -o asm_funcs.o

clean:
	rm -f $(OBJS) program
```

**Symbol Visibility and Linking:**

**Global symbols:** Visible across object files. Use `.global` directive in assembly.

**Local symbols:** Private to object file. Default for symbols not declared global.

**Weak symbols:** Can be overridden by strong symbols. Use `.weak` directive.

**Example** with weak symbols:

```assembly
.weak default_handler
.type default_handler, %function

default_handler:
    B       default_handler      ; Infinite loop
.size default_handler, .-default_handler

; User can provide strong symbol to override
```

**Calling Assembly from C++ with Name Mangling:**

C++ mangles function names for overloading. Use `extern "C"` to prevent mangling:

```cpp
// C++ header
#ifdef __cplusplus
extern "C" {
#endif

int asm_function(int a, int b);
void asm_process(void);

#ifdef __cplusplus
}
#endif
```

Assembly remains unchanged—symbols don't have C++ decoration.

**Position-Independent Code (PIC):**

For shared libraries, code must be position-independent. Access global data through GOT (Global Offset Table):

```assembly
.global pic_function
.type pic_function, %function

pic_function:
    PUSH    {R4, LR}
    
    ; Access global variable through GOT
    LDR     R4, .L_got_offset
.L_pc:
    ADD     R4, PC, R4           ; R4 = GOT address
    LDR     R4, [R4]             ; Load actual address
    LDR     R0, [R4]             ; Load variable value
    
    POP     {R4, PC}

.L_got_offset:
    .word   global_var(GOT) - (.L_pc + 8)
```

Typically handled automatically by assembler/linker when using `-fPIC` flag.

**Static vs Dynamic Linking:**

**Static linking:** Embeds all code into executable. Larger binary, no runtime dependencies.

```bash
gcc main.o asm_funcs.o -static -o program
```

**Dynamic linking:** Uses shared libraries (.so). Smaller binary, requires libraries at runtime.

```bash
gcc main.o asm_funcs.o -o program
```

**Creating Shared Library with Assembly:**

```bash
# Compile with PIC
gcc -fPIC -c main.c
as -march=armv7-a asm_funcs.s -o asm_funcs.o

# Create shared library
gcc -shared -o libmylib.so main.o asm_funcs.o

# Link against shared library
gcc app.c -L. -lmylib -o app
```

**Section Placement:**

Assembly code can specify sections for fine-grained control:

```assembly
.section .text.fast, "ax", %progbits
.global fast_function
fast_function:
    ; Critical function placed in specific section
    BX      LR

.section .data.shared, "aw", %progbits
.global shared_buffer
shared_buffer:
    .space 1024

.section .rodata
.global constant_table
constant_table:
    .word 1, 2, 3, 4, 5
```

Linker script controls section placement in memory:

```ld
SECTIONS
{
    .text : { *(.text .text.*) }
    .text.fast : { *(.text.fast) }  /* Place in fast memory */
    .data : { *(.data .data.*) }
    .rodata : { *(.rodata*) }
    .bss : { *(.bss .bss.*) }
}
```

**Debugging Mixed-Language Code:**

GDB supports debugging mixed C/assembly:

```bash
# Compile with debug symbols
gcc -g -c main.c
as -g asm_funcs.s -o asm_funcs.o
gcc main.o asm_funcs.o -o program

# Debug
gdb program
(gdb) break asm_add
(gdb) run
(gdb) disassemble
(gdb) info registers
(gdb) step 
(gdb) stepi # Step single instruction
````

**DWARF Debug Information in Assembly:**

Add debug directives for better debugging experience:

```assembly
.file   "asm_funcs.s"
.text
.global asm_add
.type   asm_add, %function

asm_add:
    .loc 1 5 0               # File 1, line 5, column 0
    .cfi_startproc           # Call Frame Information start
    ADD     R0, R0, R1
    .loc 1 6 0
    BX      LR
    .cfi_endproc
.size asm_add, .-asm_add
````

**Interoperability Considerations:**

**Alignment Requirements:** ARM requires different alignments for different data types:

- Byte: 1-byte alignment
- Halfword: 2-byte alignment
- Word: 4-byte alignment
- Doubleword: 8-byte alignment

Misaligned access may cause faults or performance degradation depending on CPU configuration.

```assembly
.data
.align 2                     # 4-byte alignment (2^2)
int_array:
    .word 1, 2, 3, 4

.align 3                     # 8-byte alignment (2^3)
double_value:
    .quad 0x123456789ABCDEF0
```

**Structure Packing and Padding:**

C structures may have padding for alignment. Assembly must match C layout:

```c
// C structure
struct Data {
    char a;        // Offset 0
    // 3 bytes padding
    int b;         // Offset 4
    short c;       // Offset 8
    // 2 bytes padding
    long long d;   // Offset 16 (must be 8-byte aligned)
};  // Total size: 24 bytes
```

Assembly accessing structure:

```assembly
; R0 points to struct Data
LDRB    R1, [R0, #0]         ; Load a
LDR     R2, [R0, #4]         ; Load b
LDRH    R3, [R0, #8]         ; Load c
LDRD    R4, R5, [R0, #16]    ; Load d (64-bit)
```

**Calling Conventions Across Platforms:**

Different platforms may use different conventions:

**Linux EABI (Embedded ABI):**

- Soft float: FP in integer registers
- Hard float: FP in VFP registers
- System call number in R7

**Bare metal:**

- Custom conventions possible
- No OS-enforced ABI
- Document carefully

**RTOS (e.g., FreeRTOS):**

- May define custom conventions
- Stack requirements for task context
- Interrupt handling considerations

**Example - FreeRTOS task in assembly:**

```assembly
.global task_function
.type task_function, %function

task_function:
    PUSH    {R4-R11, LR}         ; Save context
    
task_loop:
    ; Task work here
    
    ; Call FreeRTOS delay
    MOV     R0, #1000            ; Delay 1000 ticks
    BL      vTaskDelay           ; C function
    
    B       task_loop
    
    ; Task should never return, but if it does:
    POP     {R4-R11, PC}
.size task_function, .-task_function
```

**Optimization Considerations:**

**Link Time Optimization (LTO):** GCC's LTO can optimize across C/assembly boundaries, but assembly prevents many optimizations:

```bash
# Compile with LTO
gcc -flto -c main.c
gcc -c asm_funcs.s           # Assembly not optimized by LTO
gcc -flto main.o asm_funcs.o -o program
```

**Inlining:** Compiler cannot inline assembly functions. Performance-critical small functions may be better as inline assembly or intrinsics.

**Register Allocation:** Compiler has no visibility into assembly register usage. Calling many assembly functions may cause register spilling.

**Example Project - Cryptography Library:**

Mixing C for portability with assembly for performance-critical operations:

**aes.h:**

```c
#ifndef AES_H
#define AES_H

#include <stdint.h>

// C prototypes
void aes_init(uint32_t *key, int key_bits);
void aes_encrypt_block(const uint8_t *input, uint8_t *output);

// Assembly-optimized function
extern void aes_encrypt_block_asm(const uint8_t *input, 
                                   uint8_t *output,
                                   const uint32_t *round_keys,
                                   int num_rounds);

#endif
```

**aes.c:**

```c
#include "aes.h"
#include <string.h>

static uint32_t round_keys[60];
static int num_rounds;

void aes_init(uint32_t *key, int key_bits) {
    // C implementation of key expansion
    num_rounds = (key_bits == 128) ? 10 : 
                 (key_bits == 192) ? 12 : 14;
    
    // Key expansion logic (omitted for brevity)
    // Fills round_keys array
}

void aes_encrypt_block(const uint8_t *input, uint8_t *output) {
    // Use optimized assembly version
    aes_encrypt_block_asm(input, output, round_keys, num_rounds);
}
```

**aes_asm.s:**

```assembly
.global aes_encrypt_block_asm
.type aes_encrypt_block_asm, %function

aes_encrypt_block_asm:
    PUSH    {R4-R11, LR}
    
    ; R0 = input, R1 = output, R2 = round_keys, R3 = num_rounds
    
    ; Load input block (128 bits = 4 words)
    LDMIA   R0, {R4-R7}          ; Load 4 words
    
    ; Initial round key addition
    LDMIA   R2!, {R8-R11}
    EOR     R4, R4, R8
    EOR     R5, R5, R9
    EOR     R6, R6, R10
    EOR     R7, R7, R11
    
    ; Main rounds (using NEON for actual AES implementation)
    ; Simplified here - real implementation would use
    ; AES instructions or NEON-optimized substitution/mixing
    
round_loop:
    SUBS    R3, R3, #1
    BEQ     final_round
    
    ; SubBytes, ShiftRows, MixColumns, AddRoundKey
    ; (Omitted - would use table lookups or NEON)
    
    B       round_loop
    
final_round:
    ; SubBytes, ShiftRows, AddRoundKey (no MixColumns)
    
    ; Store output
    STMIA   R1, {R4-R7}
    
    POP     {R4-R11, PC}
.size aes_encrypt_block_asm, .-aes_encrypt_block_asm
```

**Makefile:**

```makefile
CC = arm-linux-gnueabihf-gcc
AS = arm-linux-gnueabihf-as
AR = arm-linux-gnueabihf-ar

CFLAGS = -O3 -march=armv7-a -mfpu=neon -Wall
ASFLAGS = -march=armv7-a -mfpu=neon

LIB_OBJS = aes.o aes_asm.o
TEST_OBJS = test.o

all: libaes.a test

libaes.a: $(LIB_OBJS)
	$(AR) rcs libaes.a $(LIB_OBJS)

test: $(TEST_OBJS) libaes.a
	$(CC) $(TEST_OBJS) -L. -laes -o test

aes.o: aes.c aes.h
	$(CC) $(CFLAGS) -c aes.c

aes_asm.o: aes_asm.s
	$(AS) $(ASFLAGS) aes_asm.s -o aes_asm.o

test.o: test.c aes.h
	$(CC) $(CFLAGS) -c test.c

clean:
	rm -f *.o libaes.a test
```

**Cross-Platform Considerations:**

When building for multiple architectures, use conditional assembly:

```assembly
#ifdef __ARM_ARCH_7A__
    ; ARMv7-A specific code
    UDIV    R0, R1, R2
#elif defined(__ARM_ARCH_6__)
    ; ARMv6 fallback
    ; Use division by repeated subtraction
#endif

#ifdef __ARM_NEON__
    ; NEON-optimized path
    VLD1.32 {Q0}, [R0]
    VADD.I32 Q0, Q0, Q1
#else
    ; Scalar fallback
    LDR     R0, [R1]
    ADD     R0, R0, R2
#endif
```

**Testing Mixed-Language Code:**

Create comprehensive test suite:

**test.c:**

```c
#include <stdio.h>
#include <assert.h>
#include "aes.h"

void test_aes_encrypt() {
    uint8_t input[16] = {0x00, 0x11, 0x22, 0x33, 0x44, 0x55, 0x66, 0x77,
                         0x88, 0x99, 0xAA, 0xBB, 0xCC, 0xDD, 0xEE, 0xFF};
    uint8_t output[16];
    uint8_t expected[16] = {/* known good output */};
    
    uint32_t key[4] = {0x00010203, 0x04050607, 0x08090A0B, 0x0C0D0E0F};
    
    aes_init(key, 128);
    aes_encrypt_block(input, output);
    
    assert(memcmp(output, expected, 16) == 0);
    printf("AES test passed\n");
}

int main() {
    test_aes_encrypt();
    return 0;
}
```

**Performance Profiling:**

Profile mixed-language code to identify bottlenecks:

```bash
# Compile with profiling
gcc -pg -O2 main.c asm_funcs.s -o program

# Run program
./program

# Analyze profile
gprof program gmon.out > analysis.txt
```

**Static Analysis:**

Use tools to verify ABI compliance:

```bash
# Check symbols in object files
nm asm_funcs.o

# Verify no undefined symbols
objdump -T program

# Check binary dependencies
ldd program

# Disassemble to verify code generation
objdump -d program > disassembly.txt
```

**Documentation Best Practices:**

Document all assembly functions with clear interface specifications:

```assembly
; ============================================================================
; Function: vector_normalize
; Description: Normalizes a 3D vector to unit length
; 
; Parameters:
;   R0 - Pointer to vector (float[3])
; 
; Returns:
;   None (modifies vector in-place)
; 
; Registers modified: R0-R3, S0-S9
; Stack usage: None
; Calling convention: AAPCS (hard float)
; 
; Example:
;   float vec[3] = {3.0, 4.0, 0.0};
;   vector_normalize(vec);
;   // vec is now {0.6, 0.8, 0.0}
; ============================================================================
.global vector_normalize
.type vector_normalize, %function

vector_normalize:
    ; Implementation here
    BX      LR
.size vector_normalize, .-vector_normalize
```

**Key Points:**

- Follow AAPCS for parameter passing: R0-R3 for arguments, R0 for return
- Preserve callee-saved registers R4-R11, SP, LR when used
- Maintain 8-byte stack alignment at public interfaces
- Use inline assembly for small code snippets with compiler integration
- Create separate assembly files for complex functions
- Mark inline assembly volatile when side effects matter
- Specify accurate clobber lists to prevent optimization bugs
- Use `extern "C"` in C++ to prevent name mangling
- Document assembly function interfaces thoroughly
- Test mixed-language code extensively across optimization levels
- Profile to verify assembly optimizations provide actual benefit

**Related Subtopics:**

Understanding AAPCS details for structure passing, variadic functions, and C++ exception handling would provide deeper insight into ABI complexities. Examining generated assembly from compilers (`gcc -S`) helps understand expected patterns. Exploring hardware-specific extensions like ARM's Pointer Authentication (ARMv8.3) and Branch Target Identification requires specialized knowledge of architectural security features.

---

## Name Mangling

Name mangling (or name decoration) is the encoding of function names to support features like function overloading, namespaces, and type information in compiled object files.

**C language (no mangling):**

```c
// C source
int add(int a, int b) {
    return a + b;
}

void process_data(void);
```

```asm
; Assembly implementation
; C uses simple names - no mangling
.global add
.type add, %function
add:
    ADD W0, W0, W1              ; a + b
    RET

.global process_data
.type process_data, %function
process_data:
    ; Implementation
    RET

; Calling from assembly
call_c_function:
    MOV W0, #5
    MOV W1, #10
    BL add                      ; Direct call, no name mangling
    RET
```

**C++ language (mangled names):**

```cpp
// C++ source
int add(int a, int b) {
    return a + b;
}

float add(float a, float b) {
    return a + b;
}

namespace math {
    int multiply(int a, int b) {
        return a * b;
    }
}

class Calculator {
public:
    int subtract(int a, int b);
};
```

```asm
; Mangled names (GCC/Clang Itanium ABI)
; int add(int, int) -> _Z3addii
.global _Z3addii
.type _Z3addii, %function
_Z3addii:
    ADD W0, W0, W1
    RET

; float add(float, float) -> _Z3addff
.global _Z3addff
.type _Z3addff, %function
_Z3addff:
    FADD S0, S0, S1
    RET

; math::multiply(int, int) -> _ZN4math8multiplyEii
.global _ZN4math8multiplyEii
.type _ZN4math8multiplyEii, %function
_ZN4math8multiplyEii:
    MUL W0, W0, W1
    RET

; Calculator::subtract(int, int) -> _ZN10Calculator8subtractEii
.global _ZN10Calculator8subtractEii
.type _ZN10Calculator8subtractEii, %function
_ZN10Calculator8subtractEii:
    ; X0 = this pointer
    ; W1 = a, W2 = b
    SUB W0, W1, W2
    RET
```

**Extern "C" linkage:**

```cpp
// C++ header for assembly functions
extern "C" {
    int asm_add(int a, int b);
    void asm_process(void);
}

// C++ implementation calling assembly
int use_asm() {
    return asm_add(5, 10);
}
```

```asm
; Assembly with C linkage (no mangling)
.global asm_add
.type asm_add, %function
asm_add:
    ADD W0, W0, W1
    RET

.global asm_process
.type asm_process, %function
asm_process:
    ; Implementation
    RET

; Calling C++ function from assembly
; Must use mangled name
.extern _Z10use_asm_v              ; void use_asm()

call_cpp:
    BL _Z10use_asm_v
    RET
```

**Name mangling patterns (Itanium C++ ABI):**

```
Pattern: _Z + <function-name-length> + <function-name> + <parameter-types>

Basic types:
  v = void
  b = bool
  c = char
  a = signed char
  h = unsigned char
  s = short
  t = unsigned short
  i = int
  j = unsigned int
  l = long
  m = unsigned long
  x = long long
  y = unsigned long long
  f = float
  d = double
  e = long double

Pointer: P + <type>        (e.g., Pi = int*)
Reference: R + <type>      (e.g., Ri = int&)
Const: K + <type>          (e.g., Ki = const int)

Namespaces: N + <namespace-length> + <namespace-name> + ... + E

Examples:
  void func()                    -> _Z4funcv
  int func(int)                  -> _Z4funci
  int func(int, float)           -> _Z4funcif
  int func(int*)                 -> _Z4funcPi
  int func(const int&)           -> _Z4funcRKi
  ns::func(int)                  -> _ZN2ns4funcEi
  Class::method(int)             -> _ZN5Class6methodEi
  operator+(int, int)            -> _Zpl1ii
```

**Practical name mangling handling:**

```asm
; Method 1: Use extern "C" wrapper
; C++ header
extern "C" {
    void my_asm_function(int x);
}

; Assembly (no mangling needed)
.global my_asm_function
my_asm_function:
    ; W0 = x
    RET

; Method 2: Get mangled name from object file
; Compile C++ to object file, then inspect:
; nm -C myfile.o | grep function_name

; Method 3: Use assembly in C++ namespace
.global _ZN6MyCode12process_dataEi
_ZN6MyCode12process_dataEi:    ; MyCode::process_data(int)
    RET
```

## ABI Compatibility

The Application Binary Interface (ABI) defines calling conventions, data layout, and system interfaces for compatibility between compilation units.

**AAPCS64 (ARM Architecture Procedure Call Standard):**

**Register usage:**

```asm
; Argument registers (caller-saved)
; X0-X7 (W0-W7): Integer/pointer arguments and return values
; V0-V7: Floating-point/SIMD arguments and return values
; X8: Indirect result location (for large return values)

; Temporary registers (caller-saved)
; X9-X15: Temporary registers, caller must save if needed
; X16-X17: Intra-procedure-call temporary (IP0, IP1)

; Callee-saved registers
; X19-X28: Must be preserved across function calls
; X29: Frame pointer (FP)
; X30: Link register (LR)

; Special registers
; SP: Stack pointer (must be 16-byte aligned)
; X18: Platform register (OS-specific, usually reserved)

; Example: ABI-compliant function
; int complex_func(int a, long b, float c, double d, 
;                  int e, int f, int g, int h, int overflow)
complex_func:
    ; W0 = a (int)
    ; X1 = b (long)
    ; S2 = c (float)
    ; D3 = d (double)
    ; W4 = e (int)
    ; W5 = f (int)
    ; W6 = g (int)
    ; W7 = h (int)
    ; [SP] = overflow (on stack)
    
    STP X29, X30, [SP, #-64]!   ; Save FP, LR
    MOV X29, SP                  ; Setup frame pointer
    STP X19, X20, [SP, #16]     ; Save callee-saved if used
    
    ; Access stack parameter
    LDR W19, [SP, #64]          ; Load overflow from caller's stack
    
    ; Function body
    ; Use W0, X1, S2, D3, W4-W7, W19
    ADD W0, W0, W4
    ADD W0, W0, W5
    ; ...
    
    LDP X19, X20, [SP, #16]     ; Restore callee-saved
    LDP X29, X30, [SP], #64     ; Restore FP, LR
    RET
```

**Return value conventions:**

```asm
; Integer/pointer return in X0 (or W0)
return_int:
    MOV W0, #42
    RET

; 64-bit return in X0
return_long:
    MOV X0, #0x123456789ABCDEF
    RET

; Pointer return in X0
return_pointer:
    ADRP X0, data_buffer
    ADD X0, X0, :lo12:data_buffer
    RET

; Float return in S0
return_float:
    FMOV S0, #1.0
    RET

; Double return in D0
return_double:
    FMOV D0, #3.14159
    RET

; 128-bit integer return in X0, X1
return_int128:
    MOV X0, #0x123456789ABCDEF  ; Low 64 bits
    MOV X1, #0xFEDCBA987654321  ; High 64 bits
    RET

; Small struct return (≤ 16 bytes) in X0, X1
; struct Point { long x, y; };
return_point:
    MOV X0, #10                 ; x
    MOV X1, #20                 ; y
    RET

; Large struct return (> 16 bytes) via memory
; X8 points to caller-allocated space
; struct LargeData { long a, b, c, d; };
return_large_struct:
    ; X8 = pointer to result location
    MOV X0, #1
    STR X0, [X8, #0]            ; a
    MOV X0, #2
    STR X0, [X8, #8]            ; b
    MOV X0, #3
    STR X0, [X8, #16]           ; c
    MOV X0, #4
    STR X0, [X8, #24]           ; d
    MOV X0, X8                  ; Return pointer in X0
    RET
```

**Variable argument functions (varargs):**

```c
// C varargs function
int sum_varargs(int count, ...) {
    va_list args;
    va_start(args, count);
    int sum = 0;
    for (int i = 0; i < count; i++) {
        sum += va_arg(args, int);
    }
    va_end(args);
    return sum;
}
```

```asm
; Assembly implementation of varargs
; AArch64: First 8 args in X0-X7, rest on stack
.global sum_varargs
sum_varargs:
    ; W0 = count
    STP X29, X30, [SP, #-96]!   ; Save FP, LR
    MOV X29, SP
    
    ; Save register arguments to stack (va_list)
    ; This allows va_arg to access them uniformly
    STP X1, X2, [SP, #16]       ; Save X1-X7
    STP X3, X4, [SP, #32]
    STP X5, X6, [SP, #48]
    STR X7, [SP, #64]
    
    MOV W9, W0                  ; Save count
    MOV W10, #0                 ; sum = 0
    MOV W11, #0                 ; index = 0
    
    CMP W9, #0
    B.LE done
    
    ; Setup va_list pointer
    ADD X12, SP, #16            ; Point to saved args
    
sum_loop:
    ; Load next argument
    CMP W11, #7
    B.GT from_stack
    
    ; From register save area
    LDR W13, [X12], #8
    B add_value
    
from_stack:
    ; From caller's stack
    ; Arguments beyond X7 are at [X29, #96]
    SUB W14, W11, #7
    ADD X15, X29, #96
    LDR W13, [X15, W14, UXTW #2]
    
add_value:
    ADD W10, W10, W13           ; sum += arg
    ADD W11, W11, #1            ; index++
    CMP W11, W9
    B.LT sum_loop
    
done:
    MOV W0, W10                 ; Return sum
    LDP X29, X30, [SP], #96
    RET

; Calling varargs function
call_sum:
    MOV W0, #5                  ; count = 5
    MOV W1, #1                  ; arg1
    MOV W2, #2                  ; arg2
    MOV W3, #3                  ; arg3
    MOV W4, #4                  ; arg4
    MOV W5, #5                  ; arg5
    BL sum_varargs
    ; Result in W0
    RET
```

**ABI-compliant stack frame:**

```asm
; Standard function prologue/epilogue
function_with_frame:
    ; Prologue
    STP X29, X30, [SP, #-48]!   ; Save FP, LR (16 bytes)
    MOV X29, SP                  ; FP = SP
    STP X19, X20, [SP, #16]     ; Save callee-saved (16 bytes)
    STP X21, X22, [SP, #32]     ; Save more callee-saved (16 bytes)
    
    ; Stack layout at this point:
    ; [SP + 0]  = saved X29 (FP)
    ; [SP + 8]  = saved X30 (LR)
    ; [SP + 16] = saved X19
    ; [SP + 24] = saved X20
    ; [SP + 32] = saved X21
    ; [SP + 40] = saved X22
    
    ; Allocate local variables
    SUB SP, SP, #32             ; 32 bytes for locals (maintain alignment)
    
    ; Function body
    ; Access locals at [SP + offset]
    ; Access saved regs via FP: [X29, #offset]
    
    ; Epilogue
    ADD SP, SP, #32             ; Deallocate locals
    LDP X21, X22, [SP, #32]     ; Restore callee-saved
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #48     ; Restore FP, LR
    RET

; Leaf function (no calls, no frame pointer needed)
leaf_function:
    ; No prologue needed if no calls and uses only X0-X15
    ADD W0, W0, W1
    MUL W0, W0, W2
    RET

; Non-leaf requiring only LR save
simple_function:
    STP X29, X30, [SP, #-16]!
    
    BL other_function
    
    LDP X29, X30, [SP], #16
    RET
```

## Volatile Registers and Clobbering

Volatile (caller-saved) registers can be modified by called functions; callee-saved registers must be preserved.

**Register classification:**

```asm
; Volatile (caller-saved) registers
; X0-X18: Can be clobbered by function calls
; V0-V7, V16-V31: Can be clobbered by function calls

; Non-volatile (callee-saved) registers
; X19-X28: Must be preserved by called functions
; X29 (FP): Frame pointer (must be preserved)
; X30 (LR): Link register (must be preserved)
; V8-V15: Must be preserved by called functions
; SP: Stack pointer (must be preserved and aligned)

; Example: Caller must save volatile registers if needed
caller_function:
    MOV X9, #100                ; X9 = volatile
    MOV X19, #200               ; X19 = non-volatile
    
    ; Before calling, save volatile registers we need later
    STP X9, X10, [SP, #-16]!    ; Save X9, X10
    
    BL some_function            ; May clobber X0-X18
    
    LDP X9, X10, [SP], #16      ; Restore X9, X10
    
    ; X19 still has value 200 (callee must preserve)
    ADD X0, X9, X19
    RET

; Example: Callee must save non-volatile registers if used
callee_function:
    STP X29, X30, [SP, #-32]!
    MOV X29, SP
    STP X19, X20, [SP, #16]     ; Must save if we use X19, X20
    
    ; Use X19, X20 for local state
    MOV X19, #42
    MOV X20, #84
    
    ; Can freely use X0-X15 without saving
    MOV X9, #100
    
    ; Must restore non-volatile registers
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #32
    RET
```

**Inline assembly with clobbers (C/C++):**

```c
// GCC/Clang inline assembly syntax
void use_inline_asm(int a, int b) {
    int result;
    
    // Simple inline assembly
    asm("add %w0, %w1, %w2"
        : "=r" (result)         // Output: result in register
        : "r" (a), "r" (b)      // Inputs: a and b in registers
    );
    
    // With clobbers
    asm("mov x9, %1\n"
        "mul x9, x9, x9\n"
        "add %0, x9, %2\n"
        : "=r" (result)         // Output
        : "r" (a), "r" (b)      // Inputs
        : "x9", "cc"            // Clobbers: x9 modified, condition codes changed
    );
    
    // Memory clobber (indicates memory changes)
    asm volatile("str %1, [%0]\n"
        :                       // No outputs
        : "r" (ptr), "r" (value)
        : "memory"              // Memory clobber
    );
}
```

```asm
; Corresponding assembly showing register usage
use_inline_asm:
    ; Compiler allocates registers for variables
    ; W0 = a, W1 = b
    
    ; First asm block: add
    ADD W2, W0, W1              ; result = a + b
    
    ; Second asm block with clobbers
    MOV X9, X0                  ; Use X9 (volatile, OK to clobber)
    MUL X9, X9, X9              ; x9 = a * a
    ADD X2, X9, X1              ; result = (a * a) + b
    ; Compiler knows X9 is clobbered, won't rely on its value
    
    ; Memory operations
    ; Compiler ensures memory is coherent due to "memory" clobber
    
    RET
```

**Handling function calls with volatile preservation:**

```asm
; Function using many temporaries
complex_calculation:
    STP X29, X30, [SP, #-80]!
    MOV X29, SP
    
    ; Load initial values into volatile registers
    MOV X9, #10
    MOV X10, #20
    MOV X11, #30
    MOV X12, #40
    MOV X13, #50
    
    ; Need to call helper function
    ; Save volatile registers we still need
    STP X9, X10, [SP, #16]
    STP X11, X12, [SP, #32]
    STR X13, [SP, #48]
    
    ; Call function (may clobber X0-X15)
    MOV X0, X9
    BL helper_function
    MOV X14, X0                 ; Save return value
    
    ; Restore volatile registers
    LDP X9, X10, [SP, #16]
    LDP X11, X12, [SP, #32]
    LDR X13, [SP, #48]
    
    ; Continue with restored values
    ADD X0, X9, X10
    ADD X0, X0, X11
    ADD X0, X0, X12
    ADD X0, X0, X13
    ADD X0, X0, X14
    
    LDP X29, X30, [SP], #80
    RET

; Alternative: Use callee-saved registers (no saving needed)
complex_calculation_opt:
    STP X29, X30, [SP, #-48]!
    MOV X29, SP
    STP X19, X20, [SP, #16]     ; Must save callee-saved
    STP X21, X22, [SP, #32]
    
    ; Use callee-saved registers for persistent state
    MOV X19, #10
    MOV X20, #20
    MOV X21, #30
    MOV X22, #40
    
    ; Call function - X19-X22 automatically preserved by callee
    MOV X0, X19
    BL helper_function
    
    ; X19-X22 still have original values
    ADD X0, X0, X19
    ADD X0, X0, X20
    ADD X0, X0, X21
    ADD X0, X0, X22
    
    LDP X21, X22, [SP, #32]
    LDP X19, X20, [SP, #16]
    LDP X29, X30, [SP], #48
    RET
```

**SIMD register preservation:**

```asm
; V0-V7: Volatile (arguments and return)
; V8-V15: Non-volatile (must preserve lower 64 bits)
; V16-V31: Volatile

simd_function:
    STP X29, X30, [SP, #-96]!
    MOV X29, SP
    
    ; Save V8-V15 if used (only lower 64 bits required)
    STP D8, D9, [SP, #16]
    STP D10, D11, [SP, #32]
    STP D12, D13, [SP, #48]
    STP D14, D15, [SP, #64]
    
    ; Or save full 128 bits if needed
    ; STP Q8, Q9, [SP, #16]     ; Requires more stack space
    
    ; Use SIMD registers
    FADD V8.4S, V0.4S, V1.4S
    FMUL V9.4S, V8.4S, V2.4S
    
    ; V0-V7 can be used freely (volatile)
    FADD V0.4S, V3.4S, V4.4S
    
    ; Restore V8-V15
    LDP D8, D9, [SP, #16]
    LDP D10, D11, [SP, #32]
    LDP D12, D13, [SP, #48]
    LDP D14, D15, [SP, #64]
    
    LDP X29, X30, [SP], #96
    RET
```

## Structure Passing

Structures are passed according to size and composition rules.

**Small structures (≤ 16 bytes):**

```c
// C structure
struct Point {
    int x;
    int y;
};

struct Point add_points(struct Point a, struct Point b) {
    struct Point result;
    result.x = a.x + b.x;
    result.y = a.y + b.y;
    return result;
}
```

```asm
; Assembly implementation
; struct Point is 8 bytes (two ints)
; Passed in single register X0 (a) and X1 (b)
; Returned in X0

.global add_points
add_points:
    ; X0 = {a.x (W0), a.y (bits [63:32])}
    ; X1 = {b.x (W1), b.y (bits [63:32])}
    
    ; Extract fields
    ; a.x in W0 (lower 32 bits)
    LSR X2, X0, #32             ; a.y in W2
    ; b.x in W1
    LSR X3, X1, #32             ; b.y in W3
    
    ; Add
    ADD W4, W0, W1              ; result.x = a.x + b.x
    ADD W5, W2, W3              ; result.y = a.y + b.y
    
    ; Pack result
    ORR X0, X4, X5, LSL #32     ; X0 = {result.x, result.y}
    RET

; Alternative: Structure passed by components
; If structure has 2 integer members, may be in W0, W1
add_points_v2:
    ; W0 = a.x, W1 = a.y, W2 = b.x, W3 = b.y
    ADD W0, W0, W2              ; result.x
    ADD W1, W1, W3              ; result.y
    ; Return in W0, W1
    RET
```

**Structure with mixed types:**

```c
struct Mixed {
    int i;
    float f;
};

float process_mixed(struct Mixed m) {
    return m.i + m.f;
}
```

```asm
; Mixed int/float structure
; Passed in X0 (int in W0, float in bits [63:32])
process_mixed:
    ; Extract int (lower 32 bits)
    ; W0 already has the int
    
    ; Extract float (upper 32 bits)
    LSR X1, X0, #32
    FMOV S1, W1                 ; Move to float register
    
    ; Convert int to float and add
    SCVTF S0, W0                ; Convert int to float
    FADD S0, S0, S1             ; Add
    
    RET
```

**Medium structures (> 16 bytes, ≤ 4 members):**

```c
struct Vector4 {
    float x, y, z, w;
};

struct Vector4 add_vectors(struct Vector4 a, struct Vector4 b) {
    struct Vector4 result;
    result.x = a.x + b.x;
    result.y = a.y + b.y;
    result.z = a.z + b.z;
    result.w = a.w + b.w;
    return result;
}
```

```asm
; Structure of 4 floats (16 bytes total)
; Can be passed in SIMD register V0, V1
; Or in S0-S3, S4-S7 depending on ABI variant

; Method 1: SIMD registers (HFA - Homogeneous Float Aggregate)
add_vectors_simd:
    ; V0 = {a.x, a.y, a.z, a.w} as .4S
    ; V1 = {b.x, b.y, b.z, b.w} as .4S
    
    FADD V0.4S, V0.4S, V1.4S    ; Vector add all components
    ; Result in V0
    RET

; Method 2: Individual float registers
add_vectors_regs:
    ; S0 = a.x, S1 = a.y, S2 = a.z, S3 = a.w
    ; S4 = b.x, S5 = b.y, S6 = b.z, S7 = b.w
    
    FADD S0, S0, S4             ; result.x
    FADD S1, S1, S5             ; result.y
    FADD S2, S2, S6             ; result.z
    FADD S3, S3, S7             ; result.w
    ; Return in S0-S3
    RET
```

**Large structures (> 16 bytes, non-HFA):**

```c
struct LargeData {
    long a, b, c, d, e;         // 40 bytes
};

struct LargeData process_large(struct LargeData data) {
    data.a += 1;
    data.b += 2;
    data.c += 3;
    data.d += 4;
    data.e += 5;
    return data;
}
```

```asm
; Large structures passed by reference
; X0 = pointer to input structure
; X8 = pointer to return value location (caller-allocated)
process_large:
    ; Load structure members
    LDP X1, X2, [X0]            ; a, b
    LDP X3, X4, [X0, #16]       ; c, d
    LDR X5, [X0, #32]           ; e
    
    ; Modify
    ADD X1, X1, #1
    ADD X2, X2, #2
    ADD X3, X3, #3
    ADD X4, X4, #4
    ADD X5, X5, #5
    
    ; Store to return location (X8)
    STP X1, X2, [X8]
    STP X3, X4, [X8, #16]
    STR X5, [X8, #32]
    
    ; Return pointer to result
    MOV X0, X8
    RET

; Calling large structure function
call_large:
    STP X29, X30, [SP, #-96]!
    MOV X29, SP
    
    ; Allocate structure on stack
    ADD X0, SP, #16             ; Input structure
    ADD X8, SP, #56             ; Return structure
    
    ; Initialize input
    MOV X1, #10
    STR X1, [X0]                ; data.a = 10
    ; ... initialize other members
    
    BL process_large
    
    ; Result now in [SP, #56]
    LDR X0, [SP, #56]           ; Access result.a
    
    LDP X29, X30, [SP], #96
    RET
```

**Homogeneous aggregates (HFA/HVA):**

```c
// Homogeneous Float Aggregate (HFA)
struct FloatArray {
    float data[4];
};

// Homogeneous Vector Aggregate (HVA)
struct Vec2Array {
    struct { float x, y; } vecs[4];
};
```

```asm
; HFA: Up to 4 float members (or arrays totaling ≤4 floats)
; Passed in S0-S3 or as a SIMD vector

process_hfa:
    ; S0-S3 contain the 4 floats
    FADD S0, S0, S1
    FADD S2, S2, S3
    FADD S0, S0, S2
    ; Return sum in S0
    RET

; HVA: Up to 4 vector members
; Passed in V0-V3

process_hva:
    ; V0-V3 each contain {x, y} as .2S
    FADD V0.2S, V0.2S, V1.2S
    FADD V2.2S, V2.2S, V3.2S
    FADD V0.2S, V0.2S, V2.2S
    ; Return in V0
    RET
```

**Bit fields in structures:**

```c
struct BitFields {
    unsigned int a : 5;
    unsigned int b : 11;
    unsigned int c : 16;
};

void set_bitfields(struct BitFields *bf, int a, int b, int c) {
    bf->a = a;
    bf->b = b;
    bf->c = c;
}
```

```asm
; Bit field structure (32 bits total)
; Layout: [a:5][b:11][c:16]
; Offset:  0-4   5-15  16-31

    ; X0 = bf pointer
    ; W1 = a (value to set in bits 0-4)
    ; W2 = b (value to set in bits 5-15)
    ; W3 = c (value to set in bits 16-31)
    
    ; Load current value
    LDR W4, [X0]
    
    ; Clear and set field 'a' (bits 0-4)
    BIC W4, W4, #0x1F           ; Clear bits 0-4
    AND W5, W1, #0x1F           ; Mask input to 5 bits
    ORR W4, W4, W5              ; Set bits 0-4
    
    ; Clear and set field 'b' (bits 5-15)
    MOV W5, #0x7FF              ; Mask for 11 bits
    BIC W4, W4, W5, LSL #5      ; Clear bits 5-15
    AND W5, W2, #0x7FF          ; Mask input to 11 bits
    ORR W4, W4, W5, LSL #5      ; Set bits 5-15
    
    ; Clear and set field 'c' (bits 16-31)
    MOV W5, #0xFFFF
    BIC W4, W4, W5, LSL #16     ; Clear bits 16-31
    AND W5, W3, #0xFFFF         ; Mask input to 16 bits
    ORR W4, W4, W5, LSL #16     ; Set bits 16-31
    
    ; Store result
    STR W4, [X0]
    RET

; Extract bit fields
get_bitfields:
    ; X0 = bf pointer
    ; Returns: W0 = a, W1 = b, W2 = c
    
    LDR W3, [X0]                ; Load structure
    
    ; Extract 'a' (bits 0-4)
    AND W0, W3, #0x1F
    
    ; Extract 'b' (bits 5-15)
    UBFX W1, W3, #5, #11        ; Extract 11 bits at offset 5
    
    ; Extract 'c' (bits 16-31)
    LSR W2, W3, #16             ; Shift right 16 bits
    
    RET
```

**Packed structures (alignment override):**

```c
struct __attribute__((packed)) PackedData {
    char c;         // 1 byte
    int i;          // 4 bytes (normally would be aligned to 4-byte boundary)
    short s;        // 2 bytes
};  // Total: 7 bytes (not 12)

void access_packed(struct PackedData *p) {
    p->i = 42;
}
```

```asm
; Packed structure - unaligned access
access_packed:
    ; X0 = pointer to packed structure
    ; Offset of 'i' is 1 (after char), not aligned
    
    MOV W1, #42
    
    ; Method 1: Byte-by-byte access (safe on all ARM)
    STRB W1, [X0, #1]           ; Store byte 0
    LSR W2, W1, #8
    STRB W2, [X0, #2]           ; Store byte 1
    LSR W2, W1, #16
    STRB W2, [X0, #3]           ; Store byte 2
    LSR W2, W1, #24
    STRB W2, [X0, #4]           ; Store byte 3
    RET

; Method 2: Unaligned access (ARMv8 supports this)
access_packed_unaligned:
    MOV W1, #42
    STR W1, [X0, #1]            ; Unaligned store (slower but works)
    RET

; Reading from packed structure
read_packed:
    ; Method 1: Byte-by-byte
    LDRB W0, [X0, #1]           ; Byte 0
    LDRB W1, [X0, #2]           ; Byte 1
    LDRB W2, [X0, #3]           ; Byte 2
    LDRB W3, [X0, #4]           ; Byte 3
    
    ORR W0, W0, W1, LSL #8      ; Combine bytes
    ORR W0, W0, W2, LSL #16
    ORR W0, W0, W3, LSL #24
    RET

; Method 2: Unaligned load
read_packed_unaligned:
    LDR W0, [X0, #1]            ; Unaligned load
    RET
```

**Nested structures:**

```c
struct Inner {
    int x;
    int y;
};

struct Outer {
    struct Inner a;
    struct Inner b;
    int z;
};

void process_nested(struct Outer *out) {
    out->a.x += out->b.x;
    out->a.y += out->b.y;
    out->z = out->a.x + out->a.y;
}
```

```asm
; Nested structure layout:
; Offset 0: a.x (4 bytes)
; Offset 4: a.y (4 bytes)
; Offset 8: b.x (4 bytes)
; Offset 12: b.y (4 bytes)
; Offset 16: z (4 bytes)
; Total: 20 bytes

process_nested:
    ; X0 = pointer to Outer
    
    ; Load a.x, a.y
    LDP W1, W2, [X0]            ; a.x, a.y
    
    ; Load b.x, b.y
    LDP W3, W4, [X0, #8]        ; b.x, b.y
    
    ; Add
    ADD W1, W1, W3              ; a.x += b.x
    ADD W2, W2, W4              ; a.y += b.y
    
    ; Store updated a
    STP W1, W2, [X0]
    
    ; Calculate z
    ADD W5, W1, W2              ; z = a.x + a.y
    STR W5, [X0, #16]           ; Store z
    
    RET
```

**Union handling:**

```c
union Value {
    int i;
    float f;
    char bytes[4];
};

void set_union_int(union Value *v, int x) {
    v->i = x;
}

float get_union_float(union Value *v) {
    return v->f;
}
```

```asm
; Union - all members share same memory
; Size = size of largest member (4 bytes)

set_union_int:
    ; X0 = pointer to union, W1 = value
    STR W1, [X0]                ; Store as int
    RET

get_union_float:
    ; X0 = pointer to union
    LDR W0, [X0]                ; Load 4 bytes
    FMOV S0, W0                 ; Move to float register
    RET

; Type punning through union
type_pun:
    ; Reinterpret float bits as int
    ; S0 = float input
    
    SUB SP, SP, #16             ; Allocate stack space
    STR S0, [SP]                ; Store as float
    LDR W0, [SP]                ; Load as int
    ADD SP, SP, #16
    RET
```

**Complex structures with mixed alignment:**

```c
struct Complex {
    char c;         // 1 byte, offset 0
    // 3 bytes padding
    int i;          // 4 bytes, offset 4
    double d;       // 8 bytes, offset 8 (needs 8-byte alignment)
    short s;        // 2 bytes, offset 16
    // 6 bytes padding at end for array alignment
};  // Total: 24 bytes
```

```asm
; Access complex structure with proper alignment
access_complex:
    ; X0 = pointer to Complex
    
    ; Load char (offset 0)
    LDRB W1, [X0]
    
    ; Load int (offset 4)
    LDR W2, [X0, #4]
    
    ; Load double (offset 8)
    LDR D0, [X0, #8]
    
    ; Load short (offset 16)
    LDRH W3, [X0, #16]
    
    ; Modify and store
    ADD W1, W1, #1
    STRB W1, [X0]
    
    ADD W2, W2, #10
    STR W2, [X0, #4]
    
    FADD D0, D0, D0
    STR D0, [X0, #8]
    
    ADD W3, W3, #100
    STRH W3, [X0, #16]
    
    RET

; Calculate structure size and alignment at runtime
sizeof_complex:
    MOV X0, #24                 ; sizeof(struct Complex)
    RET

alignof_complex:
    MOV X0, #8                  ; alignof(struct Complex) = 8 (for double)
    RET
```

**Flexible array member (FAM):**

```c
struct FlexArray {
    int count;
    int data[];     // Flexible array member
};

void init_flex_array(struct FlexArray *arr, int count) {
    arr->count = count;
    for (int i = 0; i < count; i++) {
        arr->data[i] = i;
    }
}
```

```asm
; Flexible array member - array at end of structure
init_flex_array:
    ; X0 = pointer to FlexArray
    ; W1 = count
    
    STP X29, X30, [SP, #-16]!
    
    ; Store count
    STR W1, [X0]                ; arr->count = count
    
    ; Initialize loop
    MOV W2, #0                  ; i = 0
    ADD X3, X0, #4              ; Pointer to arr->data
    
    CMP W1, #0
    B.LE done
    
loop:
    STR W2, [X3], #4            ; data[i] = i, advance pointer
    ADD W2, W2, #1              ; i++
    CMP W2, W1
    B.LT loop
    
done:
    LDP X29, X30, [SP], #16
    RET

; Allocate flexible array structure
; Size = sizeof(struct) + count * sizeof(element)
alloc_flex_array:
    ; W0 = count
    STP X29, X30, [SP, #-16]!
    
    ; Calculate size: 4 (count) + count * 4 (data)
    LSL W1, W0, #2              ; count * 4
    ADD W0, W1, #4              ; Total size
    
    ; Call malloc
    BL malloc                   ; X0 = allocated pointer
    
    LDP X29, X30, [SP], #16
    RET
```

**Structure copying:**

```c
struct Data {
    long values[10];
};

void copy_struct(struct Data *dest, const struct Data *src) {
    *dest = *src;
}
```

```asm
; Efficient structure copy (80 bytes)
copy_struct:
    ; X0 = dest, X1 = src
    
    ; Copy using register pairs (16 bytes at a time)
    LDP X2, X3, [X1]
    STP X2, X3, [X0]
    
    LDP X2, X3, [X1, #16]
    STP X2, X3, [X0, #16]
    
    LDP X2, X3, [X1, #32]
    STP X2, X3, [X0, #32]
    
    LDP X2, X3, [X1, #48]
    STP X2, X3, [X0, #48]
    
    LDP X2, X3, [X1, #64]
    STP X2, X3, [X0, #64]
    
    RET

; Generic memcpy for large structures
memcpy_struct:
    ; X0 = dest, X1 = src, X2 = size
    
    CMP X2, #0
    B.LE done
    
    ; Copy in 16-byte chunks
copy_loop:
    CMP X2, #16
    B.LT copy_remaining
    
    LDP X3, X4, [X1], #16
    STP X3, X4, [X0], #16
    SUB X2, X2, #16
    B copy_loop
    
copy_remaining:
    ; Copy remaining bytes
    CBZ X2, done
    
byte_loop:
    LDRB W3, [X1], #1
    STRB W3, [X0], #1
    SUBS X2, X2, #1
    B.NE byte_loop
    
done:
    RET

; Optimized with SIMD (copy 16 bytes per iteration)
memcpy_simd:
    ; X0 = dest, X1 = src, X2 = size
    
    CMP X2, #16
    B.LT small_copy
    
simd_loop:
    LDR Q0, [X1], #16           ; Load 128 bits
    STR Q0, [X0], #16           ; Store 128 bits
    SUBS X2, X2, #16
    B.GE simd_loop
    
    ADD X2, X2, #16             ; Adjust for overshoot
    
small_copy:
    CBZ X2, exit
    
small_loop:
    LDRB W3, [X1], #1
    STRB W3, [X0], #1
    SUBS X2, X2, #1
    B.NE small_loop
    
exit:
    RET
```

**Structure with function pointers (vtables):**

```c
struct Operations {
    int (*add)(int, int);
    int (*subtract)(int, int);
    int (*multiply)(int, int);
};

int call_operation(struct Operations *ops, int a, int b) {
    return ops->add(a, b);
}
```

```asm
; Structure containing function pointers
; Offset 0: add pointer
; Offset 8: subtract pointer
; Offset 16: multiply pointer

call_operation:
    ; X0 = ops pointer
    ; W1 = a, W2 = b
    
    STP X29, X30, [SP, #-16]!
    
    ; Load function pointer
    LDR X3, [X0]                ; ops->add
    
    ; Setup arguments (already in W1, W2)
    MOV W0, W1                  ; First arg
    MOV W1, W2                  ; Second arg
    
    ; Call through pointer
    BLR X3                      ; Indirect call
    
    ; Result in W0
    LDP X29, X30, [SP], #16
    RET

; Implement operations
add_impl:
    ADD W0, W0, W1
    RET

subtract_impl:
    SUB W0, W0, W1
    RET

multiply_impl:
    MUL W0, W0, W1
    RET

; Initialize operations structure
init_operations:
    ; X0 = pointer to Operations structure
    
    ADRP X1, add_impl
    ADD X1, X1, :lo12:add_impl
    STR X1, [X0]                ; ops->add = add_impl
    
    ADRP X1, subtract_impl
    ADD X1, X1, :lo12:subtract_impl
    STR X1, [X0, #8]            ; ops->subtract = subtract_impl
    
    ADRP X1, multiply_impl
    ADD X1, X1, :lo12:multiply_impl
    STR X1, [X0, #16]           ; ops->multiply = multiply_impl
    
    RET
```

**Interfacing with C++ classes:**

```cpp
class Calculator {
private:
    int value;
    
public:
    Calculator(int v);
    int add(int x);
    int get_value();
    virtual int compute();      // Virtual function
};
```

```asm
; C++ class layout:
; Offset 0: vtable pointer (if has virtual functions)
; Offset 8: member 'value'

; Constructor: Calculator::Calculator(int)
; Mangled name: _ZN10CalculatorC1Ei
_ZN10CalculatorC1Ei:
    ; X0 = this pointer
    ; W1 = v parameter
    
    ; Setup vtable pointer
    ADRP X2, _ZTV10Calculator   ; vtable address
    ADD X2, X2, :lo12:_ZTV10Calculator
    ADD X2, X2, #16             ; Skip typeinfo pointers
    STR X2, [X0]                ; Store vtable pointer
    
    ; Initialize member
    STR W1, [X0, #8]            ; this->value = v
    
    RET

; Member function: Calculator::add(int)
; Mangled name: _ZN10Calculator3addEi
_ZN10Calculator3addEi:
    ; X0 = this pointer
    ; W1 = x parameter
    
    LDR W2, [X0, #8]            ; Load this->value
    ADD W2, W2, W1              ; value + x
    STR W2, [X0, #8]            ; Store back
    MOV W0, W2                  ; Return new value
    RET

; Member function: Calculator::get_value()
; Mangled name: _ZN10Calculator9get_valueEv
_ZN10Calculator9get_valueEv:
    ; X0 = this pointer
    LDR W0, [X0, #8]            ; Return this->value
    RET

; Virtual function: Calculator::compute()
; Called through vtable
_ZN10Calculator7computeEv:
    ; X0 = this pointer
    LDR W0, [X0, #8]            ; Return this->value
    RET

; Call virtual function
call_virtual:
    ; X0 = Calculator object pointer
    STP X29, X30, [SP, #-16]!
    
    ; Load vtable pointer
    LDR X1, [X0]                ; vtable pointer
    
    ; Load function pointer from vtable
    LDR X2, [X1]                ; First virtual function
    
    ; Call through vtable
    BLR X2
    
    LDP X29, X30, [SP], #16
    RET

; Vtable layout (read-only data)
.section .rodata
.align 3
_ZTV10Calculator:
    .quad 0                     ; Offset to top
    .quad _ZTI10Calculator      ; Typeinfo pointer
    .quad _ZN10Calculator7computeEv  ; compute() function pointer
```

**Key Points:**

- Name mangling encodes type information in symbol names; C uses simple names while C++ mangles based on Itanium ABI conventions
- ABI compatibility requires following register conventions (X0-X7 for args, X19-X28 callee-saved) and 16-byte stack alignment
- Volatile registers (X0-X18) can be clobbered by calls; callee-saved registers (X19-X28) must be preserved
- Structure passing depends on size: ≤16 bytes in registers, HFA/HVA in SIMD registers, larger structures by reference with return location in X8
- Packed structures and bit fields require careful byte-level manipulation
- C++ classes add vtable pointers and name mangling complexity

[Inference] Specific ABI details may vary slightly between platforms (Linux, iOS, Windows) though all follow AAPCS64 baseline - platform-specific variations primarily affect system calls and dynamic linking.

[Inference] Compiler optimizations may pass structures differently than baseline ABI when inlining or using link-time optimization, but external interfaces must follow standard ABI for compatibility.

---

# Hardware Interfacing

Hardware interfacing in ARM assembly involves directly interacting with physical devices through memory-mapped I/O, controlling GPIO pins, accessing peripherals, and manipulating device registers. This low-level programming is essential for embedded systems, device drivers, and bare-metal applications.

## Memory-Mapped I/O

Memory-mapped I/O maps hardware device registers into the processor's address space, allowing hardware control using standard load and store instructions.

### Memory-Mapped I/O Concepts

**Address Space Organization:**

```
Typical ARM System Memory Map:

0x00000000 - 0x0FFFFFFF: Flash/ROM (code storage)
0x20000000 - 0x2FFFFFFF: SRAM (data, stack)
0x40000000 - 0x5FFFFFFF: Peripherals (memory-mapped I/O)
0x60000000 - 0x9FFFFFFF: External RAM/devices
0xE0000000 - 0xE00FFFFF: System peripherals (Cortex-M)
```

**Memory-Mapped Register Access:**

```assembly
; Define peripheral base addresses
.equ PERIPHERAL_BASE, 0x40000000
.equ GPIO_BASE,       0x40020000
.equ UART_BASE,       0x40011000

; Reading from device register
LDR r0, =GPIO_BASE
LDR r1, [r0]                    ; Read GPIO data register

; Writing to device register
LDR r0, =GPIO_BASE
MOV r1, #0xFF
STR r1, [r0]                    ; Write to GPIO data register
```

### Volatile Access Requirements

Memory-mapped I/O requires special handling to prevent compiler/processor optimizations that might break hardware interaction:

```assembly
; Volatile read - must actually read from hardware
volatile_read:
    LDR r0, =PERIPHERAL_ADDR
    LDR r1, [r0]                ; Hardware read
    ; Compiler cannot optimize away even if value unused
    BX lr

; Non-volatile optimization (wrong for hardware)
; Compiler might:
; - Reorder reads/writes
; - Cache values in registers
; - Eliminate "redundant" accesses
```

**Memory Barriers for Hardware Access:**

```assembly
; Ensure ordering of hardware accesses
    LDR r0, =GPIO_BASE
    MOV r1, #1
    STR r1, [r0]                ; Write to GPIO
    
    DMB                         ; Data Memory Barrier
                                ; Ensures write completes before next access
    
    LDR r2, [r0, #4]            ; Read status register
                                ; Guaranteed to see previous write
```

### Read-Modify-Write Operations

**Safe Bit Manipulation:**

```assembly
; Set specific bit in hardware register
; r0 = register address, r1 = bit position

set_bit_safe:
    LDR r2, [r0]                ; Read current value
    MOV r3, #1
    LSL r3, r3, r1              ; Create bit mask
    ORR r2, r2, r3              ; Set bit
    STR r2, [r0]                ; Write back
    BX lr

; Clear specific bit
clear_bit_safe:
    LDR r2, [r0]                ; Read
    MOV r3, #1
    LSL r3, r3, r1              ; Create mask
    BIC r2, r2, r3              ; Clear bit
    STR r2, [r0]                ; Write back
    BX lr

; Toggle bit
toggle_bit:
    LDR r2, [r0]
    MOV r3, #1
    LSL r3, r3, r1
    EOR r2, r2, r3              ; Toggle bit
    STR r2, [r0]
    BX lr
```

**Atomic Hardware Operations:**

```assembly
; ARM Cortex-M specific: Bit-band aliasing
; Allows atomic bit operations without read-modify-write

; Bit-band regions:
; 0x20000000-0x200FFFFF -> 0x22000000-0x23FFFFFF (SRAM)
; 0x40000000-0x400FFFFF -> 0x42000000-0x43FFFFFF (Peripheral)

; Calculate bit-band alias address
; alias_addr = bit_band_base + (byte_offset * 32) + (bit_number * 4)

set_bit_bitband:
    ; r0 = register address, r1 = bit number
    LDR r2, =0x42000000         ; Peripheral bit-band base
    LDR r3, =0x40000000         ; Peripheral region base
    
    SUB r0, r0, r3              ; Get offset from base
    LSL r0, r0, #5              ; Multiply by 32
    LSL r1, r1, #2              ; Multiply bit number by 4
    ADD r0, r0, r1              ; Add bit offset
    ADD r0, r0, r2              ; Add bit-band base
    
    MOV r1, #1
    STR r1, [r0]                ; Atomic bit set
    BX lr
```

### Direct Memory Access (DMA) Setup

**Configuring DMA Controller:**

```assembly
; Example: STM32 DMA configuration
.equ DMA1_BASE,     0x40020000
.equ DMA1_CH1_BASE, 0x40020008

.equ DMA_CCR_OFFSET,   0x00     ; Configuration register
.equ DMA_CNDTR_OFFSET, 0x04     ; Number of data
.equ DMA_CPAR_OFFSET,  0x08     ; Peripheral address
.equ DMA_CMAR_OFFSET,  0x0C     ; Memory address

; DMA_CCR flags
.equ DMA_CCR_EN,       (1 << 0)  ; Enable
.equ DMA_CCR_TCIE,     (1 << 1)  ; Transfer complete interrupt
.equ DMA_CCR_HTIE,     (1 << 2)  ; Half transfer interrupt
.equ DMA_CCR_TEIE,     (1 << 3)  ; Transfer error interrupt
.equ DMA_CCR_DIR,      (1 << 4)  ; Direction: 0=peripheral to memory
.equ DMA_CCR_CIRC,     (1 << 5)  ; Circular mode
.equ DMA_CCR_PINC,     (1 << 6)  ; Peripheral increment
.equ DMA_CCR_MINC,     (1 << 7)  ; Memory increment
.equ DMA_CCR_PSIZE_8,  (0 << 8)  ; Peripheral size: 8-bit
.equ DMA_CCR_PSIZE_16, (1 << 8)  ; 16-bit
.equ DMA_CCR_PSIZE_32, (2 << 8)  ; 32-bit
.equ DMA_CCR_MSIZE_8,  (0 << 10) ; Memory size: 8-bit
.equ DMA_CCR_MSIZE_16, (1 << 10) ; 16-bit
.equ DMA_CCR_MSIZE_32, (2 << 10) ; 32-bit
.equ DMA_CCR_PL_LOW,   (0 << 12) ; Priority: low
.equ DMA_CCR_PL_MED,   (1 << 12) ; Medium
.equ DMA_CCR_PL_HIGH,  (2 << 12) ; High
.equ DMA_CCR_PL_VHIGH, (3 << 12) ; Very high
.equ DMA_CCR_MEM2MEM,  (1 << 14) ; Memory to memory

dma_setup:
    PUSH {r4, r5, lr}
    
    ; r0 = source address
    ; r1 = destination address
    ; r2 = transfer count
    
    LDR r3, =DMA1_CH1_BASE
    
    ; Disable DMA channel first
    LDR r4, [r3, #DMA_CCR_OFFSET]
    BIC r4, r4, #DMA_CCR_EN
    STR r4, [r3, #DMA_CCR_OFFSET]
    
    ; Set peripheral address (source)
    STR r0, [r3, #DMA_CPAR_OFFSET]
    
    ; Set memory address (destination)
    STR r1, [r3, #DMA_CMAR_OFFSET]
    
    ; Set transfer count
    STR r2, [r3, #DMA_CNDTR_OFFSET]
    
    ; Configure: memory to memory, 32-bit, increment both, high priority
    MOV r4, #(DMA_CCR_MEM2MEM | DMA_CCR_MINC | DMA_CCR_PINC)
    ORR r4, r4, #(DMA_CCR_MSIZE_32 | DMA_CCR_PSIZE_32)
    ORR r4, r4, #DMA_CCR_PL_HIGH
    ORR r4, r4, #DMA_CCR_TCIE           ; Enable transfer complete interrupt
    STR r4, [r3, #DMA_CCR_OFFSET]
    
    ; Enable DMA channel
    LDR r4, [r3, #DMA_CCR_OFFSET]
    ORR r4, r4, #DMA_CCR_EN
    STR r4, [r3, #DMA_CCR_OFFSET]
    
    POP {r4, r5, pc}
```

### Cache Coherency Considerations

**Ensuring Cache Coherency with DMA:**

```assembly
; ARM Cortex-A with caches

; Before DMA read (peripheral -> memory):
; Invalidate data cache to prevent reading stale data

dma_prepare_read:
    ; r0 = buffer address, r1 = size
    PUSH {r4, lr}
    
    ; Clean and invalidate cache lines
    MOV r2, r0                  ; Start address
    ADD r3, r0, r1              ; End address
    
invalidate_loop:
    DC CIVAC, r2                ; Clean and invalidate cache line
    ADD r2, r2, #64             ; Cache line size (typically 64 bytes)
    CMP r2, r3
    BLT invalidate_loop
    
    DSB                         ; Ensure completion
    
    POP {r4, pc}

; After DMA write (memory -> peripheral):
; Clean data cache to ensure data written to memory

dma_prepare_write:
    ; r0 = buffer address, r1 = size
    PUSH {r4, lr}
    
    MOV r2, r0
    ADD r3, r0, r1
    
clean_loop:
    DC CVAC, r2                 ; Clean cache line
    ADD r2, r2, #64
    CMP r2, r3
    BLT clean_loop
    
    DSB                         ; Ensure completion
    
    POP {r4, pc}
```

## GPIO Programming

General Purpose Input/Output (GPIO) allows software control of digital pins for interfacing with external hardware.

### GPIO Configuration

**GPIO Register Structure:**

```assembly
; Typical GPIO register layout
.equ GPIO_MODER,   0x00         ; Mode register (input/output/alternate/analog)
.equ GPIO_OTYPER,  0x04         ; Output type (push-pull/open-drain)
.equ GPIO_OSPEEDR, 0x08         ; Output speed
.equ GPIO_PUPDR,   0x0C         ; Pull-up/pull-down
.equ GPIO_IDR,     0x10         ; Input data register
.equ GPIO_ODR,     0x14         ; Output data register
.equ GPIO_BSRR,    0x18         ; Bit set/reset register
.equ GPIO_LCKR,    0x1C         ; Lock register
.equ GPIO_AFRL,    0x20         ; Alternate function low
.equ GPIO_AFRH,    0x24         ; Alternate function high

; Mode values (2 bits per pin)
.equ GPIO_MODE_INPUT,  0b00
.equ GPIO_MODE_OUTPUT, 0b01
.equ GPIO_MODE_AF,     0b10      ; Alternate function
.equ GPIO_MODE_ANALOG, 0b11

; Output type
.equ GPIO_OTYPE_PP,    0         ; Push-pull
.equ GPIO_OTYPE_OD,    1         ; Open-drain

; Speed values (2 bits per pin)
.equ GPIO_SPEED_LOW,    0b00
.equ GPIO_SPEED_MED,    0b01
.equ GPIO_SPEED_HIGH,   0b10
.equ GPIO_SPEED_VHIGH,  0b11

; Pull-up/pull-down (2 bits per pin)
.equ GPIO_PUPD_NONE, 0b00
.equ GPIO_PUPD_UP,   0b01
.equ GPIO_PUPD_DOWN, 0b10
```

**Configuring GPIO Pin:**

```assembly
; Configure pin as output
; r0 = GPIO base address, r1 = pin number (0-15)

gpio_config_output:
    PUSH {r4, r5, lr}
    
    ; Set mode to output (2 bits per pin)
    LDR r2, [r0, #GPIO_MODER]
    MOV r3, #0b11               ; Mask for 2 bits
    LSL r4, r1, #1              ; Bit position = pin * 2
    LSL r3, r3, r4              ; Position mask
    BIC r2, r2, r3              ; Clear bits
    MOV r3, #GPIO_MODE_OUTPUT
    LSL r3, r3, r4              ; Position value
    ORR r2, r2, r3              ; Set bits
    STR r2, [r0, #GPIO_MODER]
    
    ; Set output type to push-pull
    LDR r2, [r0, #GPIO_OTYPER]
    MOV r3, #1
    LSL r3, r3, r1              ; Bit mask
    BIC r2, r2, r3              ; Clear = push-pull
    STR r2, [r0, #GPIO_OTYPER]
    
    ; Set speed to high
    LDR r2, [r0, #GPIO_OSPEEDR]
    MOV r3, #0b11
    LSL r4, r1, #1
    LSL r3, r3, r4
    BIC r2, r2, r3              ; Clear bits
    MOV r3, #GPIO_SPEED_HIGH
    LSL r3, r3, r4
    ORR r2, r2, r3              ; Set high speed
    STR r2, [r0, #GPIO_OSPEEDR]
    
    ; No pull-up/pull-down
    LDR r2, [r0, #GPIO_PUPDR]
    MOV r3, #0b11
    LSL r4, r1, #1
    LSL r3, r3, r4
    BIC r2, r2, r3              ; Clear = no pull
    STR r2, [r0, #GPIO_PUPDR]
    
    POP {r4, r5, pc}

; Configure pin as input with pull-up
gpio_config_input_pullup:
    PUSH {r4, r5, lr}
    
    ; Set mode to input
    LDR r2, [r0, #GPIO_MODER]
    MOV r3, #0b11
    LSL r4, r1, #1
    LSL r3, r3, r4
    BIC r2, r2, r3              ; Clear = input mode
    STR r2, [r0, #GPIO_MODER]
    
    ; Set pull-up
    LDR r2, [r0, #GPIO_PUPDR]
    MOV r3, #0b11
    LSL r4, r1, #1
    LSL r3, r3, r4
    BIC r2, r2, r3
    MOV r3, #GPIO_PUPD_UP
    LSL r3, r3, r4
    ORR r2, r2, r3
    STR r2, [r0, #GPIO_PUPDR]
    
    POP {r4, r5, pc}
```

### GPIO Read/Write Operations

**Writing to GPIO:**

```assembly
; Set pin high
; r0 = GPIO base, r1 = pin number

gpio_set:
    MOV r2, #1
    LSL r2, r2, r1              ; Create bit mask
    STR r2, [r0, #GPIO_BSRR]    ; BSRR lower 16 bits = set
    BX lr

; Set pin low
gpio_clear:
    MOV r2, #1
    LSL r2, r2, r1
    LSL r2, r2, #16             ; BSRR upper 16 bits = reset
    STR r2, [r0, #GPIO_BSRR]
    BX lr

; Toggle pin
gpio_toggle:
    PUSH {r4, lr}
    
    LDR r2, [r0, #GPIO_ODR]     ; Read current state
    MOV r3, #1
    LSL r3, r3, r1              ; Create mask
    EOR r2, r2, r3              ; Toggle bit
    STR r2, [r0, #GPIO_ODR]     ; Write back
    
    POP {r4, pc}

; Write multiple pins atomically
; r0 = GPIO base, r1 = value, r2 = mask
gpio_write_masked:
    LDR r3, [r0, #GPIO_ODR]     ; Read current
    BIC r3, r3, r2              ; Clear masked bits
    AND r1, r1, r2              ; Mask new value
    ORR r3, r3, r1              ; Combine
    STR r3, [r0, #GPIO_ODR]     ; Write
    BX lr
```

**Reading from GPIO:**

```assembly
; Read pin state
; r0 = GPIO base, r1 = pin number
; Returns: r0 = 0 or 1

gpio_read:
    LDR r2, [r0, #GPIO_IDR]     ; Read input data register
    LSR r2, r2, r1              ; Shift bit to position 0
    AND r0, r2, #1              ; Mask to single bit
    BX lr

; Read entire port
gpio_read_port:
    LDR r0, [r0, #GPIO_IDR]
    BX lr

; Wait for pin to go high
; r0 = GPIO base, r1 = pin number
gpio_wait_high:
    PUSH {r4, lr}
    
    MOV r4, #1
    LSL r4, r4, r1              ; Create mask
    
wait_loop:
    LDR r2, [r0, #GPIO_IDR]
    TST r2, r4                  ; Test bit
    BEQ wait_loop               ; Loop if still low
    
    POP {r4, pc}
```

### GPIO Interrupts

**Configuring External Interrupts:**

```assembly
; Example: STM32 EXTI configuration
.equ EXTI_BASE,    0x40010400
.equ SYSCFG_BASE,  0x40010000

.equ EXTI_IMR,     0x00         ; Interrupt mask register
.equ EXTI_EMR,     0x04         ; Event mask register
.equ EXTI_RTSR,    0x08         ; Rising trigger selection
.equ EXTI_FTSR,    0x0C         ; Falling trigger selection
.equ EXTI_SWIER,   0x10         ; Software interrupt event
.equ EXTI_PR,      0x14         ; Pending register

.equ SYSCFG_EXTICR1, 0x08       ; External interrupt configuration

; Configure pin for interrupt on rising edge
; r0 = GPIO port (A=0, B=1, etc.), r1 = pin number

gpio_config_interrupt:
    PUSH {r4, r5, r6, lr}
    
    ; Configure SYSCFG to route pin to EXTI line
    LDR r2, =SYSCFG_BASE
    MOV r3, r1
    LSR r3, r3, #2              ; Register index = pin / 4
    LSL r3, r3, #2              ; Multiply by 4 for offset
    ADD r2, r2, #SYSCFG_EXTICR1
    ADD r2, r2, r3              ; Point to correct EXTICR register
    
    AND r4, r1, #0x3            ; Position within register = pin % 4
    LSL r4, r4, #2              ; Each field is 4 bits
    MOV r5, #0xF
    LSL r5, r5, r4              ; Create mask
    
    LDR r6, [r2]
    BIC r6, r6, r5              ; Clear field
    LSL r0, r0, r4              ; Position port number
    ORR r6, r6, r0              ; Set port
    STR r6, [r2]
    
    ; Enable interrupt mask for this line
    LDR r2, =EXTI_BASE
    LDR r3, [r2, #EXTI_IMR]
    MOV r4, #1
    LSL r4, r4, r1              ; Bit for this pin
    ORR r3, r3, r4
    STR r3, [r2, #EXTI_IMR]
    
    ; Configure rising edge trigger
    LDR r3, [r2, #EXTI_RTSR]
    ORR r3, r3, r4
    STR r3, [r2, #EXTI_RTSR]
    
    ; Clear any pending interrupt
    STR r4, [r2, #EXTI_PR]
    
    POP {r4, r5, r6, pc}

; EXTI interrupt handler
exti_irq_handler:
    PUSH {r4, lr}
    
    LDR r0, =EXTI_BASE
    LDR r1, [r0, #EXTI_PR]      ; Read pending register
    
    ; Check which line triggered
    MOV r2, #0                  ; Pin counter
    
check_loop:
    CMP r2, #16
    BGE irq_done
    
    MOV r3, #1
    LSL r3, r3, r2
    TST r1, r3                  ; Test if this pin pending
    BEQ next_pin
    
    ; Clear pending bit
    STR r3, [r0, #EXTI_PR]
    
    ; Handle interrupt for pin r2
    PUSH {r0-r3}
    MOV r0, r2
    BL gpio_interrupt_callback  ; User callback
    POP {r0-r3}
    
next_pin:
    ADD r2, r2, #1
    B check_loop
    
irq_done:
    POP {r4, pc}
```

### Practical GPIO Examples

**Example** - LED Blink:

```assembly
; Blink LED on pin
; r0 = GPIO base, r1 = pin number, r2 = delay count

led_blink:
    PUSH {r4, r5, r6, lr}
    
    MOV r4, r0                  ; Save GPIO base
    MOV r5, r1                  ; Save pin
    MOV r6, r2                  ; Save delay
    
blink_loop:
    ; Turn LED on
    MOV r0, r4
    MOV r1, r5
    BL gpio_set
    
    ; Delay
    MOV r0, r6
    BL delay_ms
    
    ; Turn LED off
    MOV r0, r4
    MOV r1, r5
    BL gpio_clear
    
    ; Delay
    MOV r0, r6
    BL delay_ms
    
    B blink_loop                ; Infinite loop
```

**Example** - Button debouncing:

```assembly
; Read button with debouncing
; r0 = GPIO base, r1 = pin number
; Returns: r0 = 1 if button pressed (stable)

button_read_debounced:
    PUSH {r4, r5, r6, lr}
    
    MOV r4, r0                  ; Save GPIO base
    MOV r5, r1                  ; Save pin
    MOV r6, #0                  ; Stable count
    
    ; Read initial state
    MOV r0, r4
    MOV r1, r5
    BL gpio_read
    MOV r2, r0                  ; Previous state
    
debounce_loop:
    ; Small delay
    MOV r0, #1
    BL delay_ms
    
    ; Read current state
    MOV r0, r4
    MOV r1, r5
    BL gpio_read
    
    ; Compare with previous
    CMP r0, r2
    BNE state_changed
    
    ; State stable, increment counter
    ADD r6, r6, #1
    CMP r6, #10                 ; 10ms stable = valid
    BGE debounce_done
    B debounce_loop
    
state_changed:
    MOV r2, r0                  ; Update previous state
    MOV r6, #0                  ; Reset counter
    B debounce_loop
    
debounce_done:
    MOV r0, r2                  ; Return stable state
    POP {r4, r5, r6, pc}
```

**Example** - Shift register (74HC595) control:

```assembly
; Send data to 74HC595 shift register
; r0 = GPIO base
; r1 = data pin, r2 = clock pin, r3 = latch pin
; r4 = data byte

shift_out_595:
    PUSH {r5, r6, r7, lr}
    
    MOV r5, r0                  ; Save GPIO base
    MOV r6, #8                  ; Bit counter
    
    ; Latch low
    MOV r0, r5
    MOV r1, r3
    BL gpio_clear
    
shift_loop:
    ; Extract MSB
    MOV r7, r4
    LSR r7, r7, #7              ; Get bit 7
    AND r7, r7, #1
    
    ; Set data pin
    MOV r0, r5
    MOV r1, r1                  ; Data pin (from parameter)
    CMP r7, #0
    ITE EQ
    BLEQ gpio_clear
    BLNE gpio_set
    
    ; Clock high
    MOV r0, r5
    MOV r1, r2                  ; Clock pin
    BL gpio_set
    
    ; Small delay
    MOV r0, #1
    BL delay_us
    
    ; Clock low
    MOV r0, r5
    MOV r1, r2
    BL gpio_clear
    
    ; Shift data left
    LSL r4, r4, #1
    
    ; Decrement counter
    SUBS r6, r6, #1
    BNE shift_loop
    
    ; Latch high to output data
    MOV r0, r5
    MOV r1, r3
    BL gpio_set
    
    POP {r5, r6, r7, pc}
```

**Example** - Reading rotary encoder:

```assembly
; Read quadrature rotary encoder
; r0 = GPIO base, r1 = pin A, r2 = pin B
; Returns: r0 = -1 (CCW), 0 (no change), 1 (CW)

.data
encoder_state: .byte 0          ; Previous state

.text
read_encoder:
    PUSH {r4, r5, r6, lr}
    
    MOV r4, r0                  ; Save GPIO base
    MOV r5, r1                  ; Save pin A
    MOV r6, r2                  ; Save pin B
    
    ; Read pin A
    MOV r0, r4
    MOV r1, r5
    BL gpio_read
    LSL r0, r0, #1              ; Shift to bit 1
    MOV r3, r0                  ; Current state
    
    ; Read pin B
    MOV r0, r4
    MOV r1, r6
    BL gpio_read
    ORR r3, r3, r0              ; Combine: [A:B]
    
    ; Load previous state
    LDR r0, =encoder_state
    LDRB r1, [r0]
    
    ; Save current state
    STRB r3, [r0]
    
    ; Determine direction
    ; State transition table:
    ; 00->01 = CW,  01->00 = CCW
    ; 01->11 = CW,  11->01 = CCW
    ; 11->10 = CW,  10->11 = CCW
    ; 10->00 = CW,  00->10 = CCW
    
    EOR r2, r1, r3              ; XOR old and new
    CMP r2, #0
    IT EQ
    MOVEQ r0, #0                ; No change
    BEQ encoder_done
    
    ; Check which bit changed
    TST r2, #0x02               ; Did bit 1 (A) change?
    ITE NE
    ANDNE r0, r3, #0x01         ; If A changed, direction = B
    ANDEQ r0, r3, #0x02         ; If B changed, direction = A
    LSR r0, r0, #0              ; Normalize to 0 or 1
    
    ; Convert to -1 or 1
    CMP r0, #0
    ITE EQ
    MOVEQ r0, #-1               ; CCW
    MOVNE r0, #1                ; CW
    
encoder_done:
    POP {r4, r5, r6, pc}
```

## Peripheral Access

Peripherals are specialized hardware blocks that provide specific functionality like timers, communication interfaces, and analog conversion.

### Clock Configuration

**Enabling Peripheral Clocks:**

```assembly
; Example: STM32 RCC (Reset and Clock Control)
.equ RCC_BASE,      0x40021000
.equ RCC_CR,        0x00            ; Clock control register
.equ RCC_CFGR,      0x04            ; Clock configuration
.equ RCC_APB1ENR,   0x1C            ; APB1 peripheral clock enable
.equ RCC_APB2ENR,   0x18            ; APB2 peripheral clock enable
.equ RCC_AHBENR,    0x14            ; AHB peripheral clock enable

; Enable GPIOA clock
.equ RCC_AHBENR_GPIOAEN, (1 << 17)

enable_gpioa_clock:
    LDR r0, =RCC_BASE
    LDR r1, [r0, #RCC_AHBENR]
    ORR r1, r1, #RCC_AHBENR_GPIOAEN
    STR r1, [r0, #RCC_AHBENR]
    
    ; Small delay for clock to stabilize
    MOV r0, #10
    BL delay_us
    
    BX lr

; Configure system clock
; Example: Set to 72MHz using PLL
configure_system_clock:
    PUSH {r4, lr}

    LDR r4, =RCC_BASE

    ; Enable HSE (High Speed External oscillator)
    LDR r0, [r4, #RCC_CR]
    ORR r0, r0, #(1 << 16)          ; HSEON
    STR r0, [r4, #RCC_CR]

wait_hse_ready:
    LDR r0, [r4, #RCC_CR]
    TST r0, #(1 << 17)              ; HSERDY
    BEQ wait_hse_ready

    ; Configure PLL: HSE * 9 = 72MHz (8MHz * 9)
    LDR r0, [r4, #RCC_CFGR]
    BIC r0, r0, #0x003C0000         ; Clear PLL multiplier bits
    ORR r0, r0, #0x001C0000         ; PLLMUL = 9
    ORR r0, r0, #(1 << 16)          ; PLLSRC = HSE
    STR r0, [r4, #RCC_CFGR]

    ; Enable PLL
    LDR r0, [r4, #RCC_CR]
    ORR r0, r0, #(1 << 24)          ; PLLON
    STR r0, [r4, #RCC_CR]

wait_pll_ready:
    LDR r0, [r4, #RCC_CR]
    TST r0, #(1 << 25)              ; PLLRDY
    BEQ wait_pll_ready

    ; Set PLL as system clock
    LDR r0, [r4, #RCC_CFGR]
    BIC r0, r0, #0x03               ; Clear SW bits
    ORR r0, r0, #0x02               ; SW = PLL
    STR r0, [r4, #RCC_CFGR]

wait_clock_switch:
    LDR r0, [r4, #RCC_CFGR]
    AND r0, r0, #0x0C               ; SWS bits
    CMP r0, #0x08                   ; Check if PLL is system clock
    BNE wait_clock_switch

    POP {r4, pc}
````

### Timer/Counter Programming

**Basic Timer Configuration:**
```assembly
; Example: STM32 TIM2 configuration
.equ TIM2_BASE,    0x40000000
.equ TIM_CR1,      0x00         ; Control register 1
.equ TIM_CR2,      0x04         ; Control register 2
.equ TIM_DIER,     0x0C         ; DMA/Interrupt enable
.equ TIM_SR,       0x10         ; Status register
.equ TIM_CNT,      0x24         ; Counter
.equ TIM_PSC,      0x28         ; Prescaler
.equ TIM_ARR,      0x2C         ; Auto-reload register
.equ TIM_CCR1,     0x34         ; Capture/Compare 1

; TIM_CR1 bits
.equ TIM_CR1_CEN,  (1 << 0)     ; Counter enable
.equ TIM_CR1_UDIS, (1 << 1)     ; Update disable
.equ TIM_CR1_URS,  (1 << 2)     ; Update request source
.equ TIM_CR1_OPM,  (1 << 3)     ; One-pulse mode
.equ TIM_CR1_ARPE, (1 << 7)     ; Auto-reload preload enable

; TIM_DIER bits
.equ TIM_DIER_UIE, (1 << 0)     ; Update interrupt enable

; TIM_SR bits
.equ TIM_SR_UIF,   (1 << 0)     ; Update interrupt flag

; Initialize timer for 1ms interrupts
; Assumes 72MHz system clock, APB1 = 36MHz, timer clock = 72MHz
timer_init_1ms:
    PUSH {r4, lr}
    
    ; Enable TIM2 clock
    LDR r0, =RCC_BASE
    LDR r1, [r0, #RCC_APB1ENR]
    ORR r1, r1, #(1 << 0)       ; TIM2EN
    STR r1, [r0, #RCC_APB1ENR]
    
    LDR r4, =TIM2_BASE
    
    ; Set prescaler: 72MHz / 72 = 1MHz (1µs per tick)
    MOV r0, #71                 ; Prescaler = 72 - 1
    STR r0, [r4, #TIM_PSC]
    
    ; Set auto-reload: 1MHz / 1000 = 1ms
    MOV r0, #999                ; ARR = 1000 - 1
    STR r0, [r4, #TIM_ARR]
    
    ; Enable update interrupt
    MOV r0, #TIM_DIER_UIE
    STR r0, [r4, #TIM_DIER]
    
    ; Enable timer
    MOV r0, #TIM_CR1_CEN
    STR r0, [r4, #TIM_CR1]
    
    ; Enable TIM2 interrupt in NVIC
    LDR r0, =0xE000E100         ; NVIC_ISER0
    MOV r1, #(1 << 28)          ; TIM2 is IRQ 28
    STR r1, [r0]
    
    POP {r4, pc}

; Timer interrupt handler
TIM2_IRQHandler:
    PUSH {r4, lr}
    
    LDR r4, =TIM2_BASE
    
    ; Check if update interrupt
    LDR r0, [r4, #TIM_SR]
    TST r0, #TIM_SR_UIF
    BEQ timer_irq_done
    
    ; Clear interrupt flag
    LDR r0, [r4, #TIM_SR]
    BIC r0, r0, #TIM_SR_UIF
    STR r0, [r4, #TIM_SR]
    
    ; Handle 1ms tick
    BL system_tick_handler      ; User callback
    
timer_irq_done:
    POP {r4, pc}
````

**PWM Generation:**

```assembly
; Configure timer for PWM output
; r0 = frequency (Hz), r1 = duty cycle (0-100)

timer_pwm_init:
    PUSH {r4, r5, r6, lr}
    
    MOV r4, r0                  ; Save frequency
    MOV r5, r1                  ; Save duty cycle
    
    ; Enable TIM2 clock
    LDR r0, =RCC_BASE
    LDR r1, [r0, #RCC_APB1ENR]
    ORR r1, r1, #(1 << 0)
    STR r1, [r0, #RCC_APB1ENR]
    
    LDR r6, =TIM2_BASE
    
    ; Calculate prescaler and ARR
    ; Timer clock = 72MHz
    ; ARR = 72000000 / (prescaler * frequency) - 1
    
    ; Use prescaler = 72 for frequencies < 1MHz
    MOV r0, #71
    STR r0, [r6, #TIM_PSC]
    
    ; ARR = 1000000 / frequency - 1
    LDR r0, =1000000
    UDIV r0, r0, r4
    SUB r0, r0, #1
    STR r0, [r6, #TIM_ARR]
    
    ; Calculate CCR1 for duty cycle
    ; CCR1 = ARR * duty_cycle / 100
    ADD r0, r0, #1              ; ARR + 1
    MUL r0, r0, r5              ; * duty_cycle
    MOV r1, #100
    UDIV r0, r0, r1             ; / 100
    STR r0, [r6, #TIM_CCR1]
    
    ; Configure channel 1 as PWM mode 1
    ; CCMR1: OC1M = 110 (PWM mode 1), OC1PE = 1 (preload enable)
    LDR r0, [r6, #0x18]         ; TIM_CCMR1
    BIC r0, r0, #0x73           ; Clear OC1M and OC1PE
    ORR r0, r0, #0x68           ; Set PWM mode 1 and preload
    STR r0, [r6, #0x18]
    
    ; Enable channel 1 output
    LDR r0, [r6, #0x20]         ; TIM_CCER
    ORR r0, r0, #0x01           ; CC1E
    STR r0, [r6, #0x20]
    
    ; Enable auto-reload preload
    LDR r0, [r6, #TIM_CR1]
    ORR r0, r0, #TIM_CR1_ARPE
    STR r0, [r6, #TIM_CR1]
    
    ; Enable counter
    LDR r0, [r6, #TIM_CR1]
    ORR r0, r0, #TIM_CR1_CEN
    STR r0, [r6, #TIM_CR1]
    
    POP {r4, r5, r6, pc}

; Set PWM duty cycle
; r0 = duty cycle (0-100)
timer_pwm_set_duty:
    PUSH {r4, lr}
    
    LDR r4, =TIM2_BASE
    
    ; Read ARR
    LDR r1, [r4, #TIM_ARR]
    ADD r1, r1, #1
    
    ; Calculate new CCR1
    MUL r0, r0, r1
    MOV r1, #100
    UDIV r0, r0, r1
    
    ; Update CCR1
    STR r0, [r4, #TIM_CCR1]
    
    POP {r4, pc}
```

**Input Capture:**

```assembly
; Configure timer for input capture
; Measure pulse width or frequency

timer_input_capture_init:
    PUSH {r4, lr}
    
    ; Enable TIM2 clock
    LDR r0, =RCC_BASE
    LDR r1, [r0, #RCC_APB1ENR]
    ORR r1, r1, #(1 << 0)
    STR r1, [r0, #RCC_APB1ENR]
    
    LDR r4, =TIM2_BASE
    
    ; Set prescaler for desired resolution
    MOV r0, #71                 ; 1MHz (1µs resolution)
    STR r0, [r4, #TIM_PSC]
    
    ; Set ARR to maximum
    LDR r0, =0xFFFF
    STR r0, [r4, #TIM_ARR]
    
    ; Configure channel 1 as input capture
    ; CCMR1: CC1S = 01 (input, IC1 mapped to TI1)
    LDR r0, [r6, #0x18]         ; TIM_CCMR1
    BIC r0, r0, #0x03
    ORR r0, r0, #0x01           ; CC1S = 01
    STR r0, [r6, #0x18]
    
    ; Enable capture on rising edge
    LDR r0, [r4, #0x20]         ; TIM_CCER
    ORR r0, r0, #0x01           ; CC1E
    BIC r0, r0, #0x02           ; CC1P = 0 (rising edge)
    STR r0, [r4, #0x20]
    
    ; Enable capture interrupt
    LDR r0, [r4, #TIM_DIER]
    ORR r0, r0, #(1 << 1)       ; CC1IE
    STR r0, [r4, #TIM_DIER]
    
    ; Enable timer
    LDR r0, [r4, #TIM_CR1]
    ORR r0, r0, #TIM_CR1_CEN
    STR r0, [r4, #TIM_CR1]
    
    POP {r4, pc}

.data
capture_start: .word 0
pulse_width:   .word 0

.text
; Capture interrupt handler
TIM2_Capture_IRQHandler:
    PUSH {r4, lr}
    
    LDR r4, =TIM2_BASE
    
    ; Check CC1 interrupt
    LDR r0, [r4, #TIM_SR]
    TST r0, #(1 << 1)           ; CC1IF
    BEQ capture_done
    
    ; Clear flag
    LDR r0, [r4, #TIM_SR]
    BIC r0, r0, #(1 << 1)
    STR r0, [r4, #TIM_SR]
    
    ; Read captured value
    LDR r0, [r4, #TIM_CCR1]
    
    ; Check if this is start or end of pulse
    LDR r1, =capture_start
    LDR r2, [r1]
    CMP r2, #0
    BEQ capture_rising
    
    ; Falling edge - calculate pulse width
    SUB r3, r0, r2              ; End - start
    LDR r1, =pulse_width
    STR r3, [r1]
    
    ; Reset for next capture
    LDR r1, =capture_start
    MOV r2, #0
    STR r2, [r1]
    
    ; Change to rising edge
    LDR r0, [r4, #0x20]
    BIC r0, r0, #0x02           ; CC1P = 0
    STR r0, [r4, #0x20]
    
    B capture_done
    
capture_rising:
    ; Rising edge - save start time
    LDR r1, =capture_start
    STR r0, [r1]
    
    ; Change to falling edge
    LDR r0, [r4, #0x20]
    ORR r0, r0, #0x02           ; CC1P = 1
    STR r0, [r4, #0x20]
    
capture_done:
    POP {r4, pc}
```

### UART/USART Communication

**UART Initialization:**

```assembly
; Example: STM32 USART1 configuration
.equ USART1_BASE,  0x40013800
.equ USART_SR,     0x00         ; Status register
.equ USART_DR,     0x04         ; Data register
.equ USART_BRR,    0x08         ; Baud rate register
.equ USART_CR1,    0x0C         ; Control register 1
.equ USART_CR2,    0x10         ; Control register 2
.equ USART_CR3,    0x14         ; Control register 3

; USART_SR bits
.equ USART_SR_TXE,  (1 << 7)    ; Transmit data register empty
.equ USART_SR_TC,   (1 << 6)    ; Transmission complete
.equ USART_SR_RXNE, (1 << 5)    ; Read data register not empty
.equ USART_SR_ORE,  (1 << 3)    ; Overrun error
.equ USART_SR_FE,   (1 << 1)    ; Framing error
.equ USART_SR_PE,   (1 << 0)    ; Parity error

; USART_CR1 bits
.equ USART_CR1_UE,     (1 << 13) ; USART enable
.equ USART_CR1_M,      (1 << 12) ; Word length (0=8bit, 1=9bit)
.equ USART_CR1_PCE,    (1 << 10) ; Parity control enable
.equ USART_CR1_PS,     (1 << 9)  ; Parity selection
.equ USART_CR1_TXEIE,  (1 << 7)  ; TXE interrupt enable
.equ USART_CR1_TCIE,   (1 << 6)  ; TC interrupt enable
.equ USART_CR1_RXNEIE, (1 << 5)  ; RXNE interrupt enable
.equ USART_CR1_TE,     (1 << 3)  ; Transmitter enable
.equ USART_CR1_RE,     (1 << 2)  ; Receiver enable

; Initialize UART at 115200 baud, 8N1
; Assumes 72MHz system clock, USART1 on APB2 (72MHz)
uart_init:
    PUSH {r4, lr}
    
    ; Enable USART1 clock
    LDR r0, =RCC_BASE
    LDR r1, [r0, #RCC_APB2ENR]
    ORR r1, r1, #(1 << 14)      ; USART1EN
    STR r1, [r0, #RCC_APB2ENR]
    
    ; Enable GPIOA clock (for TX/RX pins)
    LDR r1, [r0, #RCC_AHBENR]
    ORR r1, r1, #(1 << 17)      ; GPIOAEN
    STR r1, [r0, #RCC_AHBENR]
    
    ; Configure PA9 (TX) and PA10 (RX) as alternate function
    LDR r0, =0x40020000         ; GPIOA_BASE
    
    ; PA9: Alternate function, push-pull, high speed
    LDR r1, [r0, #GPIO_MODER]
    BIC r1, r1, #(0x3 << 18)
    ORR r1, r1, #(0x2 << 18)    ; Alternate function
    STR r1, [r0, #GPIO_MODER]
    
    ; PA10: Alternate function
    LDR r1, [r0, #GPIO_MODER]
    BIC r1, r1, #(0x3 << 20)
    ORR r1, r1, #(0x2 << 20)
    STR r1, [r0, #GPIO_MODER]
    
    ; Set alternate function AF7 (USART1) for PA9 and PA10
    LDR r1, [r0, #GPIO_AFRH]
    BIC r1, r1, #0x00000FF0     ; Clear AF9 and AF10
    ORR r1, r1, #0x00000770     ; Set AF7
    STR r1, [r0, #GPIO_AFRH]
    
    LDR r4, =USART1_BASE
    
    ; Calculate and set baud rate
    ; BRR = fCK / (16 * baud_rate)
    ; BRR = 72000000 / (16 * 115200) = 39.0625
    ; Mantissa = 39, Fraction = 0.0625 * 16 = 1
    ; BRR = (39 << 4) | 1 = 625
    LDR r0, =625
    STR r0, [r4, #USART_BRR]
    
    ; Configure: 8 data bits, 1 stop bit, no parity
    MOV r0, #0
    STR r0, [r4, #USART_CR2]    ; 1 stop bit (default)
    
    ; Enable USART, transmitter, and receiver
    MOV r0, #(USART_CR1_UE | USART_CR1_TE | USART_CR1_RE)
    STR r0, [r4, #USART_CR1]
    
    POP {r4, pc}

; Transmit single byte
; r0 = byte to transmit
uart_putc:
    PUSH {r4, lr}
    
    LDR r4, =USART1_BASE
    MOV r1, r0                  ; Save byte
    
    ; Wait for TXE (transmit data register empty)
wait_txe:
    LDR r0, [r4, #USART_SR]
    TST r0, #USART_SR_TXE
    BEQ wait_txe
    
    ; Write data
    STRB r1, [r4, #USART_DR]
    
    POP {r4, pc}

; Receive single byte
; Returns: r0 = received byte
uart_getc:
    PUSH {r4, lr}
    
    LDR r4, =USART1_BASE
    
    ; Wait for RXNE (receive data register not empty)
wait_rxne:
    LDR r0, [r4, #USART_SR]
    TST r0, #USART_SR_RXNE
    BEQ wait_rxne
    
    ; Read data
    LDRB r0, [r4, #USART_DR]
    
    POP {r4, pc}

; Transmit string
; r0 = pointer to null-terminated string
uart_puts:
    PUSH {r4, r5, lr}
    
    MOV r4, r0                  ; Save string pointer
    
puts_loop:
    LDRB r5, [r4], #1           ; Load byte, increment pointer
    CMP r5, #0                  ; Check for null terminator
    BEQ puts_done
    
    MOV r0, r5
    BL uart_putc
    
    B puts_loop
    
puts_done:
    POP {r4, r5, pc}

; Receive string (until newline or max length)
; r0 = buffer pointer, r1 = max length
; Returns: r0 = actual length
uart_gets:
    PUSH {r4, r5, r6, lr}
    
    MOV r4, r0                  ; Save buffer pointer
    MOV r5, r1                  ; Save max length
    MOV r6, #0                  ; Current length
    
gets_loop:
    CMP r6, r5                  ; Check max length
    BGE gets_done
    
    BL uart_getc                ; Receive byte
    
    CMP r0, #'\r'               ; Check for carriage return
    BEQ gets_done
    CMP r0, #'\n'               ; Check for newline
    BEQ gets_done
    
    STRB r0, [r4], #1           ; Store byte
    ADD r6, r6, #1              ; Increment length
    
    B gets_loop
    
gets_done:
    MOV r0, #0
    STRB r0, [r4]               ; Null terminate
    MOV r0, r6                  ; Return length
    
    POP {r4, r5, r6, pc}
```

**UART with Interrupts and Ring Buffer:**

```assembly
.equ UART_BUFFER_SIZE, 128

.data
.align 2
uart_rx_buffer: .space UART_BUFFER_SIZE
uart_rx_head:   .word 0
uart_rx_tail:   .word 0
uart_rx_count:  .word 0

.text
; Initialize UART with RX interrupt
uart_init_interrupt:
    PUSH {r4, lr}
    
    ; Call basic init
    BL uart_init
    
    ; Enable RXNE interrupt
    LDR r4, =USART1_BASE
    LDR r0, [r4, #USART_CR1]
    ORR r0, r0, #USART_CR1_RXNEIE
    STR r0, [r4, #USART_CR1]
    
    ; Enable USART1 interrupt in NVIC
    LDR r0, =0xE000E100         ; NVIC_ISER1
    MOV r1, #(1 << 5)           ; USART1 is IRQ 37 (bit 5 in ISER1)
    STR r1, [r0, #4]            ; ISER1
    
    ; Initialize buffer pointers
    LDR r0, =uart_rx_head
    MOV r1, #0
    STR r1, [r0]
    LDR r0, =uart_rx_tail
    STR r1, [r0]
    LDR r0, =uart_rx_count
    STR r1, [r0]
    
    POP {r4, pc}

; USART1 interrupt handler
USART1_IRQHandler:
    PUSH {r4, r5, r6, lr}
    
    LDR r4, =USART1_BASE
    
    ; Check RXNE
    LDR r0, [r4, #USART_SR]
    TST r0, #USART_SR_RXNE
    BEQ uart_irq_done
    
    ; Read data (clears RXNE flag)
    LDRB r5, [r4, #USART_DR]
    
    ; Check if buffer is full
    LDR r0, =uart_rx_count
    LDR r1, [r0]
    CMP r1, #UART_BUFFER_SIZE
    BGE uart_irq_done           ; Buffer full, discard byte
    
    ; Store byte in buffer
    LDR r2, =uart_rx_buffer
    LDR r3, =uart_rx_head
    LDR r6, [r3]
    STRB r5, [r2, r6]           ; buffer[head] = byte
    
    ; Increment head (circular)
    ADD r6, r6, #1
    CMP r6, #UART_BUFFER_SIZE
    IT EQ
    MOVEQ r6, #0                ; Wrap around
    STR r6, [r3]
    
    ; Increment count
    ADD r1, r1, #1
    STR r1, [r0]
    
uart_irq_done:
    POP {r4, r5, r6, pc}

; Check if data available in buffer
; Returns: r0 = 1 if data available, 0 otherwise
uart_available:
    LDR r0, =uart_rx_count
    LDR r1, [r0]
    CMP r1, #0
    ITE GT
    MOVGT r0, #1
    MOVLE r0, #0
    BX lr

; Read byte from buffer (non-blocking)
; Returns: r0 = byte, or -1 if no data
uart_read_nonblocking:
    PUSH {r4, r5, lr}
    
    LDR r0, =uart_rx_count
    LDR r1, [r0]
    CMP r1, #0
    BEQ no_data
    
    ; Read from buffer
    LDR r2, =uart_rx_buffer
    LDR r3, =uart_rx_tail
    LDR r4, [r3]
    LDRB r5, [r2, r4]           ; byte = buffer[tail]
    
    ; Increment tail (circular)
    ADD r4, r4, #1
    CMP r4, #UART_BUFFER_SIZE
    IT EQ
    MOVEQ r4, #0
    STR r4, [r3]
    
    ; Decrement count
    SUB r1, r1, #1
    STR r1, [r0]
    
    MOV r0, r5                  ; Return byte
    POP {r4, r5, pc}
    
no_data:
    MOV r0, #-1
    POP {r4, r5, pc}
```

### SPI Communication

**SPI Master Configuration:**

```assembly
; Example: STM32 SPI1 configuration
.equ SPI1_BASE,    0x40013000
.equ SPI_CR1,      0x00         ; Control register 1
.equ SPI_CR2,      0x04         ; Control register 2
.equ SPI_SR,       0x08         ; Status register
.equ SPI_DR,       0x0C         ; Data register

; SPI_CR1 bits
.equ SPI_CR1_BIDIMODE, (1 << 15) ; Bidirectional data mode
.equ SPI_CR1_BIDIOE,   (1 << 14) ; Output enable in bidir mode
.equ SPI_CR1_CRCEN,    (1 << 13) ; CRC enable
.equ SPI_CR1_CRCNEXT,  (1 << 12) ; CRC next
.equ SPI_CR1_DFF,      (1 << 11) ; Data frame format (0=8bit, 1=16bit)
.equ SPI_CR1_RXONLY,   (1 << 10) ; Receive only
.equ SPI_CR1_SSM,      (1 << 9)  ; Software slave management
.equ SPI_CR1_SSI,      (1 << 8)  ; Internal slave select
.equ SPI_CR1_LSBFIRST, (1 << 7)  ; Frame format
.equ SPI_CR1_SPE,      (1 << 6)  ; SPI enable
.equ SPI_CR1_BR,       (7 << 3)  ; Baud rate control (3 bits)
.equ SPI_CR1_MSTR,     (1 << 2)  ; Master selection
.equ SPI_CR1_CPOL,     (1 << 1)  ; Clock polarity
.equ SPI_CR1_CPHA,     (1 << 0)  ; Clock phase

; SPI_SR bits
.equ SPI_SR_BSY,   (1 << 7)     ; Busy flag
.equ SPI_SR_TXE,   (1 << 1)     ; Transmit buffer empty
.equ SPI_SR_RXNE,  (1 << 0)     ; Receive buffer not empty

; Initialize SPI1 as master, mode 0, 1MHz
spi_init:
    PUSH {r4, lr}
    
    ; Enable SPI1 clock
    LDR r0, =RCC_BASE
    LDR r1, [r0, #RCC_APB2ENR]
    ORR r1, r1, #(1 << 12)          ; SPI1EN
    STR r1, [r0, #RCC_APB2ENR]
    
    ; Enable GPIOA clock (for SCK, MISO, MOSI)
    LDR r1, [r0, #RCC_AHBENR]
    ORR r1, r1, #(1 << 17)          ; GPIOAEN
    STR r1, [r0, #RCC_AHBENR]

    ; Configure GPIO pins: PA5=SCK, PA6=MISO, PA7=MOSI
    LDR r0, =0x40020000             ; GPIOA_BASE

    ; Set PA5, PA7 as alternate function, PA6 as input
    LDR r1, [r0, #GPIO_MODER]
    BIC r1, r1, #0x0000FC00         ; Clear bits for PA5-PA7
    ORR r1, r1, #0x0000A800         ; PA5=AF, PA6=AF, PA7=AF
    STR r1, [r0, #GPIO_MODER]

    ; Set alternate function AF5 (SPI1) for PA5-PA7
    LDR r1, [r0, #GPIO_AFRL]
    BIC r1, r1, #0xFFF00000         ; Clear AF5-AF7
    ORR r1, r1, #0x55500000         ; Set AF5
    STR r1, [r0, #GPIO_AFRL]

    ; Set high speed for SPI pins
    LDR r1, [r0, #GPIO_OSPEEDR]
    ORR r1, r1, #0x0000FC00         ; High speed for PA5-PA7
    STR r1, [r0, #GPIO_OSPEEDR]

    LDR r4, =SPI1_BASE

    ; Configure SPI1
    ; Master, software slave management, 8-bit, MSB first
    ; Baud rate = fPCLK/32 (72MHz/32 = 2.25MHz)
    ; Mode 0: CPOL=0, CPHA=0
    MOV r0, #(SPI_CR1_MSTR | SPI_CR1_SSM | SPI_CR1_SSI)
    ORR r0, r0, #(0x04 << 3)        ; BR = 100 (divide by 32)
    STR r0, [r4, #SPI_CR1]

    ; Enable SPI
    LDR r0, [r4, #SPI_CR1]
    ORR r0, r0, #SPI_CR1_SPE
    STR r0, [r4, #SPI_CR1]

    POP {r4, pc}

; Transfer single byte (full duplex)
; r0 = byte to send
; Returns: r0 = byte received
spi_transfer:
    PUSH {r4, lr}

    LDR r4, =SPI1_BASE
    MOV r1, r0                      ; Save byte to send

wait_spi_txe:
    LDR r0, [r4, #SPI_SR]
    TST r0, #SPI_SR_TXE
    BEQ wait_spi_txe

    ; Send byte
    STRB r1, [r4, #SPI_DR]

wait_spi_rxne:
    LDR r0, [r4, #SPI_SR]
    TST r0, #SPI_SR_RXNE
    BEQ wait_spi_rxne

    ; Read received byte
    LDRB r0, [r4, #SPI_DR]

wait_spi_busy:
    LDR r1, [r4, #SPI_SR]
    TST r1, #SPI_SR_BSY
    BNE wait_spi_busy

    POP {r4, pc}

; Send multiple bytes
; r0 = buffer pointer, r1 = count
spi_send_buffer:
    PUSH {r4, r5, r6, lr}

    MOV r4, r0                      ; Save buffer pointer
    MOV r5, r1                      ; Save count
    MOV r6, #0                      ; Index

send_loop:
    CMP r6, r5
    BGE send_done

    LDRB r0, [r4, r6]               ; Load byte
    BL spi_transfer                 ; Send (ignore received data)

    ADD r6, r6, #1
    B send_loop

send_done:
    POP {r4, r5, r6, pc}

; Receive multiple bytes
; r0 = buffer pointer, r1 = count
spi_receive_buffer:
    PUSH {r4, r5, r6, lr}

    MOV r4, r0                      ; Save buffer pointer
    MOV r5, r1                      ; Save count
    MOV r6, #0                      ; Index

receive_loop:
    CMP r6, r5
    BGE receive_done

    MOV r0, #0xFF                   ; Send dummy byte
    BL spi_transfer
    STRB r0, [r4, r6]               ; Store received byte

    ADD r6, r6, #1
    B receive_loop

receive_done:
    POP {r4, r5, r6, pc}

; Example: Read from SPI device with chip select
; r0 = register address, r1 = chip select GPIO base, r2 = CS pin
spi_read_register:
    PUSH {r4, r5, r6, lr}

    MOV r4, r0                      ; Save register address
    MOV r5, r1                      ; Save CS GPIO base
    MOV r6, r2                      ; Save CS pin

; CS low
MOV r0, r5
MOV r1, r6
BL gpio_clear

; Small delay
MOV r0, #1
BL delay_us

; Send read command (address with read bit set)
ORR r0, r4, #0x80           ; Set MSB for read
BL spi_transfer

; Read data
MOV r0, #0xFF               ; Dummy byte
BL spi_transfer
MOV r4, r0                  ; Save received data

; CS high
MOV r0, r5
MOV r1, r6
BL gpio_set

MOV r0, r4                  ; Return data
POP {r4, r5, r6, pc}

; Write to SPI device ; r0 = register address, r1 = data, r2 = CS GPIO base, r3 = CS pin spi_write_register: PUSH {r4, r5, r6, r7, lr}

MOV r4, r0                  ; Save register
MOV r5, r1                  ; Save data
MOV r6, r2                  ; Save CS GPIO
MOV r7, r3                  ; Save CS pin

; CS low
MOV r0, r6
MOV r1, r7
BL gpio_clear

; Small delay
MOV r0, #1
BL delay_us

; Send write command (address without read bit)
MOV r0, r4
BL spi_transfer

; Send data
MOV r0, r5
BL spi_transfer

; CS high
MOV r0, r6
MOV r1, r7
BL gpio_set

POP {r4, r5, r6, r7, pc}
````

### I2C Communication

**I2C Master Configuration:**
```assembly
; Example: STM32 I2C1 configuration
.equ I2C1_BASE,    0x40005400
.equ I2C_CR1,      0x00         ; Control register 1
.equ I2C_CR2,      0x04         ; Control register 2
.equ I2C_OAR1,     0x08         ; Own address register 1
.equ I2C_OAR2,     0x0C         ; Own address register 2
.equ I2C_DR,       0x10         ; Data register
.equ I2C_SR1,      0x14         ; Status register 1
.equ I2C_SR2,      0x18         ; Status register 2
.equ I2C_CCR,      0x1C         ; Clock control register
.equ I2C_TRISE,    0x20         ; Rise time register

; I2C_CR1 bits
.equ I2C_CR1_PE,     (1 << 0)   ; Peripheral enable
.equ I2C_CR1_START,  (1 << 8)   ; Start generation
.equ I2C_CR1_STOP,   (1 << 9)   ; Stop generation
.equ I2C_CR1_ACK,    (1 << 10)  ; Acknowledge enable
.equ I2C_CR1_POS,    (1 << 11)  ; Position
.equ I2C_CR1_SWRST,  (1 << 15)  ; Software reset

; I2C_SR1 bits
.equ I2C_SR1_SB,     (1 << 0)   ; Start bit
.equ I2C_SR1_ADDR,   (1 << 1)   ; Address sent/matched
.equ I2C_SR1_BTF,    (1 << 2)   ; Byte transfer finished
.equ I2C_SR1_TXE,    (1 << 7)   ; Data register empty (transmit)
.equ I2C_SR1_RXNE,   (1 << 6)   ; Data register not empty (receive)

; Initialize I2C1 at 100kHz (standard mode)
i2c_init:
    PUSH {r4, lr}
    
    ; Enable I2C1 clock
    LDR r0, =RCC_BASE
    LDR r1, [r0, #RCC_APB1ENR]
    ORR r1, r1, #(1 << 21)      ; I2C1EN
    STR r1, [r0, #RCC_APB1ENR]
    
    ; Enable GPIOB clock (for SCL, SDA)
    LDR r1, [r0, #RCC_AHBENR]
    ORR r1, r1, #(1 << 18)      ; GPIOBEN
    STR r1, [r0, #RCC_AHBENR]
    
    ; Configure PB6=SCL, PB7=SDA as alternate function, open-drain
    LDR r0, =0x40020400         ; GPIOB_BASE
    
    ; Set as alternate function
    LDR r1, [r0, #GPIO_MODER]
    BIC r1, r1, #0x0000F000     ; Clear PB6-PB7
    ORR r1, r1, #0x0000A000     ; Set alternate function
    STR r1, [r0, #GPIO_MODER]
    
    ; Set alternate function AF4 (I2C1)
    LDR r1, [r0, #GPIO_AFRL]
    BIC r1, r1, #0xFF000000
    ORR r1, r1, #0x44000000     ; AF4
    STR r1, [r0, #GPIO_AFRL]
    
    ; Set as open-drain
    LDR r1, [r0, #GPIO_OTYPER]
    ORR r1, r1, #0x000000C0     ; Open-drain for PB6-PB7
    STR r1, [r0, #GPIO_OTYPER]
    
    ; Set speed to high
    LDR r1, [r0, #GPIO_OSPEEDR]
    ORR r1, r1, #0x0000F000
    STR r1, [r0, #GPIO_OSPEEDR]
    
    ; Pull-up
    LDR r1, [r0, #GPIO_PUPDR]
    BIC r1, r1, #0x0000F000
    ORR r1, r1, #0x00005000     ; Pull-up
    STR r1, [r0, #GPIO_PUPDR]
    
    LDR r4, =I2C1_BASE
    
    ; Reset I2C
    LDR r0, [r4, #I2C_CR1]
    ORR r0, r0, #I2C_CR1_SWRST
    STR r0, [r4, #I2C_CR1]
    BIC r0, r0, #I2C_CR1_SWRST
    STR r0, [r4, #I2C_CR1]
    
    ; Set peripheral clock frequency (36MHz for APB1)
    MOV r0, #36
    STR r0, [r4, #I2C_CR2]
    
    ; Configure clock control for 100kHz
    ; CCR = fPCLK / (2 * fSCL) = 36000000 / (2 * 100000) = 180
    MOV r0, #180
    STR r0, [r4, #I2C_CCR]
    
    ; Configure rise time (max 1000ns for 100kHz)
    ; TRISE = (max_rise_time / tPCLK) + 1 = (1000ns / 27.7ns) + 1 = 37
    MOV r0, #37
    STR r0, [r4, #I2C_TRISE]
    
    ; Enable I2C
    LDR r0, [r4, #I2C_CR1]
    ORR r0, r0, #I2C_CR1_PE
    STR r0, [r4, #I2C_CR1]
    
    POP {r4, pc}

; Generate START condition
i2c_start:
    PUSH {r4, lr}
    
    LDR r4, =I2C1_BASE
    
    ; Generate START
    LDR r0, [r4, #I2C_CR1]
    ORR r0, r0, #I2C_CR1_START
    STR r0, [r4, #I2C_CR1]
    
    ; Wait for SB flag
wait_sb:
    LDR r0, [r4, #I2C_SR1]
    TST r0, #I2C_SR1_SB
    BEQ wait_sb
    
    POP {r4, pc}

; Generate STOP condition
i2c_stop:
    PUSH {r4, lr}
    
    LDR r4, =I2C1_BASE
    
    ; Generate STOP
    LDR r0, [r4, #I2C_CR1]
    ORR r0, r0, #I2C_CR1_STOP
    STR r0, [r4, #I2C_CR1]
    
    POP {r4, pc}

; Send address
; r0 = 7-bit address, r1 = direction (0=write, 1=read)
i2c_send_address:
    PUSH {r4, r5, lr}
    
    LDR r4, =I2C1_BASE
    
    ; Shift address and add direction bit
    LSL r0, r0, #1
    ORR r0, r0, r1
    
    ; Write address to DR
    STRB r0, [r4, #I2C_DR]
    
    ; Wait for ADDR flag
wait_addr:
    LDR r0, [r4, #I2C_SR1]
    TST r0, #I2C_SR1_ADDR
    BEQ wait_addr
    
    ; Clear ADDR by reading SR1 and SR2
    LDR r0, [r4, #I2C_SR1]
    LDR r0, [r4, #I2C_SR2]
    
    POP {r4, r5, pc}

; Write single byte
; r0 = data byte
i2c_write_byte:
    PUSH {r4, lr}
    
    LDR r4, =I2C1_BASE
    MOV r1, r0
    
    ; Wait for TXE
wait_txe:
    LDR r0, [r4, #I2C_SR1]
    TST r0, #I2C_SR1_TXE
    BEQ wait_txe
    
    ; Write data
    STRB r1, [r4, #I2C_DR]
    
    ; Wait for BTF (byte transfer finished)
wait_btf:
    LDR r0, [r4, #I2C_SR1]
    TST r0, #I2C_SR1_BTF
    BEQ wait_btf
    
    POP {r4, pc}

; Read single byte
; Returns: r0 = data byte
i2c_read_byte:
    PUSH {r4, lr}
    
    LDR r4, =I2C1_BASE
    
    ; Enable ACK
    LDR r0, [r4, #I2C_CR1]
    ORR r0, r0, #I2C_CR1_ACK
    STR r0, [r4, #I2C_CR1]
    
    ; Wait for RXNE
wait_rxne:
    LDR r0, [r4, #I2C_SR1]
    TST r0, #I2C_SR1_RXNE
    BEQ wait_rxne
    
    ; Read data
    LDRB r0, [r4, #I2C_DR]
    
    POP {r4, pc}

; Write to I2C device
; r0 = device address (7-bit), r1 = register, r2 = data
i2c_write_register:
    PUSH {r4, r5, r6, lr}
    
    MOV r4, r0                  ; Save address
    MOV r5, r1                  ; Save register
    MOV r6, r2                  ; Save data
    
    ; START
    BL i2c_start
    
    ; Send device address (write)
    MOV r0, r4
    MOV r1, #0                  ; Write direction
    BL i2c_send_address
    
    ; Send register address
    MOV r0, r5
    BL i2c_write_byte
    
    ; Send data
    MOV r0, r6
    BL i2c_write_byte
    
    ; STOP
    BL i2c_stop
    
    POP {r4, r5, r6, pc}

; Read from I2C device
; r0 = device address (7-bit), r1 = register
; Returns: r0 = data
i2c_read_register:
    PUSH {r4, r5, lr}
    
    MOV r4, r0                  ; Save address
    MOV r5, r1                  ; Save register
    
    ; START
    BL i2c_start
    
    ; Send device address (write)
    MOV r0, r4
    MOV r1, #0
    BL i2c_send_address
    
    ; Send register address
    MOV r0, r5
    BL i2c_write_byte
    
    ; Repeated START
    BL i2c_start
    
    ; Send device address (read)
    MOV r0, r4
    MOV r1, #1                  ; Read direction
    BL i2c_send_address
    
    ; Disable ACK before reading last byte
    LDR r1, =I2C1_BASE
    LDR r2, [r1, #I2C_CR1]
    BIC r2, r2, #I2C_CR1_ACK
    STR r2, [r1, #I2C_CR1]
    
    ; Read data
    BL i2c_read_byte
    MOV r4, r0                  ; Save data
    
    ; STOP
    BL i2c_stop
    
    MOV r0, r4                  ; Return data
    POP {r4, r5, pc}
````

## Device Registers

Device registers are memory-mapped locations that control hardware behavior and status.

### Register Types

**Control Registers:**

```assembly
; Configure device behavior
; Example: Timer control register

.equ TIM_CR1_CEN,  (1 << 0)     ; Counter enable
.equ TIM_CR1_UDIS, (1 << 1)     ; Update disable
.equ TIM_CR1_DIR,  (1 << 4)     ; Direction (0=up, 1=down)

timer_configure:
    LDR r0, =TIM2_BASE
    
    ; Set control bits
    MOV r1, #(TIM_CR1_CEN | TIM_CR1_DIR)
    STR r1, [r0, #TIM_CR1]
    
    BX lr
```

**Status Registers:**

```assembly
; Read device status
; Example: UART status

check_uart_status:
    LDR r0, =USART1_BASE
    LDR r1, [r0, #USART_SR]
    
    ; Check specific flags
    TST r1, #USART_SR_TXE       ; Transmit empty?
    BNE tx_ready
    
    TST r1, #USART_SR_ORE       ; Overrun error?
    BNE handle_overrun
    
    BX lr
```

**Data Registers:**

```assembly
; Read/write data
; Example: ADC data register

read_adc:
    LDR r0, =ADC1_BASE
    
    ; Start conversion
    LDR r1, [r0, #ADC_CR2]
    ORR r1, r1, #ADC_CR2_SWSTART
    STR r1, [r0, #ADC_CR2]
    
    ; Wait for conversion complete
wait_adc:
    LDR r1, [r0, #ADC_SR]
    TST r1, #ADC_SR_EOC
    BEQ wait_adc
    
    ; Read data
    LDR r0, [r0, #ADC_DR]
    
    BX lr
```

**Configuration Registers:**

```assembly
; Set device parameters
; Example: ADC configuration

.equ ADC_SMPR2,    0x10         ; Sample time register
.equ ADC_SQR3,     0x34         ; Regular sequence register

adc_configure_channel:
    ; r0 = channel number (0-17)
    PUSH {r4, lr}
    
    LDR r4, =ADC1_BASE
    
    ; Set sample time (55.5 cycles for channel 0)
    LDR r1, [r4, #ADC_SMPR2]
    MOV r2, #0x07               ; Sample time value
    LSL r2, r2, r0              ; Shift to channel position
    ORR r1, r1, r2
    STR r1, [r4, #ADC_SMPR2]
    
    ; Set sequence (channel 0 as first conversion)
    LDR r1, [r4, #ADC_SQR3]
    BIC r1, r1, #0x1F           ; Clear first position
    ORR r1, r1, r0              ; Set channel
    STR r1, [r4, #ADC_SQR3]
    
    POP {r4, pc}
```

### Register Access Patterns

**Read-Modify-Write Pattern:**

```assembly
; Safe modification of register bits
; r0 = register address, r1 = bits to set, r2 = bits to clear

register_modify:
    LDR r3, [r0]                ; Read current value
    BIC r3, r3, r2              ; Clear specified bits
    ORR r3, r3, r1              ; Set specified bits
    STR r3, [r0]                ; Write back
    BX lr
```

**Polling Pattern:**

```assembly
; Wait for flag with timeout
; r0 = register address, r1 = flag mask, r2 = timeout (iterations)
; Returns: r0 = 1 if flag set, 0 if timeout

poll_register:
    PUSH {r4, r5, lr}
    
    MOV r4, #0                  ; Counter
    
poll_loop:
    LDR r5, [r0]                ; Read register
    TST r5, r1                  ; Test flag
    BNE poll_success
    
    ADD r4, r4, #1              ; Increment counter
    CMP r4, r2                  ; Check timeout
    BLT poll_loop
    
    ; Timeout
    MOV r0, #0
    POP {r4, r5, pc}
    
poll_success:
    MOV r0, #1
    POP {r4, r5, pc}
```

**Atomic Bit Set/Clear:**

```assembly
; Using bit-set/bit-reset registers (if available)
; Example: GPIO BSRR register

; Set bit atomically
gpio_atomic_set:
    ; r0 = GPIO base, r1 = pin number
    MOV r2, #1
    LSL r2, r2, r1              ; Create mask
    STR r2, [r0, #GPIO_BSRR]    ; Write to BSRR (lower 16 bits set)
    BX lr

; Clear bit atomically
gpio_atomic_clear:
    MOV r2, #1
    LSL r2, r2, r1
    LSL r2, r2, #16             ; Upper 16 bits reset
    STR r2, [r0, #GPIO_BSRR]
    BX lr
```

### Interrupt-Driven I/O

**Interrupt Service Routine Template:**

```assembly
; Generic ISR pattern
device_ISR:
    PUSH {r4, r5, lr}
    
    ; Read status register
    LDR r4, =DEVICE_BASE
    LDR r5, [r4, #DEVICE_SR]
    
    ; Check interrupt source
    TST r5, #DEVICE_FLAG1
    BNE handle_interrupt1
    
    TST r5, #DEVICE_FLAG2
    BNE handle_interrupt2
    
    B isr_done
    
handle_interrupt1:
    ; Clear flag
    MOV r0, #DEVICE_FLAG1
    STR r0, [r4, #DEVICE_SR]
    
    ; Handle interrupt
    ; ... processing ...
    
    B isr_done
    
handle_interrupt2:
    ; Clear flag
    MOV r0, #DEVICE_FLAG2
    STR r0, [r4, #DEVICE_SR]
    
    ; Handle interrupt
    ; ... processing ...
    
isr_done:
    POP {r4, r5, pc}
```

**Important related topics:** ADC and DAC programming, PWM for motor control, watchdog timer configuration, real-time clock (RTC) interfacing, flash memory programming, power management and low-power modes, interrupt priority and nesting, DMA chaining and scatter-gather, bus protocols (CAN, USB, Ethernet)

---

## Bit-banging Protocols

Bit-banging implements communication protocols through direct GPIO (General Purpose Input/Output) manipulation in software, without dedicated hardware peripherals. This technique provides maximum flexibility for custom or non-standard protocols at the cost of CPU cycles and timing precision requirements.

**GPIO Memory-Mapped Registers:**

GPIO pins are controlled through memory-mapped registers. Typical register layout (implementation-specific):

```
Base Address: 0x20200000 (example for Broadcom BCM2835)

Offset   Register              Function
0x00     GPFSEL0-5            Function select (input/output/alternate)
0x1C     GPSET0-1             Set pins high
0x28     GPCLR0-1             Clear pins low
0x34     GPLEV0-1             Read pin levels
0x40     GPEDS0-1             Event detect status
0x4C     GPREN0-1             Rising edge detect enable
0x58     GPFEN0-1             Falling edge detect enable
0x94     GPPUD                Pull-up/down control
0x98     GPPUDCLK0-1          Pull-up/down clock
```

**Basic GPIO Operations:**

```assembly
; GPIO base address
.equ GPIO_BASE, 0x20200000

; Configure GPIO pin as output
; R0 = pin number (0-53)
gpio_set_output:
    LDR     R1, =GPIO_BASE
    
    ; Calculate GPFSEL register offset (pin/10)
    MOV     R2, R0
    MOV     R3, #10
    UDIV    R4, R2, R3           ; R4 = register index
    MLS     R2, R4, R3, R2       ; R2 = pin % 10 (bit position)
    
    ; Read current GPFSEL register
    LDR     R5, [R1, R4, LSL #2]
    
    ; Clear 3 bits for this pin
    MOV     R3, #7
    LSL     R3, R3, R2
    BIC     R5, R5, R3
    
    ; Set to output (001)
    MOV     R3, #1
    LSL     R3, R3, R2
    ORR     R5, R5, R3
    
    ; Write back
    STR     R5, [R1, R4, LSL #2]
    BX      LR

; Set GPIO pin high
; R0 = pin number
gpio_set_high:
    LDR     R1, =GPIO_BASE
    ADD     R1, R1, #0x1C        ; GPSET0 offset
    
    CMP     R0, #32
    ADDGE   R1, R1, #4           ; Use GPSET1 if pin >= 32
    SUBGE   R0, R0, #32
    
    MOV     R2, #1
    LSL     R2, R2, R0           ; Create bit mask
    STR     R2, [R1]             ; Write to set register
    BX      LR

; Set GPIO pin low
; R0 = pin number
gpio_set_low:
    LDR     R1, =GPIO_BASE
    ADD     R1, R1, #0x28        ; GPCLR0 offset
    
    CMP     R0, #32
    ADDGE   R1, R1, #4           ; Use GPCLR1 if pin >= 32
    SUBGE   R0, R0, #32
    
    MOV     R2, #1
    LSL     R2, R2, R0
    STR     R2, [R1]
    BX      LR

; Read GPIO pin state
; R0 = pin number
; Returns: R0 = 0 or 1
gpio_read:
    LDR     R1, =GPIO_BASE
    ADD     R1, R1, #0x34        ; GPLEV0 offset
    
    CMP     R0, #32
    ADDGE   R1, R1, #4           ; Use GPLEV1 if pin >= 32
    SUBGE   R0, R0, #32
    
    LDR     R2, [R1]
    LSR     R2, R2, R0           ; Shift to LSB
    AND     R0, R2, #1           ; Mask bit
    BX      LR
```

**Software Delay Implementation:**

Accurate timing requires calibrated delays. Simple loop-based delays depend on CPU frequency:

```assembly
; Delay in microseconds
; R0 = microseconds to delay
; Assumes CPU frequency known (e.g., 1GHz = 1000 cycles/μs)
; [Inference] Actual cycle count varies by implementation
delay_us:
    PUSH    {R1, R2}
    LDR     R1, =CYCLES_PER_US   ; Implementation-specific constant
    MUL     R2, R0, R1           ; Total cycles
delay_loop:
    SUBS    R2, R2, #3           ; ~3 cycles per iteration (approximate)
    BGT     delay_loop
    POP     {R1, R2}
    BX      LR

.equ CYCLES_PER_US, 1000         ; Adjust based on actual CPU frequency
```

**I²C Bit-bang Implementation:**

I²C uses two wires: SDA (data) and SCL (clock). Both are open-drain with pull-up resistors.

```assembly
.equ I2C_SDA_PIN, 2
.equ I2C_SCL_PIN, 3
.equ I2C_DELAY, 5                ; Microseconds (for 100kHz bus)

; Initialize I²C pins as inputs (high impedance = high with pull-ups)
i2c_init:
    MOV     R0, #I2C_SDA_PIN
    BL      gpio_set_input
    MOV     R0, #I2C_SCL_PIN
    BL      gpio_set_input
    BX      LR

; I²C start condition: SDA high→low while SCL high
i2c_start:
    PUSH    {LR}
    
    ; Ensure both lines high
    MOV     R0, #I2C_SDA_PIN
    BL      gpio_set_input       ; Release SDA (goes high)
    MOV     R0, #I2C_SCL_PIN
    BL      gpio_set_input       ; Release SCL (goes high)
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    ; Pull SDA low while SCL high (START condition)
    MOV     R0, #I2C_SDA_PIN
    BL      gpio_set_output
    BL      gpio_set_low
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    ; Pull SCL low
    MOV     R0, #I2C_SCL_PIN
    BL      gpio_set_output
    BL      gpio_set_low
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    POP     {PC}

; I²C stop condition: SDA low→high while SCL high
i2c_stop:
    PUSH    {LR}
    
    ; Ensure SDA is low
    MOV     R0, #I2C_SDA_PIN
    BL      gpio_set_output
    BL      gpio_set_low
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    ; Release SCL (goes high)
    MOV     R0, #I2C_SCL_PIN
    BL      gpio_set_input
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    ; Release SDA while SCL high (STOP condition)
    MOV     R0, #I2C_SDA_PIN
    BL      gpio_set_input
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    POP     {PC}

; Write one bit to I²C bus
; R0 = bit value (0 or 1)
i2c_write_bit:
    PUSH    {R4, LR}
    MOV     R4, R0               ; Save bit value
    
    ; Set SDA according to bit value
    MOV     R0, #I2C_SDA_PIN
    CMP     R4, #0
    BEQ     write_bit_low
    
write_bit_high:
    BL      gpio_set_input       ; Release SDA (high)
    B       write_bit_clock
    
write_bit_low:
    BL      gpio_set_output
    BL      gpio_set_low
    
write_bit_clock:
    ; Clock pulse
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    MOV     R0, #I2C_SCL_PIN
    BL      gpio_set_input       ; Release SCL (clock high)
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    MOV     R0, #I2C_SCL_PIN
    BL      gpio_set_output
    BL      gpio_set_low         ; Pull SCL low
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    POP     {R4, PC}

; Read one bit from I²C bus
; Returns: R0 = bit value
i2c_read_bit:
    PUSH    {R4, LR}
    
    ; Release SDA (allow slave to drive)
    MOV     R0, #I2C_SDA_PIN
    BL      gpio_set_input
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    ; Clock high
    MOV     R0, #I2C_SCL_PIN
    BL      gpio_set_input
    MOV     R0, #I2C_DELAY
    BL      delay_us
    
    ; Read SDA
    MOV     R0, #I2C_SDA_PIN
    BL      gpio_read
    MOV     R4, R0               ; Save bit value
    
    ; Clock low
    MOV     R0, #I2C_DELAY
    BL      delay_us
    MOV     R0, #I2C_SCL_PIN
    BL      gpio_set_output
    BL      gpio_set_low
    
    MOV     R0, R4               ; Return bit value
    POP     {R4, PC}

; Write one byte to I²C bus
; R0 = byte to write
; Returns: R0 = ACK bit (0=ACK, 1=NACK)
i2c_write_byte:
    PUSH    {R4, R5, LR}
    MOV     R4, R0               ; Save byte
    MOV     R5, #8               ; Bit counter
    
write_byte_loop:
    ; Extract MSB
    MOV     R0, R4, LSR #7
    AND     R0, R0, #1
    BL      i2c_write_bit
    
    LSL     R4, R4, #1           ; Shift to next bit
    SUBS    R5, R5, #1
    BNE     write_byte_loop
    
    ; Read ACK bit
    BL      i2c_read_bit
    
    POP     {R4, R5, PC}

; Read one byte from I²C bus
; R0 = send ACK (0) or NACK (1)
; Returns: R0 = byte read
i2c_read_byte:
    PUSH    {R4, R5, R6, LR}
    MOV     R6, R0               ; Save ACK/NACK flag
    MOV     R4, #0               ; Accumulator
    MOV     R5, #8               ; Bit counter
    
read_byte_loop:
    BL      i2c_read_bit
    LSL     R4, R4, #1           ; Shift left
    ORR     R4, R4, R0           ; Add new bit
    SUBS    R5, R5, #1
    BNE     read_byte_loop
    
    ; Send ACK/NACK
    MOV     R0, R6
    BL      i2c_write_bit
    
    MOV     R0, R4               ; Return byte
    POP     {R4, R5, R6, PC}

; Complete I²C transaction example: write to device
; R0 = device address (7-bit)
; R1 = register address
; R2 = data byte
i2c_write_register:
    PUSH    {R4, R5, R6, LR}
    MOV     R4, R0
    MOV     R5, R1
    MOV     R6, R2
    
    ; Start condition
    BL      i2c_start
    
    ; Send device address with write bit (0)
    LSL     R0, R4, #1           ; Address << 1
    BIC     R0, R0, #1           ; Clear bit 0 (write)
    BL      i2c_write_byte
    CMP     R0, #0               ; Check ACK
    BNE     i2c_error
    
    ; Send register address
    MOV     R0, R5
    BL      i2c_write_byte
    CMP     R0, #0
    BNE     i2c_error
    
    ; Send data byte
    MOV     R0, R6
    BL      i2c_write_byte
    CMP     R0, #0
    BNE     i2c_error
    
    ; Stop condition
    BL      i2c_stop
    
    MOV     R0, #0               ; Success
    POP     {R4, R5, R6, PC}
    
i2c_error:
    BL      i2c_stop
    MOV     R0, #-1              ; Error
    POP     {R4, R5, R6, PC}
```

**SPI Bit-bang Implementation:**

SPI uses four signals: MOSI (Master Out Slave In), MISO (Master In Slave Out), SCK (clock), and CS (chip select).

```assembly
.equ SPI_MOSI_PIN, 10
.equ SPI_MISO_PIN, 9
.equ SPI_SCK_PIN, 11
.equ SPI_CS_PIN, 8
.equ SPI_DELAY, 1                ; Microseconds

; Initialize SPI pins
spi_init:
    PUSH    {LR}
    
    MOV     R0, #SPI_MOSI_PIN
    BL      gpio_set_output
    MOV     R0, #SPI_SCK_PIN
    BL      gpio_set_output
    MOV     R0, #SPI_CS_PIN
    BL      gpio_set_output
    MOV     R0, #SPI_MISO_PIN
    BL      gpio_set_input
    
    ; CS high (inactive), SCK low
    MOV     R0, #SPI_CS_PIN
    BL      gpio_set_high
    MOV     R0, #SPI_SCK_PIN
    BL      gpio_set_low
    
    POP     {PC}

; SPI transfer byte (full duplex)
; R0 = byte to send
; Returns: R0 = byte received
spi_transfer:
    PUSH    {R4, R5, R6, LR}
    MOV     R4, R0               ; Byte to send
    MOV     R5, #0               ; Byte received
    MOV     R6, #8               ; Bit counter
    
spi_transfer_loop:
    ; Write MOSI bit (MSB first)
    MOV     R0, R4, LSR #7
    AND     R0, R0, #1
    
    CMP     R0, #0
    MOV     R0, #SPI_MOSI_PIN
    BEQ     spi_mosi_low
    BL      gpio_set_high
    B       spi_clock_pulse
    
spi_mosi_low:
    BL      gpio_set_low
    
spi_clock_pulse:
    ; Small delay
    MOV     R0, #SPI_DELAY
    BL      delay_us
    
    ; Clock high
    MOV     R0, #SPI_SCK_PIN
    BL      gpio_set_high
    MOV     R0, #SPI_DELAY
    BL      delay_us
    
    ; Read MISO bit
    MOV     R0, #SPI_MISO_PIN
    BL      gpio_read
    LSL     R5, R5, #1           ; Shift received byte
    ORR     R5, R5, R0           ; Add new bit
    
    ; Clock low
    MOV     R0, #SPI_SCK_PIN
    BL      gpio_set_low
    
    ; Next bit
    LSL     R4, R4, #1
    SUBS    R6, R6, #1
    BNE     spi_transfer_loop
    
    MOV     R0, R5               ; Return received byte
    POP     {R4, R5, R6, PC}

; SPI transaction with CS control
; R0 = pointer to TX buffer
; R1 = pointer to RX buffer
; R2 = length
spi_transaction:
    PUSH    {R4, R5, R6, R7, LR}
    MOV     R4, R0               ; TX buffer
    MOV     R5, R1               ; RX buffer
    MOV     R6, R2               ; Length
    
    ; Assert CS (low)
    MOV     R0, #SPI_CS_PIN
    BL      gpio_set_low
    MOV     R0, #SPI_DELAY
    BL      delay_us
    
spi_trans_loop:
    ; Load byte to send
    LDRB    R0, [R4], #1
    BL      spi_transfer
    
    ; Store received byte
    STRB    R0, [R5], #1
    
    SUBS    R6, R6, #1
    BNE     spi_trans_loop
    
    ; Deassert CS (high)
    MOV     R0, #SPI_DELAY
    BL      delay_us
    MOV     R0, #SPI_CS_PIN
    BL      gpio_set_high
    
    POP     {R4, R5, R6, R7, PC}
```

**1-Wire Bit-bang Implementation:**

1-Wire protocol uses a single bidirectional data line with specific timing requirements.

```assembly
.equ ONEWIRE_PIN, 4

; 1-Wire reset pulse
; Returns: R0 = 0 if device present, 1 if no device
onewire_reset:
    PUSH    {R4, LR}
    
    ; Pull line low for 480μs
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_set_output
    BL      gpio_set_low
    MOV     R0, #480
    BL      delay_us
    
    ; Release line and wait 70μs
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_set_input
    MOV     R0, #70
    BL      delay_us
    
    ; Read presence pulse
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_read
    MOV     R4, R0
    
    ; Wait for line to go high
    MOV     R0, #410
    BL      delay_us
    
    MOV     R0, R4
    EOR     R0, R0, #1           ; Invert (0=present, 1=not present)
    POP     {R4, PC}

; Write 1-Wire bit
; R0 = bit value
onewire_write_bit:
    PUSH    {R4, LR}
    MOV     R4, R0
    
    ; Pull line low
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_set_output
    BL      gpio_set_low
    
    CMP     R4, #0
    BEQ     onewire_write_0
    
    ; Write 1: release after 6μs
    MOV     R0, #6
    BL      delay_us
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_set_input
    MOV     R0, #64
    BL      delay_us
    B       onewire_write_done
    
onewire_write_0:
    ; Write 0: keep low for 60μs
    MOV     R0, #60
    BL      delay_us
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_set_input
    MOV     R0, #10
    BL      delay_us
    
onewire_write_done:
    POP     {R4, PC}

; Read 1-Wire bit
; Returns: R0 = bit value
onewire_read_bit:
    PUSH    {R4, LR}
    
    ; Pull line low for 3μs
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_set_output
    BL      gpio_set_low
    MOV     R0, #3
    BL      delay_us
    
    ; Release and wait 10μs
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_set_input
    MOV     R0, #10
    BL      delay_us
    
    ; Read bit
    MOV     R0, #ONEWIRE_PIN
    BL      gpio_read
    MOV     R4, R0
    
    ; Wait for slot end
    MOV     R0, #53
    BL      delay_us
    
    MOV     R0, R4
    POP     {R4, PC}
```

**Timing Accuracy Considerations:**

Bit-banging requires precise timing. [Inference] Factors affecting accuracy include:

- CPU frequency variation
- Interrupt latency (interrupts must be disabled during critical timing)
- Cache misses and memory access timing
- Pipeline stalls

**Example** with interrupt protection:

```assembly
i2c_write_byte_protected:
    PUSH    {R4, LR}
    MOV     R4, R0
    
    ; Disable interrupts
    CPSID   i                    ; Disable IRQ and FIQ
    
    ; Perform timing-critical operation
    MOV     R0, R4
    BL      i2c_write_byte_internal
    
    ; Re-enable interrupts
    CPSIE   i
    
    POP     {R4, PC}
```

## Timer Programming

Hardware timers provide precise time measurement and event generation without CPU intervention. ARM-based systems typically include multiple timer peripherals with varying capabilities.

**ARM Generic Timer (ARMv7-A/ARMv8):**

The ARM Generic Timer provides system-wide timekeeping with both physical and virtual counters.

**Generic Timer Registers:**

- **CNTFRQ:** Counter frequency (Hz)
- **CNTPCT:** Physical count value (64-bit)
- **CNTVCT:** Virtual count value (64-bit)
- **CNTP_CTL:** Physical timer control
- **CNTP_CVAL:** Physical timer compare value
- **CNTP_TVAL:** Physical timer interval value
- **CNTV_CTL:** Virtual timer control
- **CNTV_CVAL:** Virtual timer compare value
- **CNTV_TVAL:** Virtual timer interval value

**Accessing Generic Timer:**

```assembly
; Read counter frequency
MRC     p15, 0, R0, c14, c0, 0       ; Read CNTFRQ

; Read physical counter (64-bit)
MRRC    p15, 0, R0, R1, c14          ; Read CNTPCT (R0=low, R1=high)

; Read virtual counter (64-bit)
MRRC    p15, 1, R0, R1, c14          ; Read CNTVCT

; Set physical timer compare value
MCRR    p15, 2, R0, R1, c14          ; Write CNTP_CVAL

; Set physical timer interval
MCR     p15, 0, R0, c14, c2, 0       ; Write CNTP_TVAL

; Enable physical timer
MRC     p15, 0, R0, c14, c2, 1       ; Read CNTP_CTL
ORR     R0, R0, #1                   ; Set enable bit
MCR     p15, 0, R0, c14, c2, 1       ; Write CNTP_CTL
```

**Timer Interrupt Setup Example:**

```assembly
; Configure physical timer for 1-second interval
setup_timer_1sec:
    PUSH    {R4, R5, LR}
    
    ; Read counter frequency
    MRC     p15, 0, R4, c14, c0, 0   ; CNTFRQ
    
    ; Set timer interval (frequency = ticks per second)
    MCR     p15, 0, R4, c14, c2, 0   ; Write CNTP_TVAL
    
    ; Enable timer and interrupt
    MOV     R0, #3                   ; Enable + interrupt enable
    MCR     p15, 0, R0, c14, c2, 1   ; Write CNTP_CTL
    
    ; Configure GIC for timer interrupt (implementation-specific)
    ; Timer IRQ typically ID 30 for physical timer
    MOV     R0, #30
    BL      gic_enable_interrupt
    
    POP     {R4, R5, PC}

; Timer interrupt handler
timer_irq_handler:
    PUSH    {R0-R3, LR}
    
    ; Read and acknowledge timer status
    MRC     p15, 0, R0, c14, c2, 1   ; Read CNTP_CTL
    ORR     R0, R0, #2               ; Clear ISTATUS
    MCR     p15, 0, R0, c14, c2, 1   ; Write CNTP_CTL
    
    ; Reload timer for next interrupt
    MRC     p15, 0, R1, c14, c0, 0   ; Read CNTFRQ
    MCR     p15, 0, R1, c14, c2, 0   ; Write CNTP_TVAL
    
    ; User timer handler code here
    BL      timer_callback
    
    POP     {R0-R3, PC}
```

**Peripheral Timers (Example: ARM SP804 Dual Timer):**

Many ARM SoCs include peripheral timers with memory-mapped registers.

```assembly
; SP804 Timer register offsets
.equ TIMER_BASE, 0x10011000          ; Example base address
.equ TIMER1_LOAD, 0x00               ; Load register
.equ TIMER1_VALUE, 0x04              ; Current value (read-only)
.equ TIMER1_CONTROL, 0x08            ; Control register
.equ TIMER1_INTCLR, 0x0C             ; Interrupt clear
.equ TIMER1_RIS, 0x10                ; Raw interrupt status
.equ TIMER1_MIS, 0x14                ; Masked interrupt status
.equ TIMER1_BGLOAD, 0x18             ; Background load

; Timer control register bits
.equ TIMER_CTRL_ONESHOT, (0 << 0)    ; One-shot mode
.equ TIMER_CTRL_PERIODIC, (1 << 0)   ; Periodic mode (wrapping)
.equ TIMER_CTRL_32BIT, (1 << 1)      ; 32-bit counter
.equ TIMER_CTRL_PRESCALE_1, (0 << 2) ; No prescale
.equ TIMER_CTRL_PRESCALE_16, (1 << 2)
.equ TIMER_CTRL_PRESCALE_256, (2 << 2)
.equ TIMER_CTRL_INTEN, (1 << 5)      ; Interrupt enable
.equ TIMER_CTRL_ENABLE, (1 << 7)     ; Timer enable

; Initialize timer for periodic interrupts
; R0 = reload value
; R1 = prescaler (0=1, 1=16, 2=256)
timer_init_periodic:
    PUSH    {R4, R5, LR}
    MOV     R4, R0
    MOV     R5, R1
    
    LDR     R0, =TIMER_BASE
    
    ; Disable timer first
    MOV     R1, #0
    STR     R1, [R0, #TIMER1_CONTROL]
    
    ; Set load value
    STR     R4, [R0, #TIMER1_LOAD]
    
    ; Configure control register
    MOV     R1, #TIMER_CTRL_PERIODIC
    ORR     R1, R1, #TIMER_CTRL_32BIT
    ORR     R1, R1, #TIMER_CTRL_INTEN
    ORR     R1, R1, #TIMER_CTRL_ENABLE
    
    ; Add prescaler
    LSL     R5, R5, #2
    ORR     R1, R1, R5
    
    STR     R1, [R0, #TIMER1_CONTROL]
    
    POP     {R4, R5, PC}

; Timer interrupt handler
timer_sp804_handler:
    PUSH    {R0-R1, LR}
    
    ; Clear interrupt
    LDR     R0, =TIMER_BASE
    MOV     R1, #1
    STR     R1, [R0, #TIMER1_INTCLR]
    
    ; User handler
    BL      timer_callback
    
    POP     {R0-R1, PC}

; Read current timer value
; Returns: R0 = current count
timer_read:
    LDR     R0, =TIMER_BASE
    LDR     R0, [R0, #TIMER1_VALUE]
    BX      LR

; Stop timer
timer_stop:
    LDR     R0, =TIMER_BASE
    MOV     R1, #0
    STR     R1, [R0, #TIMER1_CONTROL]
    BX      LR
```

**Watchdog Timer Programming:**

Watchdog timers reset the system if not periodically refreshed, detecting software hangs.

```assembly
; Watchdog register offsets (example)
.equ    WDT_BASE,         0x10010000
.equ    WDT_LOAD,         0x00            ; Load/counter value
.equ    WDT_VALUE,        0x04            ; Current value
.equ    WDT_CONTROL,      0x08            ; Control register
.equ    WDT_INTCLR,       0x0C            ; Interrupt clear
.equ    WDT_LOCK,         0xC00           ; Lock register

.equ    WDT_UNLOCK_VALUE, 0x1ACCE551      ; Unlock value


; Initialize watchdog
; R0 = timeout value
watchdog_init:
    PUSH    {R4, LR}
    MOV     R4, R0

    LDR     R0, =WDT_BASE

    ; Unlock watchdog
    LDR     R1, =WDT_UNLOCK_VALUE
    STR     R1, [R0, #WDT_LOCK]

    ; Set load value
    STR     R4, [R0, #WDT_LOAD]

    ; Enable watchdog with interrupt then reset
    MOV     R1, #3                       ; Interrupt enable + reset enable
    STR     R1, [R0, #WDT_CONTROL]

    ; Lock watchdog
    MOV     R1, #0
    STR     R1, [R0, #WDT_LOCK]

    POP     {R4, PC}


; Refresh/kick watchdog (prevent reset)
watchdog_kick:
    LDR     R0, =WDT_BASE

    ; Unlock
    LDR     R1, =WDT_UNLOCK_VALUE
    STR     R1, [R0, #WDT_LOCK]

    ; Reload counter (write any value to LOAD)
    LDR     R1, [R0, #WDT_LOAD]
    STR     R1, [R0, #WDT_LOAD]

    ; Lock
    MOV     R1, #0
    STR     R1, [R0, #WDT_LOCK]

    BX      LR


; Clear watchdog interrupt
watchdog_clear_interrupt:
    LDR     R0, =WDT_BASE
    MOV     R1, #1
    STR     R1, [R0, #WDT_INTCLR]
    BX      LR

````

**PWM Generation with Timers:**

Pulse Width Modulation can be generated using timer compare/match functionality.

```assembly
; Software PWM using timer interrupt
; Variables in memory
.data
pwm_period:     .word 1000           ; Timer ticks per PWM cycle
pwm_duty:       .word 500            ; Timer ticks for high state
pwm_counter:    .word 0              ; Current position in cycle
pwm_state:      .word 0              ; Current output state
pwm_pin:        .word 12             ; GPIO pin number

.text
; Initialize PWM
; R0 = period (timer ticks)
; R1 = duty cycle (timer ticks)
; R2 = GPIO pin
pwm_init:
    PUSH    {LR}
    
    ; Store parameters
    LDR     R3, =pwm_period
    STR     R0, [R3]
    LDR     R3, =pwm_duty
    STR     R1, [R3]
    LDR     R3, =pwm_pin
    STR     R2, [R3]
    
    ; Configure GPIO as output
    MOV     R0, R2
    BL      gpio_set_output
    
    ; Initialize timer for fast periodic interrupt
    MOV     R0, #100                 ; Timer interval (adjust for resolution)
    BL      timer_init_periodic
    
    POP     {PC}

; PWM timer interrupt handler
pwm_timer_handler:
    PUSH    {R0-R4, LR}
    
    ; Clear timer interrupt
    BL      timer_clear_interrupt
    
    ; Load PWM parameters
    LDR     R0, =pwm_counter
    LDR     R1, [R0]                 ; Current counter
    LDR     R2, =pwm_duty
    LDR     R2, [R2]                 ; Duty cycle value
    LDR     R3, =pwm_period
    LDR     R3, [R3]                 ; Period value
    
    ; Increment counter
    ADD     R1, R1, #1
    CMP     R1, R3
    MOVGE   R1, #0                   ; Wrap at period
    STR     R1, [R0]
    
    ; Determine output state
    CMP     R1, R2
    MOVLT   R4, #1                   ; Counter < duty: output high
    MOVGE   R4, #0                   ; Counter >= duty: output low
    
    ; Update GPIO
    LDR     R0, =pwm_pin
    LDR     R0, [R0]
    CMP     R4, #0
    BEQ     pwm_set_low
    
    BL      gpio_set_high
    B       pwm_done
    
pwm_set_low:
    BL      gpio_set_low
    
pwm_done:
    POP     {R0-R4, PC}

; Update PWM duty cycle
; R0 = new duty cycle
pwm_set_duty:
    LDR     R1, =pwm_duty
    STR     R0, [R1]
    BX      LR
````

**High-Resolution Timing:**

Using counters for precise time measurement:

```assembly
; Measure execution time using Generic Timer
; R0 = pointer to function
; Returns: R0 = elapsed cycles (low 32 bits)
measure_cycles:
    PUSH    {R4, R5, R6, LR}
    MOV     R6, R0                   ; Save function pointer
    
    ; Read start time
    MRRC    p15, 0, R4, R5, c14      ; Read CNTPCT (64-bit)
    
    ; Execute function
    BLX     R6
    
    ; Read end time
    MRRC    p15, 0, R0, R1, c14      ; Read CNTPCT
    
    ; Calculate difference (low 32 bits)
    SUB     R0, R0, R4
    
    POP     {R4, R5, R6, PC}

; Precise delay using counter
; R0 = microseconds to delay
precise_delay_us:
    PUSH    {R4, R5, R6, R7}
    
    ; Read counter frequency
    MRC     p15, 0, R4, c14, c0, 0   ; CNTFRQ
    
    ; Calculate target cycles: (us * freq) / 1000000
    MUL     R5, R0, R4
    LDR     R6, =1000000
    UDIV    R5, R5, R6
    
    ; Read start count
    MRRC    p15, 0, R0, R1, c14      ; CNTPCT
    ADD     R5, R5, R0               ; Target = start + delta
    
delay_loop:
    MRRC    p15, 0, R6, R7, c14      ; Read current count
    CMP     R6, R5
    BLT     delay_loop
    
    POP     {R4, R5, R6, R7}
    BX      LR
```

## UART Communication

Universal Asynchronous Receiver/Transmitter provides serial communication. UART implementations vary, but the PrimeCell UART (PL011) is common in ARM systems.

**PL011 UART Registers:**

```assembly
; UART register offsets from base address
.equ UART_BASE, 0x10009000           ; Example base address

.equ UART_DR, 0x00                   ; Data register
.equ UART_RSR_ECR, 0x04              ; Receive status/error clear
.equ UART_FR, 0x18                   ; Flag register
.equ UART_ILPR, 0x20                 ; IrDA low-power counter
.equ UART_IBRD, 0x24                 ; Integer baud rate divisor
.equ UART_FBRD, 0x28                 ; Fractional baud rate divisor
.equ UART_LCRH, 0x2C                 ; Line control register
.equ UART_CR, 0x30                   ; Control register
.equ UART_IFLS, 0x34                 ; Interrupt FIFO level select
.equ UART_IMSC, 0x38                 ; Interrupt mask set/clear
.equ UART_RIS, 0x3C                  ; Raw interrupt status
.equ UART_MIS, 0x40                  ; Masked interrupt status
.equ UART_ICR, 0x44                  ; Interrupt clear
.equ UART_DMACR, 0x48                ; DMA control

; Flag register bits
.equ UART_FR_TXFF, (1 << 5)          ; Transmit FIFO full
.equ UART_FR_RXFE, (1 << 4)          ; Receive FIFO empty
.equ UART_FR_BUSY, (1 << 3)          ; UART busy
.equ UART_FR_TXFE, (1 << 7)          ; Transmit FIFO empty

; Control register bits
.equ UART_CR_UARTEN, (1 << 0)        ; UART enable
.equ UART_CR_TXE, (1 << 8)           ; Transmit enable
.equ UART_CR_RXE, (1 << 9)           ; Receive enable

; Line control register bits
.equ UART_LCRH_WLEN_8, (3 << 5)      ; 8-bit word length
.equ UART_LCRH_WLEN_7, (2 << 5)      ; 7-bit word length
.equ UART_LCRH_FEN, (1 << 4)         ; FIFO enable
.equ UART_LCRH_STP2, (1 << 3)        ; 2 stop bits
.equ UART_LCRH_EPS, (1 << 2)         ; Even parity select
.equ UART_LCRH_PEN, (1 << 1)         ; Parity enable

; Interrupt bits
.equ UART_INT_OE, (1 << 10)          ; Overrun error
.equ UART_INT_BE, (1 << 9)           ; Break error
.equ UART_INT_PE, (1 << 8)           ; Parity error
.equ UART_INT_FE, (1 << 7)           ; Framing error
.equ UART_INT_RT, (1 << 6)           ; Receive timeout
.equ UART_INT_TX, (1 << 5)           ; Transmit
.equ UART_INT_RX, (1 << 4)           ; Receive
```

**UART Initialization:**

Baud rate calculation: `BAUDDIV = (UARTCLK) / (16 × Baud rate)`

Integer part: `IBRD = integer(BAUDDIV)`

Fractional part: `FBRD = integer((BAUDDIV - IBRD) × 64 + 0.5)`

```assembly
; Initialize UART
; R0 = baud rate (e.g., 115200)
uart_init:
    PUSH    {R4, R5, R6, LR}
    MOV     R6, R0                   ; Save baud rate
    
    LDR     R4, =UART_BASE
    
    ; Disable UART
    MOV     R0, #0
    STR     R0, [R4, #UART_CR]
    
    ; Wait for current transmission to complete
    LDR     R0, [R4, #UART_FR]
    TST     R0, #UART_FR_BUSY
    BNE     .-8
    
    ; Flush FIFO
    LDR     R0, [R4, #UART_LCRH]
    BIC     R0, R0, #UART_LCRH_FEN
    STR     R0, [R4, #UART_LCRH]
    
    ; Calculate baud rate divisor
    ; UART clock frequency (example: 24 MHz)
    LDR     R0, =24000000
    LSL     R1, R6, #4               ; Baud × 16
    UDIV    R2, R0, R1               ; Integer part
    MUL     R3, R2, R1
    SUB     R3, R0, R3               ; Remainder
    MOV     R0, #64
    MUL     R3, R3, R0
    UDIV    R3, R3, R1               ; Fractional part
    
    ; Write baud rate divisors
    STR     R2, [R4, #UART_IBRD]
    STR     R3, [R4, #UART_FBRD]
    
    ; Configure line control: 8N1 (8 data, no parity, 1 stop)
    MOV     R0, #UART_LCRH_WLEN_8
    ORR     R0, R0, #UART_LCRH_FEN   ; Enable FIFO
    STR     R0, [R4, #UART_LCRH]
    
    ; Disable all interrupts (polling mode)
    MOV     R0, #0
    STR     R0, [R4, #UART_IMSC]
    
    ; Enable UART, TX, and RX
    MOV     R0, #UART_CR_UARTEN
    ORR     R0, R0, #UART_CR_TXE
    ORR     R0, R0, #UART_CR_RXE
    STR     R0, [R4, #UART_CR]
    
    POP     {R4, R5, R6, PC}

; Send one character (blocking)
; R0 = character to send
uart_putc:
    PUSH    {R4, R5}
    MOV     R5, R0                   ; Save character
    LDR     R4, =UART_BASE
    
    ; Wait until TX FIFO not full
wait_tx:
    LDR     R0, [R4, #UART_FR]
    TST     R0, #UART_FR_TXFF
    BNE     wait_tx
    
    ; Write character to data register
    STR     R5, [R4, #UART_DR]
    
    POP     {R4, R5}
    BX      LR

; Receive one character (blocking)
; Returns: R0 = received character
uart_getc:
    PUSH    {R4}
    LDR     R4, =UART_BASE
    
    ; Wait until RX FIFO not empty
wait_rx:
    LDR     R0, [R4, #UART_FR]
    TST     R0, #UART_FR_RXFE
    BNE     wait_rx
    
    ; Read character from data register
    LDR     R0, [R4, #UART_DR]
    AND     R0, R0, #0xFF            ; Mask to 8 bits
    
    POP     {R4}
    BX      LR

; Send string (null-terminated)
; R0 = pointer to string
uart_puts:
    PUSH    {R4, R5, LR}
    MOV     R4, R0                   ; Save string pointer
    
puts_loop:
    LDRB    R5, [R4], #1             ; Load character, increment
    CMP     R5, #0                   ; Check for null terminator
    BEQ     puts_done
    
    MOV     R0, R5
    BL      uart_putc                ; Send character
    B       puts_loop
    
puts_done:
    POP     {R4, R5, PC}

; Check if character available (non-blocking)
; Returns: R0 = 1 if character available, 0 otherwise
uart_available:
    LDR     R0, =UART_BASE
    LDR     R0, [R0, #UART_FR]
    TST     R0, #UART_FR_RXFE
    MOVEQ   R0, #1                   ; Not empty = available
    MOVNE   R0, #0                   ; Empty = not available
    BX      LR

; Read with timeout
; R0 = timeout in microseconds
; Returns: R0 = character (0-255), or -1 if timeout
uart_getc_timeout:
    PUSH    {R4, R5, R6, LR}
    MOV     R6, R0                   ; Save timeout
    
    ; Read start time
    MRRC    p15, 0, R4, R5, c14      ; CNTPCT
    
    ; Calculate timeout cycles
    MRC     p15, 0, R0, c14, c0, 0   ; CNTFRQ
    MUL     R1, R6, R0
    LDR     R2, =1000000
    UDIV    R1, R1, R2
    ADD     R5, R4, R1               ; End time = start + timeout
    
getc_timeout_loop:
    ; Check if character available
    BL      uart_available
    CMP     R0, #1
    BEQ     getc_timeout_read
    
    ; Check timeout
    MRRC    p15, 0, R0, R1, c14      ; Current time
    CMP     R0, R5
    BGE     getc_timeout_expired
    
    B       getc_timeout_loop
    
getc_timeout_read:
    BL      uart_getc
    B       getc_timeout_done
    
getc_timeout_expired:
    MVN     R0, #0                   ; Return -1
    
getc_timeout_done:
    POP     {R4, R5, R6, PC}
```

**UART with Interrupts:**

```assembly
; Enable UART interrupts
; R0 = interrupt mask (UART_INT_* flags)
uart_enable_interrupts:
    LDR     R1, =UART_BASE
    STR     R0, [R1, #UART_IMSC]
    BX      LR

; UART interrupt handler
uart_irq_handler:
    PUSH    {R0-R4, LR}
    LDR     R4, =UART_BASE
    
    ; Read interrupt status
    LDR     R0, [R4, #UART_MIS]      ; Masked interrupt status
    
    ; Check RX interrupt
    TST     R0, #UART_INT_RX
    BEQ     check_tx
    
    ; Handle received data
    LDR     R1, [R4, #UART_DR]
    AND     R1, R1, #0xFF
    BL      uart_rx_handler          ; User handler
    
    ; Clear RX interrupt
    MOV     R2, #UART_INT_RX
    STR     R2, [R4, #UART_ICR]
    
check_tx:
    ; Check TX interrupt
    TST     R0, #UART_INT_TX
    BEQ     check_errors
    
    ; Handle transmit ready
    BL      uart_tx_handler          ; User handler
    
    ; Clear TX interrupt
    MOV     R2, #UART_INT_TX
    STR     R2, [R4, #UART_ICR]
    
check_errors:
    ; Check for errors
    TST     R0, #(UART_INT_OE | UART_INT_BE | UART_INT_PE | UART_INT_FE)
    BEQ     uart_irq_done
    
    ; Handle errors
    BL      uart_error_handler
    
    ; Clear error interrupts
    MOV     R2, #(UART_INT_OE | UART_INT_BE | UART_INT_PE | UART_INT_FE)
    STR     R2, [R4, #UART_ICR]
    
uart_irq_done:
    POP     {R0-R4, PC}
```

**Circular Buffer for UART:**

```assembly
.data
.align 2
rx_buffer:      .space 256           ; Receive buffer
rx_head:        .word 0              ; Write pointer
rx_tail:        .word 0              ; Read pointer
rx_buffer_size: .word 256

tx_buffer:      .space 256           ; Transmit buffer
tx_head:        .word 0
tx_tail:        .word 0
tx_buffer_size: .word 256

.text
; UART RX interrupt handler (called from uart_irq_handler)
; R1 = received character
uart_rx_handler:
    PUSH    {R4-R6}
    
    ; Load buffer pointers
    LDR     R4, =rx_head
    LDR     R5, [R4]                 ; Head index
    LDR     R6, =rx_buffer_size
    LDR     R6, [R6]
    
    ; Store character in buffer
    LDR     R2, =rx_buffer
    STRB    R1, [R2, R5]
    
    ; Increment head with wrap
    ADD     R5, R5, #1
    CMP     R5, R6
    MOVGE   R5, #0
    STR     R5, [R4]
    
    ; Check for buffer overflow (head == tail after increment)
    LDR     R4, =rx_tail
    LDR     R2, [R4]
    CMP     R5, R2
    BEQ     rx_overflow
    
    POP     {R4-R6}
    BX      LR
    
rx_overflow:
    ; Handle overflow (discard oldest data)
    ADD     R2, R2, #1
    CMP     R2, R6
    MOVGE   R2, #0
    STR     R2, [R4]
    POP     {R4-R6}
    BX      LR

; Read character from RX buffer
; Returns: R0 = character, or -1 if buffer empty
uart_buffer_getc:
    PUSH    {R4-R6}
    
    ; Load pointers
    LDR     R4, =rx_head
    LDR     R5, =rx_tail
    LDR     R0, [R4]                 ; Head
    LDR     R1, [R5]                 ; Tail
    
    ; Check if buffer empty
    CMP     R0, R1
    BEQ     buffer_empty
    
    ; Read character
    LDR     R2, =rx_buffer
    LDRB    R0, [R2, R1]
    
    ; Increment tail with wrap
    LDR     R6, =rx_buffer_size
    LDR     R6, [R6]
    ADD     R1, R1, #1
    CMP     R1, R6
    MOVGE   R1, #0
    STR     R1, [R5]
    
    POP     {R4-R6}
    BX      LR
    
buffer_empty:
    MVN     R0, #0                   ; Return -1
    POP     {R4-R6}
    BX      LR

; UART TX interrupt handler
uart_tx_handler:
    PUSH    {R4-R6, LR}
    
    ; Load pointers
    LDR     R4, =tx_head
    LDR     R5, =tx_tail
    LDR     R0, [R4]                 ; Head
    LDR     R1, [R5]                 ; Tail
    
    ; Check if buffer empty
    CMP     R0, R1
    BEQ     tx_buffer_empty
    
    ; Read character from buffer
    LDR     R2, =tx_buffer
    LDRB    R3, [R2, R1]
    
    ; Send character
    LDR     R6, =UART_BASE
    STR     R3, [R6, #UART_DR]
    
    ; Increment tail with wrap
    LDR     R6, =tx_buffer_size
    LDR     R6, [R6]
    ADD     R1, R1, #1
    CMP     R1, R6
    MOVGE   R1, #0
    STR     R1, [R5]
    
    POP     {R4-R6, PC}
    
tx_buffer_empty:
    ; Disable TX interrupt when buffer empty
    LDR     R0, =UART_BASE
    LDR     R1, [R0, #UART_IMSC]
    BIC     R1, R1, #UART_INT_TX
    STR     R1, [R0, #UART_IMSC]
    
    POP     {R4-R6, PC}

; Write character to TX buffer
; R0 = character
uart_buffer_putc:
    PUSH    {R4-R6}
    MOV     R6, R0                   ; Save character
    
    ; Load pointers
    LDR     R4, =tx_head
    LDR     R5, [R4]                 ; Head
    LDR     R3, =tx_buffer_size
    LDR     R3, [R3]
    
    ; Calculate next head position
    ADD     R0, R5, #1
    CMP     R0, R3
    MOVGE   R0, #0
    
    ; Wait if buffer full (head+1 == tail)
    LDR     R1, =tx_tail
wait_tx_space:
    LDR     R2, [R1]
    CMP     R0, R2
    BEQ     wait_tx_space
    
    ; Store character
    LDR     R2, =tx_buffer
    STRB    R6, [R2, R5]
    STR     R0, [R4]                 ; Update head
    
    ; Enable TX interrupt
    LDR     R0, =UART_BASE
    LDR     R1, [R0, #UART_IMSC]
    ORR     R1, R1, #UART_INT_TX
    STR     R1, [R0, #UART_IMSC]
    
    POP     {R4-R6}
    BX      LR
```

**DMA-based UART Transfer:**

```assembly
; Configure UART for DMA transfer
; R0 = source buffer address
; R1 = length
uart_dma_transmit:
    PUSH    {R4-R6, LR}
    MOV     R4, R0                   ; Source address
    MOV     R5, R1                   ; Length
    
    ; Enable UART DMA
    LDR     R6, =UART_BASE
    MOV     R0, #1                   ; TX DMA enable
    STR     R0, [R6, #UART_DMACR]
    
    ; Configure DMA controller (implementation-specific)
    ; Example: PL080 DMA
    LDR     R0, =DMA_BASE
    MOV     R1, R4                   ; Source address
    LDR     R2, =UART_BASE
    ADD     R2, R2, #UART_DR         ; Destination (UART DR)
    MOV     R3, R5                   ; Transfer size
    
    ; DMA configuration (simplified)
    STR     R1, [R0, #DMA_SRC]
    STR     R2, [R0, #DMA_DST]
    STR     R3, [R0, #DMA_LEN]
    
    ; Set DMA control: memory-to-peripheral, increment source
    LDR     R1, =DMA_CTRL_M2P | DMA_CTRL_SRC_INC
    STR     R1, [R0, #DMA_CTRL]
    
    ; Enable DMA channel
    MOV     R1, #1
    STR     R1, [R0, #DMA_ENABLE]
    
    POP     {R4-R6, PC}
```

## Interrupt Controllers

ARM systems use interrupt controllers to manage multiple interrupt sources. The Generic Interrupt Controller (GIC) is the standard for Cortex-A processors.

**GIC Architecture:**

The GIC consists of two main components:

**Distributor (GICD):** Manages interrupt prioritization, routing to CPU cores, and enabling/disabling interrupts.

**CPU Interface (GICC):** Per-core interface for acknowledging interrupts and signaling end-of-interrupt (EOI).

**Interrupt Types:**

- **SGI (Software Generated Interrupt):** IDs 0-15, used for inter-processor communication
- **PPI (Private Peripheral Interrupt):** IDs 16-31, per-core peripherals (timers, watchdog)
- **SPI (Shared Peripheral Interrupt):** IDs 32-1019, shared peripherals

**GIC Register Offsets:**

```assembly
; Distributor registers
.equ GICD_BASE, 0x10001000           ; Example base address
.equ GICD_CTLR, 0x000                ; Distributor control
.equ GICD_TYPER, 0x004               ; Interrupt controller type
.equ GICD_IIDR, 0x008                ; Distributor implementer ID
.equ GICD_IGROUPR, 0x080             ; Interrupt group registers
.equ GICD_ISENABLER, 0x100           ; Interrupt set-enable
.equ GICD_ICENABLER, 0x180           ; Interrupt clear-enable
.equ GICD_ISPENDR, 0x200             ; Interrupt set-pending
.equ GICD_ICPENDR, 0x280             ; Interrupt clear-pending
.equ GICD_ISACTIVER, 0x300           ; Interrupt set-active
.equ GICD_ICACTIVER, 0x380           ; Interrupt clear-active
.equ GICD_IPRIORITYR, 0x400          ; Interrupt priority
.equ GICD_ITARGETSR, 0x800           ; Interrupt processor targets
.equ GICD_ICFGR, 0xC00               ; Interrupt configuration
.equ GICD_SGIR, 0xF00                ; Software generated interrupt

; CPU Interface registers
.equ GICC_BASE, 0x10002000           ; Example base address
.equ GICC_CTLR, 0x000                ; CPU interface control
.equ GICC_PMR, 0x004                 ; Interrupt priority mask
.equ GICC_BPR, 0x008                 ; Binary point register
.equ GICC_IAR, 0x00C                 ; Interrupt acknowledge
.equ GICC_EOIR, 0x010                ; End of interrupt
.equ GICC_RPR, 0x014                 ; Running priority
.equ GICC_HPPIR, 0x018               ; Highest priority pending interrupt
.equ GICC_ABPR, 0x01C                ; Aliased binary point
.equ GICC_AIAR, 0x020                ; Aliased interrupt acknowledge
.equ GICC_AEOIR, 0x024               ; Aliased end of interrupt
.equ GICC_AHPPIR, 0x028              ; Aliased highest priority pending interrupt
.equ GICC_IIDR, 0x00FC               ; CPU interface implementer ID

; Constants
.equ GIC_DIST_ENABLE, 0x1
.equ GIC_CPU_ENABLE, 0x1
.equ GIC_PRIO_MASK_ALL, 0xFF
```

**GIC Initialization:**

```assembly
; Initialize GIC
gic_init:
    PUSH    {R4-R6, LR}
    
    ; Disable distributor
    LDR     R4, =GICD_BASE
    MOV     R0, #0
    STR     R0, [R4, #GICD_CTLR]
    
    ; Get number of interrupt lines
    LDR     R0, [R4, #GICD_TYPER]
	AND     R0, R0, #0x1F            ; Extract ITLinesNumber
    ADD     R5, R0, #1               ; Number of registers
    LSL     R5, R5, #5               ; × 32 interrupts per register
    
    ; Disable all interrupts
    LDR     R6, =GICD_ICENABLER
    ADD     R6, R4, R6
    MOV     R0, #0
disable_loop:
    MVN     R1, #0                   ; 0xFFFFFFFF (all bits set)
    STR     R1, [R6], #4
    ADD     R0, R0, #32
    CMP     R0, R5
    BLT     disable_loop
    
    ; Clear all pending interrupts
    LDR     R6, =GICD_ICPENDR
    ADD     R6, R4, R6
    MOV     R0, #0
clear_pending_loop:
    MVN     R1, #0
    STR     R1, [R6], #4
    ADD     R0, R0, #32
    CMP     R0, R5
    BLT     clear_pending_loop
    
    ; Set all interrupts to lowest priority (0xFF)
    LDR     R6, =GICD_IPRIORITYR
    ADD     R6, R4, R6
    MOV     R0, #0
    MVN     R2, #0                   ; 0xFFFFFFFF
set_priority_loop:
    STR     R2, [R6], #4
    ADD     R0, R0, #4               ; 4 interrupts per register
    CMP     R0, R5
    BLT     set_priority_loop
    
    ; Route all SPIs to CPU 0
    LDR     R6, =GICD_ITARGETSR
    ADD     R6, R4, R6
    ADD     R6, R6, #32              ; Start at interrupt 32 (first SPI)
    MOV     R0, #32
    MOV     R2, #0x01010101          ; Target CPU 0 for all 4 interrupts
set_target_loop:
    STR     R2, [R6], #4
    ADD     R0, R0, #4
    CMP     R0, R5
    BLT     set_target_loop
    
    ; Configure all interrupts as level-sensitive (default)
    ; ICFGR: 0 = level-sensitive, 1 = edge-triggered
    LDR     R6, =GICD_ICFGR
    ADD     R6, R4, R6
    MOV     R0, #0
    MOV     R2, #0
config_loop:
    STR     R2, [R6], #4
    ADD     R0, R0, #16              ; 16 interrupts per register
    CMP     R0, R5
    BLT     config_loop
    
    ; Enable distributor
    MOV     R0, #GIC_DIST_ENABLE
    STR     R0, [R4, #GICD_CTLR]
    
    ; Initialize CPU interface
    LDR     R4, =GICC_BASE
    
    ; Set priority mask (allow all priorities)
    MOV     R0, #GIC_PRIO_MASK_ALL
    STR     R0, [R4, #GICC_PMR]
    
    ; Set binary point (no priority grouping)
    MOV     R0, #0
    STR     R0, [R4, #GICC_BPR]
    
    ; Enable CPU interface
    MOV     R0, #GIC_CPU_ENABLE
    STR     R0, [R4, #GICC_CTLR]
    
    ; Enable IRQ interrupts in CPSR
    CPSIE   i
    
    POP     {R4-R6, PC}

; Enable specific interrupt
; R0 = interrupt ID
gic_enable_interrupt:
    PUSH    {R4, R5}
    
    LDR     R4, =GICD_BASE
    
    ; Calculate register offset and bit position
    MOV     R5, R0, LSR #5           ; Register index = ID / 32
    AND     R1, R0, #0x1F            ; Bit position = ID % 32
    MOV     R2, #1
    LSL     R2, R2, R1               ; Create bit mask
    
    ; Write to ISENABLER
    LDR     R3, =GICD_ISENABLER
    STR     R2, [R4, R3, LSL #0]     ; Base + offset
    ADD     R4, R4, R3
    STR     R2, [R4, R5, LSL #2]     ; + register index × 4
    
    POP     {R4, R5}
    BX      LR

; Disable specific interrupt
; R0 = interrupt ID
gic_disable_interrupt:
    PUSH    {R4, R5}
    
    LDR     R4, =GICD_BASE
    
    ; Calculate register offset and bit position
    MOV     R5, R0, LSR #5
    AND     R1, R0, #0x1F
    MOV     R2, #1
    LSL     R2, R2, R1
    
    ; Write to ICENABLER
    LDR     R3, =GICD_ICENABLER
    ADD     R4, R4, R3
    STR     R2, [R4, R5, LSL #2]
    
    POP     {R4, R5}
    BX      LR

; Set interrupt priority
; R0 = interrupt ID
; R1 = priority (0-255, lower = higher priority)
gic_set_priority:
    PUSH    {R4, R5}
    
    LDR     R4, =GICD_BASE
    
    ; Calculate register offset and byte position
    MOV     R5, R0, LSR #2           ; Register index = ID / 4
    AND     R2, R0, #0x3             ; Byte position = ID % 4
    LSL     R2, R2, #3               ; × 8 bits per byte
    
    ; Read-modify-write
    LDR     R3, =GICD_IPRIORITYR
    ADD     R4, R4, R3
    LDR     R3, [R4, R5, LSL #2]     ; Read current value
    
    MOV     R0, #0xFF
    LSL     R0, R0, R2               ; Create mask
    BIC     R3, R3, R0               ; Clear old priority
    LSL     R1, R1, R2               ; Position new priority
    ORR     R3, R3, R1               ; Set new priority
    
    STR     R3, [R4, R5, LSL #2]     ; Write back
    
    POP     {R4, R5}
    BX      LR

; Set interrupt target CPU
; R0 = interrupt ID (must be SPI: 32-1019)
; R1 = CPU mask (bit 0 = CPU0, bit 1 = CPU1, etc.)
gic_set_target:
    PUSH    {R4, R5}
    
    ; SPIs only (ID >= 32)
    CMP     R0, #32
    BLT     gic_set_target_done
    
    LDR     R4, =GICD_BASE
    
    ; Calculate register offset and byte position
    MOV     R5, R0, LSR #2           ; Register index = ID / 4
    AND     R2, R0, #0x3             ; Byte position = ID % 4
    LSL     R2, R2, #3               ; × 8 bits
    
    ; Read-modify-write
    LDR     R3, =GICD_ITARGETSR
    ADD     R4, R4, R3
    LDR     R3, [R4, R5, LSL #2]
    
    MOV     R0, #0xFF
    LSL     R0, R0, R2
    BIC     R3, R3, R0               ; Clear old target
    AND     R1, R1, #0xFF
    LSL     R1, R1, R2
    ORR     R3, R3, R1               ; Set new target
    
    STR     R3, [R4, R5, LSL #2]
    
gic_set_target_done:
    POP     {R4, R5}
    BX      LR

; Configure interrupt as edge or level triggered
; R0 = interrupt ID
; R1 = 0 (level-sensitive) or 1 (edge-triggered)
gic_set_config:
    PUSH    {R4, R5}
    
    LDR     R4, =GICD_BASE
    
    ; Calculate register offset and bit position
    MOV     R5, R0, LSR #4           ; Register index = ID / 16
    AND     R2, R0, #0xF             ; Field position = ID % 16
    LSL     R2, R2, #1               ; × 2 bits per interrupt
    
    ; Read-modify-write
    LDR     R3, =GICD_ICFGR
    ADD     R4, R4, R3
    LDR     R3, [R4, R5, LSL #2]
    
    MOV     R0, #0x3
    LSL     R0, R0, R2
    BIC     R3, R3, R0               ; Clear config bits
    
    CMP     R1, #0
    LSLNE   R1, R1, #1               ; Edge = bit 1 set
    LSL     R1, R1, R2
    ORR     R3, R3, R1
    
    STR     R3, [R4, R5, LSL #2]
    
    POP     {R4, R5}
    BX      LR

; Main IRQ handler (called from exception vector)
irq_handler:
    ; Save context
    SUB     LR, LR, #4               ; Adjust return address
    PUSH    {R0-R3, R12, LR}
    
    ; Read interrupt acknowledge register
    LDR     R0, =GICC_BASE
    LDR     R1, [R0, #GICC_IAR]
    
    ; Extract interrupt ID
    MOV     R2, R1
    AND     R2, R2, #0x3FF           ; Mask to 10 bits
    
    ; Check for spurious interrupt (ID 1023)
    LDR     R3, =1023
    CMP     R2, R3
    BEQ     irq_spurious
    
    ; Call interrupt-specific handler
    ; Jump table approach
    LDR     R0, =irq_handler_table
    LDR     R3, [R0, R2, LSL #2]     ; Load handler address
    CMP     R3, #0
    BEQ     irq_no_handler
    
    MOV     R0, R2                   ; Pass interrupt ID
    BLX     R3                       ; Call handler
    
irq_no_handler:
    ; Signal end of interrupt
    LDR     R0, =GICC_BASE
    STR     R1, [R0, #GICC_EOIR]     ; Write original IAR value
    
irq_spurious:
    ; Restore context and return
    POP     {R0-R3, R12, LR}
    MOVS    PC, LR                   ; Return and restore CPSR

; Interrupt handler table (array of function pointers)
.data
.align 2
irq_handler_table:
    .word   0                        ; ID 0 (SGI)
    .word   0                        ; ID 1
    ; ... (fill with handler addresses)
    .skip   1020 * 4                 ; Remaining entries

.text
; Register interrupt handler
; R0 = interrupt ID
; R1 = handler function pointer
gic_register_handler:
    PUSH    {R4}
    
    LDR     R4, =irq_handler_table
    STR     R1, [R4, R0, LSL #2]
    
    POP     {R4}
    BX      LR

; Software Generated Interrupt
; R0 = target CPU mask
; R1 = SGI ID (0-15)
gic_send_sgi:
    PUSH    {R4}
    
    LDR     R4, =GICD_BASE
    
    ; Build SGIR value
    LSL     R0, R0, #16              ; Target list in bits 23:16
    AND     R1, R1, #0xF             ; SGI ID in bits 3:0
    ORR     R0, R0, R1
    
    ; Write to SGIR
    STR     R0, [R4, #GICD_SGIR]
    
    POP     {R4}
    BX      LR

; Broadcast SGI to all other CPUs
; R0 = SGI ID (0-15)
gic_send_sgi_broadcast:
    PUSH    {R4}
    
    LDR     R4, =GICD_BASE
    
    ; Build SGIR value with target filter = 1 (all but self)
    MOV     R1, #1
    LSL     R1, R1, #24              ; Target filter in bits 25:24
    AND     R0, R0, #0xF
    ORR     R0, R0, R1
    
    STR     R0, [R4, #GICD_SGIR]
    
    POP     {R4}
    BX      LR
```

**FIQ (Fast Interrupt) Handling:**

FIQ has higher priority than IRQ and a dedicated register bank (R8-R14_fiq) for faster context switching.

```assembly
; Configure interrupt as FIQ (Group 0)
; R0 = interrupt ID
gic_set_fiq:
    PUSH    {R4, R5}
    
    LDR     R4, =GICD_BASE
    
    ; Calculate register and bit position
    MOV     R5, R0, LSR #5
    AND     R1, R0, #0x1F
    MOV     R2, #1
    LSL     R2, R2, R1
    
    ; Clear bit in IGROUPR (Group 0 = FIQ)
    LDR     R3, =GICD_IGROUPR
    ADD     R4, R4, R3
    LDR     R3, [R4, R5, LSL #2]
    BIC     R3, R3, R2
    STR     R3, [R4, R5, LSL #2]
    
    POP     {R4, R5}
    BX      LR

; FIQ handler (minimal latency)
fiq_handler:
    ; FIQ typically handles single high-priority interrupt
    ; No context save needed if using FIQ register bank only
    
    ; Read IAR
    LDR     R8, =GICC_BASE
    LDR     R9, [R8, #GICC_IAR]
    
    ; Handle interrupt (inline for speed)
    ; ... custom FIQ handling code ...
    
    ; Signal EOI
    STR     R9, [R8, #GICC_EOIR]
    
    ; Return
    SUBS    PC, LR, #4               ; Return and restore CPSR
```

**Nested Interrupt Support:**

```assembly
; IRQ handler with nesting enabled
irq_handler_nested:
    ; Adjust return address
    SUB     LR, LR, #4
    
    ; Save minimal context
    PUSH    {R0-R3, R12, LR}
    
    ; Read and acknowledge interrupt
    LDR     R0, =GICC_BASE
    LDR     R1, [R0, #GICC_IAR]
    MOV     R2, R1
    AND     R2, R2, #0x3FF
    
    ; Check spurious
    LDR     R3, =1023
    CMP     R2, R3
    BEQ     nested_spurious
    
    ; Save SPSR and re-enable interrupts for nesting
    MRS     R3, SPSR
    PUSH    {R1, R3}                 ; Save IAR and SPSR
    
    CPSIE   i                        ; Enable IRQ (allow nesting)
    
    ; Call handler in System mode
    CPS     #0x1F                    ; Switch to System mode
    
    ; Save full context on system stack
    PUSH    {R0-R12, LR}
    
    ; Call interrupt handler
    LDR     R0, =irq_handler_table
    LDR     R3, [R0, R2, LSL #2]
    MOV     R0, R2
    BLX     R3
    
    ; Restore context
    POP     {R0-R12, LR}
    
    ; Return to IRQ mode
    CPS     #0x12                    ; IRQ mode
    CPSID   i                        ; Disable interrupts
    
    ; Restore IAR and SPSR
    POP     {R1, R3}
    MSR     SPSR, R3
    
    ; Signal EOI
    LDR     R0, =GICC_BASE
    STR     R1, [R0, #GICC_EOIR]
    
nested_spurious:
    ; Restore minimal context and return
    POP     {R0-R3, R12, LR}
    MOVS    PC, LR

```

**Complete Interrupt Example:**

```assembly
; Complete example: Timer interrupt via GIC
example_timer_interrupt:
    PUSH    {LR}
    
    ; Initialize GIC
    BL      gic_init
    
    ; Register timer handler
    MOV     R0, #30                  ; Physical timer IRQ ID
    LDR     R1, =timer_interrupt_handler
    BL      gic_register_handler
    
    ; Set timer priority (high)
    MOV     R0, #30
    MOV     R1, #0x20                ; Priority 32
    BL      gic_set_priority
    
    ; Configure as level-sensitive
    MOV     R0, #30
    MOV     R1, #0
    BL      gic_set_config
    
    ; Enable timer interrupt
    MOV     R0, #30
    BL      gic_enable_interrupt
    
    ; Setup physical timer
    BL      setup_timer_1sec
    
    ; Main loop
main_loop:
    WFI                              ; Wait for interrupt
    B       main_loop
    
    POP     {PC}

; Timer interrupt handler
timer_interrupt_handler:
    PUSH    {R0-R3, LR}
    
    ; Clear timer interrupt
    MRC     p15, 0, R0, c14, c2, 1
    ORR     R0, R0, #2
    MCR     p15, 0, R0, c14, c2, 1
    
    ; Reload timer
    MRC     p15, 0, R1, c14, c0, 0
    MCR     p15, 0, R1, c14, c2, 0
    
    ; Process timer event
    LDR     R0, =tick_count
    LDR     R1, [R0]
    ADD     R1, R1, #1
    STR     R1, [R0]
    
    POP     {R0-R3, PC}

.data
tick_count: .word 0
```

**Key Points:**
- Bit-banging implements protocols through direct GPIO manipulation with precise software timing
- Hardware timers provide time measurement and periodic interrupts without CPU overhead
- UART provides serial communication with configurable baud rates, parity, and stop bits
- Circular buffers enable efficient interrupt-driven UART communication
- GIC manages interrupt prioritization, routing, and acknowledgment for multiple interrupt sources
- Interrupt handlers must acknowledge interrupts via IAR/EOIR registers
- Nested interrupts require careful context management and priority configuration
- SGIs enable inter-processor communication in multi-core systems

---

# Embedded Systems in ARM Assembly

Embedded systems programming on ARM architecture involves direct hardware interaction without an operating system layer. This requires deep understanding of the processor's startup sequence, memory layout configuration, and interrupt handling mechanisms.

## Bare-metal Programming

Bare-metal programming refers to writing code that runs directly on hardware without an operating system. The programmer has complete control over all resources but must manually manage everything the OS would typically handle.

**Core Responsibilities:**

**Hardware Initialization** The processor starts in an undefined state. Code must configure clock sources, enable peripheral clocks, set up memory controllers (DRAM timing, chip selects), and initialize GPIO pin modes. On Cortex-M processors, the System Control Block (SCB) registers control core behavior including vector table location and priority grouping.

**Memory Management** Without virtual memory, physical addresses are used directly. Memory regions are defined with specific purposes: code in flash/ROM, data in SRAM, peripheral registers at fixed addresses. Memory Protection Units (MPU) on Cortex-M3/M4/M7 can enforce access restrictions even without an MMU.

**Direct Hardware Access** Peripherals are controlled through memory-mapped registers. Writing to specific addresses controls UART transmission, SPI clock rates, ADC sampling, timer configurations. Each register bit has hardware-defined meaning documented in the chip reference manual.

**Interrupt Handling** Interrupts must be manually enabled in the NVIC (Nested Vectored Interrupt Controller). Priority levels, interrupt service routine addresses, and enable flags require explicit configuration. Context saving (register preservation) is partially automatic on Cortex-M but manual on Cortex-A/R.

**Example** - Basic GPIO toggle on ARM Cortex-M:

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ Memory-mapped register addresses
.equ RCC_AHB1ENR,  0x40023830
.equ GPIOA_MODER,  0x40020000
.equ GPIOA_ODR,    0x40020014

.global main
.type main, %function

main:
    @ Enable GPIOA clock
    LDR r0, =RCC_AHB1ENR
    LDR r1, [r0]
    ORR r1, r1, #0x01        @ Set bit 0
    STR r1, [r0]
    
    @ Configure PA5 as output
    LDR r0, =GPIOA_MODER
    LDR r1, [r0]
    BIC r1, r1, #(0x3 << 10) @ Clear bits 10-11
    ORR r1, r1, #(0x1 << 10) @ Set as output
    STR r1, [r0]
    
loop:
    @ Toggle PA5
    LDR r0, =GPIOA_ODR
    LDR r1, [r0]
    EOR r1, r1, #(1 << 5)    @ XOR bit 5
    STR r1, [r0]
    
    @ Simple delay
    LDR r2, =500000
delay_loop:
    SUBS r2, r2, #1
    BNE delay_loop
    
    B loop
```

## Boot Process and Startup Code

The ARM boot sequence is hardware-defined and varies between Cortex-M and Cortex-A architectures. Understanding this sequence is critical for reliable system initialization.

**Cortex-M Boot Sequence:**

**Power-On Reset** When power is applied or reset is released, the processor reads two 32-bit values from memory address 0x00000000 (or remapped location):

- **Initial Stack Pointer (SP)**: Loaded from address 0x00000000
- **Reset Handler Address**: Loaded from address 0x00000004

The processor automatically loads these values, sets the stack pointer, and branches to the reset handler. This happens in hardware before any instruction executes.

**Reset Handler Execution** The reset handler is the first C-callable code. Standard operations include:

1. **Copy initialized data** from flash (.data section) to SRAM
2. **Zero-initialize BSS section** (uninitialized global/static variables)
3. **Initialize FPU** (if Cortex-M4F/M7F)
4. **Set up clock system** (PLL configuration, clock source selection)
5. **Call C library initialization** (if using newlib/standard library)
6. **Call main()** function

**Example** - Cortex-M startup code:

```assembly
.syntax unified
.cpu cortex-m4
.fpu fpv4-sp-d16
.thumb

.global Reset_Handler
.type Reset_Handler, %function

Reset_Handler:
    @ Load stack pointer (already done by hardware, but shown for clarity)
    LDR r0, =_estack
    MOV sp, r0
    
    @ Copy .data section from flash to SRAM
    LDR r0, =_sdata        @ Destination start
    LDR r1, =_edata        @ Destination end
    LDR r2, =_sidata       @ Source start
    MOVS r3, #0
    B copy_data_check

copy_data_loop:
    LDR r4, [r2, r3]       @ Read from flash
    STR r4, [r0, r3]       @ Write to SRAM
    ADDS r3, r3, #4

copy_data_check:
    ADDS r4, r0, r3
    CMP r4, r1
    BCC copy_data_loop
    
    @ Zero-initialize .bss section
    LDR r0, =_sbss
    LDR r1, =_ebss
    MOVS r2, #0
    B clear_bss_check

clear_bss_loop:
    STR r2, [r0]
    ADDS r0, r0, #4

clear_bss_check:
    CMP r0, r1
    BCC clear_bss_loop
    
    @ Enable FPU (Cortex-M4F)
    LDR r0, =0xE000ED88    @ CPACR address
    LDR r1, [r0]
    ORR r1, r1, #(0xF << 20) @ Enable CP10 and CP11
    STR r1, [r0]
    DSB
    ISB
    
    @ Call system initialization
    BL SystemInit
    
    @ Call C library initialization
    BL __libc_init_array
    
    @ Call main
    BL main
    
    @ If main returns, loop forever
hang:
    B hang
```

**Cortex-A Boot Sequence:**

**Multiple Exception Levels** Cortex-A processors boot in privileged modes (often Supervisor mode on ARMv7-A or EL3 on ARMv8-A). The boot process is more complex:

1. **ROM bootloader** executes (manufacturer-provided)
2. **Secondary bootloader** (U-Boot, etc.) loads from flash/SD
3. **Kernel/application** loaded to DRAM
4. **MMU configuration** for virtual memory
5. **Cache enablement** (separate I-cache and D-cache)
6. **Jump to application** entry point

[Inference] Most production systems use multi-stage boot due to size limitations of on-chip ROM.

## Linker Scripts

Linker scripts are text files that instruct the linker how to organize compiled object files into the final executable. They define memory regions, section placement, and symbol creation.

**Memory Definition:**

The `MEMORY` command defines available memory regions with their base addresses and sizes:

```ld
MEMORY
{
    FLASH (rx)  : ORIGIN = 0x08000000, LENGTH = 512K
    SRAM (rwx)  : ORIGIN = 0x20000000, LENGTH = 128K
}
```

Attributes: `r` (read), `w` (write), `x` (execute), `a` (allocatable), `i` (initialized).

**Section Definitions:**

The `SECTIONS` command places code/data sections into memory regions:

**Example** - Complete linker script for Cortex-M:

```ld
/* Entry point */
ENTRY(Reset_Handler)

/* Highest address of the user mode stack */
_estack = ORIGIN(SRAM) + LENGTH(SRAM);

/* Minimum heap size */
_Min_Heap_Size = 0x200;
_Min_Stack_Size = 0x400;

MEMORY
{
    FLASH (rx)  : ORIGIN = 0x08000000, LENGTH = 512K
    SRAM (rwx)  : ORIGIN = 0x20000000, LENGTH = 128K
}

SECTIONS
{
    /* Vector table must be at start of flash */
    .isr_vector :
    {
        . = ALIGN(4);
        KEEP(*(.isr_vector))
        . = ALIGN(4);
    } >FLASH
    
    /* Program code */
    .text :
    {
        . = ALIGN(4);
        *(.text)           /* .text sections (code) */
        *(.text*)          /* .text* sections (code) */
        *(.glue_7)         /* ARM/Thumb interworking */
        *(.glue_7t)
        *(.eh_frame)
        
        KEEP(*(.init))
        KEEP(*(.fini))
        
        . = ALIGN(4);
        _etext = .;        /* End of code section */
    } >FLASH
    
    /* Read-only data */
    .rodata :
    {
        . = ALIGN(4);
        *(.rodata)
        *(.rodata*)
        . = ALIGN(4);
    } >FLASH
    
    /* ARM exception unwinding */
    .ARM.extab :
    {
        *(.ARM.extab* .gnu.linkonce.armextab.*)
    } >FLASH
    
    .ARM.exidx :
    {
        __exidx_start = .;
        *(.ARM.exidx* .gnu.linkonce.armexidx.*)
        __exidx_end = .;
    } >FLASH
    
    /* Used by startup to initialize data */
    _sidata = LOADADDR(.data);
    
    /* Initialized data goes to SRAM, loaded from FLASH */
    .data :
    {
        . = ALIGN(4);
        _sdata = .;        /* Start of data section */
        *(.data)
        *(.data*)
        
        . = ALIGN(4);
        _edata = .;        /* End of data section */
    } >SRAM AT>FLASH
    
    /* Uninitialized data section */
    .bss :
    {
        . = ALIGN(4);
        _sbss = .;
        __bss_start__ = _sbss;
        *(.bss)
        *(.bss*)
        *(COMMON)
        
        . = ALIGN(4);
        _ebss = .;
        __bss_end__ = _ebss;
    } >SRAM
    
    /* Heap and stack */
    ._user_heap_stack :
    {
        . = ALIGN(8);
        PROVIDE(end = .);
        PROVIDE(_end = .);
        . = . + _Min_Heap_Size;
        . = . + _Min_Stack_Size;
        . = ALIGN(8);
    } >SRAM
    
    /* Remove debug sections */
    /DISCARD/ :
    {
        libc.a (*)
        libm.a (*)
        libgcc.a (*)
    }
}
```

**Key Concepts:**

**LMA vs VMA**

- **Load Memory Address (LMA)**: Where section is stored in flash
- **Virtual Memory Address (VMA)**: Where section runs in SRAM

The `.data` section has different LMA (FLASH) and VMA (SRAM). Startup code copies from LMA to VMA.

**KEEP Directive** Prevents linker from removing sections during garbage collection (`--gc-sections`). Critical for vector tables and initialization code.

**Symbol Creation** `_sdata`, `_edata`, `_sbss`, `_ebss` are symbols used by startup code to know where sections begin and end.

## Vector Tables

Vector tables are arrays of function pointers that the processor uses to handle exceptions and interrupts. The table layout is architecture-defined and must match processor expectations.

**Cortex-M Vector Table Structure:**

The vector table contains:

1. **Initial stack pointer** (entry 0)
2. **Exception handlers** (entries 1-15, processor exceptions)
3. **IRQ handlers** (entries 16+, device-specific interrupts)

**Standard Exception Vectors (Cortex-M):**

- **0**: Initial SP value
- **1**: Reset handler
- **2**: NMI (Non-Maskable Interrupt)
- **3**: HardFault
- **4**: MemManage fault (if MPU exists)
- **5**: BusFault
- **6**: UsageFault
- **7-10**: Reserved
- **11**: SVCall (Supervisor Call)
- **12**: Debug Monitor
- **13**: Reserved
- **14**: PendSV (Pendable Service)
- **15**: SysTick timer

Entries 16+ are device-specific IRQs (UART, SPI, timers, etc.).

**Example** - Vector table definition:

```assembly
.syntax unified
.cpu cortex-m4
.thumb

.section .isr_vector,"a",%progbits
.type vector_table, %object
.size vector_table, .-vector_table

vector_table:
    .word _estack              @ 0: Initial stack pointer
    .word Reset_Handler        @ 1: Reset handler
    .word NMI_Handler          @ 2: NMI
    .word HardFault_Handler    @ 3: Hard fault
    .word MemManage_Handler    @ 4: MPU fault
    .word BusFault_Handler     @ 5: Bus fault
    .word UsageFault_Handler   @ 6: Usage fault
    .word 0                    @ 7: Reserved
    .word 0                    @ 8: Reserved
    .word 0                    @ 9: Reserved
    .word 0                    @ 10: Reserved
    .word SVC_Handler          @ 11: SVCall
    .word DebugMon_Handler     @ 12: Debug monitor
    .word 0                    @ 13: Reserved
    .word PendSV_Handler       @ 14: PendSV
    .word SysTick_Handler      @ 15: SysTick
    
    @ External interrupts (device-specific)
    .word WWDG_IRQHandler           @ 16: Window watchdog
    .word PVD_IRQHandler            @ 17: PVD through EXTI
    .word TAMP_STAMP_IRQHandler     @ 18: Tamper/timestamp
    .word RTC_WKUP_IRQHandler       @ 19: RTC wakeup
    .word FLASH_IRQHandler          @ 20: Flash global
    .word RCC_IRQHandler            @ 21: RCC global
    .word EXTI0_IRQHandler          @ 22: EXTI line 0
    .word EXTI1_IRQHandler          @ 23: EXTI line 1
    @ ... more device-specific interrupts
```

**Default Handler Implementation:**

Unimplemented interrupt handlers typically point to a default handler that loops forever, making debugging easier:

```assembly
.section .text

@ Weak definitions allow user to override
.weak NMI_Handler
.weak HardFault_Handler
.weak MemManage_Handler
.weak BusFault_Handler
.weak UsageFault_Handler
.weak SVC_Handler
.weak DebugMon_Handler
.weak PendSV_Handler
.weak SysTick_Handler

@ All default to Default_Handler
.thumb_set NMI_Handler, Default_Handler
.thumb_set HardFault_Handler, Default_Handler
.thumb_set MemManage_Handler, Default_Handler
.thumb_set BusFault_Handler, Default_Handler
.thumb_set UsageFault_Handler, Default_Handler
.thumb_set SVC_Handler, Default_Handler
.thumb_set DebugMon_Handler, Default_Handler
.thumb_set PendSV_Handler, Default_Handler
.thumb_set SysTick_Handler, Default_Handler

.type Default_Handler, %function
Default_Handler:
    B .                    @ Infinite loop
.size Default_Handler, .-Default_Handler
```

**Vector Table Relocation:**

The VTOR (Vector Table Offset Register) allows relocating the vector table to SRAM for dynamic modification:

```assembly
.equ SCB_VTOR, 0xE000ED08    @ VTOR address

relocate_vector_table:
    @ Copy vector table to SRAM
    LDR r0, =vector_table_ram    @ Destination (must be 128-byte aligned)
    LDR r1, =vector_table        @ Source in flash
    LDR r2, =vector_table_size
    
copy_vectors:
    LDM r1!, {r3-r10}           @ Load 8 words
    STM r0!, {r3-r10}           @ Store 8 words
    SUBS r2, r2, #32
    BGT copy_vectors
    
    @ Set VTOR to new location
    LDR r0, =SCB_VTOR
    LDR r1, =vector_table_ram
    STR r1, [r0]
    
    DSB                         @ Data synchronization barrier
    ISB                         @ Instruction synchronization barrier
    BX lr
```

**Interrupt Handler Example:**

```assembly
.global EXTI0_IRQHandler
.type EXTI0_IRQHandler, %function

EXTI0_IRQHandler:
    @ Context automatically saved by hardware:
    @ R0-R3, R12, LR, PC, xPSR
    
    PUSH {r4-r7, lr}           @ Save additional registers
    
    @ Clear EXTI pending bit
    LDR r0, =EXTI_PR           @ Pending register
    LDR r1, =0x01              @ Clear bit 0
    STR r1, [r0]
    
    @ Handle interrupt (call C function)
    BL handle_button_press
    
    POP {r4-r7, pc}            @ Restore and return
    @ Hardware restores R0-R3, R12, PC, xPSR
    
.size EXTI0_IRQHandler, .-EXTI0_IRQHandler
```

**Cortex-A Vector Table:**

Cortex-A processors use exception vectors at fixed addresses (typically 0x00000000 or 0xFFFF0000):

```assembly
.section .vectors,"ax"
.align 5

vector_table_a:
    B Reset_Handler        @ 0x00: Reset
    B Undefined_Handler    @ 0x04: Undefined instruction
    B SVC_Handler          @ 0x08: Supervisor call
    B Prefetch_Handler     @ 0x0C: Prefetch abort
    B Data_Handler         @ 0x10: Data abort
    NOP                    @ 0x14: Reserved
    B IRQ_Handler          @ 0x18: IRQ
    B FIQ_Handler          @ 0x1C: FIQ
```

Each entry is a branch instruction (4 bytes). The processor jumps to these fixed addresses on exceptions.

**Key Points:**

- Vector tables define processor behavior on reset, faults, and interrupts
- Table layout is architecture-defined and must be at specific memory locations
- Cortex-M uses word-sized function pointers; Cortex-A uses branch instructions
- Weak symbols allow default handlers to be overridden
- VTOR allows runtime relocation on Cortex-M
- Proper interrupt acknowledgment (clearing pending flags) is critical to prevent repeated triggers

---

## Real-time Constraints

Real-time constraints define the temporal requirements that embedded systems must satisfy to function correctly. In real-time systems, correctness depends not only on logical results but also on the time at which results are produced.

### Hard Real-time vs Soft Real-time

Hard real-time systems must meet deadlines absolutely—missing a deadline constitutes system failure. Examples include automotive airbag controllers, medical device monitors, and aircraft control systems. Soft real-time systems tolerate occasional deadline misses with degraded performance rather than catastrophic failure, such as video streaming or user interface responsiveness.

### Interrupt Latency

Interrupt latency is the time between an interrupt request and the execution of the first instruction in the interrupt service routine (ISR). ARM processors minimize this through:

**Interrupt handling sequence:**

- Interrupt signal assertion
- Current instruction completion
- Pipeline flush
- Context saving (automatic in ARM Cortex-M)
- Vector fetch and branch
- ISR execution begins

In ARM Cortex-M processors, hardware automatically saves registers R0-R3, R12, LR, PC, and xPSR to the stack, reducing interrupt latency to typically 12 cycles. The Cortex-M4 with FPU adds lazy stacking—floating-point registers are only saved if the ISR uses them.

```assembly
// Minimal ISR with fast execution
    .syntax unified
    .thumb
    
// Timer interrupt handler
TIM2_IRQHandler:
    PUSH    {LR}                    // Save return address
    
    // Clear interrupt flag (memory-mapped register)
    LDR     R0, =TIM2_SR            // Status register address
    LDR     R1, [R0]                // Read current value
    BIC     R1, R1, #0x01           // Clear UIF bit
    STR     R1, [R0]                // Write back
    
    // Critical time-sensitive code here
    LDR     R0, =GPIO_ODR           // Output data register
    LDR     R1, [R0]
    EOR     R1, R1, #(1<<5)         // Toggle pin
    STR     R1, [R0]
    
    POP     {PC}                    // Return from interrupt
```

### Deterministic Execution

Real-time systems require deterministic execution times. ARM assembly allows precise cycle counting:

```assembly
// Deterministic delay function (Cortex-M at known clock)
// R0 contains delay count
delay_cycles:
    SUBS    R0, R0, #1              // 1 cycle
    BNE     delay_cycles            // 2 cycles when taken, 1 when not
    BX      LR                      // 3 cycles
    
// Each loop iteration: 3 cycles
// Formula: (3 * count) + 4 cycles total
```

### Priority Inversion

Priority inversion occurs when a high-priority task waits for a resource held by a low-priority task, while a medium-priority task preempts the low-priority task. ARM Cortex-M processors implement priority levels (0-255, configurable groups) with hardware priority boosting:

```assembly
// Setting interrupt priorities (Cortex-M)
    LDR     R0, =NVIC_IPR0          // Interrupt priority register
    
    // Set UART interrupt to priority 2 (high priority)
    MOV     R1, #(2 << 5)           // Bits [7:5] for priority
    STRB    R1, [R0, #UART_IRQn]    // Set priority
    
    // Set Timer interrupt to priority 5 (lower priority)
    MOV     R1, #(5 << 5)
    STRB    R1, [R0, #TIM_IRQn]
```

### RTOS Context Switching

Real-time operating systems manage task scheduling. Context switching in ARM involves saving and restoring task states:

```assembly
// Simplified context switch (Cortex-M with RTOS)
PendSV_Handler:
    // Disable interrupts
    CPSID   I
    
    // Save context of current task
    MRS     R0, PSP                 // Get process stack pointer
    STMDB   R0!, {R4-R11}          // Save R4-R11 (R0-R3 already saved)
    
    // Save stack pointer to TCB
    LDR     R1, =current_task
    LDR     R1, [R1]
    STR     R0, [R1]                // Store SP in task control block
    
    // Load next task context
    LDR     R0, =next_task
    LDR     R0, [R0]
    LDR     R1, =current_task
    STR     R0, [R1]                // Update current task pointer
    
    LDR     R0, [R0]                // Load new SP from TCB
    LDMIA   R0!, {R4-R11}          // Restore R4-R11
    MSR     PSP, R0                 // Set process stack pointer
    
    // Re-enable interrupts
    CPSIE   I
    
    BX      LR                      // Return (hardware restores R0-R3)
```

## Power Management

Power management is critical in battery-operated embedded systems. ARM processors offer multiple power-saving modes and techniques to extend battery life while maintaining responsiveness.

### Sleep Modes

ARM Cortex-M processors provide several sleep modes with varying wake-up latencies and power savings:

**Sleep mode:** CPU clock stopped, peripherals and RAM active. Wake on any interrupt.

**Deep sleep:** CPU and high-speed clocks stopped, low-power oscillator active. Longer wake-up time.

**Standby/Shutdown:** Maximum power savings, only backup domain active. Requires reset to wake.

```assembly
// Enter sleep mode (Wait for Interrupt)
    WFI                             // Halt until interrupt
    // Execution continues here after interrupt
    
// Enter sleep mode (Wait for Event)
    WFE                             // Halt until event
    
// Configuring deep sleep
    LDR     R0, =SCB_SCR            // System Control Register
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<2)         // Set SLEEPDEEP bit
    STR     R1, [R0]
    WFI                             // Enter deep sleep
```

### Clock Gating

Selectively disabling clocks to unused peripherals reduces dynamic power consumption:

```assembly
// Enable peripheral clocks only when needed (STM32 example)
    LDR     R0, =RCC_AHB1ENR        // AHB1 peripheral clock enable
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<0)         // Enable GPIOA clock
    STR     R1, [R0]
    
    // Use GPIOA...
    
    // Disable when done
    LDR     R1, [R0]
    BIC     R1, R1, #(1<<0)         // Disable GPIOA clock
    STR     R1, [R0]
```

### Dynamic Voltage and Frequency Scaling (DVFS)

Adjusting processor frequency and voltage based on workload reduces power consumption:

```assembly
// Switch to lower clock frequency (example sequence)
    // First reduce voltage regulator scale
    LDR     R0, =PWR_CR              // Power control register
    LDR     R1, [R0]
    BIC     R1, R1, #(3<<14)        // Clear VOS bits
    ORR     R1, R1, #(2<<14)        // Set scale 2
    STR     R1, [R0]
    
    // Wait for voltage ready
wait_vos:
    LDR     R1, =PWR_CSR
    LDR     R1, [R1]
    TST     R1, #(1<<14)            // Check VOSRDY
    BEQ     wait_vos
    
    // Now switch PLL to lower frequency
    LDR     R0, =RCC_CFGR
    LDR     R1, [R0]
    BIC     R1, R1, #(3<<0)         // Clear SW bits
    ORR     R1, R1, #(1<<0)         // Use PLL as system clock
    STR     R1, [R0]
```

### Peripheral Power Control

Individual peripherals can be powered down independently:

```assembly
// ADC power control
    LDR     R0, =ADC1_CR2           // ADC control register 2
    
    // Power on ADC
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<0)         // Set ADON bit
    STR     R1, [R0]
    
    // Wait for stabilization (typically 10 µs)
    MOV     R2, #1000               // Delay count
adc_delay:
    SUBS    R2, R2, #1
    BNE     adc_delay
    
    // Use ADC...
    
    // Power off ADC when done
    LDR     R1, [R0]
    BIC     R1, R1, #(1<<0)         // Clear ADON bit
    STR     R1, [R0]
```

### Efficient Code Patterns

Power-efficient assembly code minimizes memory accesses and computational overhead:

```assembly
// Power-efficient loop: minimize memory traffic
    LDR     R0, =data_array         // Load address once
    MOV     R1, #0                  // Initialize sum
    MOV     R2, #ARRAY_SIZE
    
process_loop:
    LDR     R3, [R0], #4            // Load and post-increment
    ADD     R1, R1, R3              // Accumulate in register
    SUBS    R2, R2, #1
    BNE     process_loop
    
    // Write result once
    LDR     R0, =result
    STR     R1, [R0]
```

## Watchdog Timers

Watchdog timers are hardware safety mechanisms that reset the system if software fails to respond within a specified timeout period. They detect software hangs, infinite loops, and system crashes.

### Watchdog Operation

A watchdog timer counts down from a preset value. Software must periodically "kick" or "refresh" the watchdog before it reaches zero. If the counter expires, the watchdog triggers a system reset.

**Independent Watchdog (IWDG):** Runs from independent low-speed oscillator, continues in sleep modes. Used for detecting complete system failure.

**Window Watchdog (WWDG):** Requires refresh within a specific time window. Detects early refresh (runaway code) and late refresh (hang). Stops in debug/sleep modes.

### IWDG Configuration and Usage

```assembly
// Initialize Independent Watchdog (STM32 example)
    LDR     R0, =IWDG_KR            // Key register
    
    // Unlock IWDG registers
    LDR     R1, =0x5555
    STR     R1, [R0]
    
    // Set prescaler (divide LSI clock)
    LDR     R0, =IWDG_PR
    MOV     R1, #4                  // Divide by 64 (LSI=32kHz -> 500Hz)
    STR     R1, [R0]
    
    // Set reload value (timeout period)
    LDR     R0, =IWDG_RLR
    LDR     R1, =2000               // 2000/500Hz = 4 second timeout
    STR     R1, [R0]
    
    // Start watchdog
    LDR     R0, =IWDG_KR
    LDR     R1, =0xCCCC
    STR     R1, [R0]

// Watchdog refresh routine (call periodically)
watchdog_refresh:
    LDR     R0, =IWDG_KR
    LDR     R1, =0xAAAA             // Reload key
    STR     R1, [R0]
    BX      LR
```

### WWDG Configuration

```assembly
// Initialize Window Watchdog
    // Enable WWDG clock
    LDR     R0, =RCC_APB1ENR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<11)        // WWDGEN bit
    STR     R1, [R0]
    
    // Configure prescaler
    LDR     R0, =WWDG_CFR
    LDR     R1, =0x0000007F         // Max window value, prescaler /8
    STR     R1, [R0]
    
    // Set counter and enable
    LDR     R0, =WWDG_CR
    LDR     R1, =0x000000FF         // Counter=0x7F, enable bit
    STR     R1, [R0]

// Window watchdog refresh (must be within window)
wwdg_refresh:
    LDR     R0, =WWDG_CR
    LDR     R1, =0x0000007F         // Reset counter value
    STR     R1, [R0]
    BX      LR
```

### Watchdog in Main Loop

Typical embedded main loop with watchdog:

```assembly
main:
    BL      system_init
    BL      watchdog_init
    BL      peripheral_init
    
main_loop:
    // Critical task 1
    BL      read_sensors
    
    // Critical task 2
    BL      process_data
    
    // Refresh watchdog before timeout
    BL      watchdog_refresh
    
    // Critical task 3
    BL      update_outputs
    
    // Check for events
    BL      handle_communications
    
    B       main_loop               // Loop forever
```

### Watchdog Reset Detection

Detecting watchdog resets allows error logging and recovery:

```assembly
// Check reset source on startup
startup_check:
    LDR     R0, =RCC_CSR            // Clock control & status register
    LDR     R1, [R0]
    
    // Check IWDG reset flag
    TST     R1, #(1<<29)            // IWDGRSTF bit
    BNE     iwdg_reset_occurred
    
    // Check WWDG reset flag
    TST     R1, #(1<<30)            // WWDGRSTF bit
    BNE     wwdg_reset_occurred
    
    // Normal startup
    B       normal_init
    
iwdg_reset_occurred:
    // Log error, increment counter, recovery action
    BL      log_watchdog_error
    // Clear flag
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<24)        // RMVF - clear reset flags
    STR     R1, [R0]
    B       recovery_init
```

### Safe Watchdog Patterns

```assembly
// Task with watchdog protection
protected_task:
    PUSH    {R4, LR}
    
    // Set timeout flag
    LDR     R4, =task_timeout_flag
    MOV     R1, #0
    STR     R1, [R4]
    
    // Enable timeout timer interrupt
    BL      start_timeout_timer
    
    // Execute task
    BL      potentially_slow_operation
    
    // Check if we exceeded soft timeout
    LDR     R1, [R4]
    CMP     R1, #0
    BNE     task_timeout_error
    
    // Success - refresh watchdog
    BL      watchdog_refresh
    
    POP     {R4, PC}
    
task_timeout_error:
    // Handle timeout without letting watchdog expire
    BL      emergency_shutdown
    BL      watchdog_refresh        // Still refresh to allow clean shutdown
    B       error_handler
```

## DMA Operations

Direct Memory Access (DMA) allows peripherals to transfer data to/from memory without CPU intervention, freeing the processor for other tasks and reducing power consumption.

### DMA Architecture

ARM-based microcontrollers typically include DMA controllers with multiple channels/streams, each configurable for different transfer types:

**Memory-to-memory:** Copy data between memory locations **Peripheral-to-memory:** ADC readings, UART reception **Memory-to-peripheral:** DAC output, UART transmission **Peripheral-to-peripheral:** [Inference: Less common, hardware-dependent]

DMA transfers can be:

- Single transfer: One data element per trigger
- Burst transfer: Multiple elements per trigger
- Circular mode: Automatic restart after completion

### DMA Configuration

```assembly
// Configure DMA stream for ADC to memory transfer
    // Enable DMA clock
    LDR     R0, =RCC_AHB1ENR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<22)        // DMA2EN
    STR     R1, [R0]
    
    // Disable stream before configuration
    LDR     R0, =DMA2_Stream0_CR
    LDR     R1, [R0]
    BIC     R1, R1, #(1<<0)         // Clear EN bit
    STR     R1, [R0]
    
    // Wait until disabled
wait_disable:
    LDR     R1, [R0]
    TST     R1, #(1<<0)
    BNE     wait_disable
    
    // Configure peripheral address
    LDR     R0, =DMA2_Stream0_PAR
    LDR     R1, =ADC1_DR            // ADC data register
    STR     R1, [R0]
    
    // Configure memory address
    LDR     R0, =DMA2_Stream0_M0AR
    LDR     R1, =adc_buffer         // Destination buffer
    STR     R1, [R0]
    
    // Configure number of data items
    LDR     R0, =DMA2_Stream0_NDTR
    MOV     R1, #BUFFER_SIZE
    STR     R1, [R0]
    
    // Configure stream control
    LDR     R0, =DMA2_Stream0_CR
    LDR     R1, =(0<<25)            // Channel 0
    ORR     R1, R1, #(1<<10)        // Memory increment mode
    ORR     R1, R1, #(1<<8)         // Circular mode
    ORR     R1, R1, #(1<<4)         // Transfer complete interrupt
    ORR     R1, R1, #(1<<1)         // Direct mode error interrupt
    STR     R1, [R0]
```

### Starting DMA Transfer

```assembly
// Start DMA transfer
start_dma:
    LDR     R0, =DMA2_Stream0_CR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<0)         // Set EN bit
    STR     R1, [R0]
    BX      LR
```

### DMA Interrupt Handling

```assembly
// DMA transfer complete interrupt handler
DMA2_Stream0_IRQHandler:
    PUSH    {LR}
    
    // Check transfer complete flag
    LDR     R0, =DMA2_LISR          // Low interrupt status register
    LDR     R1, [R0]
    TST     R1, #(1<<5)             // TCIF0 - Transfer complete
    BEQ     dma_error_check
    
    // Clear flag
    LDR     R0, =DMA2_LIFCR         // Low interrupt flag clear
    MOV     R1, #(1<<5)
    STR     R1, [R0]
    
    // Process received data
    BL      process_adc_buffer
    
    POP     {PC}
    
dma_error_check:
    // Check for errors
    LDR     R0, =DMA2_LISR
    LDR     R1, [R0]
    TST     R1, #(1<<3)             // TEIF0 - Transfer error
    BNE     dma_transfer_error
    
    POP     {PC}
```

### Memory-to-Memory DMA

```assembly
// Fast memory copy using DMA
dma_memcpy:
    // R0 = source, R1 = destination, R2 = size in words
    PUSH    {R4-R6, LR}
    MOV     R4, R0
    MOV     R5, R1
    MOV     R6, R2
    
    // Disable DMA stream
    LDR     R0, =DMA2_Stream1_CR
    LDR     R1, [R0]
    BIC     R1, R1, #(1<<0)
    STR     R1, [R0]
    
wait_dma_disable:
    LDR     R1, [R0]
    TST     R1, #(1<<0)
    BNE     wait_dma_disable
    
    // Configure source address (memory 0)
    LDR     R0, =DMA2_Stream1_M0AR
    STR     R4, [R0]
    
    // Configure destination (peripheral address used as memory)
    LDR     R0, =DMA2_Stream1_PAR
    STR     R5, [R0]
    
    // Configure transfer size
    LDR     R0, =DMA2_Stream1_NDTR
    STR     R6, [R0]
    
    // Configure for memory-to-memory
    LDR     R0, =DMA2_Stream1_CR
    LDR     R1, =(2<<16)            // Memory-to-memory mode
    ORR     R1, R1, #(2<<13)        // Memory size: 32-bit
    ORR     R1, R1, #(2<<11)        // Peripheral size: 32-bit
    ORR     R1, R1, #(1<<10)        // Memory increment
    ORR     R1, R1, #(1<<9)         // Peripheral increment
    ORR     R1, R1, #(1<<4)         // Transfer complete interrupt
    STR     R1, [R0]
    
    // Enable stream
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<0)
    STR     R1, [R0]
    
    // Wait for completion (polling version)
wait_dma_complete:
    LDR     R0, =DMA2_LISR
    LDR     R1, [R0]
    TST     R1, #(1<<11)            // TCIF1
    BEQ     wait_dma_complete
    
    // Clear flag
    LDR     R0, =DMA2_LIFCR
    MOV     R1, #(1<<11)
    STR     R1, [R0]
    
    POP     {R4-R6, PC}
```

### Double Buffering

Double buffering allows continuous data acquisition while processing previous data:

```assembly
// Configure DMA with double buffering
    LDR     R0, =DMA2_Stream0_CR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<18)        // Enable double buffer mode
    STR     R1, [R0]
    
    // Set memory 0 address
    LDR     R0, =DMA2_Stream0_M0AR
    LDR     R1, =buffer0
    STR     R1, [R0]
    
    // Set memory 1 address
    LDR     R0, =DMA2_Stream0_M1AR
    LDR     R1, =buffer1
    STR     R1, [R0]

// In interrupt handler, determine active buffer
DMA_double_buffer_handler:
    PUSH    {LR}
    
    // Check which buffer was filled
    LDR     R0, =DMA2_Stream0_CR
    LDR     R1, [R0]
    TST     R1, #(1<<19)            // CT bit indicates current target
    BNE     buffer1_filled
    
    // Buffer 0 was filled, process it while DMA fills buffer 1
    LDR     R0, =buffer0
    BL      process_buffer
    B       clear_flag
    
buffer1_filled:
    // Buffer 1 was filled, process it while DMA fills buffer 0
    LDR     R0, =buffer1
    BL      process_buffer
    
clear_flag:
    LDR     R0, =DMA2_LIFCR
    MOV     R1, #(1<<5)
    STR     R1, [R0]
    
    POP     {PC}
```

### DMA with Peripheral Control

```assembly
// UART transmission with DMA
uart_dma_transmit:
    // R0 = buffer address, R1 = length
    PUSH    {R4, R5, LR}
    MOV     R4, R0
    MOV     R5, R1
    
    // Disable DMA stream
    LDR     R0, =DMA1_Stream6_CR
    LDR     R1, [R0]
    BIC     R1, R1, #(1<<0)
    STR     R1, [R0]
    
wait_uart_dma_disable:
    LDR     R1, [R0]
    TST     R1, #(1<<0)
    BNE     wait_uart_dma_disable
    
    // Set memory address
    LDR     R0, =DMA1_Stream6_M0AR
    STR     R4, [R0]
    
    // Set peripheral address (USART2 data register)
    LDR     R0, =DMA1_Stream6_PAR
    LDR     R1, =USART2_DR
    STR     R1, [R0]
    
    // Set transfer count
    LDR     R0, =DMA1_Stream6_NDTR
    STR     R5, [R0]
    
    // Configure stream: channel 4, memory increment, memory->peripheral
    LDR     R0, =DMA1_Stream6_CR
    LDR     R1, =(4<<25)            // Channel 4 (USART2_TX)
    ORR     R1, R1, #(1<<10)        // Memory increment
    ORR     R1, R1, #(1<<6)         // Memory to peripheral direction
    ORR     R1, R1, #(1<<4)         // Transfer complete interrupt
    STR     R1, [R0]
    
    // Enable USART2 DMA transmit
    LDR     R0, =USART2_CR3
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<7)         // DMAT - DMA enable transmitter
    STR     R1, [R0]
    
    // Enable DMA stream
    LDR     R0, =DMA1_Stream6_CR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<0)
    STR     R1, [R0]
    
    POP     {R4, R5, PC}
```

### DMA Priority and Arbitration

Multiple DMA streams may compete for bus access. Priority levels prevent starvation:

```assembly
// Set DMA stream priority
    LDR     R0, =DMA2_Stream0_CR
    LDR     R1, [R0]
    BIC     R1, R1, #(3<<16)        // Clear priority bits
    ORR     R1, R1, #(3<<16)        // Very high priority
    STR     R1, [R0]
    
    // Priority levels:
    // 00: Low
    // 01: Medium
    // 10: High
    // 11: Very high
```

### DMA Scatter-Gather [Inference]

Some ARM implementations support scatter-gather DMA using linked list descriptors, allowing complex multi-buffer transfers without CPU intervention. However, this is hardware-specific and not universally available across all ARM Cortex-M microcontrollers.

**Key Points**

- Real-time systems require deterministic timing, managed through interrupt priorities, context switching, and careful cycle counting
- Power management involves multiple sleep modes, clock gating, DVFS, and efficient code patterns to extend battery life
- Watchdog timers provide system reliability by detecting hangs and triggering resets, with periodic refresh required
- DMA offloads data transfers from CPU, supporting peripheral-to-memory, memory-to-peripheral, and memory-to-memory operations with interrupt-driven or circular modes

**Important related topics:** Exception handling and nested vectored interrupt controller (NVIC), memory protection units (MPU), cache management in Cortex-A/R series, peripheral bus architectures (AHB/APB), bootloader design for embedded systems.

---

# Security Features

## ARM TrustZone Basics

ARM TrustZone is a hardware-based security technology that creates an isolated secure execution environment within a processor. It divides the system into two parallel worlds: the Secure world and the Normal (Non-secure) world, enabling the processor to handle sensitive operations separately from general-purpose computing tasks.

TrustZone operates at the hardware level, implemented across the processor, memory, and peripherals. The architecture uses a single physical processor core that can switch between security states, rather than requiring separate processors. This approach provides strong isolation while maintaining efficiency.

The security model relies on the NS (Non-secure) bit, which propagates through the entire system including bus transactions, memory accesses, and peripheral communications. Hardware enforcement ensures that Non-secure software cannot access Secure resources, while Secure software can access both worlds.

**Key Points:**

- TrustZone provides hardware-enforced isolation without requiring a separate security processor
- The architecture supports both AArch32 (32-bit) and AArch64 (64-bit) execution states
- Security state transitions occur through controlled entry points called Secure Monitor Calls (SMC)
- The technology is designed to protect sensitive assets like cryptographic keys, biometric data, and payment credentials
- TrustZone can coexist with virtualization extensions, creating multiple security domains

## Secure and Non-secure States

The processor operates in one of two security states at any time: Secure state or Non-secure state. Each state has its own set of resources, exception levels, and memory regions.

### Security State Architecture

In AArch64, the security states interact with Exception Levels (EL0-EL3):

- **EL3**: Secure Monitor - Always executes in Secure state, handles transitions between security states
- **EL2**: Hypervisor - Can exist in both Secure and Non-secure states (implementation dependent)
- **EL1**: Operating System kernel level
- **EL0**: Application level

Non-secure state typically runs the Rich OS (like Linux or Android) and applications, while Secure state runs a Trusted OS and trusted applications. EL3 serves as the gatekeeper between these worlds.

### State Transition Mechanisms

Transitions from Non-secure to Secure state occur through:

- **SMC (Secure Monitor Call)**: Explicit instruction to request Secure services
- **Hardware exceptions**: Certain interrupts configured as Secure can trigger transitions
- **Reset**: System reset always enters Secure state

Transitions from Secure to Non-secure state occur through:

- **ERET (Exception Return)**: Returns from EL3 to lower exception level in Non-secure state
- **Secure interrupts completing**: After handling Secure interrupt, control returns to Non-secure world

### Resource Partitioning

**Secure State Resources:**

- Dedicated register banks (some registers are banked by security state)
- Secure memory regions (marked in translation tables)
- Secure peripherals (configured via hardware signals)
- Secure interrupts (FIQ typically configured as Secure)
- Private cache ways (optional, implementation dependent)

**Non-secure State Resources:**

- Normal world register banks
- Non-secure memory regions
- Non-secure peripherals
- Non-secure interrupts (IRQ typically configured as Non-secure)

### Memory Access Rules

The fundamental security rule: Non-secure accesses cannot read or write Secure memory, but Secure accesses can access Non-secure memory. This is enforced by the memory system using the NS bit on all transactions.

Memory controllers and peripherals check the NS bit to determine whether to permit or deny access. Attempting to access Secure memory from Non-secure state results in an abort or returns dummy data, depending on configuration.

**Example:**

```assembly
// Non-secure world calling Secure service
// Running at EL0 or EL1 in Non-secure state

        MOV     X0, #0x1000         // Function ID for Secure service
        MOV     X1, #0x42           // Parameter 1
        MOV     X2, buffer_addr     // Parameter 2 (Non-secure buffer)
        SMC     #0                  // Trigger Secure Monitor Call

        // Execution switches to EL3 Secure Monitor
        // Monitor validates request and dispatches to Secure OS
        // After service completion, ERET returns here
        
        CMP     X0, #0              // Check return status
        B.NE    error_handler
```

**Example:**

```assembly
// Secure Monitor at EL3 handling SMC
// This code runs in Secure state

secure_monitor_handler:
        // Save Non-secure context
        STP     X0, X1, [SP, #-16]!
        STP     X2, X3, [SP, #-16]!
        
        // Read exception syndrome to identify SMC
        MRS     X0, ESR_EL3
        
        // Check if call is from Non-secure state
        MRS     X1, SCR_EL3         // Secure Configuration Register
        TBNZ    X1, #0, handle_ns_call  // NS bit = 1 means Non-secure
        
        // Handle Secure world SMC differently
        B       handle_secure_call

handle_ns_call:
        // Switch to Secure EL1 to service the request
        // Set up return to Non-secure state
        MOV     X0, secure_service_addr
        MSR     ELR_EL3, X0         // Set return address
        
        // Configure return to Non-secure EL1
        MOV     X0, #0b01001        // EL1h, Secure state
        MSR     SPSR_EL3, X0
        
        ERET                        // Enter Secure EL1
```

### System Control Registers

**SCR_EL3 (Secure Configuration Register)**: Controls security state behavior

- NS bit (bit 0): Current security state (0=Secure, 1=Non-secure)
- IRQ, FIQ bits: Route interrupts to EL3
- EA bit: Route external aborts to EL3
- SMD bit: Disable SMC instruction in Non-secure state

**Key Points:**

- Security state is orthogonal to Exception Level - you can be at EL1 in either Secure or Non-secure state
- The processor always boots into Secure state at EL3
- SMC instruction is undefined in Secure EL0 and Non-secure EL0 (generates exception)
- Context switching between states must save/restore all necessary architectural state
- FIQ is typically dedicated to Secure world, IRQ to Non-secure world

## Cryptography Instructions

ARM processors include optional cryptographic extensions that provide hardware-accelerated instructions for common cryptographic algorithms. These instructions significantly improve performance and can provide protection against timing attacks when properly implemented.

### AES (Advanced Encryption Standard) Instructions

The AES instructions operate on 128-bit values in SIMD registers and support encryption, decryption, and key expansion operations.

**AES Encryption/Decryption Instructions:**

- **AESE**: AES single round encryption
- **AESD**: AES single round decryption
- **AESMC**: AES mix columns (encryption)
- **AESIMC**: AES inverse mix columns (decryption)

These instructions work on SIMD&FP registers (V registers) in 128-bit mode.

**Example:**

```assembly
// AES-128 encryption of a single block
// V0 = plaintext block (128 bits)
// V1-V11 = round keys (11 keys for AES-128)

aes_encrypt:
        // Initial round
        EOR     V0.16B, V0.16B, V1.16B      // Add round key 0
        
        // Rounds 1-9
        AESE    V0.16B, V2.16B              // SubBytes + ShiftRows + AddRoundKey
        AESMC   V0.16B, V0.16B              // MixColumns
        
        AESE    V0.16B, V3.16B
        AESMC   V0.16B, V0.16B
        
        AESE    V0.16B, V4.16B
        AESMC   V0.16B, V0.16B
        
        AESE    V0.16B, V5.16B
        AESMC   V0.16B, V0.16B
        
        AESE    V0.16B, V6.16B
        AESMC   V0.16B, V0.16B
        
        AESE    V0.16B, V7.16B
        AESMC   V0.16B, V0.16B
        
        AESE    V0.16B, V8.16B
        AESMC   V0.16B, V0.16B
        
        AESE    V0.16B, V9.16B
        AESMC   V0.16B, V0.16B
        
        AESE    V0.16B, V10.16B
        AESMC   V0.16B, V0.16B
        
        // Final round (no MixColumns)
        AESE    V0.16B, V11.16B
        
        // V0 now contains ciphertext
        RET
```

**Example:**

```assembly
// AES-128 decryption of a single block
// V0 = ciphertext block (128 bits)
// V1-V11 = round keys (applied in reverse order)

aes_decrypt:
        // Initial round (with inverse round key)
        EOR     V0.16B, V0.16B, V11.16B
        
        // Rounds 9-1
        AESD    V0.16B, V10.16B             // InvShiftRows + InvSubBytes + AddRoundKey
        AESIMC  V0.16B, V0.16B              // InvMixColumns
        
        AESD    V0.16B, V9.16B
        AESIMC  V0.16B, V0.16B
        
        AESD    V0.16B, V8.16B
        AESIMC  V0.16B, V0.16B
        
        AESD    V0.16B, V7.16B
        AESIMC  V0.16B, V0.16B
        
        AESD    V0.16B, V6.16B
        AESIMC  V0.16B, V0.16B
        
        AESD    V0.16B, V5.16B
        AESIMC  V0.16B, V0.16B
        
        AESD    V0.16B, V4.16B
        AESIMC  V0.16B, V0.16B
        
        AESD    V0.16B, V3.16B
        AESIMC  V0.16B, V0.16B
        
        AESD    V0.16B, V2.16B
        AESIMC  V0.16B, V0.16B
        
        // Final round (no InvMixColumns)
        AESD    V0.16B, V1.16B
        
        // V0 now contains plaintext
        RET
```

### SHA (Secure Hash Algorithm) Instructions

ARM provides instructions for SHA-1, SHA-256, and SHA-512 (in ARMv8.2 and later) hash computations.

**SHA-1 Instructions:**

- **SHA1C**: SHA-1 hash update (choose function)
- **SHA1P**: SHA-1 hash update (parity function)
- **SHA1M**: SHA-1 hash update (majority function)
- **SHA1H**: SHA-1 fixed rotate
- **SHA1SU0**, **SHA1SU1**: SHA-1 schedule update

**SHA-256 Instructions:**

- **SHA256H**: SHA-256 hash update part 1
- **SHA256H2**: SHA-256 hash update part 2
- **SHA256SU0**, **SHA256SU1**: SHA-256 schedule update

**Example:**

```assembly
// SHA-256 compression function (partial)
// V0 = state words A, B (64 bits each)
// V1 = state words C, D
// V2 = state words E, F
// V3 = state words G, H
// V4-V7 = message schedule

sha256_round:
        // Process 4 rounds
        LD1     {V16.4S}, [X0], #16         // Load 4 round constants
        
        ADD     V4.4S, V4.4S, V16.4S        // Add constants to schedule
        MOV     V17.16B, V2.16B             // Copy E,F,G,H
        
        SHA256H Q2, Q3, V4.4S               // Hash update part 1
        SHA256H2 Q3, Q17, V4.4S             // Hash update part 2
        
        // Update schedule for future rounds
        SHA256SU0 V4.4S, V5.4S
        SHA256SU1 V4.4S, V6.4S, V7.4S
        
        RET
```

### SHA-3/SHA-512 Instructions (ARMv8.2+)

**SHA-512 Instructions:**

- **SHA512H**: SHA-512 hash update part 1
- **SHA512H2**: SHA-512 hash update part 2
- **SHA512SU0**, **SHA512SU1**: SHA-512 schedule update

**SHA-3 Instructions:**

- **EOR3**: Three-way XOR
- **RAX1**: Rotate and XOR
- **XAR**: XOR and rotate
- **BCAX**: Bit clear and XOR

### Polynomial Multiply Instructions

Used for Galois/Counter Mode (GCM) and other cryptographic operations:

- **PMULL**, **PMULL2**: Polynomial multiply long

**Example:**

```assembly
// GCM multiplication (128-bit carry-less multiply)
// V0, V1 = input operands (128 bits each)

gcm_mult:
        PMULL   V2.1Q, V0.1D, V1.1D         // Low × Low
        PMULL2  V3.1Q, V0.2D, V1.2D         // High × High
        
        EXT     V4.16B, V0.16B, V0.16B, #8  // Swap halves of V0
        EOR     V4.16B, V4.16B, V0.16B      // V4 = Low ⊕ High of V0
        
        EXT     V5.16B, V1.16B, V1.16B, #8
        EOR     V5.16B, V5.16B, V1.16B      // V5 = Low ⊕ High of V1
        
        PMULL   V4.1Q, V4.1D, V5.1D         // Middle term
        
        EOR     V4.16B, V4.16B, V2.16B      // Combine with Low
        EOR     V4.16B, V4.16B, V3.16B      // Combine with High
        
        // Continue with reduction modulo GCM polynomial
        // ...
        
        RET
```

**Key Points:**

- Cryptographic instructions are optional; check ID registers (ID_AA64ISAR0_EL1) to detect availability
- Instructions operate on SIMD&FP registers (V registers) in various element sizes
- Hardware implementations provide constant-time execution, helping prevent timing side-channel attacks
- Multiple blocks can be processed in parallel using different register sets
- Round keys must be pre-expanded and stored in registers for optimal performance
- For production use, implementations should consider cache-timing attacks and use constant-time table lookups where applicable

## Memory Protection

ARM architectures provide multiple layers of memory protection mechanisms to enforce security policies, isolate processes, and prevent unauthorized access.

### Memory Management Unit (MMU)

The MMU translates virtual addresses to physical addresses while enforcing access permissions. It uses a hierarchical page table structure with configurable page sizes.

### Translation Table Structure

**AArch64 Translation Tables** support multiple granule sizes:

- 4KB pages: 4-level page tables (512 entries per level)
- 16KB pages: 4-level page tables (2048 entries per level)
- 64KB pages: 3-level page tables (8192 entries per level)

Each translation table entry contains:

- Output address (physical address)
- Access permissions (AP bits)
- Memory attributes (AttrIndx)
- Security attributes (NS bit in Secure state)
- Shareability attributes
- Access flag and dirty bit

### Access Permission Control

**AP[2:1] bits** in translation table entries control read/write permissions:

- `00`: Read/Write at EL1, No access at EL0
- `01`: Read/Write at EL1 and EL0
- `10`: Read-only at EL1, No access at EL0
- `11`: Read-only at EL1 and EL0

**UXN/PXN bits** control execution permissions:

- **UXN (Unprivileged Execute Never)**: Prevents execution at EL0
- **PXN (Privileged Execute Never)**: Prevents execution at EL1/EL2

**Example:**

```assembly
// Creating a page table entry for user read-only, kernel read/write page
// X0 = virtual address of page table entry
// X1 = physical address of page
// Assume 4KB granule

create_pte:
        // Set up the descriptor
        AND     X1, X1, #0xFFFFFFFFF000     // Mask to page boundary
        ORR     X1, X1, #0x3                // Valid, page descriptor
        
        // Set access permissions: AP[2:1] = 11 (read-only all levels)
        // But we want kernel RW, user RO, so use AP[2:1] = 10
        ORR     X1, X1, #(0x2 << 6)         // AP[2:1] = 10
        
        // Allow user execution (clear UXN), prevent kernel execution
        ORR     X1, X1, #(0x1 << 53)        // PXN = 1
        // UXN = 0 (bit 54 cleared)
        
        // Set memory attributes (index to MAIR)
        ORR     X1, X1, #(0x0 << 2)         // AttrIndx = 0 (normal memory)
        
        // Set shareability
        ORR     X1, X1, #(0x3 << 8)         // Inner shareable
        
        // Set access flag
        ORR     X1, X1, #(0x1 << 10)        // AF = 1 (accessed)
        
        // Store the entry
        STR     X1, [X0]
        
        // Ensure visibility
        DSB     ISH
        
        RET
```

### Memory Attributes

**MAIR_ELx registers** (Memory Attribute Indirection Registers) define memory types referenced by translation table entries. Each MAIR register contains 8 attribute encoding fields.

**Memory Types:**

- **Device memory**: Unpredictable behavior if speculative access occurs; used for memory-mapped I/O
    
    - Device-nGnRnE: Non-gathering, non-reordering, no early write acknowledgment
    - Device-nGnRE: Non-gathering, non-reordering, early write acknowledgment
    - Device-GRE: Gathering, reordering, early write acknowledgment
- **Normal memory**: Safe for speculative access; used for code and data
    
    - Non-cacheable
    - Write-through cacheable
    - Write-back cacheable
    - Write-back transient cacheable

**Example:**

```assembly
// Setting up MAIR_EL1 with common memory attributes
setup_mair:
        // Attr0: Normal memory, write-back cacheable
        MOV     X0, #0xFF                   // Inner/Outer Write-Back
        
        // Attr1: Device-nGnRnE (strongly ordered device)
        ORR     X0, X0, #(0x00 << 8)
        
        // Attr2: Normal memory, non-cacheable
        ORR     X0, X0, #(0x44 << 16)
        
        // Attr3: Device-GRE
        ORR     X0, X0, #(0x0C << 24)
        
        MSR     MAIR_EL1, X0
        ISB                                 // Synchronize context change
        
        RET
```

### Memory Protection Unit (MPU)

Some ARM processors (especially embedded/real-time variants) use an MPU instead of or alongside an MMU. The MPU divides memory into regions with configurable attributes but without address translation.

**MPU Regions** are defined by:

- Base address
- Size (must be power of 2, minimum 32 bytes)
- Access permissions
- Memory attributes
- Enable/disable bit

**[Inference]** MPU configuration typically occurs at system initialization and requires privileged access to modify. The number of supported regions varies by implementation.

### Pointer Authentication (ARMv8.3-PAuth)

Pointer Authentication cryptographically signs pointers to detect corruption or malicious modification. This provides protection against return-oriented programming (ROP) and jump-oriented programming (JOP) attacks.

**Pointer Authentication Instructions:**

- **PACIA, PACIB**: Sign pointer using key A or B (instruction address)
- **PACDA, PACDB**: Sign pointer using key A or B (data address)
- **AUTIA, AUTIB**: Authenticate pointer using key A or B (instruction)
- **AUTDA, AUTDB**: Authenticate pointer using key A or B (data)
- **XPACI, XPACD**: Strip authentication code

**Example:**

```assembly
// Function prologue with return address signing
function_with_pac:
        PACIASP                             // Sign LR using SP as modifier, key A
        STP     X29, X30, [SP, #-16]!      // Save frame pointer and signed LR
        MOV     X29, SP
        
        // Function body
        // ...
        
        // Function epilogue
        LDP     X29, X30, [SP], #16         // Restore frame pointer and signed LR
        AUTIASP                             // Authenticate LR using SP, key A
        RET                                 // Return; authentication failure causes exception
```

**Example:**

```assembly
// Protecting a function pointer in a structure
// X0 = pointer to structure
// X1 = function pointer to protect
// X2 = context value (discriminator)

store_protected_fptr:
        PACIA   X1, X2                      // Sign function pointer with context
        STR     X1, [X0, #FPTR_OFFSET]      // Store signed pointer
        RET

call_protected_fptr:
        LDR     X1, [X0, #FPTR_OFFSET]      // Load signed pointer
        AUTIA   X1, X2                      // Authenticate with same context
        BLR     X1                          // Call authenticated pointer
        RET
```

### Memory Tagging Extension (ARMv8.5-MTE)

MTE assigns 4-bit tags to memory allocations and pointers, detecting memory safety violations at runtime.

**Tag Management Instructions:**

- **IRG**: Insert Random Tag
- **GMI**: Tag Mask Insert
- **ADDG**: Add with tag
- **SUBG**: Subtract with tag
- **STG**: Store Allocation Tag
- **LDG**: Load Allocation Tag
- **ST2G, STZ2G, STZG**: Store multiple tags

**Load/Store Instructions** with tag checking:

- **LDGM, STGM**: Load/Store multiple tags
- Normal load/store instructions check tags when MTE is enabled

**Example:**

```assembly
// Allocating tagged memory
// X0 = untagged pointer to allocated memory
// X1 = size of allocation

tag_allocation:
        IRG     X0, X0, XZR                 // Generate random tag for pointer
        MOV     X2, X0                      // Copy tagged pointer
        
        // Set allocation tags in memory
tag_loop:
        STG     X2, [X2], #16               // Store tag, advance by 16 bytes
        SUB     X1, X1, #16
        CBNZ    X1, tag_loop
        
        // X0 contains tagged pointer
        RET

// Using tagged pointer
tagged_store:
        // X0 = tagged pointer
        // X1 = value to store
        STR     X1, [X0]                    // Tag checked automatically
        // If pointer tag doesn't match memory tag, generates exception
        RET
```

### Domain-Based Protection (AArch32)

**[Inference]** AArch32 state supports domain-based memory protection through the DACR (Domain Access Control Register). Domains partition memory into up to 16 sections with independent access control.

**Domain Access Types:**

- No access (00): Any access generates a fault
- Client (01): Access permissions checked
- Manager (11): Access permissions not checked

This feature is not available in AArch64 state.

### Execute-Never (XN) Protection

**XN attribute** in translation table entries prevents instruction fetches from specific memory regions, critical for preventing code injection attacks.

**Implementation Strategy:**

- Mark stack and heap as XN
- Mark data sections as XN
- Only mark explicitly designated code regions as executable

**Example:**

```assembly
// Setting up translation table entry for non-executable data page
// X0 = virtual address of page table entry
// X1 = physical address of data page

create_data_pte:
        AND     X1, X1, #0xFFFFFFFFF000
        ORR     X1, X1, #0x3                // Valid page
        ORR     X1, X1, #(0x1 << 54)        // UXN = 1 (user execute-never)
        ORR     X1, X1, #(0x1 << 53)        // PXN = 1 (privileged execute-never)
        ORR     X1, X1, #(0x1 << 6)         // AP[1] = 1 (read/write)
        
        STR     X1, [X0]
        DSB     ISH
        
        RET
```

### Cache Maintenance and Security

Cache maintenance operations are security-relevant because they affect timing and data visibility across security boundaries.

**Cache Maintenance Instructions:**

- **DC CIVAC**: Clean and invalidate by VA to PoC
- **DC CVAU**: Clean by VA to PoU
- **IC IVAU**: Invalidate instruction cache by VA to PoU
- **DC ZVA**: Zero cache line by VA

**Security Considerations:**

- Secure data in cache may leak to Non-secure world if not properly cleaned
- Cache timing can reveal information about execution patterns
- Translation table walks can be cached, affecting access times

**Example:**

```assembly
// Securely clearing a buffer before returning to Non-secure state
// X0 = buffer address
// X1 = buffer size

secure_clear_buffer:
        MOV     X2, X0
        MOV     X3, X1
        
        // Zero the buffer
clear_loop:
        STR     XZR, [X2], #8
        SUB     X3, X3, #8
        CBNZ    X3, clear_loop
        
        // Clean cache to ensure zeros written to memory
        MOV     X2, X0
        MOV     X3, X1
        
cache_clean_loop:
        DC      CIVAC, X2                   // Clean and invalidate by VA
        ADD     X2, X2, #64                 // Assume 64-byte cache line
        SUB     X3, X3, #64
        CBNZ    X3, cache_clean_loop
        
        DSB     SY                          // Ensure completion
        
        RET
```

**Key Points:**

- Memory protection operates at multiple privilege levels and security states
- Translation table entries combine address translation with access control
- Modern ARM architectures provide hardware-based protection against common memory exploits through Pointer Authentication and Memory Tagging
- Execute-Never protection should be default for data regions
- Cache maintenance is critical when transitioning between security domains
- Permission checks occur on each memory access; privileged software cannot bypass them without explicit configuration
- TLB maintenance operations must be performed after modifying translation tables

---

## Privilege Levels

ARM processors implement hierarchical execution levels that control access to system resources and instructions.

### Exception Levels (ARMv8/AArch64)

ARM defines four Exception Levels (EL0-EL3) with increasing privilege:

**EL0 (User Mode)**: Unprivileged execution level where application code runs. Applications at EL0 cannot directly access system registers, perform privileged operations, or access memory outside their permitted regions. Any attempt to execute privileged instructions triggers an exception to a higher level.

**EL1 (Operating System Kernel)**: Privileged level where OS kernel code executes. The kernel manages virtual memory through page tables, handles system calls from EL0, configures interrupt controllers, and manages device access. Most operating systems run at this level with full control over application execution.

**EL2 (Hypervisor)**: Provides virtualization support allowing multiple operating systems to run on the same hardware. The hypervisor at EL2 manages virtual machines, traps and emulates privileged operations from guest OSes at EL1, controls stage-2 address translation for VM memory isolation, and manages virtual CPU scheduling.

**EL3 (Secure Monitor)**: Highest privilege level managing transitions between Normal World and Secure World in TrustZone implementations. The Secure Monitor handles secure boot verification, manages cryptographic keys in secure storage, mediates communication between secure and non-secure states, and controls access to secure peripherals.

### ARM Cortex-A Exception Level Transitions

Exception level changes occur through specific mechanisms:

```assembly
// Taking an exception from EL0 to EL1
// Hardware automatically:
// 1. Saves processor state to ELR_EL1 (Exception Link Register)
// 2. Saves PSTATE to SPSR_EL1 (Saved Program Status Register)
// 3. Updates current EL to EL1
// 4. Branches to exception vector

exception_entry:
    stp x29, x30, [sp, #-16]!    // Save frame pointer and link register
    stp x27, x28, [sp, #-16]!
    // ... save other registers
    
    // Handle exception at EL1
    bl  handle_syscall
    
    // Return to EL0
    ldp x27, x28, [sp], #16
    ldp x29, x30, [sp], #16
    eret                          // Exception Return - restores EL0
```

### Privilege Level Registers

Each exception level has dedicated system registers:

```assembly
// Reading current exception level
mrs x0, CurrentEL          // Read current EL into x0
lsr x0, x0, #2             // EL is in bits [3:2]
// x0 now contains 0, 1, 2, or 3

// Accessing EL-specific registers
mrs x1, SCTLR_EL1          // System Control Register for EL1
mrs x2, VBAR_EL1           // Vector Base Address Register for EL1
mrs x3, TCR_EL1            // Translation Control Register for EL1

// Attempting to access higher-EL register from lower EL causes exception
// This instruction at EL0 would trap to EL1:
mrs x4, SCTLR_EL1          // UNDEFINED at EL0
```

### ARMv7 Privilege Modes

Legacy 32-bit ARM uses processor modes instead of exception levels:

- **User mode**: Unprivileged application execution
- **FIQ mode**: Fast Interrupt Request handling
- **IRQ mode**: Standard Interrupt Request handling
- **Supervisor mode**: OS kernel execution after reset or SVC instruction
- **Abort mode**: Memory access violation handling
- **Undefined mode**: Undefined instruction handling
- **System mode**: Privileged mode with user-mode register bank

```assembly
// ARMv7 mode switching via CPSR manipulation
mrs r0, CPSR               // Read Current Program Status Register
bic r0, r0, #0x1F          // Clear mode bits
orr r0, r0, #0x13          // Set Supervisor mode (10011)
msr CPSR_c, r0             // Write back to CPSR
```

## Secure Boot Concepts

Secure boot establishes a chain of trust from initial power-on through operating system loading, ensuring only authenticated code executes.

### Root of Trust

The secure boot process begins with immutable code in ROM (Boot ROM) that cannot be modified after chip manufacturing. This code contains cryptographic public keys or key hashes used to verify the next stage.

**Boot Sequence**:

1. **Boot ROM (BL1)**: Executes immediately after reset from internal ROM. Verifies digital signature of next bootloader stage using embedded public key. Only proceeds if signature verification succeeds.
    
2. **Trusted Boot Firmware (BL2)**: Second stage bootloader verified by BL1. Initializes additional hardware, verifies and loads subsequent stages, and establishes secure execution environment.
    
3. **Secure Monitor (BL31)**: Establishes EL3 secure world environment. Provides runtime services for secure and non-secure worlds.
    
4. **Bootloader (BL33)**: OS bootloader (U-Boot, UEFI) verified by previous stage. Loads and verifies OS kernel before execution.
    

### Digital Signature Verification

Each boot stage verifies the next using cryptographic signatures:

```assembly
// Simplified signature verification flow (conceptual)
secure_boot_verify:
    // Load next stage image header
    ldr x0, =next_stage_addr
    ldr x1, [x0, #IMAGE_SIGNATURE_OFFSET]
    ldr x2, [x0, #IMAGE_HASH_OFFSET]
    
    // Calculate hash of image
    ldr x3, [x0, #IMAGE_SIZE]
    bl  sha256_hash              // Calculate SHA-256 of image
    
    // Verify signature using public key in ROM
    ldr x4, =public_key_rom
    mov x5, x1                   // Signature
    mov x6, x2                   // Hash
    bl  rsa_verify               // RSA signature verification
    
    cbz x0, boot_failure         // If verification fails, halt
    
    // Verification succeeded, jump to next stage
    ldr x0, =next_stage_addr
    br  x0

boot_failure:
    // Secure failure - cannot proceed
    wfi                          // Wait for interrupt (halt)
    b   boot_failure             // Infinite loop
```

### TrustZone Integration

Secure boot leverages ARM TrustZone to partition system into Normal World and Secure World:

```assembly
// Transitioning to Secure World via SMC (Secure Monitor Call)
// From EL1 (Normal World)
mov x0, #SECURE_BOOT_VERIFY_FID    // Function ID
ldr x1, =image_address             // Image to verify
ldr x2, =image_size
smc #0                             // Triggers exception to EL3

// In EL3 Secure Monitor
secure_monitor_handler:
    // Check FID in x0
    cmp x0, #SECURE_BOOT_VERIFY_FID
    b.ne unknown_smc
    
    // Execute secure verification
    mov x0, x1                     // Image address
    mov x1, x2                     // Image size
    bl  secure_verify_image
    
    // Return to Normal World with result in x0
    eret
```

### Anti-Rollback Protection

Secure boot systems include version tracking to prevent downgrade attacks:

```assembly
// Check image version against secure storage
check_version:
    // Read current minimum version from secure OTP/eFuses
    ldr x0, =OTP_VERSION_ADDR
    ldr w1, [x0]                   // Minimum allowed version
    
    // Read image version from header
    ldr x2, =image_header
    ldr w3, [x2, #VERSION_OFFSET]
    
    // Compare versions
    cmp w3, w1
    b.lt version_too_old           // Reject if image version < minimum
    
    // Update secure counter if newer version
    cmp w3, w1
    b.le version_ok
    str w3, [x0]                   // Write new minimum version
    
version_ok:
    ret

version_too_old:
    mov x0, #-1                    // Return error
    ret
```

## Side-Channel Considerations

Side-channel attacks exploit information leaked through physical implementation rather than algorithmic weaknesses. ARM processors face various side-channel threats that require careful consideration in secure code.

### Timing Attacks

Execution time variations can leak information about secret data:

```assembly
// VULNERABLE: Timing varies based on secret value
vulnerable_compare:
    ldrb w2, [x0], #1              // Load byte from string 1
    ldrb w3, [x1], #1              // Load byte from string 2
    cmp w2, w3
    b.ne not_equal                 // Early exit leaks position of difference
    cbnz w2, vulnerable_compare
    // Strings equal
    mov x0, #1
    ret
not_equal:
    mov x0, #0
    ret

// CONSTANT-TIME: Always takes same time regardless of input
constant_time_compare:
    mov w4, #0                     // Accumulator for differences
compare_loop:
    ldrb w2, [x0], #1
    ldrb w3, [x1], #1
    eor w5, w2, w3                 // XOR difference
    orr w4, w4, w5                 // Accumulate all differences
    cbnz w2, compare_loop          // Continue to end of string
    cmp w4, #0                     // Check if any differences
    cset x0, eq                    // Set result
    ret
```

### Cache Timing Attacks

Cache state differences create timing variations exploitable in attacks like Spectre and Meltdown:

**[Inference]** Cache-based side channels occur when memory access patterns affect cache state, creating measurable timing differences. Attackers can potentially infer accessed addresses by measuring cache hit/miss timing.

```assembly
// Table lookup vulnerable to cache timing
vulnerable_table_lookup:
    ldr x2, =lookup_table
    ldrb w3, [x2, x0]              // Index with secret - cache state leaks info
    ret

// Constant-time alternative using masking
constant_time_lookup:
    ldr x2, =lookup_table
    mov x3, #0                     // Result accumulator
    mov x4, #0                     // Counter
lookup_loop:
    ldrb w5, [x2, x4]              // Load each entry
    cmp x4, x0                     // Compare with target index
    csel x3, x5, x3, eq            // Select if match (constant-time)
    add x4, x4, #1
    cmp x4, #256                   // Iterate through all entries
    b.lt lookup_loop
    mov x0, x3
    ret
```

### Speculative Execution Mitigations

Modern ARM processors implement speculation barriers to prevent speculative side-channel attacks:

```assembly
// Speculation barrier prevents speculative execution past this point
// Use after bounds check to prevent Spectre variant 1
bounds_check:
    cmp x0, x1                     // Check index < bound
    b.hs out_of_bounds             // Branch if out of bounds
    
    csdb                           // Conditional Speculation Barrier (ARMv8.5)
    // or
    dsb sy                         // Data Synchronization Barrier (older)
    isb                            // Instruction Synchronization Barrier
    
    // Safe to access array - speculation prevented
    ldr x2, [x3, x0, lsl #3]
    ret

out_of_bounds:
    mov x0, #-1
    ret
```

**[Inference]** The `csdb` instruction provides hardware-level protection against speculative execution vulnerabilities when placed after conditional branches, though exact implementation details and effectiveness depend on specific processor microarchitecture.

### Power Analysis Considerations

Current consumption variations during cryptographic operations can leak key material through Differential Power Analysis (DPA). **[Inference]** Countermeasures in software include randomizing operation order, adding dummy operations, and using masked implementations, though hardware-level protections are generally more effective.

```assembly
// Random delay insertion to mask timing
add_random_delay:
    // Read hardware random number generator
    mrs x0, RNDR                   // ARMv8.5 random number instruction
    and x0, x0, #0x1F              // Limit to reasonable range
delay_loop:
    nop
    sub x0, x0, #1
    cbnz x0, delay_loop
    ret
```

## Pointer Authentication (ARMv8.3+)

Pointer Authentication Codes (PAC) protect against return-oriented programming (ROP) and jump-oriented programming (JOP) attacks by cryptographically signing pointers before use.

### PAC Mechanism

PAC uses cryptographic algorithms to generate authentication codes embedded in unused upper bits of 64-bit pointers. ARMv8.3-A introduces five 128-bit keys:

- **APIAKey/APIBKey**: Instruction address keys (A and B variants)
- **APDAKey/APDBKey**: Data address keys (A and B variants)
- **APGAKey**: Generic authentication key

```assembly
// Sign return address before storing
function_with_pac:
    paciasp                        // Sign LR with SP and APIAKey
    stp x29, x30, [sp, #-16]!      // Store frame pointer and signed LR
    mov x29, sp
    
    // Function body
    bl  some_other_function
    
    // Restore and authenticate return address
    ldp x29, x30, [sp], #16
    autiasp                        // Authenticate LR with SP and APIAKey
    ret                            // Return using authenticated address

// If authentication fails, top bits are corrupted
// causing a fault when used as address
```

### PAC Instructions

**Signing Instructions**:

```assembly
// Sign instruction pointer with context and key A
pacia x0, x1                   // Sign x0 using x1 as context, APIAKey

// Sign instruction pointer with SP as context
paciasp                        // Sign LR using SP as context

// Sign instruction pointer with zero context
paciaz                         // Sign LR using zero as context

// Sign data pointer
pacda x0, x1                   // Sign x0 using x1 as context, APDAKey

// Generic authentication
pacga x0, x1, x2               // Sign x1 with x2 context using APGAKey
```

**Authentication Instructions**:

```assembly
// Authenticate instruction pointer
autia x0, x1                   // Authenticate x0 using x1 as context

// Authenticate with SP
autiasp                        // Authenticate LR using SP

// Authenticate with zero
autiaz                         // Authenticate LR using zero

// Authenticate data pointer
autda x0, x1                   // Authenticate x0 using x1 as context
```

**Strip Instructions**:

```assembly
// Remove PAC without authentication (for debugging/logging)
xpaci x0                       // Strip PAC from instruction pointer in x0
xpacd x0                       // Strip PAC from data pointer in x0
```

### PAC in Practice

**Function Call Protection**:

```assembly
// Caller
call_protected_function:
    // Return address automatically protected if PAC enabled
    bl  protected_function        // LR contains signed return address
    // Execution continues here after return

// Callee
protected_function:
    paciasp                       // Sign LR with SP
    stp x29, x30, [sp, #-32]!
    stp x19, x20, [sp, #16]
    mov x29, sp
    
    // If attacker overwrites LR on stack, authentication will fail
    // Function body...
    
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #32
    autiasp                       // Authenticate before using
    ret                           // Fault if authentication failed
```

**Data Pointer Protection**:

```assembly
// Protect function pointers in structures
struct_with_callback:
    .quad data_field1
    .quad data_field2
    .quad callback_ptr            // Function pointer to protect

protect_callback:
    ldr x0, =struct_with_callback
    ldr x1, [x0, #16]             // Load callback pointer
    mov x2, x0                    // Use struct address as context
    pacia x1, x2                  // Sign callback with struct address
    str x1, [x0, #16]             // Store signed pointer
    ret

call_callback:
    ldr x0, =struct_with_callback
    ldr x1, [x0, #16]             // Load signed callback
    mov x2, x0                    // Context
    autia x1, x2                  // Authenticate
    blr x1                        // Call authenticated pointer
    ret
```

**Virtual Function Tables**:

```assembly
// C++ vtable with PAC protection
// vtable layout: [method1_ptr, method2_ptr, method3_ptr]

// Signing vtable entries at initialization
sign_vtable:
    ldr x0, =object_vtable
    mov x1, x0                    // Use vtable address as context
    
    ldr x2, [x0, #0]              // Load method1 pointer
    pacia x2, x1                  // Sign with vtable address
    str x2, [x0, #0]
    
    ldr x2, [x0, #8]              // Load method2 pointer
    pacia x2, x1
    str x2, [x0, #8]
    
    ldr x2, [x0, #16]             // Load method3 pointer
    pacia x2, x1
    str x2, [x0, #16]
    ret

// Calling virtual method
call_virtual_method:
    ldr x0, [x8, #0]              // Load object pointer (x8 = this)
    ldr x1, [x0, #0]              // Load vtable pointer
    ldr x2, [x1, #8]              // Load signed method pointer
    autia x2, x1                  // Authenticate with vtable address
    blr x2                        // Call method
    ret
```

### PAC Configuration

System registers control PAC behavior:

```assembly
// Enable PAC in SCTLR_EL1
enable_pac:
    mrs x0, SCTLR_EL1
    orr x0, x0, #(1 << 30)        // EnIA - Enable PAC for instruction addresses
    orr x0, x0, #(1 << 27)        // EnDA - Enable PAC for data addresses
    msr SCTLR_EL1, x0
    isb
    ret

// Configure PAC keys (EL1 or higher)
set_pac_keys:
    // Load keys from secure storage
    ldr x0, =apiakeylo
    ldr x1, =apiakeyhi
    msr APIAKeyLo_EL1, x0
    msr APIAKeyHi_EL1, x1
    
    // Set other keys similarly
    ldr x0, =apibkeylo
    ldr x1, =apibkeyhi
    msr APIBKeyLo_EL1, x0
    msr APIBKeyHi_EL1, x1
    isb
    ret
```

### PAC Security Properties

**[Inference]** PAC provides probabilistic protection against pointer corruption. The authentication code uses unused upper bits (typically bits 55-63 on current implementations), providing roughly 2^8 = 256 possible values per context. An attacker attempting to forge a valid PAC has approximately 1/256 chance of success per attempt, though triggering a fault on failure limits practical exploitation.

**Limitations**:

- **[Unverified]** PAC does not protect against all control-flow attacks; attackers may use valid signed pointers from elsewhere in memory
- Context selection is critical: weak or predictable contexts reduce security
- **[Inference]** PAC effectiveness depends on key management; compromised keys eliminate protection
- Performance overhead exists from additional sign/authenticate instructions
- Limited to 64-bit architectures with available upper address bits

**Key Points:**

- ARM privilege levels (EL0-EL3) create hierarchical isolation between applications, OS, hypervisor, and secure monitor with hardware-enforced access controls
- Secure boot establishes cryptographic chain of trust from immutable ROM through each boot stage using digital signatures and anti-rollback counters
- Side-channel attacks exploit timing, cache state, power consumption, and speculative execution requiring constant-time algorithms and hardware barriers
- Pointer Authentication (ARMv8.3+) cryptographically signs pointers using dedicated keys to detect and prevent ROP/JOP attacks with probabilistic protection

**Important related topics:** TrustZone architecture and secure world isolation, Memory Tagging Extension (MTE) for spatial and temporal memory safety, Branch Target Identification (BTI) for forward-edge control flow integrity, Secure EL2 virtualization features in ARMv8.4+

---

# Debugging and Profiling ARM Assembly

Debugging embedded ARM systems requires understanding both software debugging techniques and hardware debug interfaces. ARM processors include dedicated debug hardware that enables real-time inspection and control without significantly impacting program execution.

## Debug Symbols and DWARF

Debug symbols are metadata that map machine code back to source code, enabling debuggers to show meaningful information instead of raw addresses. DWARF (Debugging With Attributed Record Formats) is the standard debug information format used on ARM systems.

**Debug Symbol Types:**

**Function Symbols** Map addresses to function names, showing which function contains the current program counter. Generated by assembler directives like `.type` and `.size`:

```assembly
.global my_function
.type my_function, %function

my_function:
    PUSH {r4, lr}
    MOV r4, r0
    BL helper_function
    MOV r0, r4
    POP {r4, pc}
    
.size my_function, .-my_function
```

**Variable Symbols** Associate memory addresses with variable names. For global variables:

```assembly
.data
.align 4
.global counter
.type counter, %object
.size counter, 4

counter:
    .word 0
```

**Line Number Information** DWARF line number tables map instruction addresses to source file line numbers. Generated automatically by assemblers when debug flags are enabled (`-g` flag for GNU assembler):

```bash
arm-none-eabi-as -g -mcpu=cortex-m4 -mthumb source.s -o source.o
```

**DWARF Structure:**

DWARF organizes debug information into sections within the ELF file:

- `.debug_info`: Compilation unit information, types, variables
- `.debug_line`: Line number program (address to source line mapping)
- `.debug_frame`: Call frame information for stack unwinding
- `.debug_abbrev`: Abbreviation tables for compression
- `.debug_str`: String table for names
- `.debug_aranges`: Address ranges for compilation units
- `.debug_loc`: Location lists for variables that move
- `.debug_ranges`: Non-contiguous address ranges

**Example** - Viewing debug sections:

```bash
arm-none-eabi-readelf -S program.elf | grep debug
```

**Output:**

```
[26] .debug_aranges    PROGBITS        00000000 001234 000020 00
[27] .debug_info       PROGBITS        00000000 001254 000a84 00
[28] .debug_abbrev     PROGBITS        00000000 001cd8 000235 00
[29] .debug_line       PROGBITS        00000000 001f0d 000456 00
```

**DWARF Call Frame Information (CFI):**

CFI directives help debuggers unwind the stack to display call traces correctly. Essential when debugging without frame pointers:

```assembly
.cfi_sections .debug_frame

my_function:
    .cfi_startproc
    
    PUSH {r4, r5, lr}
    .cfi_def_cfa_offset 12
    .cfi_offset r4, -12
    .cfi_offset r5, -8
    .cfi_offset lr, -4
    
    SUB sp, sp, #16
    .cfi_def_cfa_offset 28
    
    @ Function body
    
    ADD sp, sp, #16
    .cfi_def_cfa_offset 12
    
    POP {r4, r5, pc}
    .cfi_endproc
```

These directives inform the debugger about stack frame changes, enabling accurate backtraces.

**Generating Debug Information:**

For assembly files:

```bash
arm-none-eabi-as -g -mcpu=cortex-m4 -mthumb file.s -o file.o
```

For C files with inline assembly:

```bash
arm-none-eabi-gcc -g3 -O0 -mcpu=cortex-m4 -mthumb file.c -o file.o
```

The `-g3` flag includes macro definitions in debug information. `-O0` disables optimization for easier debugging.

**Stripping Debug Symbols:**

Production binaries often have debug symbols removed to reduce size:

```bash
arm-none-eabi-strip --strip-debug program.elf -o program_stripped.elf
```

Debug symbols can be kept in a separate file:

```bash
arm-none-eabi-objcopy --only-keep-debug program.elf program.debug
arm-none-eabi-strip --strip-debug program.elf
arm-none-eabi-objcopy --add-gnu-debuglink=program.debug program.elf
```

This allows debugging with the original ELF while deploying smaller stripped binaries.

## Breakpoints and Watchpoints

ARM debug hardware provides breakpoint and watchpoint capabilities through the Debug Access Port (DAP) and dedicated debug registers. These operate independently of program execution, with minimal performance impact.

**Hardware Breakpoints:**

Hardware breakpoints halt execution when the program counter reaches a specific address. Implemented using comparator units in the Flash Patch and Breakpoint (FPB) unit on Cortex-M or debug registers on Cortex-A.

**Cortex-M FPB Unit:**

The FPB typically provides 6-8 instruction comparators:

- **FP_CTRL**: Control and status register
- **FP_COMP0-7**: Comparator registers for addresses
- **FP_REMAP**: Instruction remapping (literal patch)

Setting a breakpoint writes the target address to a comparator register:

```c
// Pseudocode for FPB configuration
#define FP_CTRL   (*((volatile uint32_t*)0xE0002000))
#define FP_COMP0  (*((volatile uint32_t*)0xE0002008))

// Enable FPB
FP_CTRL |= 0x00000003;  // KEY=1, ENABLE=1

// Set breakpoint at 0x08000400
FP_COMP0 = 0x08000400 | 0x1;  // Address with ENABLE bit
```

When PC matches the comparator value, the processor generates a debug event and halts.

**Software Breakpoints:**

When hardware breakpoints are exhausted, debuggers use software breakpoints by replacing instructions with breakpoint instructions:

- **ARM mode**: `0xE1200070` (BKPT instruction)
- **Thumb mode**: `0xBE00` (BKPT #0)

**Example** - BKPT instruction in assembly:

```assembly
.thumb
main:
    MOV r0, #5
    BKPT #0          @ Software breakpoint
    MOV r1, #10
```

The debugger saves the original instruction, replaces it with BKPT, and restores it when removing the breakpoint. This modifies program memory, which can be problematic in flash or when code is checksummed.

**Watchpoints (Data Breakpoints):**

Watchpoints monitor memory accesses and trigger when specific addresses are read, written, or accessed. Implemented via the Data Watchpoint and Trace (DWT) unit on Cortex-M.

**DWT Comparators:**

Typically 4 comparators support:

- **Read watchpoints**: Break on memory read
- **Write watchpoints**: Break on memory write
- **Access watchpoints**: Break on read or write

**Example** - Setting a write watchpoint:

```c
// DWT registers
#define DWT_CTRL       (*((volatile uint32_t*)0xE0001000))
#define DWT_COMP0      (*((volatile uint32_t*)0xE0001020))
#define DWT_MASK0      (*((volatile uint32_t*)0xE0001024))
#define DWT_FUNCTION0  (*((volatile uint32_t*)0xE0001028))

// Enable DWT
DWT_CTRL |= 0x00000001;

// Watch writes to 0x20000100
DWT_COMP0 = 0x20000100;
DWT_MASK0 = 0;              // Match exact address
DWT_FUNCTION0 = 0x5;        // FUNCTION=5 (write watchpoint)
```

**GDB Usage:**

Setting breakpoints and watchpoints in GDB:

```gdb
# Hardware breakpoint at function
hbreak my_function

# Hardware breakpoint at address
hbreak *0x08000400

# Software breakpoint
break my_function

# Write watchpoint
watch variable_name
watch *0x20000100

# Read watchpoint
rwatch variable_name

# Access watchpoint (read or write)
awatch variable_name

# Conditional breakpoint
break my_function if r0 == 5
```

**Breakpoint Limitations:**

On Cortex-M4:

- 6-8 hardware instruction breakpoints (FPB)
- 4 hardware data watchpoints (DWT)
- Unlimited software breakpoints (if flash can be modified)

[Inference] Software breakpoints are typically used for most breakpoints, reserving hardware breakpoints for flash/ROM code that cannot be modified.

**Tracepoints:**

Some ARM cores support tracing without halting via the Embedded Trace Macrocell (ETM). The processor logs execution information to a trace buffer while running at full speed.

## Single-Stepping

Single-stepping executes one instruction at a time, allowing detailed program flow examination. ARM debug hardware provides multiple stepping modes.

**Cortex-M Debug Halting:**

The Debug Halting Control and Status Register (DHCSR) controls debug state:

```c
#define DHCSR  (*((volatile uint32_t*)0xE000EDF0))
#define DCRSR  (*((volatile uint32_t*)0xE000EDF4))
#define DCRDR  (*((volatile uint32_t*)0xE000EDF8))

// Halt the processor
DHCSR = 0xA05F0003;  // Key=0xA05F, C_DEBUGEN=1, C_HALT=1

// Single-step (step one instruction and halt)
DHCSR = 0xA05F0007;  // C_DEBUGEN=1, C_HALT=1, C_STEP=1
```

**Step Modes:**

**Instruction Step** Executes exactly one instruction regardless of type. Branches, calls, and returns all count as single steps.

```gdb
stepi    # Step one instruction
si       # Shorthand
```

**Step Over** Executes one source line or instruction, treating function calls as single operations. The debugger sets a temporary breakpoint after the call instruction.

```gdb
next     # Step over (source level)
nexti    # Step over (instruction level)
ni       # Shorthand
```

**Example** sequence:

```assembly
@ PC at this line
BL function_call    @ Step-over places breakpoint at next line
MOV r0, #1          @ Execution stops here
```

**Step Into** Steps into function calls, allowing inspection of called functions:

```gdb
step     # Step into (source level)
stepi    # Step into (instruction level)
si       # Shorthand
```

**Step Out** Continues execution until the current function returns:

```gdb
finish   # Run until function returns
```

This sets a temporary breakpoint at the return address on the stack.

**Hardware Single-Stepping:**

On Cortex-M, the C_STEP bit in DHCSR enables hardware single-step. The processor executes one instruction then immediately re-enters debug state. The debugger reads registers and memory after each step.

**Interrupt Behavior During Stepping:**

**C_MASKINTS Bit:** Controls whether interrupts are masked during single-step:

```c
// Step with interrupts masked
DHCSR = 0xA05F000F;  // C_MASKINTS=1, C_STEP=1, C_HALT=1, C_DEBUGEN=1

// Step with interrupts enabled
DHCSR = 0xA05F0007;  // C_MASKINTS=0, C_STEP=1, C_HALT=1, C_DEBUGEN=1
```

With interrupts enabled during debug, interrupt handlers may execute between steps, complicating debugging. [Inference] Most debuggers mask interrupts by default during single-stepping to prevent confusion.

**Step Over Special Instructions:**

**WFI (Wait For Interrupt):** Single-stepping over WFI can be problematic. The instruction waits for an interrupt, potentially hanging the debugger. Modern debuggers handle this by:

1. Detecting WFI instructions
2. Setting a breakpoint after WFI
3. Continuing execution
4. Halting at the breakpoint when an interrupt occurs

**Branch with Link (BL):** Step-over detects BL instructions and sets a temporary breakpoint at the instruction following the BL, then continues execution.

**Example** - Manual single-step implementation:

```assembly
debug_single_step:
    PUSH {lr}
    
    @ Enable debug
    LDR r0, =DHCSR
    LDR r1, =0xA05F0003     @ Halt with debug enabled
    STR r1, [r0]
    
    @ Wait for halt
1:  LDR r1, [r0]
    TST r1, #0x00020000     @ S_HALT bit
    BEQ 1b
    
    @ Single-step
    LDR r1, =0xA05F0007     @ Step, halt, debug enabled
    STR r1, [r0]
    
    @ Wait for step complete
2:  LDR r1, [r0]
    TST r1, #0x00020000
    BEQ 2b
    
    POP {pc}
```

**GDB Stepping Commands Summary:**

```gdb
si / stepi          # Step one instruction (into calls)
ni / nexti          # Step one instruction (over calls)
step                # Step one source line (into calls)
next                # Step one source line (over calls)
finish              # Run until current function returns
until               # Run until past current loop
advance <location>  # Run until specific location
```

## Register and Memory Inspection

Debug interfaces provide full access to processor registers and memory, even while the processor is halted. This enables complete system state inspection.

**Cortex-M Debug Registers:**

**DHCSR (Debug Halting Control and Status Register):** Shows current debug state and controls halting:

```
Bits [31:16]: Debug key (0xA05F for writes)
Bit 25: S_RESET_ST - Reset status
Bit 24: S_RETIRE_ST - Instruction retired
Bit 19: S_LOCKUP - Processor in lockup
Bit 18: S_SLEEP - Processor sleeping
Bit 17: S_HALT - Processor halted
Bit 16: S_REGRDY - Register read/write available
Bit 3: C_MASKINTS - Mask interrupts when stepping
Bit 2: C_STEP - Single-step
Bit 1: C_HALT - Halt request
Bit 0: C_DEBUGEN - Debug enabled
```

**DCRSR (Debug Core Register Selector):** Selects which register to read/write via DCRDR:

```
Bit 16: REGWnR - 1=write, 0=read
Bits [4:0]: REGSEL - Register number
```

Register numbers:

- 0-12: R0-R12
- 13: SP (current stack pointer)
- 14: LR
- 15: PC
- 16: xPSR (combined program status)
- 17: MSP (Main Stack Pointer)
- 18: PSP (Process Stack Pointer)
- 20: CONTROL/FAULTMASK/BASEPRI/PRIMASK

**DCRDR (Debug Core Register Data):** Holds register value for transfer:

**Example** - Reading R0:

```c
// Select R0 for read
DCRSR = 0;  // REGWnR=0 (read), REGSEL=0 (R0)

// Wait for ready
while (!(DHCSR & 0x00010000));

// Read value from DCRDR
uint32_t r0_value = DCRDR;
```

**Example** - Writing to PC:

```c
// Write new PC value
DCRDR = 0x08001000;

// Select PC for write
DCRSR = 0x0001000F;  // REGWnR=1 (write), REGSEL=15 (PC)

// Wait for complete
while (!(DHCSR & 0x00010000));
```

**GDB Register Inspection:**

```gdb
# Display all general-purpose registers
info registers

# Display specific register
print $r0
p/x $r0          # Hexadecimal format
p/d $r0          # Decimal format
p/t $r0          # Binary format

# Display special registers
info all-registers
print $sp
print $pc
print $cpsr      # Program status register (Cortex-A)
print $xpsr      # Combined status register (Cortex-M)

# Modify register
set $r0 = 0x1234
set $pc = main
```

**CPSR/xPSR Flags:**

On Cortex-A (CPSR) and Cortex-M (xPSR), the status register contains condition flags:

```
Bit 31: N - Negative flag
Bit 30: Z - Zero flag
Bit 29: C - Carry flag
Bit 28: V - Overflow flag
Bit 27: Q - Saturation flag
Bits [15:10]: ICI/IT - If-Then state (Thumb)
Bit 9: E - Endianness (Cortex-A)
Bit 24: T - Thumb state (Cortex-A)
Bits [8:0]: Exception number (Cortex-M)
```

**Example** - Checking flags in GDB:

```gdb
# Display status register
print $xpsr

# Check if zero flag is set
print ($xpsr & (1 << 30)) != 0
```

**Memory Inspection:**

**Direct Memory Access:** Debug interfaces access memory through the AHB-AP (AHB Access Port) or APB-AP, bypassing the processor core.

**GDB Memory Commands:**

```gdb
# Examine memory
x/nfu address

# n = count
# f = format (x=hex, d=decimal, u=unsigned, o=octal, t=binary, a=address, c=char, s=string)
# u = unit size (b=byte, h=halfword, w=word, g=giant 8 bytes)

# Examples:
x/16wx 0x20000000    # 16 words in hex starting at 0x20000000
x/32xb 0x08000000    # 32 bytes in hex
x/10i $pc            # 10 instructions at PC
x/s 0x20001000       # String at address

# Display array
x/10dw array_name

# Write memory
set *((uint32_t*)0x20000100) = 0x12345678
set {unsigned int}0x20000100 = 0x12345678
```

**Memory Regions:**

Understanding memory regions is critical for correct debugging:

**Cortex-M Memory Map:**

```
0x00000000 - 0x1FFFFFFF: Code (512MB)
0x20000000 - 0x3FFFFFFF: SRAM (512MB)
0x40000000 - 0x5FFFFFFF: Peripheral (512MB)
0x60000000 - 0x9FFFFFFF: External RAM (1GB)
0xA0000000 - 0xDFFFFFFF: External device (1GB)
0xE0000000 - 0xE00FFFFF: Private peripheral bus
0xE0100000 - 0xFFFFFFFF: System (vendor-specific)
```

**Example** - Inspecting stack:

```gdb
# Display current stack pointer
print $sp

# Show stack contents (32 words)
x/32wx $sp

# Display stack frame
info frame

# Show all stack frames
backtrace
bt
```

**Peripheral Register Inspection:**

Memory-mapped peripheral registers can be examined like any memory:

```gdb
# Define peripheral base
set $GPIOA_BASE = 0x40020000

# Read GPIO input data register
x/1wx ($GPIOA_BASE + 0x10)

# Read with symbolic name
print/x *((uint32_t*)0x40020010)

# Write to GPIO output register
set *((uint32_t*)0x40020014) = 0x0020
```

**Automatic Register Display:**

Configure GDB to display registers after each step:

```gdb
# Show registers after every command
define hook-stop
    info registers
end

# Or display specific registers
define hook-stop
    printf "R0: 0x%08x  R1: 0x%08x  R2: 0x%08x\n", $r0, $r1, $r2
    printf "PC: 0x%08x  SP: 0x%08x  LR: 0x%08x\n", $pc, $sp, $lr
end
```

**Memory Dump to File:**

```gdb
# Dump memory region to binary file
dump binary memory filename.bin 0x20000000 0x20010000

# Dump as hex
dump ihex memory filename.hex 0x08000000 0x08020000

# Restore memory from file
restore filename.bin binary 0x20000000
```

**JTAG/SWD Memory Access:**

Debug probes (J-Link, ST-Link, CMSIS-DAP) access memory through debug protocols:

**SWD (Serial Wire Debug):** Two-wire protocol (SWDIO data, SWCLK clock) providing full debug access. SWD reads/writes use the Debug Port (DP) to access the Access Port (AP):

```
1. Select MEM-AP (memory access port)
2. Write target address to TAR (Transfer Address Register)
3. Read/write DRW (Data Read/Write register)
```

This happens transparently when using debuggers, but understanding the mechanism helps diagnose connection issues.

**OpenOCD Memory Commands:**

```tcl
# Read word at address
mdw 0x20000000

# Read 16 words
mdw 0x20000000 16

# Write word
mww 0x20000100 0x12345678

# Read byte
mdb 0x20000000

# Write byte
mwb 0x20000000 0xFF

# Read peripheral register
mdw 0x40020000

# Fill memory region
mww 0x20000000 0xDEADBEEF 256
```

**Flash Memory Inspection:**

Reading flash memory works identically to RAM, but writing requires flash programming algorithms:

```gdb
# Read flash
x/256xw 0x08000000

# Disassemble flash code
disassemble 0x08000000, 0x08000100

# Cannot directly write flash - use load instead
load program.elf
```

**Register Context During Exceptions:**

When an exception occurs on Cortex-M, hardware automatically stacks registers:

```
[SP-32]: R0
[SP-28]: R1
[SP-24]: R2
[SP-20]: R3
[SP-16]: R12
[SP-12]: LR (return address)
[SP-8]:  PC (exception address)
[SP-4]:  xPSR
```

**Example** - Examining exception frame:

```assembly
HardFault_Handler:
    @ Get stack pointer used for exception
    TST lr, #4
    ITE EQ
    MRSEQ r0, MSP
    MRSNE r0, PSP
    
    @ r0 now points to exception frame
    @ Can be examined in debugger or passed to C function
    B .
```

```gdb
# After hitting HardFault_Handler
x/8wx $r0
# Shows: R0, R1, R2, R3, R12, LR, PC, xPSR
```

**Debugging Optimized Code:**

Compiler optimizations can make debugging difficult. Registers hold multiple variables, values cached, code reordered:

```gdb
# Variables may be optimized out
print my_variable
# Result: <optimized out>

# Use disassembly to see actual instructions
disassemble

# Examine registers where values might be
info registers
```

**Key Points:**

- Debug symbols map machine code to source code using DWARF format
- Hardware breakpoints use comparator units; software breakpoints modify code with BKPT instructions
- Watchpoints monitor memory access and trigger on read/write operations
- Single-stepping executes one instruction at a time with hardware support
- Debug registers (DHCSR, DCRSR, DCRDR) provide full processor and memory access while halted
- Memory-mapped peripherals can be inspected and modified like any memory location
- GDB and OpenOCD provide high-level interfaces to low-level debug hardware

---

# Debugging and Profiling ARM Assembly

Debugging embedded ARM systems requires understanding both software debugging techniques and hardware debug interfaces. ARM processors include dedicated debug hardware that enables real-time inspection and control without significantly impacting program execution.

## Debug Symbols and DWARF

Debug symbols are metadata that map machine code back to source code, enabling debuggers to show meaningful information instead of raw addresses. DWARF (Debugging With Attributed Record Formats) is the standard debug information format used on ARM systems.

**Debug Symbol Types:**

**Function Symbols** Map addresses to function names, showing which function contains the current program counter. Generated by assembler directives like `.type` and `.size`:

```assembly
.global my_function
.type my_function, %function

my_function:
    PUSH {r4, lr}
    MOV r4, r0
    BL helper_function
    MOV r0, r4
    POP {r4, pc}
    
.size my_function, .-my_function
```

**Variable Symbols** Associate memory addresses with variable names. For global variables:

```assembly
.data
.align 4
.global counter
.type counter, %object
.size counter, 4

counter:
    .word 0
```

**Line Number Information** DWARF line number tables map instruction addresses to source file line numbers. Generated automatically by assemblers when debug flags are enabled (`-g` flag for GNU assembler):

```bash
arm-none-eabi-as -g -mcpu=cortex-m4 -mthumb source.s -o source.o
```

**DWARF Structure:**

DWARF organizes debug information into sections within the ELF file:

- `.debug_info`: Compilation unit information, types, variables
- `.debug_line`: Line number program (address to source line mapping)
- `.debug_frame`: Call frame information for stack unwinding
- `.debug_abbrev`: Abbreviation tables for compression
- `.debug_str`: String table for names
- `.debug_aranges`: Address ranges for compilation units
- `.debug_loc`: Location lists for variables that move
- `.debug_ranges`: Non-contiguous address ranges

**Example** - Viewing debug sections:

```bash
arm-none-eabi-readelf -S program.elf | grep debug
```

**Output:**

```
[26] .debug_aranges    PROGBITS        00000000 001234 000020 00
[27] .debug_info       PROGBITS        00000000 001254 000a84 00
[28] .debug_abbrev     PROGBITS        00000000 001cd8 000235 00
[29] .debug_line       PROGBITS        00000000 001f0d 000456 00
```

**DWARF Call Frame Information (CFI):**

CFI directives help debuggers unwind the stack to display call traces correctly. Essential when debugging without frame pointers:

```assembly
.cfi_sections .debug_frame

my_function:
    .cfi_startproc
    
    PUSH {r4, r5, lr}
    .cfi_def_cfa_offset 12
    .cfi_offset r4, -12
    .cfi_offset r5, -8
    .cfi_offset lr, -4
    
    SUB sp, sp, #16
    .cfi_def_cfa_offset 28
    
    @ Function body
    
    ADD sp, sp, #16
    .cfi_def_cfa_offset 12
    
    POP {r4, r5, pc}
    .cfi_endproc
```

These directives inform the debugger about stack frame changes, enabling accurate backtraces.

**Generating Debug Information:**

For assembly files:

```bash
arm-none-eabi-as -g -mcpu=cortex-m4 -mthumb file.s -o file.o
```

For C files with inline assembly:

```bash
arm-none-eabi-gcc -g3 -O0 -mcpu=cortex-m4 -mthumb file.c -o file.o
```

The `-g3` flag includes macro definitions in debug information. `-O0` disables optimization for easier debugging.

**Stripping Debug Symbols:**

Production binaries often have debug symbols removed to reduce size:

```bash
arm-none-eabi-strip --strip-debug program.elf -o program_stripped.elf
```

Debug symbols can be kept in a separate file:

```bash
arm-none-eabi-objcopy --only-keep-debug program.elf program.debug
arm-none-eabi-strip --strip-debug program.elf
arm-none-eabi-objcopy --add-gnu-debuglink=program.debug program.elf
```

This allows debugging with the original ELF while deploying smaller stripped binaries.

## Breakpoints and Watchpoints

ARM debug hardware provides breakpoint and watchpoint capabilities through the Debug Access Port (DAP) and dedicated debug registers. These operate independently of program execution, with minimal performance impact.

**Hardware Breakpoints:**

Hardware breakpoints halt execution when the program counter reaches a specific address. Implemented using comparator units in the Flash Patch and Breakpoint (FPB) unit on Cortex-M or debug registers on Cortex-A.

**Cortex-M FPB Unit:**

The FPB typically provides 6-8 instruction comparators:

- **FP_CTRL**: Control and status register
- **FP_COMP0-7**: Comparator registers for addresses
- **FP_REMAP**: Instruction remapping (literal patch)

Setting a breakpoint writes the target address to a comparator register:

```c
// Pseudocode for FPB configuration
#define FP_CTRL   (*((volatile uint32_t*)0xE0002000))
#define FP_COMP0  (*((volatile uint32_t*)0xE0002008))

// Enable FPB
FP_CTRL |= 0x00000003;  // KEY=1, ENABLE=1

// Set breakpoint at 0x08000400
FP_COMP0 = 0x08000400 | 0x1;  // Address with ENABLE bit
```

When PC matches the comparator value, the processor generates a debug event and halts.

**Software Breakpoints:**

When hardware breakpoints are exhausted, debuggers use software breakpoints by replacing instructions with breakpoint instructions:

- **ARM mode**: `0xE1200070` (BKPT instruction)
- **Thumb mode**: `0xBE00` (BKPT #0)

**Example** - BKPT instruction in assembly:

```assembly
.thumb
main:
    MOV r0, #5
    BKPT #0          @ Software breakpoint
    MOV r1, #10
```

The debugger saves the original instruction, replaces it with BKPT, and restores it when removing the breakpoint. This modifies program memory, which can be problematic in flash or when code is checksummed.

**Watchpoints (Data Breakpoints):**

Watchpoints monitor memory accesses and trigger when specific addresses are read, written, or accessed. Implemented via the Data Watchpoint and Trace (DWT) unit on Cortex-M.

**DWT Comparators:**

Typically 4 comparators support:

- **Read watchpoints**: Break on memory read
- **Write watchpoints**: Break on memory write
- **Access watchpoints**: Break on read or write

**Example** - Setting a write watchpoint:

```c
// DWT registers
#define DWT_CTRL       (*((volatile uint32_t*)0xE0001000))
#define DWT_COMP0      (*((volatile uint32_t*)0xE0001020))
#define DWT_MASK0      (*((volatile uint32_t*)0xE0001024))
#define DWT_FUNCTION0  (*((volatile uint32_t*)0xE0001028))

// Enable DWT
DWT_CTRL |= 0x00000001;

// Watch writes to 0x20000100
DWT_COMP0 = 0x20000100;
DWT_MASK0 = 0;              // Match exact address
DWT_FUNCTION0 = 0x5;        // FUNCTION=5 (write watchpoint)
```

**GDB Usage:**

Setting breakpoints and watchpoints in GDB:

```gdb
# Hardware breakpoint at function
hbreak my_function

# Hardware breakpoint at address
hbreak *0x08000400

# Software breakpoint
break my_function

# Write watchpoint
watch variable_name
watch *0x20000100

# Read watchpoint
rwatch variable_name

# Access watchpoint (read or write)
awatch variable_name

# Conditional breakpoint
break my_function if r0 == 5
```

**Breakpoint Limitations:**

On Cortex-M4:

- 6-8 hardware instruction breakpoints (FPB)
- 4 hardware data watchpoints (DWT)
- Unlimited software breakpoints (if flash can be modified)

[Inference] Software breakpoints are typically used for most breakpoints, reserving hardware breakpoints for flash/ROM code that cannot be modified.

**Tracepoints:**

Some ARM cores support tracing without halting via the Embedded Trace Macrocell (ETM). The processor logs execution information to a trace buffer while running at full speed.

## Single-Stepping

Single-stepping executes one instruction at a time, allowing detailed program flow examination. ARM debug hardware provides multiple stepping modes.

**Cortex-M Debug Halting:**

The Debug Halting Control and Status Register (DHCSR) controls debug state:

```c
#define DHCSR  (*((volatile uint32_t*)0xE000EDF0))
#define DCRSR  (*((volatile uint32_t*)0xE000EDF4))
#define DCRDR  (*((volatile uint32_t*)0xE000EDF8))

// Halt the processor
DHCSR = 0xA05F0003;  // Key=0xA05F, C_DEBUGEN=1, C_HALT=1

// Single-step (step one instruction and halt)
DHCSR = 0xA05F0007;  // C_DEBUGEN=1, C_HALT=1, C_STEP=1
```

**Step Modes:**

**Instruction Step** Executes exactly one instruction regardless of type. Branches, calls, and returns all count as single steps.

```gdb
stepi    # Step one instruction
si       # Shorthand
```

**Step Over** Executes one source line or instruction, treating function calls as single operations. The debugger sets a temporary breakpoint after the call instruction.

```gdb
next     # Step over (source level)
nexti    # Step over (instruction level)
ni       # Shorthand
```

**Example** sequence:

```assembly
@ PC at this line
BL function_call    @ Step-over places breakpoint at next line
MOV r0, #1          @ Execution stops here
```

**Step Into** Steps into function calls, allowing inspection of called functions:

```gdb
step     # Step into (source level)
stepi    # Step into (instruction level)
si       # Shorthand
```

**Step Out** Continues execution until the current function returns:

```gdb
finish   # Run until function returns
```

This sets a temporary breakpoint at the return address on the stack.

**Hardware Single-Stepping:**

On Cortex-M, the C_STEP bit in DHCSR enables hardware single-step. The processor executes one instruction then immediately re-enters debug state. The debugger reads registers and memory after each step.

**Interrupt Behavior During Stepping:**

**C_MASKINTS Bit:** Controls whether interrupts are masked during single-step:

```c
// Step with interrupts masked
DHCSR = 0xA05F000F;  // C_MASKINTS=1, C_STEP=1, C_HALT=1, C_DEBUGEN=1

// Step with interrupts enabled
DHCSR = 0xA05F0007;  // C_MASKINTS=0, C_STEP=1, C_HALT=1, C_DEBUGEN=1
```

With interrupts enabled during debug, interrupt handlers may execute between steps, complicating debugging. [Inference] Most debuggers mask interrupts by default during single-stepping to prevent confusion.

**Step Over Special Instructions:**

**WFI (Wait For Interrupt):** Single-stepping over WFI can be problematic. The instruction waits for an interrupt, potentially hanging the debugger. Modern debuggers handle this by:

1. Detecting WFI instructions
2. Setting a breakpoint after WFI
3. Continuing execution
4. Halting at the breakpoint when an interrupt occurs

**Branch with Link (BL):** Step-over detects BL instructions and sets a temporary breakpoint at the instruction following the BL, then continues execution.

**Example** - Manual single-step implementation:

```assembly
debug_single_step:
    PUSH {lr}
    
    @ Enable debug
    LDR r0, =DHCSR
    LDR r1, =0xA05F0003     @ Halt with debug enabled
    STR r1, [r0]
    
    @ Wait for halt
1:  LDR r1, [r0]
    TST r1, #0x00020000     @ S_HALT bit
    BEQ 1b
    
    @ Single-step
    LDR r1, =0xA05F0007     @ Step, halt, debug enabled
    STR r1, [r0]
    
    @ Wait for step complete
2:  LDR r1, [r0]
    TST r1, #0x00020000
    BEQ 2b
    
    POP {pc}
```

**GDB Stepping Commands Summary:**

```gdb
si / stepi          # Step one instruction (into calls)
ni / nexti          # Step one instruction (over calls)
step                # Step one source line (into calls)
next                # Step one source line (over calls)
finish              # Run until current function returns
until               # Run until past current loop
advance <location>  # Run until specific location
```

## Register and Memory Inspection

Debug interfaces provide full access to processor registers and memory, even while the processor is halted. This enables complete system state inspection.

**Cortex-M Debug Registers:**

**DHCSR (Debug Halting Control and Status Register):** Shows current debug state and controls halting:

```
Bits [31:16]: Debug key (0xA05F for writes)
Bit 25: S_RESET_ST - Reset status
Bit 24: S_RETIRE_ST - Instruction retired
Bit 19: S_LOCKUP - Processor in lockup
Bit 18: S_SLEEP - Processor sleeping
Bit 17: S_HALT - Processor halted
Bit 16: S_REGRDY - Register read/write available
Bit 3: C_MASKINTS - Mask interrupts when stepping
Bit 2: C_STEP - Single-step
Bit 1: C_HALT - Halt request
Bit 0: C_DEBUGEN - Debug enabled
```

**DCRSR (Debug Core Register Selector):** Selects which register to read/write via DCRDR:

```
Bit 16: REGWnR - 1=write, 0=read
Bits [4:0]: REGSEL - Register number
```

Register numbers:

- 0-12: R0-R12
- 13: SP (current stack pointer)
- 14: LR
- 15: PC
- 16: xPSR (combined program status)
- 17: MSP (Main Stack Pointer)
- 18: PSP (Process Stack Pointer)
- 20: CONTROL/FAULTMASK/BASEPRI/PRIMASK

**DCRDR (Debug Core Register Data):** Holds register value for transfer:

**Example** - Reading R0:

```c
// Select R0 for read
DCRSR = 0;  // REGWnR=0 (read), REGSEL=0 (R0)

// Wait for ready
while (!(DHCSR & 0x00010000));

// Read value from DCRDR
uint32_t r0_value = DCRDR;
```

**Example** - Writing to PC:

```c
// Write new PC value
DCRDR = 0x08001000;

// Select PC for write
DCRSR = 0x0001000F;  // REGWnR=1 (write), REGSEL=15 (PC)

// Wait for complete
while (!(DHCSR & 0x00010000));
```

**GDB Register Inspection:**

```gdb
# Display all general-purpose registers
info registers

# Display specific register
print $r0
p/x $r0          # Hexadecimal format
p/d $r0          # Decimal format
p/t $r0          # Binary format

# Display special registers
info all-registers
print $sp
print $pc
print $cpsr      # Program status register (Cortex-A)
print $xpsr      # Combined status register (Cortex-M)

# Modify register
set $r0 = 0x1234
set $pc = main
```

**CPSR/xPSR Flags:**

On Cortex-A (CPSR) and Cortex-M (xPSR), the status register contains condition flags:

```
Bit 31: N - Negative flag
Bit 30: Z - Zero flag
Bit 29: C - Carry flag
Bit 28: V - Overflow flag
Bit 27: Q - Saturation flag
Bits [15:10]: ICI/IT - If-Then state (Thumb)
Bit 9: E - Endianness (Cortex-A)
Bit 24: T - Thumb state (Cortex-A)
Bits [8:0]: Exception number (Cortex-M)
```

**Example** - Checking flags in GDB:

```gdb
# Display status register
print $xpsr

# Check if zero flag is set
print ($xpsr & (1 << 30)) != 0
```

**Memory Inspection:**

**Direct Memory Access:** Debug interfaces access memory through the AHB-AP (AHB Access Port) or APB-AP, bypassing the processor core.

**GDB Memory Commands:**

```gdb
# Examine memory
x/nfu address

# n = count
# f = format (x=hex, d=decimal, u=unsigned, o=octal, t=binary, a=address, c=char, s=string)
# u = unit size (b=byte, h=halfword, w=word, g=giant 8 bytes)

# Examples:
x/16wx 0x20000000    # 16 words in hex starting at 0x20000000
x/32xb 0x08000000    # 32 bytes in hex
x/10i $pc            # 10 instructions at PC
x/s 0x20001000       # String at address

# Display array
x/10dw array_name

# Write memory
set *((uint32_t*)0x20000100) = 0x12345678
set {unsigned int}0x20000100 = 0x12345678
```

**Memory Regions:**

Understanding memory regions is critical for correct debugging:

**Cortex-M Memory Map:**

```
0x00000000 - 0x1FFFFFFF: Code (512MB)
0x20000000 - 0x3FFFFFFF: SRAM (512MB)
0x40000000 - 0x5FFFFFFF: Peripheral (512MB)
0x60000000 - 0x9FFFFFFF: External RAM (1GB)
0xA0000000 - 0xDFFFFFFF: External device (1GB)
0xE0000000 - 0xE00FFFFF: Private peripheral bus
0xE0100000 - 0xFFFFFFFF: System (vendor-specific)
```

**Example** - Inspecting stack:

```gdb
# Display current stack pointer
print $sp

# Show stack contents (32 words)
x/32wx $sp

# Display stack frame
info frame

# Show all stack frames
backtrace
bt
```

**Peripheral Register Inspection:**

Memory-mapped peripheral registers can be examined like any memory:

```gdb
# Define peripheral base
set $GPIOA_BASE = 0x40020000

# Read GPIO input data register
x/1wx ($GPIOA_BASE + 0x10)

# Read with symbolic name
print/x *((uint32_t*)0x40020010)

# Write to GPIO output register
set *((uint32_t*)0x40020014) = 0x0020
```

**Automatic Register Display:**

Configure GDB to display registers after each step:

```gdb
# Show registers after every command
define hook-stop
    info registers
end

# Or display specific registers
define hook-stop
    printf "R0: 0x%08x  R1: 0x%08x  R2: 0x%08x\n", $r0, $r1, $r2
    printf "PC: 0x%08x  SP: 0x%08x  LR: 0x%08x\n", $pc, $sp, $lr
end
```

**Memory Dump to File:**

```gdb
# Dump memory region to binary file
dump binary memory filename.bin 0x20000000 0x20010000

# Dump as hex
dump ihex memory filename.hex 0x08000000 0x08020000

# Restore memory from file
restore filename.bin binary 0x20000000
```

**JTAG/SWD Memory Access:**

Debug probes (J-Link, ST-Link, CMSIS-DAP) access memory through debug protocols:

**SWD (Serial Wire Debug):** Two-wire protocol (SWDIO data, SWCLK clock) providing full debug access. SWD reads/writes use the Debug Port (DP) to access the Access Port (AP):

```
1. Select MEM-AP (memory access port)
2. Write target address to TAR (Transfer Address Register)
3. Read/write DRW (Data Read/Write register)
```

This happens transparently when using debuggers, but understanding the mechanism helps diagnose connection issues.

**OpenOCD Memory Commands:**

```tcl
# Read word at address
mdw 0x20000000

# Read 16 words
mdw 0x20000000 16

# Write word
mww 0x20000100 0x12345678

# Read byte
mdb 0x20000000

# Write byte
mwb 0x20000000 0xFF

# Read peripheral register
mdw 0x40020000

# Fill memory region
mww 0x20000000 0xDEADBEEF 256
```

**Flash Memory Inspection:**

Reading flash memory works identically to RAM, but writing requires flash programming algorithms:

```gdb
# Read flash
x/256xw 0x08000000

# Disassemble flash code
disassemble 0x08000000, 0x08000100

# Cannot directly write flash - use load instead
load program.elf
```

**Register Context During Exceptions:**

When an exception occurs on Cortex-M, hardware automatically stacks registers:

```
[SP-32]: R0
[SP-28]: R1
[SP-24]: R2
[SP-20]: R3
[SP-16]: R12
[SP-12]: LR (return address)
[SP-8]:  PC (exception address)
[SP-4]:  xPSR
```

**Example** - Examining exception frame:

```assembly
HardFault_Handler:
    @ Get stack pointer used for exception
    TST lr, #4
    ITE EQ
    MRSEQ r0, MSP
    MRSNE r0, PSP
    
    @ r0 now points to exception frame
    @ Can be examined in debugger or passed to C function
    B .
```

```gdb
# After hitting HardFault_Handler
x/8wx $r0
# Shows: R0, R1, R2, R3, R12, LR, PC, xPSR
```

**Debugging Optimized Code:**

Compiler optimizations can make debugging difficult. Registers hold multiple variables, values cached, code reordered:

```gdb
# Variables may be optimized out
print my_variable
# Result: <optimized out>

# Use disassembly to see actual instructions
disassemble

# Examine registers where values might be
info registers
```

**Key Points:**

- Debug symbols map machine code to source code using DWARF format
- Hardware breakpoints use comparator units; software breakpoints modify code with BKPT instructions
- Watchpoints monitor memory access and trigger on read/write operations
- Single-stepping executes one instruction at a time with hardware support
- Debug registers (DHCSR, DCRSR, DCRDR) provide full processor and memory access while halted
- Memory-mapped peripherals can be inspected and modified like any memory location
- GDB and OpenOCD provide high-level interfaces to low-level debug hardware

---

# Debugging and Profiling in ARM Assembly

Debugging and profiling are essential processes for developing reliable and efficient embedded systems. ARM processors provide extensive hardware support for debugging through dedicated debug interfaces, performance monitoring units, and trace capabilities. These features allow developers to identify bugs, measure execution characteristics, and optimize code performance at the instruction level.

# Core Dumps

Core dumps capture the complete system state at the moment of failure, including register contents, memory snapshots, and stack frames. In embedded systems without traditional file systems, core dumps must be stored in non-volatile memory or transmitted over debug interfaces.

## Core Dump Components

A comprehensive core dump includes:

- CPU register state (R0-R15, PSR, special registers)
- Stack memory contents
- Critical data sections
- Peripheral register states
- Exception/fault information
- Program counter and link register chain

## Fault Handler Implementation

```assembly
// Hard fault handler with state capture
    .syntax unified
    .thumb
    
    .section .bss
    .align 4
core_dump_buffer:
    .space 512                      // Reserve space for dump

    .section .text
    .global HardFault_Handler
    .type HardFault_Handler, %function
    
HardFault_Handler:
    // Determine which stack was in use
    TST     LR, #4                  // Check EXC_RETURN bit 2
    ITE     EQ
    MRSEQ   R0, MSP                 // Main stack pointer
    MRSNE   R0, PSP                 // Process stack pointer
    
    // R0 now points to stack frame
    // Save to core dump structure
    B       save_fault_info
    
save_fault_info:
    // Save stacked registers (hardware saved these)
    // Stack frame: R0, R1, R2, R3, R12, LR, PC, xPSR
    LDR     R1, =core_dump_buffer
    
    // Copy hardware-saved registers
    LDMIA   R0!, {R2-R9}            // Load 8 stacked registers
    STMIA   R1!, {R2-R9}            // Save to buffer
    
    // Save remaining core registers
    MRS     R2, MSP
    STR     R2, [R1], #4
    MRS     R2, PSP
    STR     R2, [R1], #4
    MRS     R2, PRIMASK
    STR     R2, [R1], #4
    MRS     R2, FAULTMASK
    STR     R2, [R1], #4
    MRS     R2, BASEPRI
    STR     R2, [R1], #4
    MRS     R2, CONTROL
    STR     R2, [R1], #4
    
    // Save fault status registers
    LDR     R2, =SCB_CFSR           // Configurable Fault Status
    LDR     R3, [R2]
    STR     R3, [R1], #4
    
    LDR     R2, =SCB_HFSR           // Hard Fault Status
    LDR     R3, [R2]
    STR     R3, [R1], #4
    
    LDR     R2, =SCB_MMFAR          // MemManage Fault Address
    LDR     R3, [R2]
    STR     R3, [R1], #4
    
    LDR     R2, =SCB_BFAR           // Bus Fault Address
    LDR     R3, [R2]
    STR     R3, [R1], #4
    
    // Trigger breakpoint for debugger
    BKPT    #0
    
    // If no debugger, infinite loop
fault_loop:
    B       fault_loop
```

## Stack Unwinding

Stack unwinding reconstructs the call chain leading to a fault:

```assembly
// Stack backtrace function
// R0 = current stack pointer
// R1 = buffer to store PC values
// R2 = max depth
stack_backtrace:
    PUSH    {R4-R7, LR}
    MOV     R4, R0                  // Current SP
    MOV     R5, R1                  // Output buffer
    MOV     R6, R2                  // Depth counter
    MOV     R7, #0                  // Frame counter
    
backtrace_loop:
    CMP     R7, R6
    BGE     backtrace_done
    
    // Validate stack pointer
    LDR     R0, =STACK_START
    CMP     R4, R0
    BLO     backtrace_invalid
    LDR     R0, =STACK_END
    CMP     R4, R0
    BHS     backtrace_invalid
    
    // Read saved LR from stack frame
    // ARM stack frame: R0-R3, R12, LR, PC, xPSR (8 words)
    LDR     R0, [R4, #24]           // PC at offset 24
    STR     R0, [R5], #4            // Store to output
    
    // Move to previous frame
    // [Inference: Frame size depends on function prologue]
    ADD     R4, R4, #32             // Typical frame size
    ADD     R7, R7, #1
    B       backtrace_loop
    
backtrace_done:
    MOV     R0, R7                  // Return frame count
    POP     {R4-R7, PC}
    
backtrace_invalid:
    MOV     R0, #-1                 // Error indicator
    POP     {R4-R7, PC}
```

## Memory Dump Functions

```assembly
// Dump memory region to buffer
// R0 = source address, R1 = destination, R2 = size in bytes
memory_dump:
    PUSH    {R4-R6, LR}
    MOV     R4, R0
    MOV     R5, R1
    MOV     R6, R2
    
    // Align to word boundary
    ANDS    R3, R6, #3
    BEQ     dump_aligned
    ADD     R6, R6, #4
    BIC     R6, R6, #3
    
dump_aligned:
    MOVS    R3, #0
dump_loop:
    CMP     R3, R6
    BGE     dump_done
    
    LDR     R0, [R4, R3]            // Read word
    STR     R0, [R5, R3]            // Write to dump
    ADD     R3, R3, #4
    B       dump_loop
    
dump_done:
    POP     {R4-R6, PC}
```

## Non-Volatile Storage

Storing core dumps in flash or EEPROM for post-mortem analysis:

```assembly
// Write core dump to flash
    LDR     R0, =core_dump_buffer
    LDR     R1, =FLASH_COREDUMP_ADDR
    LDR     R2, =512                // Dump size
    
    // Unlock flash
    BL      flash_unlock
    
    // Erase sector
    LDR     R0, =FLASH_COREDUMP_SECTOR
    BL      flash_erase_sector
    
    // Write data
    LDR     R0, =core_dump_buffer
    LDR     R1, =FLASH_COREDUMP_ADDR
    LDR     R2, =512
    BL      flash_write
    
    // Lock flash
    BL      flash_lock
    
    // Set flag indicating valid dump
    LDR     R0, =FLASH_DUMP_VALID_FLAG
    LDR     R1, =0xDEADBEEF
    STR     R1, [R0]
```

## Core Dump Analysis

Reading and interpreting stored dumps on next boot:

```assembly
// Check for core dump on startup
startup_check_dump:
    PUSH    {LR}
    
    // Check validity flag
    LDR     R0, =FLASH_DUMP_VALID_FLAG
    LDR     R1, [R0]
    LDR     R2, =0xDEADBEEF
    CMP     R1, R2
    BNE     no_dump_found
    
    // Valid dump exists
    LDR     R0, =FLASH_COREDUMP_ADDR
    BL      analyze_core_dump
    
    // Clear flag
    LDR     R0, =FLASH_DUMP_VALID_FLAG
    MOV     R1, #0
    STR     R1, [R0]
    
no_dump_found:
    POP     {PC}

analyze_core_dump:
    PUSH    {R4, LR}
    MOV     R4, R0                  // Dump address
    
    // Extract PC at fault
    LDR     R0, [R4, #24]           // PC offset in stack frame
    BL      log_fault_pc
    
    // Extract fault status
    LDR     R0, [R4, #64]           // CFSR offset
    BL      decode_fault_status
    
    // Extract fault address if applicable
    LDR     R0, [R4, #72]           // MMFAR offset
    BL      log_fault_address
    
    POP     {R4, PC}
```

# Performance Counters

ARM processors include Performance Monitoring Units (PMU) that provide hardware counters for measuring execution characteristics. These counters track events such as instruction counts, cache misses, branch mispredictions, and bus cycles.

## PMU Architecture

ARM Cortex-M processors (M4, M7) include Data Watchpoint and Trace (DWT) unit with basic performance counters. ARM Cortex-A/R processors have full PMU with multiple configurable counters.

**DWT Cycle Counter:** Counts processor clock cycles **Event Counters:** Count specific microarchitectural events **Overflow Detection:** Interrupts when counters overflow

## DWT Configuration (Cortex-M)

```assembly
// Enable DWT cycle counter (Cortex-M4/M7)
    // Enable trace system
    LDR     R0, =DEM_CR             // Debug Exception and Monitor Control
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<24)        // Set TRCENA bit
    STR     R1, [R0]
    
    // Reset cycle counter
    LDR     R0, =DWT_CYCCNT
    MOV     R1, #0
    STR     R1, [R0]
    
    // Enable cycle counter
    LDR     R0, =DWT_CTRL
    LDR     R1, [R0]
    ORR     R1, R1, #1              // CYCCNTENA bit
    STR     R1, [R0]

// Read cycle counter
read_cycle_count:
    LDR     R0, =DWT_CYCCNT
    LDR     R0, [R0]
    BX      LR
```

## Measuring Function Execution Time

```assembly
// Measure cycles for function execution
measure_function:
    PUSH    {R4, R5, LR}
    
    // Read start cycle count
    BL      read_cycle_count
    MOV     R4, R0                  // Save start count
    
    // Execute function to measure
    BL      target_function
    
    // Read end cycle count
    BL      read_cycle_count
    MOV     R5, R0                  // Save end count
    
    // Calculate elapsed cycles
    SUB     R0, R5, R4
    
    // Store result
    LDR     R1, =measurement_result
    STR     R0, [R1]
    
    POP     {R4, R5, PC}
```

## PMU Configuration (Cortex-A/R)

```assembly
// Configure PMU on Cortex-A series
    // Enable user-mode access to PMU
    MRC     p15, 0, R0, c9, c14, 0  // Read PMUSERENR
    ORR     R0, R0, #1              // Enable user access
    MCR     p15, 0, R0, c9, c14, 0  // Write PMUSERENR
    
    // Enable all counters
    MRC     p15, 0, R0, c9, c12, 0  // Read PMCR
    ORR     R0, R0, #1              // Enable all counters
    ORR     R0, R0, #2              // Reset event counters
    ORR     R0, R0, #4              // Reset cycle counter
    MCR     p15, 0, R0, c9, c12, 0  // Write PMCR
    
    // Enable cycle counter
    MOV     R0, #0x80000000
    MCR     p15, 0, R0, c9, c12, 1  // Write PMCNTENSET
```

## Event Counter Configuration

```assembly
// Configure event counter to track cache misses (Cortex-A)
    // Select counter 0
    MOV     R0, #0
    MCR     p15, 0, R0, c9, c12, 5  // Write PMSELR
    
    // Configure to count L1 data cache misses (event 0x03)
    MOV     R0, #0x03
    MCR     p15, 0, R0, c9, c13, 1  // Write PMXEVTYPER
    
    // Enable counter 0
    MOV     R0, #1
    MCR     p15, 0, R0, c9, c12, 1  // Write PMCNTENSET
    
    // Reset counter
    MOV     R0, #1
    MCR     p15, 0, R0, c9, c12, 3  // Write PMCNTENCLEAR

// Read event counter
    MOV     R0, #0                  // Select counter 0
    MCR     p15, 0, R0, c9, c12, 5  // Write PMSELR
    MRC     p15, 0, R0, c9, c13, 2  // Read PMXEVCNTR
```

## Multiple Counter Monitoring

```assembly
// Configure multiple performance counters
setup_perf_counters:
    PUSH    {R4, LR}
    
    // Counter 0: L1 I-cache misses (0x01)
    MOV     R0, #0
    MCR     p15, 0, R0, c9, c12, 5  // Select counter 0
    MOV     R0, #0x01
    MCR     p15, 0, R0, c9, c13, 1  // Set event type
    
    // Counter 1: L1 D-cache misses (0x03)
    MOV     R0, #1
    MCR     p15, 0, R0, c9, c12, 5  // Select counter 1
    MOV     R0, #0x03
    MCR     p15, 0, R0, c9, c13, 1  // Set event type
    
    // Counter 2: Branch mispredictions (0x10)
    MOV     R0, #2
    MCR     p15, 0, R0, c9, c12, 5  // Select counter 2
    MOV     R0, #0x10
    MCR     p15, 0, R0, c9, c13, 1  // Set event type
    
    // Enable all configured counters
    MOV     R0, #0x07               // Bits 0-2 for counters 0-2
    MCR     p15, 0, R0, c9, c12, 1  // Write PMCNTENSET
    
    POP     {R4, PC}

// Read all counters
read_all_counters:
    PUSH    {R4-R6, LR}
    LDR     R4, =perf_results
    
    // Read cycle counter
    MRC     p15, 0, R0, c9, c13, 0
    STR     R0, [R4], #4
    
    // Read counter 0
    MOV     R0, #0
    MCR     p15, 0, R0, c9, c12, 5
    MRC     p15, 0, R0, c9, c13, 2
    STR     R0, [R4], #4
    
    // Read counter 1
    MOV     R0, #1
    MCR     p15, 0, R0, c9, c12, 5
    MRC     p15, 0, R0, c9, c13, 2
    STR     R0, [R4], #4
    
    // Read counter 2
    MOV     R0, #2
    MCR     p15, 0, R0, c9, c12, 5
    MRC     p15, 0, R0, c9, c13, 2
    STR     R0, [R4], #4
    
    POP     {R4-R6, PC}
```

## Overflow Handling

```assembly
// Configure overflow interrupt
    // Set overflow flag on cycle counter overflow
    MOV     R0, #0x80000000
    MCR     p15, 0, R0, c9, c14, 1  // Write PMINTENSET
    
    // Enable PMU interrupt in NVIC
    LDR     R0, =NVIC_ISER
    LDR     R1, =(1 << PMU_IRQn)
    STR     R1, [R0]

// PMU overflow interrupt handler
PMU_IRQHandler:
    PUSH    {LR}
    
    // Read overflow status
    MRC     p15, 0, R0, c9, c12, 3  // Read PMOVSR
    
    // Check cycle counter overflow
    TST     R0, #0x80000000
    BNE     cycle_overflow
    
    // Check event counter overflows
    TST     R0, #0x01
    BNE     counter0_overflow
    
    // Clear overflow flags
    MCR     p15, 0, R0, c9, c12, 3  // Write PMOVSR
    
    POP     {PC}
    
cycle_overflow:
    // Increment software counter for cycle overflow
    LDR     R1, =cycle_overflow_count
    LDR     R2, [R1]
    ADD     R2, R2, #1
    STR     R2, [R1]
    B       clear_overflow
```

# Profiling Techniques

Profiling analyzes program execution to identify performance bottlenecks, hot paths, and optimization opportunities. ARM systems support both instrumentation-based and sampling-based profiling.

## Instrumentation Profiling

Adding measurement code at function entry/exit points:

```assembly
// Function instrumentation prologue
instrumented_function:
    PUSH    {R4, R5, LR}
    
    // Record entry time
    LDR     R4, =DWT_CYCCNT
    LDR     R4, [R4]                // Save start cycles
    
    // Function body
    // ... actual function code ...
    
    // Record exit time
    LDR     R5, =DWT_CYCCNT
    LDR     R5, [R5]                // Read end cycles
    
    // Calculate elapsed
    SUB     R5, R5, R4
    
    // Update profile data
    LDR     R0, =function_profile
    LDR     R1, [R0]                // Load call count
    ADD     R1, R1, #1
    STR     R1, [R0]
    
    LDR     R1, [R0, #4]            // Load total cycles
    ADD     R1, R1, R5
    STR     R1, [R0, #4]
    
    LDR     R1, [R0, #8]            // Load max cycles
    CMP     R5, R1
    STRGT   R5, [R0, #8]            // Update if greater
    
    LDR     R1, [R0, #12]           // Load min cycles
    CMP     R1, #0
    CMPNE   R5, R1
    STRLT   R5, [R0, #12]           // Update if less
    
    POP     {R4, R5, PC}

// Profile data structure per function
function_profile:
    .word   0                       // Call count
    .word   0                       // Total cycles
    .word   0                       // Max cycles
    .word   0xFFFFFFFF              // Min cycles
```

## Sampling Profiler

Periodic sampling using timer interrupts to build execution histogram:

```assembly
// Sampling profiler setup
setup_sampling_profiler:
    PUSH    {LR}
    
    // Allocate histogram (PC ranges to sample buckets)
    LDR     R0, =CODE_START
    LDR     R1, =CODE_END
    SUB     R2, R1, R0              // Code size
    LSR     R2, R2, #4              // Divide by 16 (bucket size)
    LDR     R1, =histogram_buckets
    STR     R2, [R1]
    
    // Configure timer for sampling (1ms interval)
    LDR     R0, =1000               // 1ms at 1MHz timer
    BL      start_sample_timer
    
    POP     {PC}

// Sample timer ISR
sample_timer_ISR:
    PUSH    {R4-R6, LR}
    
    // Get interrupted PC from stack
    MRS     R4, PSP                 // Or MSP depending on context
    LDR     R5, [R4, #24]           // PC at stack offset 24
    
    // Calculate histogram bucket
    LDR     R0, =CODE_START
    SUB     R5, R5, R0              // Offset from code start
    LSR     R5, R5, #4              // Divide by bucket size (16 bytes)
    
    // Increment bucket counter
    LDR     R0, =histogram_data
    LDR     R1, [R0, R5, LSL #2]    // Load current count
    ADD     R1, R1, #1
    STR     R1, [R0, R5, LSL #2]    // Store incremented count
    
    // Increment total samples
    LDR     R0, =total_samples
    LDR     R1, [R0]
    ADD     R1, R1, #1
    STR     R1, [R0]
    
    // Clear timer interrupt flag
    BL      clear_timer_interrupt
    
    POP     {R4-R6, PC}
```

## Call Graph Profiling

Building call relationships and inclusive/exclusive time:

```assembly
// Call graph node structure
// Offset 0: Function address
// Offset 4: Call count
// Offset 8: Exclusive time (time in function only)
// Offset 12: Inclusive time (time including callees)
// Offset 16: Parent pointer
// Offset 20: Caller count

// Function entry hook
profile_function_entry:
    PUSH    {R4-R6, LR}
    MOV     R4, LR                  // Save return address (caller)
    
    // Get current function address from return address
    SUB     R5, R4, #4              // Approximate function start
    
    // Find or create call graph node
    LDR     R0, =call_graph_root
    MOV     R1, R5
    BL      find_or_create_node
    MOV     R6, R0                  // Node pointer
    
    // Increment call count
    LDR     R1, [R6, #4]
    ADD     R1, R1, #1
    STR     R1, [R6, #4]
    
    // Record entry timestamp
    BL      read_cycle_count
    STR     R0, [R6, #24]           // Temporary timestamp storage
    
    // Push node onto call stack
    LDR     R0, =profile_call_stack
    LDR     R1, =profile_stack_ptr
    LDR     R2, [R1]
    STR     R6, [R0, R2, LSL #2]
    ADD     R2, R2, #1
    STR     R2, [R1]
    
    POP     {R4-R6, PC}

// Function exit hook
profile_function_exit:
    PUSH    {R4-R6, LR}
    
    // Read exit timestamp
    BL      read_cycle_count
    MOV     R4, R0
    
    // Pop from call stack
    LDR     R0, =profile_stack_ptr
    LDR     R1, [R0]
    SUB     R1, R1, #1
    STR     R1, [R0]
    
    LDR     R0, =profile_call_stack
    LDR     R5, [R0, R1, LSL #2]    // Node pointer
    
    // Calculate elapsed time
    LDR     R6, [R5, #24]           // Entry timestamp
    SUB     R4, R4, R6              // Elapsed cycles
    
    // Update exclusive time (subtract callee time)
    LDR     R0, [R5, #8]
    ADD     R0, R0, R4
    STR     R0, [R5, #8]
    
    // Update inclusive time
    LDR     R0, [R5, #12]
    ADD     R0, R0, R4
    STR     R0, [R5, #12]
    
    // Update parent's exclusive time (subtract this call)
    CMP     R1, #0
    BEQ     no_parent
    SUB     R1, R1, #1
    LDR     R0, =profile_call_stack
    LDR     R6, [R0, R1, LSL #2]    // Parent node
    LDR     R0, [R6, #8]
    SUB     R0, R0, R4
    STR     R0, [R6, #8]
    
no_parent:
    POP     {R4-R6, PC}
```

## Memory Access Profiling

Using DWT comparators to profile memory access patterns:

```assembly
// Configure DWT comparator for memory access tracking
    // Enable trace
    LDR     R0, =DEM_CR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<24)
    STR     R1, [R0]
    
    // Configure comparator 0 for address range
    LDR     R0, =DWT_COMP0
    LDR     R1, =MONITORED_ADDR_START
    STR     R1, [R0]
    
    // Configure mask for range
    LDR     R0, =DWT_MASK0
    MOV     R1, #4                  // Monitor 16-byte range
    STR     R1, [R0]
    
    // Configure function: match on read/write
    LDR     R0, =DWT_FUNCTION0
    MOV     R1, #0x07               // Match on R/W, generate event
    STR     R1, [R0]
```

## Branch Profiling

Tracking branch taken/not-taken statistics:

```assembly
// Instrumented conditional branch
    CMP     R0, R1
    BEQ     branch_taken
    
    // Branch not taken path
    LDR     R2, =branch_not_taken_count
    LDR     R3, [R2]
    ADD     R3, R3, #1
    STR     R3, [R2]
    B       branch_continue
    
branch_taken:
    // Branch taken path
    LDR     R2, =branch_taken_count
    LDR     R3, [R2]
    ADD     R3, R3, #1
    STR     R3, [R2]
    
branch_continue:
    // Continue execution
```

# Trace Analysis

Hardware trace capabilities capture detailed execution information including instruction flow, data accesses, and timing. ARM CoreSight architecture provides extensive trace support through Embedded Trace Macrocell (ETM) and other components.

## Instrumentation Trace Macrocell (ITM)

ITM provides printf-style debugging with minimal overhead:

```assembly
// ITM stimulus port write
// R0 = port number (0-31), R1 = data
itm_send_char:
    PUSH    {R4, R5, LR}
    MOV     R4, R0
    MOV     R5, R1
    
    // Check if ITM and stimulus port enabled
    LDR     R0, =ITM_TCR
    LDR     R1, [R0]
    TST     R1, #1                  // ITM enabled?
    BEQ     itm_done
    
    LDR     R0, =ITM_TER
    LDR     R1, [R0]
    MOV     R2, #1
    LSL     R2, R2, R4              // Check port bit
    TST     R1, R2
    BEQ     itm_done
    
    // Wait for port ready
    LDR     R0, =ITM_STIM0
    LSL     R4, R4, #2
    ADD     R0, R0, R4              // Calculate port address
    
itm_wait:
    LDR     R1, [R0]
    TST     R1, #1                  // Check ready bit
    BEQ     itm_wait
    
    // Write data
    STRB    R5, [R0]
    
itm_done:
    POP     {R4, R5, PC}

// ITM printf-like function
itm_print:
    // R0 = string pointer
    PUSH    {R4, R5, LR}
    MOV     R4, R0
    MOV     R5, #0                  // Use port 0
    
print_loop:
    LDRB    R1, [R4], #1
    CMP     R1, #0
    BEQ     print_done
    
    MOV     R0, R5
    BL      itm_send_char
    B       print_loop
    
print_done:
    POP     {R4, R5, PC}
```

## ETM Configuration

Embedded Trace Macrocell captures instruction execution trace:

```assembly
// Basic ETM configuration (Cortex-M4)
    // Unlock ETM
    LDR     R0, =ETM_LAR
    LDR     R1, =0xC5ACCE55
    STR     R1, [R0]
    
    // Configure main control
    LDR     R0, =ETM_CR
    LDR     R1, =0x00001000         // Basic configuration
    ORR     R1, R1, #(1<<11)        // Timestamp enable
    ORR     R1, R1, #(1<<10)        // Return stack enable
    STR     R1, [R0]
    
    // Set trace enable event (trace always)
    LDR     R0, =ETM_TRACEIDR
    MOV     R1, #0x01               // Trace ID
    STR     R1, [R0]
    
    // Configure trigger event
    LDR     R0, =ETM_TRIGGER
    LDR     R1, =0x0000406F         // Default trigger
    STR     R1, [R0]
    
    // Enable ETM
    LDR     R0, =ETM_CR
    LDR     R1, [R0]
    ORR     R1, R1, #1
    STR     R1, [R0]
```

## Trace Filtering

Configuring address range comparators to filter trace:

```assembly
// Configure ETM address comparator for code region filtering
setup_etm_filtering:
    PUSH {R4, R5, LR}

    // Unlock ETM
    LDR     R0, =ETM_LAR
    LDR     R1, =0xC5ACCE55
    STR     R1, [R0]

    // Configure address comparator 0 (start address)
    LDR     R0, =ETM_ACVR0          // Address Comparator Value Register 0
    LDR     R1, =TRACE_START_ADDR
    STR     R1, [R0]

    // Configure address comparator 1 (end address)
    LDR     R0, =ETM_ACVR1
    LDR     R1, =TRACE_END_ADDR
    STR     R1, [R0]

    // Configure address comparator access type
    LDR     R0, =ETM_ACTR0
    MOV     R1, #0x01               // Instruction execute
    STR     R1, [R0]

    LDR     R0, =ETM_ACTR1
    MOV     R1, #0x01
    STR     R1, [R0]

    // Configure trace enable to use address range
    LDR     R0, =ETM_TECR1          // Trace Enable Control Register
    MOV     R1, #0x01000100         // Enable on comparator pair 0
    STR     R1, [R0]

    POP     {R4, R5, PC}
````

## Trace Port Interface Unit (TPIU)

TPIU formats and outputs trace data:

```assembly
// Configure TPIU for trace output
    // Unlock TPIU
    LDR     R0, =TPIU_LAR
    LDR     R1, =0xC5ACCE55
    STR     R1, [R0]
    
    // Set protocol (synchronous or asynchronous)
    LDR     R0, =TPIU_SPPR
    MOV     R1, #0x02               // SWO (Serial Wire Output)
    STR     R1, [R0]
    
    // Configure async prescaler for baud rate
    // Baud = (System Clock / (Prescaler + 1))
    LDR     R0, =TPIU_ACPR
    LDR     R1, =(72000000/2000000 - 1)  // 2 Mbps at 72 MHz
    STR     R1, [R0]
    
    // Set formatter and flush control
    LDR     R0, =TPIU_FFCR
    MOV     R1, #0x00000100         // Formatter enabled
    STR     R1, [R0]
````

## Data Trace

Capturing data read/write operations using DWT:

```assembly
// Configure DWT for data trace
setup_data_trace:
    PUSH    {R4, LR}
    
    // Enable trace
    LDR     R0, =DEM_CR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<24)        // TRCENA
    STR     R1, [R0]
    
    // Configure comparator 0 for variable monitoring
    LDR     R0, =DWT_COMP0
    LDR     R1, =monitored_variable
    STR     R1, [R0]
    
    // Set function to generate trace on R/W
    LDR     R0, =DWT_FUNCTION0
    MOV     R1, #0x0C               // Data trace: read and write
    ORR     R1, R1, #(1<<8)         // DATAVMATCH
    STR     R1, [R0]
    
    // Configure comparator 1 for array range
    LDR     R0, =DWT_COMP1
    LDR     R1, =monitored_array
    STR     R1, [R0]
    
    // Set mask for range
    LDR     R0, =DWT_MASK1
    MOV     R1, #7                  // Monitor 256-byte range (2^8)
    STR     R1, [R0]
    
    // Set function
    LDR     R0, =DWT_FUNCTION1
    MOV     R1, #0x0C
    STR     R1, [R0]
    
    POP     {R4, PC}
```

## Timestamp Generation

Generating timestamps for trace correlation:

```assembly
// Configure timestamp generation
    LDR     R0, =DWT_CTRL
    LDR     R1, [R0]
    
    // Enable exception trace
    ORR     R1, R1, #(1<<16)        // EXCTRCENA
    
    // Enable PC sampling
    ORR     R1, R1, #(1<<12)        // PCSAMPLENA
    
    // Configure PC sample rate
    BIC     R1, R1, #(3<<10)        // Clear SYNCTAP
    ORR     R1, R1, #(1<<10)        // SYNCTAP = 1 (every 2^24 cycles)
    
    // Enable timestamp counter
    BIC     R1, R1, #(15<<1)        // Clear TSTAMP_PRESCALE
    ORR     R1, R1, #(1<<1)         // Prescale = 1 (divide by 4)
    
    STR     R1, [R0]
```

## Trace Buffer Management

Managing circular trace buffers in embedded trace buffer (ETB):

```assembly
// Configure ETB (Embedded Trace Buffer)
setup_etb:
    PUSH    {R4, LR}
    
    // Unlock ETB
    LDR     R0, =ETB_LAR
    LDR     R1, =0xC5ACCE55
    STR     R1, [R0]
    
    // Read buffer size
    LDR     R0, =ETB_RDP            // RAM Depth Register
    LDR     R1, [R0]
    LDR     R2, =etb_size
    STR     R1, [R2]                // Store for later use
    
    // Configure formatter and flush control
    LDR     R0, =ETB_FFCR
    MOV     R1, #0x00001403         // Stop on flush, formatter enabled
    STR     R1, [R0]
    
    // Configure circular buffer mode
    LDR     R0, =ETB_MODE
    MOV     R1, #0x00000000         // Circular mode
    STR     R1, [R0]
    
    // Enable ETB
    LDR     R0, =ETB_CTL
    MOV     R1, #0x00000001
    STR     R1, [R0]
    
    POP     {R4, PC}

// Read trace data from ETB
read_etb_trace:
    PUSH    {R4-R7, LR}
    MOV     R4, R0                  // Destination buffer
    MOV     R5, R1                  // Max words to read
    
    // Disable ETB capture
    LDR     R0, =ETB_CTL
    MOV     R1, #0
    STR     R1, [R0]
    
    // Get write pointer (shows how much data)
    LDR     R0, =ETB_RWP
    LDR     R6, [R0]                // RAM write pointer
    
    // Reset read pointer
    LDR     R0, =ETB_RRP
    MOV     R1, #0
    STR     R1, [R0]
    
    // Read loop
    MOV     R7, #0                  // Counter
read_etb_loop:
    CMP     R7, R5
    BGE     read_etb_done
    CMP     R7, R6
    BGE     read_etb_done
    
    // Read data
    LDR     R0, =ETB_RRD            // RAM Read Data
    LDR     R1, [R0]
    STR     R1, [R4], #4
    
    ADD     R7, R7, #1
    B       read_etb_loop
    
read_etb_done:
    MOV     R0, R7                  // Return words read
    
    // Re-enable ETB
    LDR     R1, =ETB_CTL
    MOV     R2, #1
    STR     R2, [R1]
    
    POP     {R4-R7, PC}
```

## PC Sampling

Periodic program counter sampling for statistical profiling:

```assembly
// Configure PC sampling via DWT
    LDR     R0, =DWT_CTRL
    LDR     R1, [R0]
    
    // Enable PC sampling
    ORR     R1, R1, #(1<<12)        // PCSAMPLENA
    
    // Configure sample rate (CYCTAP + SYNCTAP)
    // CYCTAP=0, SYNCTAP=01 -> sample every 2^10 cycles
    BIC     R1, R1, #(1<<9)         // CYCTAP = 0
    BIC     R1, R1, #(3<<10)        // Clear SYNCTAP
    ORR     R1, R1, #(1<<10)        // SYNCTAP = 01
    
    STR     R1, [R0]
    
    // PC samples will be output via trace port
```

## Exception Trace

Tracing exception entry/exit and prioritization:

```assembly
// Enable exception trace
    LDR     R0, =DWT_CTRL
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<16)        // EXCTRCENA
    STR     R1, [R0]
    
    // Configure ITM to output exception events
    LDR     R0, =ITM_TCR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<11)        // Enable exception trace
    STR     R1, [R0]

// Exception overhead measurement
measure_exception_overhead:
    PUSH    {R4-R6, LR}
    
    // Record pre-exception cycle count
    LDR     R4, =DWT_CYCCNT
    LDR     R5, [R4]
    
    // Trigger software interrupt
    LDR     R0, =NVIC_STIR          // Software Trigger Interrupt Register
    MOV     R1, #TEST_IRQn
    STR     R1, [R0]
    
    // Wait for ISR completion (set by ISR)
    LDR     R0, =isr_completed_flag
wait_isr:
    LDR     R1, [R0]
    CMP     R1, #0
    BEQ     wait_isr
    
    // Record post-exception cycle count
    LDR     R6, [R4]
    
    // Calculate overhead
    SUB     R0, R6, R5
    LDR     R1, =exception_overhead
    STR     R0, [R1]
    
    POP     {R4-R6, PC}

// Test ISR
test_ISR:
    PUSH    {LR}
    
    // Minimal ISR - just set flag
    LDR     R0, =isr_completed_flag
    MOV     R1, #1
    STR     R1, [R0]
    
    POP     {PC}
```

## Trace Synchronization

Synchronizing multiple trace sources:

```assembly
// Generate trace synchronization packet
    // ITM sends sync packet periodically
    LDR     R0, =ITM_TCR
    LDR     R1, [R0]
    
    // Trigger sync packet now
    ORR     R1, R1, #(1<<2)         // Trigger sync
    STR     R1, [R0]
    
    // Configure periodic sync
    BIC     R1, R1, #(3<<8)         // Clear prescaler
    ORR     R1, R1, #(2<<8)         // Sync every 2^16 cycles
    STR     R1, [R0]
```

## Trace Decoding Support

Markers and annotations for offline trace analysis:

```assembly
// Insert trace marker
insert_trace_marker:
    // R0 = marker ID
    PUSH    {R4, LR}
    MOV     R4, R0
    
    // Send marker via ITM port 31 (reserved for markers)
    MOV     R0, #31
    MOV     R1, R4
    BL      itm_send_char
    
    // Also log to trace buffer if enabled
    LDR     R0, =trace_marker_buffer
    LDR     R1, =trace_marker_index
    LDR     R2, [R1]
    
    // Get timestamp
    LDR     R3, =DWT_CYCCNT
    LDR     R3, [R3]
    
    // Store marker entry (timestamp + ID)
    ADD     R2, R2, R2, LSL #1      // R2 *= 3 (3 words per entry)
    LSL     R2, R2, #2              // Convert to byte offset
    STR     R3, [R0, R2]            // Timestamp
    STR     R4, [R0, R2, #4]        // Marker ID
    
    // Increment index
    LDR     R2, [R1]
    ADD     R2, R2, #1
    STR     R2, [R1]
    
    POP     {R4, PC}

// Checkpoint function for trace analysis
trace_checkpoint:
    // R0 = checkpoint name pointer
    // R1 = checkpoint ID
    PUSH    {R4, R5, LR}
    MOV     R4, R0
    MOV     R5, R1
    
    // Send checkpoint ID via trace
    MOV     R0, R5
    BL      insert_trace_marker
    
    // Record checkpoint data
    LDR     R0, =checkpoint_log
    LDR     R1, =checkpoint_count
    LDR     R2, [R1]
    
    // Calculate offset (4 words per checkpoint)
    LSL     R3, R2, #4              // R3 = R2 * 16 bytes
    
    // Store checkpoint info
    LDR     R6, =DWT_CYCCNT
    LDR     R6, [R6]
    STR     R6, [R0, R3]            // Timestamp
    STR     R5, [R0, R3, #4]        // ID
    STR     R4, [R0, R3, #8]        // Name pointer
    
    // Increment count
    ADD     R2, R2, #1
    STR     R2, [R1]
    
    POP     {R4, R5, PC}
```

## Real-time Trace Streaming

Streaming trace data to external host:

```assembly
// Configure trace streaming via SWO
setup_swo_streaming:
    PUSH    {R4, LR}
    
    // Configure TPIU for SWO
    BL      configure_tpiu_swo
    
    // Enable all ITM stimulus ports
    LDR     R0, =ITM_TER
    LDR     R1, =0xFFFFFFFF         // Enable all 32 ports
    STR     R1, [R0]
    
    // Configure ITM
    LDR     R0, =ITM_TCR
    MOV     R1, #0x00010005         // Enable ITM, sync, DWT packets
    STR     R1, [R0]
    
    // Start streaming thread/task
    BL      start_trace_stream_task
    
    POP     {R4, PC}

// Streaming task (called periodically)
trace_stream_task:
    PUSH    {R4-R7, LR}
    
    // Get trace buffer pointers
    LDR     R4, =trace_buffer_read
    LDR     R5, [R4]                // Read pointer
    LDR     R6, =trace_buffer_write
    LDR     R7, [R6]                // Write pointer
    
    // Check if data available
    CMP     R5, R7
    BEQ     stream_done
    
stream_loop:
    // Read byte from buffer
    LDR     R0, =trace_buffer_base
    LDRB    R1, [R0, R5]
    
    // Send via ITM port 0
    MOV     R0, #0
    BL      itm_send_char
    
    // Advance read pointer (circular buffer)
    ADD     R5, R5, #1
    LDR     R0, =TRACE_BUFFER_SIZE
    CMP     R5, R0
    MOVGE   R5, #0                  // Wrap around
    
    // Check if more data
    CMP     R5, R7
    BNE     stream_loop
    
    // Update read pointer
    STR     R5, [R4]
    
stream_done:
    POP     {R4-R7, PC}
```

## Profiling Report Generation

Analyzing collected profiling data:

```assembly
// Generate profiling report
generate_profile_report:
    PUSH    {R4-R8, LR}
    
    // Sort functions by total cycles
    LDR     R4, =function_profiles
    LDR     R5, =function_count
    LDR     R5, [R5]
    
    MOV     R6, #0                  // Outer loop counter
sort_outer:
    CMP     R6, R5
    BGE     sort_done
    
    MOV     R7, R6
    ADD     R7, R7, #1              // Inner loop starts at outer+1
    
sort_inner:
    CMP     R7, R5
    BGE     sort_next_outer
    
    // Compare total cycles
    LSL     R0, R6, #4              // 16 bytes per entry
    LDR     R1, [R4, R0, #4]        // Total cycles for R6
    
    LSL     R0, R7, #4
    LDR     R2, [R4, R0, #4]        // Total cycles for R7
    
    CMP     R2, R1
    BLE     sort_no_swap
    
    // Swap entries
    LSL     R0, R6, #4
    LSL     R1, R7, #4
    MOV     R8, #0
swap_loop:
    CMP     R8, #16
    BGE     sort_no_swap
    
    LDR     R2, [R4, R0, R8]
    LDR     R3, [R4, R1, R8]
    STR     R3, [R4, R0, R8]
    STR     R2, [R4, R1, R8]
    
    ADD     R8, R8, #4
    B       swap_loop
    
sort_no_swap:
    ADD     R7, R7, #1
    B       sort_inner
    
sort_next_outer:
    ADD     R6, R6, #1
    B       sort_outer
    
sort_done:
    // Calculate percentages and output
    LDR     R4, =function_profiles
    LDR     R5, =function_count
    LDR     R5, [R5]
    
    // Calculate total cycles
    MOV     R6, #0                  // Total accumulator
    MOV     R7, #0                  // Index
calc_total:
    CMP     R7, R5
    BGE     calc_done
    
    LSL     R0, R7, #4
    LDR     R1, [R4, R0, #4]        // Total cycles
    ADD     R6, R6, R1
    
    ADD     R7, R7, #1
    B       calc_total
    
calc_done:
    // Output report (via ITM or store in memory)
    MOV     R7, #0
output_loop:
    CMP     R7, R5
    BGE     report_done
    
    LSL     R0, R7, #4
    
    // Get function data
    LDR     R1, [R4, R0]            // Call count
    LDR     R2, [R4, R0, #4]        // Total cycles
    LDR     R3, [R4, R0, #8]        // Max cycles
    
    // Calculate percentage: (cycles * 100) / total
    MOV     R0, #100
    MUL     R0, R2, R0
    // [Inference: Division implementation depends on CPU features]
    // UDIV available on Cortex-M3+
    UDIV    R0, R0, R6              // Percentage
    
    // Store or output results
    BL      output_profile_line
    
    ADD     R7, R7, #1
    B       output_loop
    
report_done:
    POP     {R4-R8, PC}
```

## Watchpoint-based Profiling

Using hardware watchpoints for variable access profiling:

```assembly
// Configure watchpoint for variable profiling
profile_variable_access:
    // R0 = variable address
    PUSH    {R4, LR}
    MOV     R4, R0
    
    // Configure DWT comparator 2
    LDR     R0, =DWT_COMP2
    STR     R4, [R0]
    
    // Configure for read/write detection
    LDR     R0, =DWT_FUNCTION2
    MOV     R1, #0x06               // Generate watchpoint on R/W
    ORR     R1, R1, #(1<<4)         // Generate debug event
    STR     R1, [R0]
    
    // Enable DebugMonitor exception
    LDR     R0, =SCB_DEMCR
    LDR     R1, [R0]
    ORR     R1, R1, #(1<<16)        // MON_EN
    STR     R1, [R0]
    
    POP     {R4, PC}

// Debug Monitor exception handler
DebugMon_Handler:
    PUSH    {R4-R6, LR}
    
    // Check DWT match
    LDR     R0, =DWT_FUNCTION2
    LDR     R1, [R0]
    TST     R1, #(1<<24)            // MATCHED bit
    BEQ     debugmon_done
    
    // Record access
    LDR     R4, =variable_access_log
    LDR     R5, =access_count
    LDR     R6, [R5]
    
    // Get PC of access (from stack)
    MRS     R0, MSP
    LDR     R1, [R0, #24]           // PC
    
    // Get timestamp
    LDR     R2, =DWT_CYCCNT
    LDR     R2, [R2]
    
    // Store log entry
    LSL     R3, R6, #3              // 8 bytes per entry
    STR     R1, [R4, R3]            // PC
    STR     R2, [R4, R3, #4]        // Timestamp
    
    // Increment count
    ADD     R6, R6, #1
    STR     R6, [R5]
    
    // Clear matched bit
    LDR     R1, [R0]
    BIC     R1, R1, #(1<<24)
    STR     R1, [R0]
    
debugmon_done:
    POP     {R4-R6, PC}
```

**Key Points**

- Core dumps capture system state at failure including registers, stack, and fault information for post-mortem analysis
- Performance counters provide hardware-based measurement of cycles, cache misses, branch predictions, and other microarchitectural events
- Profiling techniques include instrumentation (adding measurement code), sampling (periodic PC capture), and call graph analysis for identifying bottlenecks
- Trace analysis uses CoreSight components (ITM, ETM, ETB, TPIU) to capture detailed execution flow, data accesses, and timing information with minimal overhead

**Important related topics:** JTAG and SWD debug protocols, GDB remote debugging integration, real-time operating system awareness in debuggers, symbol table and DWARF debug information, flash breakpoint implementation, semihosting for I/O during debugging.

---

# Bootloader Development

A bootloader is the first software that executes when a system powers on. It initializes hardware, establishes a secure execution environment, and loads the operating system. ARM bootloaders must handle processor initialization, memory setup, security configuration, and chain-of-trust establishment.

## Boot Process Overview

The typical ARM boot sequence follows these stages:

**Boot ROM (First-stage bootloader):**

- Executes from on-chip ROM immediately after reset
- Initializes minimal hardware (CPU core, internal RAM)
- Verifies and loads second-stage bootloader from external storage
- **[Inference]** Usually implemented by SoC vendor and immutable

**Second-stage bootloader (SPL/U-Boot SPL):**

- Initializes external DRAM
- Sets up clocks and critical peripherals
- Loads main bootloader into DRAM
- Size-constrained to fit in internal SRAM

**Main bootloader (U-Boot/GRUB):**

- Full-featured environment with filesystem support
- Loads kernel and device tree
- Provides boot menu and command interface
- Implements verified boot if required

## Processor Initialization

After reset, the ARM processor begins execution at the reset vector in Secure EL3. The bootloader must configure the processor for proper operation.

**Example:**

```assembly
// Reset vector - execution starts here after power-on
// Processor is in Secure EL3, AArch64 state

.section .text.reset
.global _reset_vector

_reset_vector:
        // Disable interrupts
        MSR     DAIFSet, #0xF               // Mask all interrupts
        
        // Check current exception level
        MRS     X0, CurrentEL
        CMP     X0, #(3 << 2)               // Check if EL3
        B.NE    unexpected_el
        
        // Initialize processor state
        BL      init_cpu_state
        
        // Initialize system control registers
        BL      init_system_control
        
        // Set up exception vectors
        LDR     X0, =exception_vector_table
        MSR     VBAR_EL3, X0
        
        // Initialize stack pointer for EL3
        LDR     X0, =__stack_el3_end
        MOV     SP, X0
        
        // Branch to main bootloader code
        B       bootloader_main

unexpected_el:
        // Should never happen after reset
        WFI
        B       unexpected_el
```

**Example:**

```assembly
// Initialize CPU state registers
init_cpu_state:
        // Initialize SCTLR_EL3 - System Control Register
        MOV     X0, #0x0
        ORR     X0, X0, #(1 << 12)          // I bit - Enable instruction cache
        ORR     X0, X0, #(1 << 2)           // C bit - Enable data cache
        ORR     X0, X0, #(1 << 0)           // M bit - Enable MMU (set later)
        BIC     X0, X0, #(1 << 0)           // Initially disable MMU
        MSR     SCTLR_EL3, X0
        
        // Initialize SCR_EL3 - Secure Configuration Register
        MOV     X0, #0x0
        ORR     X0, X0, #(1 << 10)          // RW bit - EL2/EL1 are AArch64
        ORR     X0, X0, #(1 << 0)           // NS bit - Initially 0 (Secure)
        ORR     X0, X0, #(1 << 3)           // EA bit - External aborts to EL3
        ORR     X0, X0, #(1 << 2)           // FIQ to EL3
        MSR     SCR_EL3, X0
        
        // Initialize CPTR_EL3 - Architectural Feature Trap Register
        MSR     CPTR_EL3, XZR               // Don't trap FP/SIMD
        
        ISB                                 // Synchronize context
        
        RET

init_system_control:
        // Disable MMU and caches initially
        MRS     X0, SCTLR_EL3
        BIC     X0, X0, #(1 << 0)           // Disable MMU
        BIC     X0, X0, #(1 << 2)           // Disable D-cache
        BIC     X0, X0, #(1 << 12)          // Disable I-cache
        MSR     SCTLR_EL3, X0
        ISB
        
        // Invalidate instruction cache
        IC      IALLUIS
        ISB
        
        // Invalidate data cache
        BL      invalidate_dcache_all
        
        // Invalidate TLB
        TLBI    ALLE3
        DSB     SY
        ISB
        
        RET
```

## Memory Initialization

The bootloader must initialize DRAM controllers before accessing external memory. This involves configuring timing parameters, training sequences, and controller registers.

**Example:**

```assembly
// Initialize external DRAM controller
// [Inference] Specific register addresses and values depend on SoC
// This example shows the general pattern

init_dram:
        STP     X29, X30, [SP, #-16]!
        MOV     X29, SP
        
        // Define base addresses
        LDR     X19, =DRAM_CTRL_BASE
        
        // Reset DRAM controller
        MOV     W0, #0x1
        STR     W0, [X19, #DRAM_CTRL_RESET]
        
        // Wait for reset completion
        MOV     W1, #10000
reset_wait:
        SUB     W1, W1, #1
        CBNZ    W1, reset_wait
        
        // Configure DRAM timing parameters
        LDR     W0, =DRAM_TIMING_VALUE      // [Unverified] Value depends on DRAM type
        STR     W0, [X19, #DRAM_TIMING_REG]
        
        // Configure DRAM size and organization
        LDR     W0, =DRAM_CONFIG_VALUE
        STR     W0, [X19, #DRAM_CONFIG_REG]
        
        // Enable DRAM controller
        MOV     W0, #0x1
        STR     W0, [X19, #DRAM_CTRL_ENABLE]
        
        // Perform DRAM training sequence
        BL      dram_training
        
        // Test DRAM connectivity
        BL      dram_test
        CMP     X0, #0
        B.NE    dram_init_failed
        
        LDP     X29, X30, [SP], #16
        RET

dram_init_failed:
        // Handle initialization failure
        // [Inference] Typically enters recovery mode or infinite loop
        B       .
```

**Example:**

```assembly
// Simple DRAM connectivity test
// Returns 0 in X0 if test passes, non-zero otherwise

dram_test:
        LDR     X0, =DRAM_BASE_ADDR
        LDR     X1, =DRAM_TEST_SIZE
        
        // Write test pattern
        MOV     X2, X0                      // Current address
        MOV     X3, #0x0
write_loop:
        // Create walking-ones pattern
        MOV     X4, #1
        LSL     X4, X4, X3
        STR     X4, [X2], #8
        
        ADD     X3, X3, #1
        AND     X3, X3, #0x3F               // Wrap at 64
        SUB     X1, X1, #8
        CBNZ    X1, write_loop
        
        // Read and verify pattern
        LDR     X0, =DRAM_BASE_ADDR
        LDR     X1, =DRAM_TEST_SIZE
        MOV     X3, #0x0
        
read_loop:
        MOV     X4, #1
        LSL     X4, X4, X3
        LDR     X5, [X0], #8
        
        CMP     X4, X5
        B.NE    test_failed
        
        ADD     X3, X3, #1
        AND     X3, X3, #0x3F
        SUB     X1, X1, #8
        CBNZ    X1, read_loop
        
        // Test passed
        MOV     X0, #0
        RET

test_failed:
        // Return error code indicating failure address
        MOV     X0, #1
        RET
```

## Translation Table Setup

Bootloaders must configure the MMU to enable caching and access control for different memory regions.

**Example:**

```assembly
// Set up identity-mapped translation tables for bootloader
// Creates 1GB sections for simplicity

setup_page_tables:
        STP     X29, X30, [SP, #-16]!
        
        // Get page table base
        LDR     X0, =__page_table_start
        
        // Clear page tables
        MOV     X1, #4096                   // 4KB table
        MOV     X2, X0
clear_tables:
        STP     XZR, XZR, [X2], #16
        SUB     X1, X1, #16
        CBNZ    X1, clear_tables
        
        // Set up MAIR_EL3
        BL      setup_mair
        
        // Create level 1 table entries (1GB blocks)
        LDR     X0, =__page_table_start
        
        // Entry 0: Device memory (0x0000_0000 - 0x3FFF_FFFF)
        MOV     X1, #0x00000000
        ORR     X1, X1, #0x1                // Valid
        ORR     X1, X1, #(0x0 << 2)         // AttrIndx = 0 (Device)
        ORR     X1, X1, #(0x1 << 10)        // AF = 1
        ORR     X1, X1, #(0x1 << 54)        // UXN
        ORR     X1, X1, #(0x1 << 53)        // PXN
        STR     X1, [X0], #8
        
        // Entry 1: Normal memory (0x4000_0000 - 0x7FFF_FFFF)
        MOV     X1, #0x40000000
        ORR     X1, X1, #0x1                // Valid
        ORR     X1, X1, #(0x1 << 2)         // AttrIndx = 1 (Normal)
        ORR     X1, X1, #(0x3 << 8)         // Inner shareable
        ORR     X1, X1, #(0x1 << 10)        // AF = 1
        STR     X1, [X0], #8
        
        // Entry 2: DRAM (0x8000_0000 - 0xBFFF_FFFF)
        MOV     X1, #0x80000000
        ORR     X1, X1, #0x1
        ORR     X1, X1, #(0x1 << 2)         // Normal memory
        ORR     X1, X1, #(0x3 << 8)         // Inner shareable
        ORR     X1, X1, #(0x1 << 10)        // AF = 1
        STR     X1, [X0], #8
        
        // Configure TCR_EL3
        MOV     X1, #0x0
        ORR     X1, X1, #(0x1 << 20)        // TBI (Top Byte Ignore)
        ORR     X1, X1, #(25 << 0)          // T0SZ = 25 (39-bit address space)
        ORR     X1, X1, #(0x0 << 14)        // TG0 = 4KB granule
        ORR     X1, X1, #(0x3 << 12)        // SH0 = Inner shareable
        ORR     X1, X1, #(0x1 << 10)        // ORGN0 = Write-back write-alloc
        ORR     X1, X1, #(0x1 << 8)         // IRGN0 = Write-back write-alloc
        MSR     TCR_EL3, X1
        
        // Set TTBR0_EL3
        LDR     X1, =__page_table_start
        MSR     TTBR0_EL3, X1
        ISB
        
        LDP     X29, X30, [SP], #16
        RET

setup_mair:
        // MAIR0: Device-nGnRnE
        // MAIR1: Normal memory, write-back cacheable
        MOV     X0, #0x00                   // Device-nGnRnE
        ORR     X0, X0, #(0xFF << 8)        // Normal WB
        MSR     MAIR_EL3, X0
        RET
```

## MMU Enablement

**Example:**

```assembly
// Enable MMU and caches
enable_mmu:
        // Ensure all prior operations complete
        DSB     SY
        ISB
        
        // Invalidate TLB
        TLBI    ALLE3
        DSB     SY
        ISB
        
        // Enable MMU and caches
        MRS     X0, SCTLR_EL3
        ORR     X0, X0, #(1 << 0)           // M bit - Enable MMU
        ORR     X0, X0, #(1 << 2)           // C bit - Enable data cache
        ORR     X0, X0, #(1 << 12)          // I bit - Enable instruction cache
        MSR     SCTLR_EL3, X0
        ISB
        
        RET
```

## Loading Kernel Image

The bootloader must read the kernel from storage (SD card, eMMC, flash) and load it into memory.

**Example:**

```assembly
// Load kernel from storage device
// X0 = storage device base address
// X1 = kernel load address in DRAM
// X2 = kernel size in bytes

load_kernel:
        STP     X29, X30, [SP, #-48]!
        STP     X19, X20, [SP, #16]
        STP     X21, X22, [SP, #32]
        MOV     X29, SP
        
        MOV     X19, X0                     // Storage device base
        MOV     X20, X1                     // Load address
        MOV     X21, X2                     // Size
        
        // Initialize storage device
        MOV     X0, X19
        BL      storage_init
        CBNZ    X0, load_failed
        
        // Read kernel image
        MOV     X0, X19                     // Device base
        MOV     X1, #KERNEL_STORAGE_OFFSET  // Offset in storage
        MOV     X2, X20                     // Destination
        MOV     X3, X21                     // Size
        BL      storage_read
        CBNZ    X0, load_failed
        
        // Verify kernel signature/checksum
        MOV     X0, X20
        MOV     X1, X21
        BL      verify_kernel
        CBNZ    X0, verification_failed
        
        // Clean cache to ensure kernel is in DRAM
        MOV     X0, X20
        MOV     X1, X21
        BL      clean_cache_range
        
        MOV     X0, #0                      // Success
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #48
        RET

load_failed:
        MOV     X0, #1                      // Error code
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #48
        RET

verification_failed:
        MOV     X0, #2                      // Verification error
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #48
        RET
```

## Device Tree Handling

Modern ARM bootloaders pass hardware information to the kernel via Device Tree Blob (DTB).

**Example:**

```assembly
// Load and prepare device tree
// X0 = DTB load address
// Returns DTB address in X0

prepare_dtb:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        MOV     X29, SP
        
        MOV     X19, X0                     // Save DTB address
        
        // Verify DTB magic number (0xD00DFEED)
        LDR     W1, [X19]
        REV     W1, W1                      // DTB is big-endian
        LDR     W2, =0xD00DFEED
        CMP     W1, W2
        B.NE    dtb_invalid
        
        // Get DTB size
        LDR     W1, [X19, #4]
        REV     W1, W1
        MOV     W20, W1                     // Save size
        
        // Add bootloader-specific properties
        MOV     X0, X19
        BL      add_bootloader_props
        
        // Clean cache for DTB area
        MOV     X0, X19
        MOV     X1, X20
        BL      clean_cache_range
        
        MOV     X0, X19                     // Return DTB address
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET

dtb_invalid:
        MOV     X0, #0                      // Return NULL
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

## Exception Level Transition

Before jumping to the kernel, the bootloader must transition from EL3 to EL2 or EL1, depending on virtualization support.

**Example:**

```assembly
// Drop to EL2/EL1 and jump to kernel
// X0 = DTB address
// X1 = kernel entry point

jump_to_kernel:
        // Save kernel parameters
        MOV     X20, X0                     // DTB address for kernel
        MOV     X21, X1                     // Kernel entry
        
        // Disable MMU before transition
        MRS     X0, SCTLR_EL3
        BIC     X0, X0, #(1 << 0)           // Disable MMU
        BIC     X0, X0, #(1 << 2)           // Disable D-cache
        MSR     SCTLR_EL3, X0
        ISB
        
        // Prepare EL2 state
        MRS     X0, SCR_EL3
        ORR     X0, X0, #(1 << 0)           // NS bit - Non-secure
        ORR     X0, X0, #(1 << 10)          // RW bit - EL2 is AArch64
        MSR     SCR_EL3, X0
        
        // Set return address to kernel
        MSR     ELR_EL3, X21
        
        // Configure processor state after ERET
        MOV     X0, #0x3C9                  // EL2h, IRQ/FIQ masked
        MSR     SPSR_EL3, X0
        
        // Set up registers for kernel
        MOV     X0, X20                     // X0 = DTB address
        MOV     X1, #0                      // X1 = 0 (reserved)
        MOV     X2, #0                      // X2 = 0 (reserved)
        MOV     X3, #0                      // X3 = 0 (reserved)
        
        // Jump to kernel in EL2
        ERET
```

## Secure Boot Implementation

Secure boot ensures only trusted code executes by verifying digital signatures at each boot stage.

**Example:**

```assembly
// Verify kernel image signature using RSA
// X0 = image address
// X1 = image size
// X2 = signature address
// X3 = public key address
// Returns 0 if valid, non-zero if invalid

verify_kernel:
        STP     X29, X30, [SP, #-64]!
        STP     X19, X20, [SP, #16]
        STP     X21, X22, [SP, #32]
        STP     X23, X24, [SP, #48]
        MOV     X29, SP
        
        MOV     X19, X0                     // Image address
        MOV     X20, X1                     // Image size
        MOV     X21, X2                     // Signature
        MOV     X22, X3                     // Public key
        
        // Calculate SHA-256 hash of image
        MOV     X0, X19
        MOV     X1, X20
        LDR     X2, =hash_buffer
        BL      sha256_hash
        CBNZ    X0, verify_fail
        
        // Verify RSA signature
        LDR     X0, =hash_buffer
        MOV     X1, X21                     // Signature
        MOV     X2, X22                     // Public key
        BL      rsa_verify
        CBNZ    X0, verify_fail
        
        MOV     X0, #0                      // Success
        LDP     X23, X24, [SP, #48]
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #64
        RET

verify_fail:
        MOV     X0, #1                      // Failure
        LDP     X23, X24, [SP, #48]
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #64
        RET
```

## Serial Console Output

Debug output is critical during bootloader development.

**Example:**

```assembly
// Simple UART output function
// X0 = null-terminated string address

uart_puts:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        
        LDR     X19, =UART_BASE
        MOV     X20, X0
        
puts_loop:
        LDRB    W0, [X20], #1
        CBZ     W0, puts_done
        
        // Wait for UART ready
wait_ready:
        LDR     W1, [X19, #UART_STATUS]
        TST     W1, #UART_TX_FULL
        B.NE    wait_ready
        
        // Send character
        STR     W0, [X19, #UART_DATA]
        
        B       puts_loop

puts_done:
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

**Key Points:**

- Bootloaders execute from reset vector in Secure EL3
- Processor initialization must occur before any other operations
- External DRAM initialization is SoC-specific and requires careful timing configuration
- Translation tables should be set up before enabling MMU
- Cache maintenance is critical when transitioning between boot stages
- Secure boot verification should occur before executing any loaded code
- Exception level transitions require careful register configuration
- Device tree provides hardware information to the kernel in a standardized format
- **[Inference]** Production bootloaders typically implement fallback mechanisms for recovery from failures

# Operating System Kernel Modules

Kernel modules are dynamically loadable code that extends operating system functionality without requiring kernel recompilation. On ARM systems, modules must handle architecture-specific details including exception levels, cache coherency, and hardware register access.

## Module Structure and Loading

Linux kernel modules on ARM follow the standard ELF format with ARM-specific relocations. The module loader resolves symbols and applies relocations at runtime.

**Example:**

```assembly
// Basic kernel module structure in ARM assembly
// This would typically be combined with C code

.section .modinfo
.ascii "license=GPL\0"
.ascii "author=Developer\0"
.ascii "description=Example ARM module\0"

.section .text

// Module initialization function
.global init_module
.type init_module, %function

init_module:
        STP     X29, X30, [SP, #-16]!
        MOV     X29, SP
        
        // Print initialization message
        LDR     X0, =init_msg
        BL      printk
        
        // Perform module-specific initialization
        BL      setup_hardware
        CBNZ    X0, init_failed
        
        // Register device or interfaces
        BL      register_device
        CBNZ    X0, init_failed
        
        MOV     X0, #0                      // Return 0 for success
        LDP     X29, X30, [SP], #16
        RET

init_failed:
        // Cleanup on failure
        BL      cleanup_hardware
        MOV     X0, #-1                     // Return error
        LDP     X29, X30, [SP], #16
        RET

// Module cleanup function
.global cleanup_module
.type cleanup_module, %function

cleanup_module:
        STP     X29, X30, [SP, #-16]!
        MOV     X29, SP
        
        // Unregister device
        BL      unregister_device
        
        // Clean up hardware
        BL      cleanup_hardware
        
        // Print cleanup message
        LDR     X0, =exit_msg
        BL      printk
        
        LDP     X29, X30, [SP], #16
        RET

.section .rodata
init_msg:
        .asciz "Example module loaded\n"
exit_msg:
        .asciz "Example module unloaded\n"
```

## Memory Management in Modules

Kernel modules must use kernel memory allocation functions and handle virtual-to-physical address translation.

**Example:**

```assembly
// Allocate DMA-capable memory buffer
// X0 = size in bytes
// Returns virtual address in X0, physical in X1

alloc_dma_buffer:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        MOV     X29, SP
        
        MOV     X19, X0                     // Save size
        
        // Call kmalloc with GFP_DMA flag
        MOV     X1, #0x00000001             // GFP_DMA flag
        BL      kmalloc
        CBZ     X0, alloc_failed
        
        MOV     X20, X0                     // Save virtual address
        
        // Get physical address
        BL      virt_to_phys
        MOV     X1, X0                      // Physical address in X1
        MOV     X0, X20                     // Virtual address in X0
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET

alloc_failed:
        MOV     X0, #0
        MOV     X1, #0
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

## Hardware Register Access

Modules that control hardware must perform memory-mapped I/O with proper barriers and cache maintenance.

**Example:**

```assembly
// Read from memory-mapped hardware register
// X0 = register address
// Returns value in X0

hw_read_reg:
        // Use load-acquire to ensure ordering
        LDAR    W0, [X0]
        RET

// Write to memory-mapped hardware register
// X0 = register address
// X1 = value to write

hw_write_reg:
        // Use store-release to ensure ordering
        STLR    W1, [X0]
        
        // Data synchronization barrier
        DSB     SY
        
        RET

// Read-modify-write operation on hardware register
// X0 = register address
// X1 = bits to set
// X2 = bits to clear

hw_rmw_reg:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        
        MOV     X19, X0                     // Save address
        MOV     X20, X1                     // Save set mask
        
        // Read current value
        LDAR    W0, [X19]
        
        // Clear specified bits
        BIC     W0, W0, W2
        
        // Set specified bits
        ORR     W0, W0, W20
        
        // Write back
        STLR    W0, [X19]
        DSB     SY
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

## Interrupt Handling

Kernel modules can register interrupt handlers that execute when hardware interrupts occur.

**Example:**

```assembly
// Interrupt handler for device
// X0 = IRQ number
// X1 = device structure pointer
// Returns IRQ_HANDLED (1) or IRQ_NONE (0)

.global device_irq_handler
.type device_irq_handler, %function

device_irq_handler:
        STP     X29, X30, [SP, #-48]!
        STP     X19, X20, [SP, #16]
        STP     X21, X22, [SP, #32]
        MOV     X29, SP
        
        MOV     X19, X1                     // Save device structure
        
        // Get device base address from structure
        LDR     X20, [X19, #DEV_BASE_OFFSET]
        
        // Read interrupt status register
        LDR     W21, [X20, #IRQ_STATUS_REG]
        
        // Check if this device generated the interrupt
        CBZ     W21, not_our_irq
        
        // Handle different interrupt types
        TBNZ    W21, #RX_IRQ_BIT, handle_rx_irq
        TBNZ    W21, #TX_IRQ_BIT, handle_tx_irq
        TBNZ    W21, #ERR_IRQ_BIT, handle_err_irq
        
        B       clear_irq

handle_rx_irq:
        // Process received data
        MOV     X0, X19
        BL      process_rx_data
        B       clear_irq

handle_tx_irq:
        // Handle transmit completion
        MOV     X0, X19
        BL      process_tx_complete
        B       clear_irq

handle_err_irq:
        // Handle error condition
        MOV     X0, X19
        BL      process_error
        B       clear_irq

clear_irq:
        // Clear interrupt status
        STR     W21, [X20, #IRQ_STATUS_REG]
        DSB     SY
        
        // Schedule tasklet for deferred processing
        MOV     X0, X19
        BL      schedule_tasklet
        
        MOV     X0, #1                      // IRQ_HANDLED
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #48
        RET

not_our_irq:
        MOV     X0, #0                      // IRQ_NONE
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #48
        RET

// Tasklet handler for bottom-half processing
.global device_tasklet
.type device_tasklet, %function

device_tasklet:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        MOV     X29, SP
        
        MOV     X19, X0                     // Device structure
        
        // Process data from ring buffer
        LDR     X20, [X19, #RING_BUFFER_OFFSET]
        
process_loop:
        // Check if ring buffer has data
        LDR     W0, [X20, #RING_HEAD]
        LDR     W1, [X20, #RING_TAIL]
        CMP     W0, W1
        B.EQ    processing_done
        
        // Get next item from ring
        MOV     X0, X20
        BL      ring_buffer_get
        CBZ     X0, processing_done
        
        // Process item
        MOV     X1, X19
        BL      process_item
        
        // Free item
        BL      kfree
        
        B       process_loop

processing_done:
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

## DMA Operations

Direct Memory Access requires careful cache management and proper address translation.

**Example:**

```assembly
// Set up DMA transfer
// X0 = device structure
// X1 = source buffer (virtual address)
// X2 = size in bytes
// X3 = direction (0=to device, 1=from device)

setup_dma_transfer:
        STP     X29, X30, [SP, #-64]!
        STP     X19, X20, [SP, #16]
        STP     X21, X22, [SP, #32]
        STP     X23, X24, [SP, #48]
        MOV     X29, SP
        
        MOV     X19, X0                     // Device structure
        MOV     X20, X1                     // Source buffer
        MOV     X21, X2                     // Size
        MOV     X22, X3                     // Direction
        
        // Get device base address
        LDR     X23, [X19, #DEV_BASE_OFFSET]
        
        // Map buffer for DMA
        MOV     X0, X19
        MOV     X1, X20
        MOV     X2, X21
        MOV     X3, X22
        BL      dma_map_single
        CMN     X0, #1                      // Check for DMA_MAPPING_ERROR
        B.EQ    dma_map_failed
        
        MOV     X24, X0                     // Save DMA address
        
        // Configure DMA controller
        // Write source address
        STR     X24, [X23, #DMA_SRC_ADDR]
        
        // Write transfer size
        STR     W21, [X23, #DMA_SIZE]
        
        // Configure control register
        MOV     W0, #0
        ORR     W0, W0, #DMA_ENABLE
        CBZ     W22, write_direction
        ORR     W0, W0, #DMA_DIR_FROM_DEV
        
write_direction:
        ORR     W0, W0, #DMA_INT_ENABLE     // Enable completion interrupt
        STR     W0, [X23, #DMA_CTRL]
        
        // Ensure writes are visible to device
        DSB     SY
        
        // Start DMA transfer
        MOV     W0, #DMA_START
        STR     W0, [X23, #DMA_CMD]
        DSB     SY
        
        MOV     X0, #0                      // Success
        LDP     X23, X24, [SP, #48]
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #64
        RET

dma_map_failed:
        MOV     X0, #-1
        LDP     X23, X24, [SP, #48]
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #64
        RET

// DMA completion handler
dma_complete_handler:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        MOV     X29, SP
        
        MOV     X19, X0                     // Device structure
        
        // Get device base address
        LDR     X20, [X19, #DEV_BASE_OFFSET]
        
        // Read DMA status
        LDR     W0, [X20, #DMA_STATUS]
        
        // Check for errors
        TBNZ    W0, #DMA_ERROR_BIT, dma_error
        
        // Clear completion flag
        MOV     W1, #DMA_COMPLETE
        STR     W1, [X20, #DMA_STATUS]
        DSB     SY
        
        // Unmap DMA buffer
        LDR     X0, [X19, #DMA_ADDR_SAVE]
        LDR     X1, [X19, #DMA_SIZE_SAVE]
        LDR     X2, [X19, #DMA_DIR_SAVE]
        BL      dma_unmap_single
        
        // Notify completion
        MOV     X0, X19
        BL      complete_dma_transfer
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET

dma_error:
        // Handle DMA error
        MOV     X0, X19
        BL      handle_dma_error
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

## Spinlock Implementation

Modules use spinlocks for protecting critical sections in interrupt context.

**Example:**

```assembly
// Acquire spinlock with interrupts disabled
// X0 = spinlock address
// Returns previous interrupt state in X0

spin_lock_irqsave:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        MOV     X29, SP
        
        MOV     X19, X0                     // Save lock address
        
        // Save and disable interrupts
        MRS     X20, DAIF
        MSR     DAIFSet, #0x3               // Disable IRQ and FIQ
        
        // Try to acquire lock
spin_wait:
        // Load-exclusive
        LDAXR   W1, [X19]
        CBNZ    W1, spin_wait               // Lock held, retry
        
        // Try to set lock
        MOV     W2, #1
        STXR    W3, W2, [X19]
        CBNZ    W3, spin_wait               // Store failed, retry
        
        // Memory barrier
        DMB     ISH
        
        // Return saved interrupt state
        MOV     X0, X20
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET

// Release spinlock and restore interrupts
// X0 = spinlock address
// X1 = saved interrupt state

spin_unlock_irqrestore:
        // Memory barrier
        DMB     ISH
        
        // Release lock
        STR     WZR, [X0]
        
        // Restore interrupt state
        MSR     DAIF, X1
        
        RET
```

## Cache Management for DMA

Modules performing DMA must ensure cache coherency between CPU and device.

**Example:**

```assembly
// Flush cache for DMA buffer (CPU to device)
// X0 = virtual address
// X1 = size in bytes

dma_cache_flush:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        
        MOV     X19, X0                     // Start address
        ADD     X20, X0, X1                 // End address
        
        // Align to cache line size (assume 64 bytes)
        AND     X19, X19, #~63
        ADD     X20, X20, #63
        AND     X20, X20, #~63

flush_loop:
        // Clean and invalidate data cache line
        DC      CIVAC, X19
        ADD     X19, X19, #64
        CMP     X19, X20
        B.LT    flush_loop
        
        // Data synchronization barrier
        DSB     SY
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET

// Invalidate cache for DMA buffer (device to CPU)
// X0 = virtual address
// X1 = size in bytes

dma_cache_invalidate:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        
        MOV     X19, X0
        ADD     X20, X0, X1
        
        // Align to cache line
        AND     X19, X19, #~63
        ADD     X20, X20, #63
        AND     X20, X20, #~63

inval_loop:
        // Invalidate data cache line
        DC      IVAC, X19
        ADD     X19, X19, #64
        CMP     X19, X20
        B.LT    inval_loop
        
        // Data synchronization barrier
        DSB     SY
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

## System Call Implementation

Kernel modules can implement custom system calls or ioctl handlers.

**Example:**

```assembly
// Device ioctl handler
// X0 = file structure pointer
// X1 = command code
// X2 = user argument

device_ioctl:
        STP     X29, X30, [SP, #-48]!
        STP     X19, X20, [SP, #16]
        STP     X21, X22, [SP, #32]
        MOV     X29, SP
        
        MOV     X19, X0                     // File structure
        MOV     X20, X1                     // Command
        MOV     X21, X2                     // Argument
        
        // Dispatch based on command
        LDR     W0, =IOCTL_CMD_START
        CMP     W20, W0
        B.EQ    handle_start
        
        LDR     W0, =IOCTL_CMD_STOP
        CMP     W20, W0
        B.EQ    handle_stop
        
        LDR     W0, =IOCTL_CMD_GET_STATUS
        CMP     W20, W0
        B.EQ    handle_get_status
        
        LDR     W0, =IOCTL_CMD_SET_CONFIG
        CMP     W20, W0
        B.EQ    handle_set_config
        
        // Unknown command
        MOV     X0, #-22                    // -EINVAL
        B       ioctl_done

handle_start:
        MOV     X0, X19
        BL      device_start
        B       ioctl_done

handle_stop:
        MOV     X0, X19
        BL      device_stop
        B       ioctl_done

handle_get_status:
        // Allocate kernel buffer for status
        MOV     X0, #64
        MOV     X1, #0x20                   // GFP_KERNEL
        BL      kmalloc
        CBZ     X0, ioctl_nomem
        
        MOV     X22, X0                     // Save buffer
        
        // Get device status
        MOV     X0, X19
        MOV     X1, X22
        BL      get_device_status
        
        // Copy to user space
        MOV     X0, X21                     // User buffer
        MOV     X1, X22                     // Kernel buffer
        MOV     X2, #64                     // Size
        BL      copy_to_user
        CBNZ    X0, copy_failed
        
        // Free kernel buffer
        MOV     X0, X22
        BL      kfree
        
        MOV     X0, #0                      // Success
        B       ioctl_done

handle_set_config:
        // Allocate kernel buffer
        MOV     X0, #128
        MOV     X1, #0x20                   // GFP_KERNEL
        BL      kmalloc
        CBZ     X0, ioctl_nomem
        
        MOV     X22, X0
        
        // Copy from user space
        MOV     X0, X22                     // Kernel buffer
        MOV     X1, X21                     // User buffer
        MOV     X2, #128
        BL      copy_from_user
        CBNZ    X0, copy_failed
        
        // Apply configuration
        MOV     X0, X19
        MOV     X1, X22
        BL      set_device_config
        
        // Free buffer
        MOV     X0, X22
        BL      kfree
        
        MOV     X0, #0
        B       ioctl_done

copy_failed:
        MOV     X0, X22
        BL      kfree
        MOV     X0, #-14                    // -EFAULT
        B       ioctl_done

ioctl_nomem:
        MOV     X0, #-12                    // -ENOMEM
        B       ioctl_done

ioctl_done:
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #48
        RET
```

## Timer Management

Kernel modules can register timers for periodic or delayed operations.

**Example:**

```assembly
// Initialize and start kernel timer
// X0 = timer structure pointer
// X1 = timeout in jiffies
// X2 = callback function

setup_timer:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        MOV     X29, SP
        
        MOV     X19, X0                     // Timer structure
        MOV     X20, X1                     // Timeout
        
        // Initialize timer
        MOV     X0, X19
        BL      init_timer
        
        // Set callback function
        STR     X2, [X19, #TIMER_FUNC_OFFSET]
        
        // Set data pointer (self-reference)
        STR     X19, [X19, #TIMER_DATA_OFFSET]
        
        // Calculate expiry time
        BL      get_jiffies
        ADD     X0, X0, X20
        STR     X0, [X19, #TIMER_EXPIRES_OFFSET]
        
        // Add timer to system
        MOV     X0, X19
        BL      add_timer
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET

// Timer callback function
.global timer_callback
.type timer_callback, %function

timer_callback:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        MOV     X29, SP
        
        MOV     X19, X0                     // Timer data
        
        // Perform periodic task
        MOV     X0, X19
        BL      periodic_task
        
        // Re-arm timer if needed
        LDR     X0, [X19, #TIMER_PERIODIC_FLAG]
        CBZ     X0, timer_done
        
        // Calculate next expiry
        BL      get_jiffies
        LDR     X1, [X19, #TIMER_INTERVAL]
        ADD     X0, X0, X1
        STR     X0, [X19, #TIMER_EXPIRES_OFFSET]
        
        // Re-add timer
        MOV     X0, X19
        BL      add_timer

timer_done:
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

## Workqueue Operations

Workqueues allow deferring work to kernel threads.

**Example:**

```assembly
// Schedule work on system workqueue
// X0 = work structure pointer

schedule_work_item:
        STP     X29, X30, [SP, #-16]!
        MOV     X29, SP
        
        // Initialize work structure if needed
        LDR     X1, [X0, #WORK_FLAGS]
        TBNZ    X1, #WORK_INITIALIZED_BIT, work_ready
        
        // Initialize work
        BL      init_work
        
work_ready:
        // Schedule on system workqueue
        BL      schedule_work
        
        LDP     X29, X30, [SP], #16
        RET

// Work handler function
.global work_handler
.type work_handler, %function

work_handler:
        STP     X29, X30, [SP, #-32]!
        STP     X19, X20, [SP, #16]
        MOV     X29, SP
        
        MOV     X19, X0                     // Work structure
        
        // Get containing device structure
        LDR     X20, [X19, #WORK_DEVICE_OFFSET]
        
        // Acquire device lock
        LDR     X0, [X20, #DEVICE_LOCK_OFFSET]
        BL      mutex_lock
        
        // Perform deferred work
        MOV     X0, X20
        BL      do_deferred_work
        
        // Release lock
        LDR     X0, [X20, #DEVICE_LOCK_OFFSET]
        BL      mutex_unlock
        
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #32
        RET
```

## Atomic Operations

Kernel modules use atomic operations for lock-free synchronization.

**Example:**

```assembly
// Atomic increment
// X0 = pointer to atomic variable
// Returns old value in X0

atomic_inc:
        MOV     X1, X0                      // Save address

inc_retry:
        LDAXR   W2, [X1]                    // Load-exclusive with acquire
        ADD     W3, W2, #1                  // Increment
        STLXR   W4, W3, [X1]                // Store-exclusive with release
        CBNZ    W4, inc_retry               // Retry if store failed
        
        MOV     W0, W2                      // Return old value
        RET

// Atomic compare-and-swap
// X0 = pointer to atomic variable
// X1 = old value (expected)
// X2 = new value
// Returns 1 if successful, 0 if failed

atomic_cmpxchg:
        MOV     X3, X0                      // Save address

cmpxchg_retry:
        LDAXR   W4, [X3]                    // Load-exclusive
        CMP     W4, W1                      // Compare with expected
        B.NE    cmpxchg_failed              // Mismatch, fail
        
        STLXR   W5, W2, [X3]                // Try to store new value
        CBNZ    W5, cmpxchg_retry           // Retry if exclusive failed
        
        MOV     X0, #1                      // Success
        RET

cmpxchg_failed:
        CLREX                               // Clear exclusive monitor
        MOV     X0, #0                      // Failure
        RET

// Atomic bit operations
// X0 = pointer to word
// X1 = bit number

atomic_set_bit:
        MOV     X2, X0
        MOV     X3, #1
        LSL     X3, X3, X1                  // Create bit mask

setbit_retry:
        LDAXR   X4, [X2]
        ORR     X4, X4, X3                  // Set bit
        STLXR   W5, X4, [X2]
        CBNZ    W5, setbit_retry
        
        RET

atomic_clear_bit:
        MOV     X2, X0
        MOV     X3, #1
        LSL     X3, X3, X1
        MVN     X3, X3                      // Invert mask

clrbit_retry:
        LDAXR   X4, [X2]
        AND     X4, X4, X3                  // Clear bit
        STLXR   W5, X4, [X2]
        CBNZ    W5, clrbit_retry
        
        RET

atomic_test_bit:
        LDR     X2, [X0]
        LSR     X2, X2, X1
        AND     X0, X2, #1
        RET
```

## Character Device Operations

Implementing file operations for character devices.

**Example:**

```assembly
// Character device read operation
// X0 = file structure
// X1 = user buffer
// X2 = count
// X3 = offset pointer

device_read:
        STP     X29, X30, [SP, #-64]!
        STP     X19, X20, [SP, #16]
        STP     X21, X22, [SP, #32]
        STP     X23, X24, [SP, #48]
        MOV     X29, SP
        
        MOV     X19, X0                     // File
        MOV     X20, X1                     // User buffer
        MOV     X21, X2                     // Count
        MOV     X22, X3                     // Offset
        
        // Get device structure from file private data
        LDR     X23, [X19, #FILE_PRIVATE_DATA]
        
        // Acquire read lock
        LDR     X0, [X23, #DEV_READ_LOCK]
        BL      mutex_lock
        
        // Check if data available
        LDR     X0, [X23, #DEV_BUFFER_HEAD]
        LDR     X1, [X23, #DEV_BUFFER_TAIL]
        CMP     X0, X1
        B.EQ    no_data_available
        
        // Calculate available data
        SUB     X24, X1, X0
        CMP     X24, X21
        CSEL    X24, X24, X21, LT           // min(available, count)
        
        // Copy to user space
        MOV     X0, X20                     // Destination (user)
        ADD     X1, X23, #DEV_BUFFER_DATA   // Source (kernel)
        LDR     X2, [X23, #DEV_BUFFER_HEAD]
        ADD     X1, X1, X2
        MOV     X2, X24                     // Size
        BL      copy_to_user
        CBNZ    X0, read_fault
        
        // Update buffer head
        LDR     X0, [X23, #DEV_BUFFER_HEAD]
        ADD     X0, X0, X24
        STR     X0, [X23, #DEV_BUFFER_HEAD]
        
        // Release lock
        LDR     X0, [X23, #DEV_READ_LOCK]
        BL      mutex_unlock
        
        // Return bytes read
        MOV     X0, X24
        LDP     X23, X24, [SP, #48]
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #64
        RET

no_data_available:
        // Check if non-blocking
        LDR     W0, [X19, #FILE_FLAGS]
        TST     W0, #O_NONBLOCK
        B.NE    return_eagain
        
        // Wait for data
        LDR     X0, [X23, #DEV_READ_LOCK]
        BL      mutex_unlock
        
        LDR     X0, [X23, #DEV_WAIT_QUEUE]
        BL      wait_event_interruptible
        
        // Check for signal
        CBNZ    X0, interrupted
        
        // Retry read
        B       device_read

return_eagain:
        LDR     X0, [X23, #DEV_READ_LOCK]
        BL      mutex_unlock
        MOV     X0, #-11                    // -EAGAIN
        B       read_done

read_fault:
        LDR     X0, [X23, #DEV_READ_LOCK]
        BL      mutex_unlock
        MOV     X0, #-14                    // -EFAULT
        B       read_done

interrupted:
        MOV     X0, #-4                     // -EINTR
        B       read_done

read_done:
        LDP     X23, X24, [SP, #48]
        LDP     X21, X22, [SP, #32]
        LDP     X19, X20, [SP, #16]
        LDP     X29, X30, [SP], #64
        RET
```

## Module Parameter Handling

**Example:**

```assembly
// Parse module parameters during initialization
// Module parameters are typically set up in C, but assembly
// can access them

.section .data
.global param_buffer_size
param_buffer_size:
        .word   4096                        // Default value

.global param_debug_level
param_debug_level:
        .word   0

// Function to validate and use parameters
validate_params:
        STP     X29, X30, [SP, #-16]!
        MOV     X29, SP
        
        // Check buffer size parameter
        LDR     X0, =param_buffer_size
        LDR     W1, [X0]
        
        // Ensure minimum size
        CMP     W1, #1024
        B.GE    size_ok
        MOV     W1, #1024
        STR     W1, [X0]
        
size_ok:
        // Ensure power of 2
        SUB     W2, W1, #1
        TST     W1, W2
        B.EQ    power_of_2
        
        // Round up to next power of 2
        CLZ     W2, W1
        MOV     W3, #32
        SUB     W2, W3, W2
        MOV     W3, #1
        LSL     W3, W3, W2
        STR     W3, [X0]
        
power_of_2:
        // Validate debug level
        LDR     X0, =param_debug_level
        LDR     W1, [X0]
        CMP     W1, #3
        B.LE    debug_ok
        MOV     W1, #3
        STR     W1, [X0]
        
debug_ok:
        LDP     X29, X30, [SP], #16
        RET
```

**Key Points:**

- Kernel modules must handle architecture-specific details like exception levels and cache coherency
- Hardware register access requires proper memory barriers to ensure ordering
- Interrupt handlers should minimize work and defer processing to tasklets or workqueues
- DMA operations require careful cache management to maintain coherency
- Atomic operations use load-exclusive/store-exclusive instructions for lock-free synchronization
- Module code executes in kernel context at EL1 (typically)
- Proper locking (spinlocks, mutexes) is critical for protecting shared data structures
- User-space data must be accessed through copy_to_user/copy_from_user functions
- **[Inference]** Module loading involves dynamic symbol resolution and relocation application by the kernel loader
- Error handling must return appropriate Linux error codes (negative values)

---

# Real-World Projects in ARM Assembly

ARM assembly plays a critical role in real-world embedded systems and high-performance computing applications where direct hardware control, precise timing, and optimal performance are essential. Device drivers and DSP algorithms represent two major domains where assembly language provides advantages over higher-level languages.

## Device Drivers

Device drivers serve as the interface between hardware peripherals and operating system or application software. ARM assembly is used in performance-critical driver sections, hardware initialization routines, and interrupt handlers where precise control and minimal latency are required.

### Memory-Mapped I/O Access

ARM peripherals communicate through memory-mapped registers, where specific memory addresses correspond to hardware control and status registers.

```assembly
// GPIO (General Purpose Input/Output) driver for ARM Cortex-M
// Example: STM32F4 GPIO registers
.equ GPIOA_BASE,    0x40020000
.equ GPIO_MODER,    0x00          // Mode register offset
.equ GPIO_OTYPER,   0x04          // Output type register offset
.equ GPIO_OSPEEDR,  0x08          // Output speed register offset
.equ GPIO_PUPDR,    0x0C          // Pull-up/pull-down register offset
.equ GPIO_IDR,      0x10          // Input data register offset
.equ GPIO_ODR,      0x14          // Output data register offset
.equ GPIO_BSRR,     0x18          // Bit set/reset register offset

// Initialize GPIO pin as output
// x0 = GPIO base address
// w1 = pin number (0-15)
gpio_init_output:
    stp x29, x30, [sp, #-16]!
    mov x29, sp
    
    // Configure pin mode as output (01 in MODER)
    ldr w2, [x0, #GPIO_MODER]
    mov w3, #0x3                   // Mask for 2 bits
    lsl w3, w3, w1, lsl #1         // Shift to pin position
    bic w2, w2, w3                 // Clear existing bits
    mov w3, #0x1                   // Output mode
    lsl w3, w3, w1, lsl #1
    orr w2, w2, w3                 // Set output mode
    str w2, [x0, #GPIO_MODER]
    
    // Configure as push-pull output (0 in OTYPER)
    ldr w2, [x0, #GPIO_OTYPER]
    mov w3, #1
    lsl w3, w3, w1
    bic w2, w2, w3                 // Clear bit for push-pull
    str w2, [x0, #GPIO_OTYPER]
    
    // Set high speed (10 in OSPEEDR)
    ldr w2, [x0, #GPIO_OSPEEDR]
    mov w3, #0x3
    lsl w3, w3, w1, lsl #1
    bic w2, w2, w3
    mov w3, #0x2                   // High speed
    lsl w3, w3, w1, lsl #1
    orr w2, w2, w3
    str w2, [x0, #GPIO_OSPEEDR]
    
    // No pull-up/pull-down (00 in PUPDR)
    ldr w2, [x0, #GPIO_PUPDR]
    mov w3, #0x3
    lsl w3, w3, w1, lsl #1
    bic w2, w2, w3                 // Clear for no pull-up/down
    str w2, [x0, #GPIO_PUPDR]
    
    ldp x29, x30, [sp], #16
    ret

// Set GPIO pin high
// x0 = GPIO base address
// w1 = pin number
gpio_set_high:
    mov w2, #1
    lsl w2, w2, w1                 // Create bit mask
    str w2, [x0, #GPIO_BSRR]       // Write to set register
    ret

// Set GPIO pin low
// x0 = GPIO base address
// w1 = pin number
gpio_set_low:
    mov w2, #1
    lsl w2, w2, w1
    lsl w2, w2, #16                // Reset bits in upper 16 bits
    str w2, [x0, #GPIO_BSRR]
    ret

// Read GPIO pin state
// x0 = GPIO base address
// w1 = pin number
// Returns: w0 = 0 or 1
gpio_read:
    ldr w2, [x0, #GPIO_IDR]
    lsr w2, w2, w1
    and w0, w2, #1
    ret

// Toggle GPIO pin
// x0 = GPIO base address
// w1 = pin number
gpio_toggle:
    stp x29, x30, [sp, #-16]!
    
    ldr w2, [x0, #GPIO_ODR]
    mov w3, #1
    lsl w3, w3, w1
    eor w2, w2, w3                 // XOR to toggle
    str w2, [x0, #GPIO_ODR]
    
    ldp x29, x30, [sp], #16
    ret
```

### Interrupt Handling

Interrupt handlers require precise timing and minimal overhead. ARM assembly allows direct control over interrupt entry, context saving, and hardware acknowledgment.

```assembly
// UART interrupt handler for ARM Cortex-A
.equ UART0_BASE,    0x10009000
.equ UART_DR,       0x00          // Data register
.equ UART_FR,       0x18          // Flag register
.equ UART_IMSC,     0x38          // Interrupt mask set/clear
.equ UART_RIS,      0x3C          // Raw interrupt status
.equ UART_MIS,      0x40          // Masked interrupt status
.equ UART_ICR,      0x44          // Interrupt clear register

.equ UART_FR_RXFE,  (1 << 4)      // Receive FIFO empty
.equ UART_FR_TXFF,  (1 << 5)      // Transmit FIFO full
.equ UART_INT_RX,   (1 << 4)      // Receive interrupt
.equ UART_INT_TX,   (1 << 5)      // Transmit interrupt

// Circular buffer structure
.equ BUFFER_SIZE,   256
.struct 0
rx_buffer:      .space BUFFER_SIZE
rx_head:        .word 0
rx_tail:        .word 0
rx_count:       .word 0
tx_buffer:      .space BUFFER_SIZE
tx_head:        .word 0
tx_tail:        .word 0
tx_count:       .word 0
.text

// UART interrupt handler
uart_irq_handler:
    // Save context (caller-saved registers)
    stp x0, x1, [sp, #-16]!
    stp x2, x3, [sp, #-16]!
    stp x4, x5, [sp, #-16]!
    stp x29, x30, [sp, #-16]!
    
    // Read interrupt status
    ldr x0, =UART0_BASE
    ldr w1, [x0, #UART_MIS]
    
    // Check for receive interrupt
    tst w1, #UART_INT_RX
    b.eq check_tx_interrupt
    
    // Handle receive interrupt
    bl uart_rx_handler
    
    // Clear receive interrupt
    mov w1, #UART_INT_RX
    str w1, [x0, #UART_ICR]

check_tx_interrupt:
    ldr x0, =UART0_BASE
    ldr w1, [x0, #UART_MIS]
    tst w1, #UART_INT_TX
    b.eq irq_exit
    
    // Handle transmit interrupt
    bl uart_tx_handler
    
    // Clear transmit interrupt
    mov w1, #UART_INT_TX
    str w1, [x0, #UART_ICR]

irq_exit:
    // Restore context
    ldp x29, x30, [sp], #16
    ldp x4, x5, [sp], #16
    ldp x2, x3, [sp], #16
    ldp x0, x1, [sp], #16
    eret                          // Exception return

// Receive handler - read data from UART into circular buffer
uart_rx_handler:
    ldr x0, =UART0_BASE
    ldr x1, =rx_buffer
    
rx_loop:
    // Check if FIFO has data
    ldr w2, [x0, #UART_FR]
    tst w2, #UART_FR_RXFE
    b.ne rx_done                  // Exit if FIFO empty
    
    // Check if buffer has space
    ldr w3, [x1, #rx_count]
    cmp w3, #BUFFER_SIZE
    b.ge rx_done                  // Exit if buffer full
    
    // Read byte from UART
    ldrb w4, [x0, #UART_DR]
    
    // Store in circular buffer
    ldr w5, [x1, #rx_head]
    add x6, x1, x5
    strb w4, [x6]
    
    // Update head pointer
    add w5, w5, #1
    and w5, w5, #(BUFFER_SIZE - 1)  // Wrap around
    str w5, [x1, #rx_head]
    
    // Increment count
    add w3, w3, #1
    str w3, [x1, #rx_count]
    
    b rx_loop

rx_done:
    ret

// Transmit handler - send data from circular buffer to UART
uart_tx_handler:
    ldr x0, =UART0_BASE
    ldr x1, =tx_buffer
    
tx_loop:
    // Check if buffer has data
    ldr w2, [x1, #tx_count]
    cbz w2, tx_done               // Exit if buffer empty
    
    // Check if UART FIFO has space
    ldr w3, [x0, #UART_FR]
    tst w3, #UART_FR_TXFF
    b.ne tx_done                  // Exit if FIFO full
    
    // Read byte from circular buffer
    ldr w4, [x1, #tx_tail]
    add x5, x1, x4
    ldrb w6, [x5]
    
    // Write to UART
    strb w6, [x0, #UART_DR]
    
    // Update tail pointer
    add w4, w4, #1
    and w4, w4, #(BUFFER_SIZE - 1)  // Wrap around
    str w4, [x1, #tx_tail]
    
    // Decrement count
    sub w2, w2, #1
    str w2, [x1, #tx_count]
    
    b tx_loop

tx_done:
    // If buffer empty, disable TX interrupt
    ldr w2, [x1, #tx_count]
    cbnz w2, tx_exit
    
    ldr x0, =UART0_BASE
    ldr w3, [x0, #UART_IMSC]
    bic w3, w3, #UART_INT_TX
    str w3, [x0, #UART_IMSC]

tx_exit:
    ret
```

### DMA (Direct Memory Access) Driver

DMA controllers transfer data between memory and peripherals without CPU intervention, improving throughput and reducing processor load.

```assembly
// DMA controller driver (simplified ARM DMA architecture)
.equ DMA_BASE,          0x40020000
.equ DMA_ISR,           0x00      // Interrupt status register
.equ DMA_IFCR,          0x04      // Interrupt flag clear register
.equ DMA_CCR,           0x08      // Channel configuration register
.equ DMA_CNDTR,         0x0C      // Channel number of data register
.equ DMA_CPAR,          0x10      // Channel peripheral address register
.equ DMA_CMAR,          0x14      // Channel memory address register

// DMA_CCR bits
.equ DMA_CCR_EN,        (1 << 0)  // Channel enable
.equ DMA_CCR_TCIE,      (1 << 1)  // Transfer complete interrupt enable
.equ DMA_CCR_HTIE,      (1 << 2)  // Half transfer interrupt enable
.equ DMA_CCR_TEIE,      (1 << 3)  // Transfer error interrupt enable
.equ DMA_CCR_DIR,       (1 << 4)  // Data transfer direction (0=periph->mem)
.equ DMA_CCR_CIRC,      (1 << 5)  // Circular mode
.equ DMA_CCR_PINC,      (1 << 6)  // Peripheral increment mode
.equ DMA_CCR_MINC,      (1 << 7)  // Memory increment mode
.equ DMA_CCR_PSIZE_8,   (0 << 8)  // Peripheral size: 8 bits
.equ DMA_CCR_PSIZE_16,  (1 << 8)  // Peripheral size: 16 bits
.equ DMA_CCR_PSIZE_32,  (2 << 8)  // Peripheral size: 32 bits
.equ DMA_CCR_MSIZE_8,   (0 << 10) // Memory size: 8 bits
.equ DMA_CCR_MSIZE_16,  (1 << 10) // Memory size: 16 bits
.equ DMA_CCR_MSIZE_32,  (2 << 10) // Memory size: 32 bits
.equ DMA_CCR_PL_LOW,    (0 << 12) // Priority level: Low
.equ DMA_CCR_PL_MED,    (1 << 12) // Priority level: Medium
.equ DMA_CCR_PL_HIGH,   (2 << 12) // Priority level: High
.equ DMA_CCR_PL_VHIGH,  (3 << 12) // Priority level: Very high
.equ DMA_CCR_MEM2MEM,   (1 << 14) // Memory to memory mode

// Configure DMA transfer
// x0 = DMA channel base address
// x1 = source address
// x2 = destination address
// w3 = transfer count
// w4 = configuration flags
dma_configure_transfer:
    stp x29, x30, [sp, #-16]!
    
    // Disable channel first
    ldr w5, [x0, #DMA_CCR]
    bic w5, w5, #DMA_CCR_EN
    str w5, [x0, #DMA_CCR]
    
    // Wait for channel to be disabled
1:  ldr w5, [x0, #DMA_CCR]
    tst w5, #DMA_CCR_EN
    b.ne 1b
    
    // Configure peripheral address (source for periph->mem)
    str x1, [x0, #DMA_CPAR]
    
    // Configure memory address (destination)
    str x2, [x0, #DMA_CMAR]
    
    // Set transfer count
    str w3, [x0, #DMA_CNDTR]
    
    // Set configuration
    str w4, [x0, #DMA_CCR]
    
    ldp x29, x30, [sp], #16
    ret

// Start DMA transfer
// x0 = DMA channel base address
dma_start_transfer:
    ldr w1, [x0, #DMA_CCR]
    orr w1, w1, #DMA_CCR_EN
    str w1, [x0, #DMA_CCR]
    ret

// Stop DMA transfer
// x0 = DMA channel base address
dma_stop_transfer:
    ldr w1, [x0, #DMA_CCR]
    bic w1, w1, #DMA_CCR_EN
    str w1, [x0, #DMA_CCR]
    ret

// Example: Configure DMA for UART receive
// Peripheral -> Memory, circular buffer mode
setup_uart_rx_dma:
    stp x29, x30, [sp, #-16]!
    
    ldr x0, =DMA_CHANNEL1_BASE
    ldr x1, =UART0_DR_ADDRESS     // UART data register (source)
    ldr x2, =uart_rx_dma_buffer   // Memory buffer (destination)
    mov w3, #256                   // Transfer 256 bytes
    
    // Configuration: 
    // - Peripheral to memory
    // - Circular mode
    // - Memory increment
    // - 8-bit transfers
    // - High priority
    // - Transfer complete interrupt
    mov w4, #0
    orr w4, w4, #DMA_CCR_CIRC
    orr w4, w4, #DMA_CCR_MINC
    orr w4, w4, #DMA_CCR_PSIZE_8
    orr w4, w4, #DMA_CCR_MSIZE_8
    orr w4, w4, #DMA_CCR_PL_HIGH
    orr w4, w4, #DMA_CCR_TCIE
    
    bl dma_configure_transfer
    bl dma_start_transfer
    
    ldp x29, x30, [sp], #16
    ret
```

### Timer Driver

Timers generate precise delays, measure time intervals, and trigger periodic events. ARM processors include hardware timer peripherals with various operating modes.

```assembly
// General Purpose Timer driver
.equ TIMER_BASE,    0x40000000
.equ TIMER_CR1,     0x00          // Control register 1
.equ TIMER_CR2,     0x04          // Control register 2
.equ TIMER_DIER,    0x0C          // DMA/Interrupt enable register
.equ TIMER_SR,      0x10          // Status register
.equ TIMER_CNT,     0x24          // Counter
.equ TIMER_PSC,     0x28          // Prescaler
.equ TIMER_ARR,     0x2C          // Auto-reload register
.equ TIMER_CCR1,    0x34          // Capture/compare register 1

// TIMER_CR1 bits
.equ TIMER_CR1_CEN, (1 << 0)      // Counter enable
.equ TIMER_CR1_UDIS,(1 << 1)      // Update disable
.equ TIMER_CR1_URS, (1 << 2)      // Update request source
.equ TIMER_CR1_OPM, (1 << 3)      // One-pulse mode
.equ TIMER_CR1_ARPE,(1 << 7)      // Auto-reload preload enable

// TIMER_DIER bits
.equ TIMER_DIER_UIE,(1 << 0)      // Update interrupt enable
.equ TIMER_DIER_CC1IE,(1 << 1)    // Capture/compare 1 interrupt enable

// Initialize timer for microsecond precision
// Assumes 72 MHz system clock
// x0 = timer base address
timer_init_us:
    stp x29, x30, [sp, #-16]!
    
    // Disable timer
    ldr w1, [x0, #TIMER_CR1]
    bic w1, w1, #TIMER_CR1_CEN
    str w1, [x0, #TIMER_CR1]
    
    // Set prescaler for 1 MHz (72 MHz / 72 = 1 MHz)
    mov w1, #71                    // Prescaler = 72 - 1
    str w1, [x0, #TIMER_PSC]
    
    // Set auto-reload for maximum period
    mov w1, #0xFFFF
    str w1, [x0, #TIMER_ARR]
    
    // Enable auto-reload preload
    ldr w1, [x0, #TIMER_CR1]
    orr w1, w1, #TIMER_CR1_ARPE
    str w1, [x0, #TIMER_CR1]
    
    // Generate update event to load prescaler
    ldr w1, [x0, #TIMER_CR2]
    orr w1, w1, #(1 << 0)          // UG bit
    str w1, [x0, #TIMER_CR2]
    
    ldp x29, x30, [sp], #16
    ret

// Delay for specified microseconds
// x0 = timer base address
// w1 = delay in microseconds
timer_delay_us:
    stp x29, x30, [sp, #-16]!
    
    // Reset counter
    str wzr, [x0, #TIMER_CNT]
    
    // Enable timer
    ldr w2, [x0, #TIMER_CR1]
    orr w2, w2, #TIMER_CR1_CEN
    str w2, [x0, #TIMER_CR1]
    
    // Wait until counter reaches delay value
1:  ldr w3, [x0, #TIMER_CNT]
    cmp w3, w1
    b.lt 1b
    
    // Disable timer
    ldr w2, [x0, #TIMER_CR1]
    bic w2, w2, #TIMER_CR1_CEN
    str w2, [x0, #TIMER_CR1]
    
    ldp x29, x30, [sp], #16
    ret

// Configure timer for periodic interrupt
// x0 = timer base address
// w1 = period in microseconds
timer_setup_periodic:
    stp x29, x30, [sp, #-16]!
    
    // Disable timer
    ldr w2, [x0, #TIMER_CR1]
    bic w2, w2, #TIMER_CR1_CEN
    str w2, [x0, #TIMER_CR1]
    
    // Set auto-reload value
    str w1, [x0, #TIMER_ARR]
    
    // Reset counter
    str wzr, [x0, #TIMER_CNT]
    
    // Enable update interrupt
    mov w2, #TIMER_DIER_UIE
    str w2, [x0, #TIMER_DIER]
    
    // Enable timer
    ldr w2, [x0, #TIMER_CR1]
    orr w2, w2, #TIMER_CR1_CEN
    str w2, [x0, #TIMER_CR1]
    
    ldp x29, x30, [sp], #16
    ret

// Timer interrupt handler
timer_irq_handler:
    stp x29, x30, [sp, #-16]!
    
    ldr x0, =TIMER_BASE
    
    // Clear update interrupt flag
    ldr w1, [x0, #TIMER_SR]
    bic w1, w1, #1
    str w1, [x0, #TIMER_SR]
    
    // Call user callback
    bl timer_callback
    
    ldp x29, x30, [sp], #16
    eret

// PWM (Pulse Width Modulation) configuration
// x0 = timer base address
// w1 = period in microseconds
// w2 = duty cycle (0-100 percentage)
timer_pwm_init:
    stp x29, x30, [sp, #-16]!
    
    // Disable timer
    ldr w3, [x0, #TIMER_CR1]
    bic w3, w3, #TIMER_CR1_CEN
    str w3, [x0, #TIMER_CR1]
    
    // Set period (auto-reload)
    str w1, [x0, #TIMER_ARR]
    
    // Calculate duty cycle value
    mul w4, w1, w2
    mov w5, #100
    udiv w4, w4, w5
    str w4, [x0, #TIMER_CCR1]
    
    // Enable timer
    ldr w3, [x0, #TIMER_CR1]
    orr w3, w3, #TIMER_CR1_CEN
    str w3, [x0, #TIMER_CR1]
    
    ldp x29, x30, [sp], #16
    ret
```

### I2C (Inter-Integrated Circuit) Driver

I2C is a synchronous serial communication protocol commonly used for sensor and peripheral interfacing.

```assembly
// I2C driver for ARM processors
.equ I2C_BASE,      0x40005400
.equ I2C_CR1,       0x00          // Control register 1
.equ I2C_CR2,       0x04          // Control register 2
.equ I2C_OAR1,      0x08          // Own address register 1
.equ I2C_DR,        0x10          // Data register
.equ I2C_SR1,       0x14          // Status register 1
.equ I2C_SR2,       0x18          // Status register 2
.equ I2C_CCR,       0x1C          // Clock control register
.equ I2C_TRISE,     0x20          // Rise time register

// I2C_CR1 bits
.equ I2C_CR1_PE,    (1 << 0)      // Peripheral enable
.equ I2C_CR1_START, (1 << 8)      // Start generation
.equ I2C_CR1_STOP,  (1 << 9)      // Stop generation
.equ I2C_CR1_ACK,   (1 << 10)     // Acknowledge enable

// I2C_SR1 bits
.equ I2C_SR1_SB,    (1 << 0)      // Start bit
.equ I2C_SR1_ADDR,  (1 << 1)      // Address sent
.equ I2C_SR1_BTF,   (1 << 2)      // Byte transfer finished
.equ I2C_SR1_TXE,   (1 << 7)      // Data register empty
.equ I2C_SR1_RXNE,  (1 << 6)      // Data register not empty

// Initialize I2C peripheral
// x0 = I2C base address
// w1 = clock speed in Hz (e.g., 100000 for 100 kHz)
i2c_init:
    stp x29, x30, [sp, #-16]!

    // Disable I2C
    ldr w2, [x0, #I2C_CR1]
    bic w2, w2, #I2C_CR1_PE
    str w2, [x0, #I2C_CR1]

    // Configure clock (simplified - assumes 36 MHz peripheral clock)
    mov w2, #36
    str w2, [x0, #I2C_CR2]

    // Calculate CCR value for 100 kHz
    mov w3, #36000000
    lsl w4, w1, #1
    udiv w3, w3, w4
    str w3, [x0, #I2C_CCR]

    // Set rise time (1 µs for standard mode)
    mov w3, #37
    str w3, [x0, #I2C_TRISE]

    // Enable I2C
    ldr w2, [x0, #I2C_CR1]
    orr w2, w2, #I2C_CR1_PE
    str w2, [x0, #I2C_CR1]

    ldp x29, x30, [sp], #16
    ret


// Generate I2C start condition
// x0 = I2C base address
// Returns: w0 = 0 on success, -1 on timeout
i2c_start:
    stp x29, x30, [sp, #-16]!

    // Generate START
    ldr w1, [x0, #I2C_CR1]
    orr w1, w1, #I2C_CR1_START
    str w1, [x0, #I2C_CR1]

    // Wait for SB flag with timeout
    mov w2, #10000
1:
    ldr w3, [x0, #I2C_SR1]
    tst w3, #I2C_SR1_SB
    b.ne 2f
    subs w2, w2, #1
    b.ne 1b

    // Timeout occurred
    mov w0, #-1
    b 3f

2:
    mov w0, #0

3:
    ldp x29, x30, [sp], #16
    ret


// Send I2C address
// x0 = I2C base address
// w1 = 7-bit address
// w2 = direction (0 = write, 1 = read)
// Returns: w0 = 0 on success, -1 on error
i2c_send_address:
    stp x29, x30, [sp, #-16]!

    // Prepare address byte
    lsl w1, w1, #1
    orr w1, w1, w2
    strb w1, [x0, #I2C_DR]

    // Wait for ADDR flag
    mov w3, #10000
1:
    ldr w4, [x0, #I2C_SR1]
    tst w4, #I2C_SR1_ADDR
    b.ne 2f
    subs w3, w3, #1
    b.ne 1b

    // Timeout
    mov w0, #-1
    b 3f

2:
    // Clear ADDR flag by reading SR1 and SR2
    ldr w4, [x0, #I2C_SR1]
    ldr w4, [x0, #I2C_SR2]
    mov w0, #0

3:
    ldp x29, x30, [sp], #16
    ret


// Write byte to I2C
// x0 = I2C base address
// w1 = data byte
// Returns: w0 = 0 on success, -1 on error
i2c_write_byte:
    stp x29, x30, [sp, #-16]!

    // Wait for TXE flag
    mov w2, #10000
1:
    ldr w3, [x0, #I2C_SR1]
    tst w3, #I2C_SR1_TXE
    b.ne 2f
    subs w2, w2, #1
    b.ne 1b

    // Timeout
    mov w0, #-1
    b 3f

2:
    strb w1, [x0, #I2C_DR]
    mov w0, #0

3:
    ldp x29, x30, [sp], #16
    ret


// Read byte from I2C
// x0 = I2C base address
// w1 = send ACK (1) or NACK (0)
// Returns: w0 = data byte, or -1 on error
i2c_read_byte:
    stp x29, x30, [sp, #-16]!

    // Configure ACK/NACK
    ldr w2, [x0, #I2C_CR1]
    cbz w1, 1f
    orr w2, w2, #I2C_CR1_ACK
    b 2f

1:
    bic w2, w2, #I2C_CR1_ACK

2:
    str w2, [x0, #I2C_CR1]

    // Wait for RXNE flag
    mov w3, #10000
3:
    ldr w4, [x0, #I2C_SR1]
    tst w4, #I2C_SR1_RXNE
    b.ne 4f
    subs w3, w3, #1
    b.ne 3b

    // Timeout
    mov w0, #-1
    b 5f

4:
    ldrb w0, [x0, #I2C_DR]

5:
    ldp x29, x30, [sp], #16
    ret


// Generate I2C stop condition
// x0 = I2C base address
i2c_stop:
    ldr w1, [x0, #I2C_CR1]
    orr w1, w1, #I2C_CR1_STOP
    str w1, [x0, #I2C_CR1]
    ret


// Complete I2C write transaction
// x0 = I2C base address
// w1 = device address (7-bit)
// x2 = pointer to data buffer
// w3 = number of bytes to write
// Returns: w0 = 0 on success, negative on error
i2c_write_data:
    stp x29, x30, [sp, #-32]!
    stp x19, x20, [sp, #16]

    mov x19, x0
    mov w20, w3
    mov x21, x2
    mov w22, w1

    // Generate START
    mov x0, x19
    bl i2c_start
    cbnz w0, write_error

    // Send address with write bit
    mov x0, x19
    mov w1, w22
    mov w2, #0
    bl i2c_send_address
    cbnz w0, write_error

write_loop:
    cbz w20, write_done
    ldrb w1, [x21], #1
    mov x0, x19
    bl i2c_write_byte
    cbnz w0, write_error
    sub w20, w20, #1
    b write_loop

write_done:
    mov x0, x19
    bl i2c_stop
    mov w0, #0
    b write_exit

write_error:
    mov x0, x19
    bl i2c_stop
    mov w0, #-1

write_exit:
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #32
    ret


// Complete I2C read transaction
// x0 = I2C base address
// w1 = device address (7-bit)
// x2 = pointer to data buffer
// w3 = number of bytes to read
// Returns: w0 = 0 on success, negative on error
i2c_read_data:
    stp x29, x30, [sp, #-32]!
    stp x19, x20, [sp, #16]

    mov x19, x0
    mov w20, w3
    mov x21, x2
    mov w22, w1

    // Generate START
    mov x0, x19
    bl i2c_start
    cbnz w0, read_error

    // Send address with read bit
    mov x0, x19
    mov w1, w22
    mov w2, #1
    bl i2c_send_address
    cbnz w0, read_error

    // Enable ACK for multi-byte read
    cmp w20, #1
    b.le read_loop
    ldr w1, [x19, #I2C_CR1]
    orr w1, w1, #I2C_CR1_ACK
    str w1, [x19, #I2C_CR1]

read_loop:
    cbz w20, read_done

    // Send NACK on last byte
    cmp w20, #1
    cset w1, ne

    mov x0, x19
    bl i2c_read_byte
    cmp w0, #-1
    b.eq read_error

    strb w0, [x21], #1
    sub w20, w20, #1
    b read_loop

read_done:
    mov x0, x19
    bl i2c_stop
    mov w0, #0
    b read_exit

read_error:
    mov x0, x19
    bl i2c_stop
    mov w0, #-1

read_exit:
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #32
    ret
````

### SPI (Serial Peripheral Interface) Driver

SPI provides full-duplex synchronous communication with higher speeds than I2C, commonly used for flash memory, displays, and high-speed sensors.

```assembly
// SPI driver for ARM processors
.equ SPI_BASE,      0x40013000
.equ SPI_CR1,       0x00          // Control register 1
.equ SPI_CR2,       0x04          // Control register 2
.equ SPI_SR,        0x08          // Status register
.equ SPI_DR,        0x0C          // Data register

// SPI_CR1 bits
.equ SPI_CR1_CPHA,  (1 << 0)      // Clock phase
.equ SPI_CR1_CPOL,  (1 << 1)      // Clock polarity
.equ SPI_CR1_MSTR,  (1 << 2)      // Master selection
.equ SPI_CR1_BR,    (7 << 3)      // Baud rate control (3 bits)
.equ SPI_CR1_SPE,   (1 << 6)      // SPI enable
.equ SPI_CR1_LSBFIRST,(1 << 7)    // Frame format
.equ SPI_CR1_SSI,   (1 << 8)      // Internal slave select
.equ SPI_CR1_SSM,   (1 << 9)      // Software slave management
.equ SPI_CR1_RXONLY,(1 << 10)     // Receive only
.equ SPI_CR1_DFF,   (1 << 11)     // Data frame format (0=8bit, 1=16bit)
.equ SPI_CR1_BIDIOE,(1 << 14)     // Bidirectional mode enable

// SPI_SR bits
.equ SPI_SR_RXNE,   (1 << 0)      // Receive buffer not empty
.equ SPI_SR_TXE,    (1 << 1)      // Transmit buffer empty
.equ SPI_SR_BSY,    (1 << 7)      // Busy flag

// Initialize SPI
// x0 = SPI base address
// w1 = mode (0-3: combinations of CPOL and CPHA)
// w2 = prescaler (0-7 for divide by 2,4,8,16,32,64,128,256)
spi_init:
    stp x29, x30, [sp, #-16]!
    
    // Disable SPI
    ldr w3, [x0, #SPI_CR1]
    bic w3, w3, #SPI_CR1_SPE
    str w3, [x0, #SPI_CR1]
    
    // Configure SPI_CR1
    mov w3, #0
    orr w3, w3, #SPI_CR1_MSTR      // Master mode
    orr w3, w3, #SPI_CR1_SSM       // Software slave management
    orr w3, w3, #SPI_CR1_SSI       // Internal slave select high
    
    // Set clock polarity and phase based on mode
    tst w1, #1
    b.eq 1f
    orr w3, w3, #SPI_CR1_CPHA
1:  tst w1, #2
    b.eq 2f
    orr w3, w3, #SPI_CR1_CPOL
    
    // Set baud rate prescaler
2:  and w2, w2, #7
    lsl w2, w2, #3
    orr w3, w3, w2
    
    str w3, [x0, #SPI_CR1]
    
    // Enable SPI
    ldr w3, [x0, #SPI_CR1]
    orr w3, w3, #SPI_CR1_SPE
    str w3, [x0, #SPI_CR1]
    
    ldp x29, x30, [sp], #16
    ret

// Transfer single byte (full duplex)
// x0 = SPI base address
// w1 = byte to transmit
// Returns: w0 = received byte
spi_transfer_byte:
    stp x29, x30, [sp, #-16]!
    
    // Wait for TXE
    mov w2, #10000
1:  ldr w3, [x0, #SPI_SR]
    tst w3, #SPI_SR_TXE
    b.ne 2f
    subs w2, w2, #1
    b.ne 1b
    mov w0, #-1                    // Timeout
    b 5f
    
    // Write data
2:  strb w1, [x0, #SPI_DR]
    
    // Wait for RXNE
    mov w2, #10000
3:  ldr w3, [x0, #SPI_SR]
    tst w3, #SPI_SR_RXNE
    b.ne 4f
    subs w2, w2, #1
    b.ne 3b
    mov w0, #-1                    // Timeout
    b 5f
    
    // Read received data
4:  ldrb w0, [x0, #SPI_DR]

5:  ldp x29, x30, [sp], #16
    ret

// Transfer multiple bytes
// x0 = SPI base address
// x1 = pointer to transmit buffer (or NULL for dummy bytes)
// x2 = pointer to receive buffer (or NULL to discard)
// w3 = number of bytes
spi_transfer:
    stp x29, x30, [sp, #-32]!
    stp x19, x20, [sp, #16]
    
    mov x19, x0
    mov x20, x1
    mov x21, x2
    mov w22, w3

transfer_loop:
    cbz w22, transfer_done
    
    // Get transmit byte (or 0xFF if NULL buffer)
    cbz x20, 1f
    ldrb w1, [x20], #1
    b 2f
1:  mov w1, #0xFF

2:  // Transfer byte
    mov x0, x19
    bl spi_transfer_byte
    
    // Store received byte if buffer provided
    cbz x21, 3f
    strb w0, [x21], #1
    
3:  sub w22, w22, #1
    b transfer_loop

transfer_done:
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #32
    ret

// Wait for SPI to complete all transfers
// x0 = SPI base address
spi_wait_busy:
    mov w1, #10000
1:  ldr w2, [x0, #SPI_SR]
    tst w2, #SPI_SR_BSY
    b.eq 2f
    subs w1, w1, #1
    b.ne 1b
    mov w0, #-1                    // Timeout
    ret
2:  mov w0, #0
    ret
````

## DSP Algorithms

Digital Signal Processing algorithms transform, analyze, and manipulate digital signals. ARM processors include specialized instructions (NEON/SVE) for SIMD operations that accelerate DSP workloads.

### FIR (Finite Impulse Response) Filter

FIR filters are fundamental DSP building blocks used for frequency filtering, smoothing, and signal conditioning.

```assembly
// FIR filter implementation
// y[n] = h[0]*x[n] + h[1]*x[n-1] + h[2]*x[n-2] + ... + h[N-1]*x[n-N+1]
// 
// x0 = pointer to input sample buffer
// x1 = pointer to filter coefficients (h[])
// x2 = pointer to delay line (state buffer)
// w3 = filter order (number of taps)
// s0 = new input sample
// Returns: s0 = filtered output sample

fir_filter_scalar:
    stp x29, x30, [sp, #-16]!
    stp x19, x20, [sp, #-16]!
    
    // Shift delay line and insert new sample
    // Move samples: x[n-1] -> x[n-2], x[n-2] -> x[n-3], etc.
    sub w4, w3, #1                 // Count-1 for loop
    add x5, x2, x4, lsl #2         // Point to end of delay line
    
shift_loop:
    cbz w4, shift_done
    ldr s1, [x5, #-4]!             // Load x[i-1]
    str s1, [x5, #4]               // Store to x[i]
    sub w4, w4, #1
    b shift_loop

shift_done:
    str s0, [x2]                   // Store new sample at x[0]
    
    // Compute FIR: sum = h[i] * x[i] for all i
    fmov s2, wzr                   // Accumulator = 0
    mov w4, #0                     // Loop counter

fir_loop:
    cmp w4, w3
    b.ge fir_done
    
    ldr s3, [x1, x4, lsl #2]       // Load coefficient h[i]
    ldr s4, [x2, x4, lsl #2]       // Load delayed sample x[i]
    fmadd s2, s3, s4, s2           // acc += h[i] * x[i]
    
    add w4, w4, #1
    b fir_loop

fir_done:
    fmov s0, s2                    // Return result
    
    ldp x19, x20, [sp], #16
    ldp x29, x30, [sp], #16
    ret

// NEON-optimized FIR filter (processes 4 samples in parallel)
// x0 = pointer to input samples (must be 16-byte aligned)
// x1 = pointer to filter coefficients (must be 16-byte aligned)
// x2 = pointer to output samples (must be 16-byte aligned)
// x3 = pointer to delay line
// w4 = filter order (must be multiple of 4)
// w5 = number of samples to process
fir_filter_neon:
    stp x29, x30, [sp, #-16]!
    
    mov w6, #0                     // Sample counter

process_samples:
    cmp w6, w5
    b.ge neon_done
    
    // Load 4 input samples
    ld1 {v0.4s}, [x0], #16
    
    // Initialize accumulator to zero
    movi v4.4s, #0
    movi v5.4s, #0
    movi v6.4s, #0
    movi v7.4s, #0
    
    // Process filter taps in groups of 4
    mov w7, #0                     // Tap counter

neon_fir_loop:
    cmp w7, w4
    b.ge neon_output
    
    // Load 4 coefficients
    ld1 {v1.4s}, [x1], #16
    
    // Load 4 delay line samples
    ld1 {v2.4s}, [x3], #16
    
    // Multiply-accumulate
    fmla v4.4s, v1.4s, v2.4s
    
    add w7, w7, #4
    b neon_fir_loop

neon_output:
    // Horizontal add to get final results
    faddp v4.4s, v4.4s, v4.4s      // Pairwise add
    faddp v4.4s, v4.4s, v4.4s      // Final add
    
    // Store result
    st1 {v4.s}[0], [x2], #4
    
    add w6, w6, #1
    b process_samples

neon_done:
    ldp x29, x30, [sp], #16
    ret
```

### IIR (Infinite Impulse Response) Filter

IIR filters use feedback and require fewer coefficients than FIR filters for equivalent frequency response. **[Inference]** They are commonly implemented as cascaded second-order sections (biquads) for numerical stability.

```assembly
// Biquad IIR filter (Direct Form I)
// y[n] = b0*x[n] + b1*x[n-1] + b2*x[n-2] - a1*y[n-1] - a2*y[n-2]
//
// Biquad state structure (stored in memory):
// .float x1, x2, y1, y2  (previous samples)
//
// x0 = pointer to biquad coefficients [b0,b1,b2,a1,a2]
// x1 = pointer to biquad state [x1,x2,y1,y2]
// s0 = input sample
// Returns: s0 = output sample

biquad_filter:
    stp x29, x30, [sp, #-16]!
    
    // Load coefficients
    ld1 {v0.4s, v1.s}[0], [x0]     // v0 = [b0,b1,b2,a1], v1[0] = a2
    
    // Load state: [x1, x2, y1, y2]
    ld1 {v2.4s}, [x1]
    
    // Calculate feedforward: b0*x[n] + b1*x[n-1] + b2*x[n-2]
    fmul s3, s0, v0.s[0]           // b0 * x[n]
    fmla s3, v2.s[0], v0.s[1]      // + b1 * x[n-1]
    fmla s3, v2.s[1], v0.s[2]      // + b2 * x[n-2]
    
    // Calculate feedback: - a1*y[n-1] - a2*y[n-2]
    fmls s3, v2.s[2], v0.s[3]      // - a1 * y[n-1]
    fmls s3, v2.s[3], v1.s[0]      // - a2 * y[n-2]
    
    // Update state: shift samples
    mov v2.s[1], v2.s[0]           // x[n-2] = x[n-1]
    mov v2.s[0], v0.s[0]           // x[n-1] = x[n] (input in s0)
    mov v2.s[3], v2.s[2]           // y[n-2] = y[n-1]
    mov v2.s[2], v3.s[0]           // y[n-1] = y[n] (output in s3)
    
    // Store updated state
    st1 {v2.4s}, [x1]
    
    // Return output
    fmov s0, s3
    
    ldp x29, x30, [sp], #16
    ret

// Cascaded biquad sections for higher-order IIR
// x0 = pointer to array of biquad coefficient structures
// x1 = pointer to array of biquad state structures
// w2 = number of biquad sections
// s0 = input sample
// Returns: s0 = output sample
iir_cascade:
    stp x29, x30, [sp, #-32]!
    stp x19, x20, [sp, #16]
    
    mov x19, x0
    mov x20, x1
    mov w21, w2
    fmov s16, s0                   // Save input in s16
    
cascade_loop:
    cbz w21, cascade_done
    
    // Process one biquad section
    mov x0, x19
    mov x1, x20
    fmov s0, s16
    bl biquad_filter
    fmov s16, s0                   // Output becomes input for next stage
    
    // Move to next biquad
    add x19, x19, #20              // 5 coefficients * 4 bytes
    add x20, x20, #16              // 4 state values * 4 bytes
    sub w21, w21, #1
    b cascade_loop

cascade_done:
    fmov s0, s16                   // Final output
    
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #32
    ret
```

### FFT (Fast Fourier Transform)

FFT transforms signals between time and frequency domains, essential for spectral analysis, filtering, and many audio/video processing applications.

```assembly
// Radix-2 Decimation-In-Time FFT
// Complex numbers stored as [real, imag] pairs
//
// x0 = pointer to complex input/output array (in-place)
// w1 = N (FFT size, must be power of 2)
// w2 = direction (0 = forward, 1 = inverse)

fft_radix2:
    stp x29, x30, [sp, #-64]!
    stp x19, x20, [sp, #16]
    stp x21, x22, [sp, #32]
    stp x23, x24, [sp, #48]
    
    mov x19, x0                    // Save array pointer
    mov w20, w1                    // Save N
    mov w21, w2                    // Save direction
    
    // Bit-reverse permutation
    mov x0, x19
    mov w1, w20
    bl fft_bit_reverse
    
    // Compute stages
    mov w22, #1                    // Current block size

fft_stage_loop:
    cmp w22, w20
    b.ge fft_normalize
    
    lsl w23, w22, #1               // Next block size = 2 * current
    
    // Compute twiddle factor angle increment
    ldr d0, =pi_constant
    fmov d1, d0
    scvtf d2, w23                  // Convert block size to float
    fdiv d3, d1, d2                // angle = PI / blockSize
    
    // Negate for inverse FFT
    cbz w21, 1f
    fneg d3, d3

1:  // Process all blocks at this stage
    mov w24, #0                    // Block index

fft_block_loop:
    cmp w24, w20
    b.ge fft_next_stage
    
    // Process butterflies in this block
    mov w25, #0                    // Butterfly index

fft_butterfly_loop:
    cmp w25, w22
    b.ge fft_next_block
    
    // Calculate twiddle factor: W = exp(-j * 2 * PI * k / N)
    scvtf d4, w25
    fmul d5, d3, d4                // angle = angle_inc * k
    
    // Compute cos and sin (twiddle real and imag)
    fmov d0, d5
    bl cos_approx
    fmov d6, d0                    // wr = cos(angle)
    
    fmov d0, d5
    bl sin_approx
    fmov d7, d0                    // wi = sin(angle)
    
    // Calculate indices
    add w26, w24, w25              // i = block_start + k
    add w27, w26, w22              // j = i + blockSize
    
    // Load complex values
    add x10, x19, x26, lsl #4      // &data[i] (16 bytes per complex)
    ldp d8, d9, [x10]              // Load data[i] = [real_i, imag_i]
    
    add x11, x19, x27, lsl #4      // &data[j]
    ldp d10, d11, [x11]            // Load data[j] = [real_j, imag_j]
    
    // Complex multiply: t = data[j] * W
    // t.real = data[j].real * wr - data[j].imag * wi
    // t.imag = data[j].real * wi + data[j].imag * wr
    fmul d12, d10, d6              // real_j * wr
    fmul d13, d11, d7              // imag_j * wi
    fsub d14, d12, d13             // t.real
    
    fmul d12, d10, d7              // real_j * wi
    fmul d13, d11, d6              // imag_j * wr
    fadd d15, d12, d13             // t.imag
    
    // Butterfly operation:
    // data[i] = data[i] + t
    // data[j] = data[i] - t
    fadd d16, d8, d14              // new_real_i
    fadd d17, d9, d15              // new_imag_i
    fsub d18, d8, d14              // new_real_j
    fsub d19, d9, d15              // new_imag_j
    
    // Store results
    stp d16, d17, [x10]
    stp d18, d19, [x11]
    
    add w25, w25, #1
    b fft_butterfly_loop

fft_next_block:
    add w24, w24, w23              // Move to next block
    b fft_block_loop

fft_next_stage:
    mov w22, w23                   // blockSize *= 2
    b fft_stage_loop

fft_normalize:
    // For inverse FFT, divide by N
    cbz w21, fft_done
    
    scvtf d0, w20
    mov w22, #0
normalize_loop:
    cmp w22, w20
    b.ge fft_done
    
    add x10, x19, x22, lsl #4
    ldp d1, d2, [x10]
    fdiv d1, d1, d0
    fdiv d2, d2, d0
    stp d1, d2, [x10]
    
    add w22, w22, #1
    b normalize_loop

fft_done:
    ldp x23, x24, [sp, #48]
    ldp x21, x22, [sp, #32]
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #64
    ret

// Bit-reverse permutation for FFT
// x0 = pointer to complex array
// w1 = N (array size)
fft_bit_reverse:
    stp x29, x30, [sp, #-48]!
    stp x19, x20, [sp, #16]
    stp x21, x22, [sp, #32]
    
    mov x19, x0
    mov w20, w1
    
    // Calculate log2(N)
    clz w21, w20
    mov w22, #32
    sub w21, w22, w21
    sub w21, w21, #1               // log2(N)
    
    mov w22, #0                    // Loop counter

bitrev_loop:
    cmp w22, w20
    b.ge bitrev_done
    
    // Compute bit-reversed index
    mov w23, w22
    mov w24, #0                    // Reversed index
    mov w25, w21                   // Bit counter

bitrev_inner:
    cbz w25, bitrev_swap
    
    and w26, w23, #1
    lsl w24, w24, #1
    orr w24, w24, w26
    lsr w23, w23, #1
    sub w25, w25, #1
    b bitrev_inner

bitrev_swap:
    // Only swap if reversed index > current (avoid double swap)
    cmp w24, w22
    b.le bitrev_next
    
    // Swap data[i] and data[reversed]
    add x10, x19, x22, lsl #4
    add x11, x19, x24, lsl #4
    
    ldp d0, d1, [x10]
    ldp d2, d3, [x11]
    stp d2, d3, [x10]
    stp d0, d1, [x11]

bitrev_next:
    add w22, w22, #1
    b bitrev_loop

bitrev_done:
    ldp x21, x22, [sp, #32]
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #48
    ret
```

### Convolution

Convolution is fundamental to many DSP applications including filtering, echo/reverb effects, and image processing.

```assembly
// 1D Convolution
// y[n] = sum(x[k] * h[n-k]) for k=0 to M-1
//
// x0 = pointer to input signal x[]
// w1 = input length N
// x2 = pointer to kernel h[]
// w3 = kernel length M
// x4 = pointer to output y[]
convolution_1d:
    stp x29, x30, [sp, #-48]!
    stp x19, x20, [sp, #16]
    stp x21, x22, [sp, #32]
    
    mov x19, x0
    mov w20, w1
    mov x21, x2
    mov w22, w3
    mov x23, x4
    
    // Output length = N + M - 1
    add w24, w20, w22
    sub w24, w24, #1
    
    mov w25, #0                    // Output index

conv_outer_loop:
    cmp w25, w24
    b.ge conv_done
    
    // Initialize accumulator
    fmov s0, wzr
    
    // Determine convolution range
    mov w26, #0                    // k_start
    mov w27, w22                   // k_end = M

    // Adjust k_start if n < M-1
    cmp w25, w22
    b.ge 1f
    sub w26, w22, w25
    sub w26, w26, #1

    // Adjust k_end if n >= N
1:  cmp w25, w20
    b.lt 2f
    sub w27, w25, w20
    add w27, w27, #1

2:  mov w28, w26                   // k = k_start

conv_inner_loop:
    cmp w28, w27
    b.ge conv_store_result
    
    // Calculate indices
    sub w29, w25, w28              // n - k
    
    // Check bounds
    cmp w29, #0
    b.lt conv_next_k
    cmp w29, w20
    b.ge conv_next_k
    
    // Load x[n-k] and h[k]
    add x10, x19, x29, lsl #2
    ldr s1, [x10]
    
    add x11, x21, x28, lsl #2
    ldr s2, [x11]
    
    // Accumulate: sum += x[n-k] * h[k]
    fmadd s0, s1, s2, s0

conv_next_k:
    add w28, w28, #1
    b conv_inner_loop

conv_store_result:
    // Store output y[n]
    add x10, x23, x25, lsl #2
    str s0, [x10]
    
    add w25, w25, #1
    b conv_outer_loop

conv_done:
    ldp x21, x22, [sp, #32]
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #48
    ret

// Fast convolution using FFT (overlap-add method)
// More efficient for large kernel sizes
// x0 = pointer to input signal
// w1 = input length
// x2 = pointer to kernel (impulse response)
// w3 = kernel length
// x4 = pointer to output buffer
fast_convolution_fft:
    stp x29, x30, [sp, #-64]!
    stp x19, x20, [sp, #16]
    stp x21, x22, [sp, #32]
    stp x23, x24, [sp, #48]
    
    mov x19, x0
    mov w20, w1
    mov x21, x2
    mov w22, w3
    mov x23, x4
    
    // Find next power of 2 >= (input_len + kernel_len - 1)
    add w24, w20, w22
    sub w24, w24, #1
    clz w25, w24
    mov w26, #32
    sub w25, w26, w25              // log2(size)
    mov w24, #1
    lsl w24, w24, w25              // FFT size
    
    // Allocate temporary buffers (would use stack or heap)
    // For demonstration, assume pre-allocated buffers
    ldr x25, =fft_buffer1          // Input FFT buffer
    ldr x26, =fft_buffer2          // Kernel FFT buffer
    ldr x27, =fft_result           // Result buffer
    
    // Zero-pad and copy input to FFT buffer
    mov w28, #0
1:  cmp w28, w20
    b.ge 2f
    add x10, x19, x28, lsl #2
    ldr s0, [x10]
    add x11, x25, x28, lsl #4      // Complex: 16 bytes
    str s0, [x11]                  // Real part
    str wzr, [x11, #8]             // Imag part = 0
    add w28, w28, #1
    b 1b
    
    // Zero remaining elements
2:  cmp w28, w24
    b.ge 3f
    add x11, x25, x28, lsl #4
    stp xzr, xzr, [x11]
    add w28, w28, #1
    b 2b
    
    // Zero-pad and copy kernel to FFT buffer
3:  mov w28, #0
4:  cmp w28, w22
    b.ge 5f
    add x10, x21, x28, lsl #2
    ldr s0, [x10]
    add x11, x26, x28, lsl #4
    str s0, [x11]
    str wzr, [x11, #8]
    add w28, w28, #1
    b 4b
    
5:  cmp w28, w24
    b.ge 6f
    add x11, x26, x28, lsl #4
    stp xzr, xzr, [x11]
    add w28, w28, #1
    b 5b
    
    // Perform FFT on both signals
6:  mov x0, x25
    mov w1, w24
    mov w2, #0                     // Forward FFT
    bl fft_radix2
    
    mov x0, x26
    mov w1, w24
    mov w2, #0
    bl fft_radix2
    
    // Complex multiply in frequency domain
    mov w28, #0
multiply_loop:
    cmp w28, w24
    b.ge inverse_fft
    
    add x10, x25, x28, lsl #4
    ldp d0, d1, [x10]              // Load X[k] = (real, imag)
    
    add x11, x26, x28, lsl #4
    ldp d2, d3, [x11]              // Load H[k] = (real, imag)
    
    // Complex multiply: (a+bi)(c+di) = (ac-bd) + (ad+bc)i
    fmul d4, d0, d2                // a*c
    fmul d5, d1, d3                // b*d
    fsub d6, d4, d5                // real = ac - bd
    
    fmul d4, d0, d3                // a*d
    fmul d5, d1, d2                // b*c
    fadd d7, d4, d5                // imag = ad + bc
    
    add x12, x27, x28, lsl #4
    stp d6, d7, [x12]              // Store Y[k]
    
    add w28, w28, #1
    b multiply_loop
    
    // Inverse FFT to get convolution result
inverse_fft:
    mov x0, x27
    mov w1, w24
    mov w2, #1                     // Inverse FFT
    bl fft_radix2
    
    // Copy real parts to output (discard imaginary parts)
    mov w28, #0
copy_output:
    add w29, w20, w22
    sub w29, w29, #1
    cmp w28, w29
    b.ge fft_conv_done
    
    add x10, x27, x28, lsl #4
    ldr s0, [x10]                  // Real part
    add x11, x23, x28, lsl #2
    str s0, [x11]
    
    add w28, w28, #1
    b copy_output

fft_conv_done:
    ldp x23, x24, [sp, #48]
    ldp x21, x22, [sp, #32]
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #64
    ret
```

### Correlation (Auto and Cross)

Correlation measures similarity between signals and is used in pattern detection, synchronization, and signal analysis.

```assembly
// Cross-correlation
// R[n] = sum(x[k] * y[k+n]) for all valid k
//
// x0 = pointer to signal x[]
// w1 = length of x
// x2 = pointer to signal y[]
// w3 = length of y
// x4 = pointer to output R[]
cross_correlation:
    stp x29, x30, [sp, #-48]!
    stp x19, x20, [sp, #16]
    stp x21, x22, [sp, #32]
    
    mov x19, x0
    mov w20, w1
    mov x21, x2
    mov w22, w3
    mov x23, x4
    
    // Output length = len(x) + len(y) - 1
    add w24, w20, w22
    sub w24, w24, #1
    
    // Lag range: -(len(y)-1) to (len(x)-1)
    sub w25, w22, #1
    neg w25, w25                   // Start lag = -(len(y)-1)

corr_lag_loop:
    sub w26, w20, #1
    cmp w25, w26
    b.gt corr_done
    
    // Initialize accumulator
    fmov s0, wzr
    
    // Determine summation range
    mov w27, #0                    // k_start
    mov w28, w20                   // k_end
    
    // Adjust for negative lag
    cmp w25, #0
    b.ge 1f
    neg w27, w25                   // k_start = -lag
    
    // Adjust for lag >= len(y)
1:  add w29, w25, w22
    cmp w29, w20
    b.ge 2f
    mov w28, w29                   // k_end = lag + len(y)
    
2:  mov w29, w27                   // k = k_start

corr_sum_loop:
    cmp w29, w28
    b.ge corr_store
    
    // Load x[k]
    add x10, x19, x29, lsl #2
    ldr s1, [x10]
    
    // Calculate y index: k + lag
    add w30, w29, w25
    
    // Bounds check
    cmp w30, #0
    b.lt corr_next_k
    cmp w30, w22
    b.ge corr_next_k
    
    // Load y[k+lag]
    add x11, x21, x30, lsl #2
    ldr s2, [x11]
    
    // Accumulate
    fmadd s0, s1, s2, s0

corr_next_k:
    add w29, w29, #1
    b corr_sum_loop

corr_store:
    // Store R[lag + (len(y)-1)]
    sub w30, w22, #1
    add w30, w25, w30
    add x10, x23, w30, lsl #2
    str s0, [x10]
    
    add w25, w25, #1
    b corr_lag_loop

corr_done:
    ldp x21, x22, [sp, #32]
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #48
    ret

// Auto-correlation (special case where x = y)
// R[lag] = sum(x[k] * x[k+lag])
// x0 = pointer to signal
// w1 = signal length
// x2 = pointer to output
// w3 = max lag
auto_correlation:
    stp x29, x30, [sp, #-32]!
    stp x19, x20, [sp, #16]
    
    mov x19, x0
    mov w20, w1
    mov x21, x2
    mov w22, w3
    
    mov w23, #0                    // Current lag

auto_corr_loop:
    cmp w23, w22
    b.gt auto_corr_done
    
    // Initialize accumulator
    fmov s0, wzr
    
    // Sum from k=0 to (N-lag-1)
    sub w24, w20, w23
    mov w25, #0

auto_corr_sum:
    cmp w25, w24
    b.ge auto_corr_store
    
    // Load x[k]
    add x10, x19, x25, lsl #2
    ldr s1, [x10]
    
    // Load x[k+lag]
    add w26, w25, w23
    add x11, x19, x26, lsl #2
    ldr s2, [x11]
    
    // Accumulate
    fmadd s0, s1, s2, s0
    
    add w25, w25, #1
    b auto_corr_sum

auto_corr_store:
    add x10, x21, x23, lsl #2
    str s0, [x10]
    
    add w23, w23, #1
    b auto_corr_loop

auto_corr_done:
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #32
    ret
```

### Decimation and Interpolation

These operations change the sampling rate of digital signals, essential for multi-rate DSP systems.

```assembly
// Decimation (downsampling by factor M)
// Takes every M-th sample after low-pass filtering
// x0 = pointer to input signal
// w1 = input length
// x2 = pointer to output signal
// w3 = decimation factor M
// x4 = pointer to anti-aliasing filter coefficients
// w5 = filter length
decimate:
    stp x29, x30, [sp, #-64]!
    stp x19, x20, [sp, #16]
    stp x21, x22, [sp, #32]
    stp x23, x24, [sp, #48]
    
    mov x19, x0
    mov w20, w1
    mov x21, x2
    mov w22, w3
    mov x23, x4
    mov w24, w5
    
    // Allocate filter state buffer
    ldr x25, =decimate_state
    
    // Initialize state to zero
    mov w26, #0
1:  cmp w26, w24
    b.ge 2f
    add x10, x25, x26, lsl #2
    str wzr, [x10]
    add w26, w26, #1
    b 1b
    
    // Process input samples
2:  mov w26, #0                    // Input index
    mov w27, #0                    // Output index

decimate_loop:
    cmp w26, w20
    b.ge decimate_done
    
    // Load input sample
    add x10, x19, w26, lsl #2
    ldr s0, [x10]
    
    // Apply anti-aliasing filter
    mov x0, x23                    // Filter coefficients
    mov x1, x25                    // Filter state
    mov w2, w24                    // Filter length
    bl fir_filter_scalar           // Result in s0
    
    // Check if this sample should be kept
    udiv w28, w26, w22
    msub w29, w28, w22, w26        // w29 = w26 % w22
    cbnz w29, decimate_skip
    
    // Store decimated sample
    add x10, x21, w27, lsl #2
    str s0, [x10]
    add w27, w27, #1

decimate_skip:
    add w26, w26, #1
    b decimate_loop

decimate_done:
    ldp x23, x24, [sp, #48]
    ldp x21, x22, [sp, #32]
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #64
    ret

// Interpolation (upsampling by factor L)
// Inserts L-1 zeros between samples and applies reconstruction filter
// x0 = pointer to input signal
// w1 = input length
// x2 = pointer to output signal
// w3 = interpolation factor L
// x4 = pointer to reconstruction filter coefficients
// w5 = filter length
interpolate:
    stp x29, x30, [sp, #-64]!
    stp x19, x20, [sp, #16]
    stp x21, x22, [sp, #32]
    stp x23, x24, [sp, #48]
    
    mov x19, x0
    mov w20, w1
    mov x21, x2
    mov w22, w3
    mov x23, x4
    mov w24, w5
    
    // Calculate output length
    mul w25, w20, w22              // output_len = input_len * L
    
    // First, upsample by inserting zeros
    mov w26, #0                    // Output index

upsample_loop:
    cmp w26, w25
    b.ge filter_interpolated
    
    // Check if this is an original sample position
    udiv w27, w26, w22
    msub w28, w27, w22, w26        // w28 = w26 % w22
    cbnz w28, insert_zero
    
    // Original sample position
    add x10, x19, w27, lsl #2
    ldr s0, [x10]
    add x11, x21, w26, lsl #2
    str s0, [x11]
    b upsample_next

insert_zero:
    // Insert zero
    add x11, x21, w26, lsl #2
    str wzr, [x11]

upsample_next:
    add w26, w26, #1
    b upsample_loop

filter_interpolated:
    // Apply reconstruction filter to upsampled signal
    ldr x26, =interpolate_filtered // Temporary buffer
    
    // Allocate filter state
    ldr x27, =interpolate_state
    mov w28, #0
1:  cmp w28, w24
    b.ge 2f
    add x10, x27, w28, lsl #2
    str wzr, [x10]
    add w28, w28, #1
    b 1b
    
    // Filter each sample
2:  mov w28, #0

filter_loop:
    cmp w28, w25
    b.ge copy_filtered
    
    // Load upsampled sample
    add x10, x21, w28, lsl #2
    ldr s0, [x10]
    
    // Apply reconstruction filter
    mov x0, x23
    mov x1, x27
    mov w2, w24
    bl fir_filter_scalar
    
    // Scale by interpolation factor
    scvtf s1, w22
    fmul s0, s0, s1
    
    // Store filtered result
    add x10, x26, w28, lsl #2
    str s0, [x10]
    
    add w28, w28, #1
    b filter_loop

copy_filtered:
    // Copy filtered signal to output
    mov w28, #0
3:  cmp w28, w25
    b.ge interpolate_done
    
    add x10, x26, w28, lsl #2
    ldr s0, [x10]
    add x11, x21, w28, lsl #2
    str s0, [x11]
    
    add w28, w28, #1
    b 3b

interpolate_done:
    ldp x23, x24, [sp, #48]
    ldp x21, x22, [sp, #32]
    ldp x19, x20, [sp, #16]
    ldp x29, x30, [sp], #64
    ret
```

### Window Functions

Window functions reduce spectral leakage in FFT analysis by tapering signal edges.

```assembly
// Generate Hamming window
// w[n] = 0.54 - 0.46 * cos(2*PI*n/(N-1))
// x0 = pointer to output window array
// w1 = window length N
generate_hamming_window:
    stp x29, x30, [sp, #-16]!
    
    // Load constants
    fmov d0, #0.54                 // Alpha
    fmov d1, #0.46                 // Beta
    ldr d2, =two_pi                // 2*PI
    
    sub w2, w1, #1
    scvtf d3, w2                   // N-1 as float
    
    mov w3, #0                     // Sample index

hamming_loop:
    cmp w3, w1
    b.ge hamming_done
    
    // Calculate angle: 2*PI*n/(N-1)
    scvtf d4, w3
    fmul d5, d2, d4
    fdiv d5, d5, d3
    
    // Calculate cos(angle)
    fmov d6, d5
    bl cos_approx
    
    // w[n] = 0.54 - 0.46 * cos(...)
    fmul d7, d1, d0                // 0.46 * cos
    fsub d8, d0, d7                // 0.54 - result
    
    // Store window value
    add x10, x0, x3, lsl #3
    str d8, [x10]
    
    add w3, w3, #1
    b hamming_loop

hamming_done:
    ldp x29, x30, [sp], #16
    ret

// Generate Hann window  
// w[n] = 0.5 * (1 - cos(2*PI*n/(N-1)))
// x0 = pointer to output window array
// w1 = window length N
generate_hann_window:
    stp x29, x30, [sp, #-16]!
    
    fmov d0, #0.5
    ldr d1, =two_pi
    
    sub w2, w1, #1
    scvtf d2, w2
    
    mov w3, #0

hann_loop:
    cmp w3, w1
    b.ge hann_done
    
    scvtf d3, w3
    fmul d4, d1, d3
    fdiv d4, d4, d2
    
    fmov d5, d4
    bl cos_approx
    
    // w[n] = 0.5 * (1 - cos(...))
    fmov d6, #1.0
    fsub d6, d6, d0
    fmul d6, d0, d6
    
    add x10, x0, x3, lsl #3
    str d6, [x10]
    
    add w3, w3, #1
    b hann_loop

hann_done:
    ldp x29, x30, [sp], #16
    ret

// Apply window to signal
// x0 = pointer to signal array
// x1 = pointer to window array
// w2 = length
// x3 = pointer to output (can be same as input for in-place)
apply_window:
    mov w4, #0

apply_window_loop:
    cmp w4, w2
    b.ge apply_window_done
    
    // Load signal sample
    add x10, x0, x4, lsl #2
    ldr s0, [x10]
    
    // Load window value
    add x11, x1, x4, lsl #3
    ldr d1, [x11]
    fcvt s1, d1                    // Convert to single precision
    
    // Multiply
    fmul s2, s0, s1
    
    // Store result
    add x12, x3, x4, lsl #2
    str s2, [x12]
    
    add w4, w4, #1
    b apply_window_loop

apply_window_done:
    ret
```

### Helper Functions (Trigonometric Approximations)

```assembly
// Fast cosine approximation using Taylor series
// Input: d0 = angle in radians
// Output: d0 = cos(angle)
cos_approx:
    stp x29, x30, [sp, #-16]!
    
    // Normalize angle to [-PI, PI]
    ldr d1, =pi_constant
    ldr d2, =two_pi
    
1:  fcmp d0, d1
    b.le 2f
    fsub d0, d0, d2
    b 1b

2:  fneg d3, d1
    fcmp d0, d3
    b.ge 3f
    fadd d0, d0, d2
    b 2b
    
    // Taylor series: cos(x) ≈ 1 - x²/2! + x⁴/4! - x⁶/6!
3:  fmul d1, d0, d0                // x²
    
    fmov d2, #1.0                  // Result = 1
    
    fmov d3, #0.5
    fmul d4, d1, d3
    fsub d2, d2, d4                // - x²/2
    
    fmul d4, d1, d1                // x⁴
    fmov d5, #0.041666666          // 1/24
    fmul d4, d4, d5
    fadd d2, d2, d4                // + x⁴/24
    
    fmul d4, d1, d1
    fmul d4, d4, d1                // x⁶
    fmov d5, #0.001388888          // 1/720
    fmul d4, d4, d5
    fsub d2, d2, d4                // - x⁶/720
    
    fmov d0, d2
    
    ldp x29, x30, [sp], #16
    ret

// Fast sine approximation
// Input: d0 = angle in radians
// Output: d0 = sin(angle)
sin_approx:
    stp x29, x30, [sp, #-16]!
    
    // sin(x) = cos(PI/2 - x)
    ldr d1, =pi_over_2
    fsub d0, d1, d0
    bl cos_approx
    
    ldp x29, x30, [sp], #16
    ret

// Constants
.data
.align 8
pi_constant:
    .double 3.14159265358979323846
two_pi:
    .double 6.28318530717958647693
pi_over_2:
    .double 1.57079632679489661923

// Buffer allocations (would typically be in BSS or dynamically allocated)
.bss
.align 16
fft_buffer1:
    .space 4096 * 16               // Complex numbers: 4096 samples * 16 bytes
fft_buffer2:
    .space 4096 * 16
fft_result:
    .space 4096 * 16
decimate_state:
    .space 256 * 4                 // Filter state buffer
interpolate_state:
    .space 256 * 4
interpolate_filtered:
    .space 4096 * 4
uart_rx_dma_buffer:
    .space 256
```

**Key Points:**

- Device drivers use memory-mapped I/O to control hardware peripherals through specific register addresses with bit-level control over configuration and operation
- Interrupt handlers require minimal latency through efficient context saving, hardware acknowledgment, and deferred processing using circular buffers for data management
- FIR filters provide linear phase response and stability through direct convolution while IIR filters achieve similar frequency response with fewer coefficients using recursive feedback structures
- FFT algorithms transform between time and frequency domains in O(N log N) operations enabling efficient spectral analysis and fast convolution for large datasets
- NEON SIMD instructions accelerate DSP operations by processing multiple data elements simultaneously, particularly effective for filters and transforms with regular data access patterns

**Important related topics:** ARM NEON intrinsics and optimization techniques, Memory barriers and cache coherency in multi-core systems, Real-time operating system integration for interrupt priority management, Fixed-point arithmetic for processors without floating-point units, Power management and clock gating in embedded drivers

---

# Real-World Projects in ARM Assembly

Real-world ARM assembly projects demonstrate optimization techniques, hardware utilization, and performance-critical implementations. Graphics and cryptography represent two domains where assembly language provides measurable advantages over high-level languages.

## Graphics Primitives

Graphics primitives are fundamental drawing operations that form the building blocks of rendering systems. Efficient implementations directly impact frame rates and user experience on embedded displays.

**Framebuffer Basics:**

Embedded displays typically use memory-mapped framebuffers where each pixel's color is stored in a contiguous memory region. Common formats:

- **RGB565**: 16 bits per pixel (5-bit red, 6-bit green, 5-bit blue)
- **RGB888**: 24 bits per pixel (8 bits per channel)
- **ARGB8888**: 32 bits per pixel (8-bit alpha, 8 bits per channel)

**Example** framebuffer structure for 320x240 RGB565 display:

```
Base address: 0xC0000000 (external SRAM)
Size: 320 * 240 * 2 = 153,600 bytes
Pixel (x,y) address: base + (y * 320 + x) * 2
```

### Line Drawing (Bresenham's Algorithm)

Bresenham's line algorithm uses only integer arithmetic, making it ideal for hardware without floating-point units.

**Algorithm Overview:**

Draws a line from (x₀, y₀) to (x₁, y₁) by calculating which pixels best approximate the line. Uses accumulated error to determine when to step in the secondary axis.

**Example** - Optimized Bresenham line drawing (Cortex-M4):

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ Draw line from (r0,r1) to (r2,r3) with color in r4
@ Assumes 320x240 RGB565 framebuffer at FRAMEBUFFER_BASE
@ Registers: r0=x0, r1=y0, r2=x1, r3=y1, r4=color

.equ FRAMEBUFFER_BASE, 0xC0000000
.equ SCREEN_WIDTH, 320

.global draw_line
.type draw_line, %function

draw_line:
    PUSH {r4-r11, lr}
    
    @ Calculate dx = abs(x1 - x0)
    SUBS r5, r2, r0          @ dx = x1 - x0
    IT MI
    RSBMI r5, r5, #0         @ if negative, negate
    
    @ Determine x step direction
    MOV r6, #2               @ sx = 2 (bytes per pixel, positive direction)
    CMP r2, r0
    IT LT
    RSBLT r6, r6, #0         @ if x1 < x0, sx = -2
    
    @ Calculate dy = abs(y1 - y0)
    SUBS r7, r3, r1          @ dy = y1 - y0
    IT MI
    RSBMI r7, r7, #0         @ if negative, negate
    
    @ Determine y step direction
    MOV r8, #(SCREEN_WIDTH * 2)  @ sy = stride (positive direction)
    CMP r3, r1
    IT LT
    RSBLT r8, r8, #0         @ if y1 < y0, sy = -stride
    
    @ Calculate initial error = dx - dy
    SUBS r9, r5, r7          @ err = dx - dy
    
    @ Calculate framebuffer address for (x0, y0)
    @ addr = base + (y0 * width + x0) * 2
    LDR r10, =FRAMEBUFFER_BASE
    MOV r11, #SCREEN_WIDTH
    MLA r11, r1, r11, r0     @ r11 = y0 * width + x0
    ADD r10, r10, r11, LSL #1  @ r10 = base + offset * 2
    
line_loop:
    @ Plot pixel at current position
    STRH r4, [r10]           @ Write color (halfword)
    
    @ Check if we've reached end point
    CMP r0, r2               @ x == x1?
    BNE continue_line
    CMP r1, r3               @ y == y1?
    BEQ line_done
    
continue_line:
    @ err2 = err * 2
    LSL r11, r9, #1          @ err2 = err * 2
    
    @ if (err2 > -dy)
    RSB r12, r7, #0          @ r12 = -dy
    CMP r11, r12
    BLE skip_x_step
    
    @ err -= dy
    SUB r9, r9, r7
    
    @ x0 += sx
    CMP r6, #0
    IT GT
    ADDGT r0, r0, #1
    IT LT
    SUBLT r0, r0, #1
    
    @ Update framebuffer pointer
    ADD r10, r10, r6
    
skip_x_step:
    @ if (err2 < dx)
    CMP r11, r5
    BGE skip_y_step
    
    @ err += dx
    ADD r9, r9, r5
    
    @ y0 += sy
    CMP r8, #0
    IT GT
    ADDGT r1, r1, #1
    IT LT
    SUBLT r1, r1, #1
    
    @ Update framebuffer pointer
    ADD r10, r10, r8
    
skip_y_step:
    B line_loop
    
line_done:
    POP {r4-r11, pc}
    
.size draw_line, .-draw_line
```

**Optimization Techniques:**

**Pre-multiplied Stride** Instead of recalculating `y * width + x` each iteration, the code maintains a running framebuffer pointer and adds stride values directly.

**Conditional Execution (IT blocks)** ARM Thumb-2 IT (If-Then) instructions allow conditional execution without branches, reducing pipeline stalls:

```assembly
CMP r0, #0
IT MI
RSBMI r0, r0, #0    @ Executes only if negative
```

**Direct Memory Access** Using `STRH` (store halfword) writes 16-bit pixels directly without byte manipulation.

### Rectangle Filling

Filling rectangles is common for backgrounds, UI elements, and clearing regions.

**Example** - Fast rectangle fill using ARM SIMD:

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ Fill rectangle at (r0,r1) with width r2, height r3, color r4
@ Uses word writes for 4x speedup on aligned regions

.global fill_rect
.type fill_rect, %function

fill_rect:
    PUSH {r4-r11, lr}
    
    @ Replicate 16-bit color to 32 bits for word writes
    ORR r4, r4, r4, LSL #16  @ r4 = color | (color << 16)
    
    @ Calculate starting address
    LDR r5, =FRAMEBUFFER_BASE
    MOV r6, #SCREEN_WIDTH
    MLA r6, r1, r6, r0       @ offset = y * width + x
    ADD r5, r5, r6, LSL #1   @ addr = base + offset * 2
    
    @ Calculate stride (bytes to next row)
    MOV r6, #SCREEN_WIDTH
    SUB r6, r6, r2           @ stride_pixels = width - rect_width
    LSL r6, r6, #1           @ stride_bytes = stride_pixels * 2
    
    @ Check if width is odd or even
    TST r2, #1
    BNE rect_odd_width
    
    @ Even width: can use word writes throughout
rect_even_row:
    MOV r7, r2, LSR #1       @ words_per_row = width / 2
    
rect_even_col:
    STR r4, [r5], #4         @ Write 2 pixels as 1 word
    SUBS r7, r7, #1
    BNE rect_even_col
    
    ADD r5, r5, r6           @ Move to next row
    SUBS r3, r3, #1          @ height--
    BNE rect_even_row
    B rect_done
    
rect_odd_width:
    @ Odd width: write words for n-1 pixels, then 1 halfword
    MOV r7, r2, LSR #1       @ words_per_row = (width - 1) / 2
    
rect_odd_row:
    MOV r8, r7
    
rect_odd_col:
    STR r4, [r5], #4         @ Write 2 pixels as 1 word
    SUBS r8, r8, #1
    BNE rect_odd_col
    
    STRH r4, [r5], #2        @ Write final pixel as halfword
    
    ADD r5, r5, r6           @ Move to next row
    SUBS r3, r3, #1
    BNE rect_odd_row
    
rect_done:
    POP {r4-r11, pc}
    
.size fill_rect, .-fill_rect
```

**Performance Analysis:**

[Inference] This implementation provides approximately 2x performance improvement over byte-wise operations by using 32-bit word writes for two 16-bit pixels simultaneously. The alignment handling ensures correct operation regardless of starting position.

### Circle Drawing (Midpoint Circle Algorithm)

Draws circles using symmetry - calculating one octant and mirroring to draw all eight octants.

**Example** - Midpoint circle with 8-way symmetry:

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ Draw circle centered at (r0,r1) with radius r2, color r3

.global draw_circle
.type draw_circle, %function

draw_circle:
    PUSH {r4-r11, lr}
    
    MOV r4, r0               @ xc = center x
    MOV r5, r1               @ yc = center y
    MOV r6, r2               @ radius
    MOV r7, r3               @ color
    
    MOV r8, #0               @ x = 0
    MOV r9, r6               @ y = radius
    
    @ d = 1 - radius
    RSB r10, r6, #1          @ d = 1 - r
    
circle_loop:
    @ Plot 8 symmetric points
    @ (xc+x, yc+y), (xc-x, yc+y), (xc+x, yc-y), (xc-x, yc-y)
    @ (xc+y, yc+x), (xc-y, yc+x), (xc+y, yc-x), (xc-y, yc-x)
    
    @ Point 1: (xc+x, yc+y)
    ADD r0, r4, r8
    ADD r1, r5, r9
    MOV r2, r7
    BL plot_pixel
    
    @ Point 2: (xc-x, yc+y)
    SUB r0, r4, r8
    ADD r1, r5, r9
    MOV r2, r7
    BL plot_pixel
    
    @ Point 3: (xc+x, yc-y)
    ADD r0, r4, r8
    SUB r1, r5, r9
    MOV r2, r7
    BL plot_pixel
    
    @ Point 4: (xc-x, yc-y)
    SUB r0, r4, r8
    SUB r1, r5, r9
    MOV r2, r7
    BL plot_pixel
    
    @ Point 5: (xc+y, yc+x)
    ADD r0, r4, r9
    ADD r1, r5, r8
    MOV r2, r7
    BL plot_pixel
    
    @ Point 6: (xc-y, yc+x)
    SUB r0, r4, r9
    ADD r1, r5, r8
    MOV r2, r7
    BL plot_pixel
    
    @ Point 7: (xc+y, yc-x)
    ADD r0, r4, r9
    SUB r1, r5, r8
    MOV r2, r7
    BL plot_pixel
    
    @ Point 8: (xc-y, yc-x)
    SUB r0, r4, r9
    SUB r1, r5, r8
    MOV r2, r7
    BL plot_pixel
    
    @ Check termination
    CMP r8, r9
    BGE circle_done
    
    @ x++
    ADD r8, r8, #1
    
    @ if (d < 0)
    CMP r10, #0
    BGE circle_update_y
    
    @ d += 2*x + 1
    LSL r11, r8, #1
    ADD r10, r10, r11
    ADD r10, r10, #1
    B circle_loop
    
circle_update_y:
    @ y--
    SUB r9, r9, #1
    
    @ d += 2*(x - y) + 1
    SUB r11, r8, r9
    LSL r11, r11, #1
    ADD r10, r10, r11
    ADD r10, r10, #1
    B circle_loop
    
circle_done:
    POP {r4-r11, pc}

@ Helper: Plot single pixel
plot_pixel:
    PUSH {r0-r2, lr}
    
    @ Bounds check
    CMP r0, #0
    BLT pixel_done
    CMP r0, #SCREEN_WIDTH
    BGE pixel_done
    CMP r1, #0
    BLT pixel_done
    CMP r1, #240
    BGE pixel_done
    
    @ Calculate address
    LDR r3, =FRAMEBUFFER_BASE
    MOV r4, #SCREEN_WIDTH
    MLA r4, r1, r4, r0
    ADD r3, r3, r4, LSL #1
    
    @ Write pixel
    STRH r2, [r3]
    
pixel_done:
    POP {r0-r2, pc}
    
.size draw_circle, .-draw_circle
```

### Bitmap Blitting with Alpha Blending

Blitting (Block Image Transfer) copies rectangular image data to the framebuffer. Alpha blending combines source and destination pixels based on transparency.

**Alpha Blending Formula:**

```
output = (src * alpha) + (dst * (255 - alpha)) / 255
```

**Example** - RGB565 bitmap blit with alpha (Cortex-M4 with DSP extensions):

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ Blit bitmap at (r0,r1), bitmap pointer r2, width r3, height on stack
@ Alpha value on stack (0-255)

.global blit_alpha
.type blit_alpha, %function

blit_alpha:
    PUSH {r4-r11, lr}
    
    @ Load parameters from stack
    LDR r4, [sp, #36]        @ height
    LDR r5, [sp, #40]        @ alpha
    
    @ Calculate destination base address
    LDR r6, =FRAMEBUFFER_BASE
    MOV r7, #SCREEN_WIDTH
    MLA r7, r1, r7, r0
    ADD r6, r6, r7, LSL #1   @ r6 = dest ptr
    
    @ Calculate source and dest stride
    MOV r7, #SCREEN_WIDTH
    SUB r7, r7, r3
    LSL r7, r7, #1           @ r7 = dest stride
    
    @ Prepare alpha values
    RSB r8, r5, #255         @ r8 = inv_alpha = 255 - alpha
    
blit_row:
    MOV r9, r3               @ column counter
    
blit_col:
    @ Load source pixel (RGB565)
    LDRH r10, [r2], #2
    
    @ Load destination pixel
    LDRH r11, [r6]
    
    @ Extract and blend red channel (5 bits)
    UBFX r12, r10, #11, #5   @ src_r
    MUL r12, r12, r5         @ src_r * alpha
    
    UBFX r14, r11, #11, #5   @ dst_r
    MLA r12, r14, r8, r12    @ (src_r*alpha) + (dst_r*inv_alpha)
    
    LSR r12, r12, #8         @ Divide by 256 (approximate /255)
    
    @ Extract and blend green channel (6 bits)
    UBFX r0, r10, #5, #6     @ src_g
    MUL r0, r0, r5
    
    UBFX r14, r11, #5, #6    @ dst_g
    MLA r0, r14, r8, r0
    LSR r0, r0, #8
    
    @ Extract and blend blue channel (5 bits)
    UBFX r1, r10, #0, #5     @ src_b
    MUL r1, r1, r5
    
    UBFX r14, r11, #0, #5    @ dst_b
    MLA r1, r14, r8, r1
    LSR r1, r1, #8
    
    @ Pack blended RGB565 value
    BFI r1, r0, #5, #6       @ Insert green
    BFI r1, r12, #11, #5     @ Insert red
    
    @ Store blended pixel
    STRH r1, [r6], #2
    
    SUBS r9, r9, #1
    BNE blit_col
    
    @ Next row
    ADD r6, r6, r7           @ Add dest stride
    SUBS r4, r4, #1
    BNE blit_row
    
    POP {r4-r11, pc}
    
.size blit_alpha, .-blit_alpha
```

**Optimization Techniques:**

**Bit Field Instructions (UBFX, BFI)** Cortex-M4 provides bit field extract and insert instructions that efficiently pack/unpack RGB565 values without multiple shift/mask operations.

**Multiply-Accumulate (MLA)** The `MLA` instruction performs `d = a*b + c` in one cycle, perfect for blending calculations.

**Approximate Division** Dividing by 255 is expensive. Right-shifting by 8 (dividing by 256) provides close approximation with single-cycle execution. [Inference] The maximum error is less than 1 per channel, typically imperceptible in graphics.

### DMA-Accelerated Graphics

Many ARM SoCs include DMA controllers that can transfer data independently of the CPU, enabling parallel rendering.

**Example** - DMA2D configuration for rectangle fill (STM32 with Chrom-ART):

```assembly
.syntax unified
.cpu cortex-m7
.thumb

@ DMA2D hardware accelerated rectangle fill
@ r0=x, r1=y, r2=width, r3=height, r4=color

.equ DMA2D_BASE, 0x4002B000
.equ DMA2D_CR,   0x00
.equ DMA2D_OCOLR, 0x34
.equ DMA2D_OMAR, 0x3C
.equ DMA2D_OOR,  0x40
.equ DMA2D_NLR,  0x44

.global dma2d_fill_rect
.type dma2d_fill_rect, %function

dma2d_fill_rect:
    PUSH {r4-r7, lr}
    
    @ Calculate destination address
    LDR r5, =FRAMEBUFFER_BASE
    MOV r6, #SCREEN_WIDTH
    MLA r6, r1, r6, r0
    ADD r5, r5, r6, LSL #1
    
    @ Wait for DMA2D ready
    LDR r6, =DMA2D_BASE
1:  LDR r7, [r6, #DMA2D_CR]
    TST r7, #1               @ Check START bit
    BNE 1b
    
    @ Configure DMA2D for register-to-memory mode
    MOV r7, #(3 << 16)       @ Mode = R2M (register to memory)
    STR r7, [r6, #DMA2D_CR]
    
    @ Set output color
    STR r4, [r6, #DMA2D_OCOLR]
    
    @ Set output memory address
    STR r5, [r6, #DMA2D_OMAR]
    
    @ Set output line offset
    MOV r7, #SCREEN_WIDTH
    SUB r7, r7, r2
    STR r7, [r6, #DMA2D_OOR]
    
    @ Set number of lines and pixels per line
    ORR r7, r3, r2, LSL #16
    STR r7, [r6, #DMA2D_NLR]
    
    @ Start transfer
    LDR r7, [r6, #DMA2D_CR]
    ORR r7, r7, #1           @ Set START bit
    STR r7, [r6, #DMA2D_CR]
    
    @ Optionally wait for completion or return immediately
    @ for async operation
    
    POP {r4-r7, pc}
    
.size dma2d_fill_rect, .-dma2d_fill_rect
```

[Inference] DMA2D hardware acceleration can provide 10-50x performance improvement for large fills and blits compared to CPU-based operations, while freeing the CPU for other tasks.

## Cryptographic Implementations

Cryptographic algorithms require careful implementation to ensure both security and performance. Assembly language provides control over timing and side-channel resistance.

### AES Encryption (ARM Cryptographic Extensions)

ARMv8 Cryptographic Extensions provide dedicated instructions for AES, significantly accelerating encryption/decryption.

**AES Instructions:**

- `AESE`: AES single round encryption
- `AESD`: AES single round decryption
- `AESMC`: AES mix columns
- `AESIMC`: AES inverse mix columns

**Example** - AES-128 encryption round (ARMv8-A, 64-bit):

```assembly
.arch armv8-a+crypto
.text

// AES-128 encrypt single block
// x0 = plaintext pointer (16 bytes)
// x1 = round keys pointer (11 round keys, 176 bytes)
// x2 = ciphertext pointer

.global aes128_encrypt
.type aes128_encrypt, %function

aes128_encrypt:
    // Load plaintext block into vector register
    LD1 {v0.16b}, [x0]
    
    // Load all round keys
    LD1 {v16.16b, v17.16b, v18.16b, v19.16b}, [x1], #64
    LD1 {v20.16b, v21.16b, v22.16b, v23.16b}, [x1], #64
    LD1 {v24.16b, v25.16b, v26.16b}, [x1]
    
    // Initial round: XOR with first round key
    EOR v0.16b, v0.16b, v16.16b
    
    // Rounds 1-9: AESE + AESMC
    AESE v0.16b, v17.16b
    AESMC v0.16b, v0.16b
    
    AESE v0.16b, v18.16b
    AESMC v0.16b, v0.16b
    
    AESE v0.16b, v19.16b
    AESMC v0.16b, v0.16b
    
    AESE v0.16b, v20.16b
    AESMC v0.16b, v0.16b
    
    AESE v0.16b, v21.16b
    AESMC v0.16b, v0.16b
    
    AESE v0.16b, v22.16b
    AESMC v0.16b, v0.16b
    
    AESE v0.16b, v23.16b
    AESMC v0.16b, v0.16b
    
    AESE v0.16b, v24.16b
    AESMC v0.16b, v0.16b
    
    AESE v0.16b, v25.16b
    AESMC v0.16b, v0.16b
    
    // Final round: AESE without AESMC
    AESE v0.16b, v26.16b
    
    // Store ciphertext
    ST1 {v0.16b}, [x2]
    
    RET
    
.size aes128_encrypt, .-aes128_encrypt
```

**Performance:** [Inference] ARMv8 crypto extensions enable AES-128 encryption at approximately 1-2 cycles per byte on modern Cortex-A processors, compared to 20-40 cycles per byte for software-only implementations.

### AES Without Hardware Acceleration (Cortex-M)

When hardware acceleration isn't available, optimized software AES implementation uses lookup tables for performance.

**Example** - AES S-box substitution (table-based):

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ AES SubBytes operation using S-box lookup
@ r0 = state pointer (16 bytes)

.global aes_sub_bytes
.type aes_sub_bytes, %function

aes_sub_bytes:
    PUSH {r4-r7, lr}
    
    LDR r1, =aes_sbox       @ Load S-box table address
    MOV r2, #16             @ 16 bytes to process
    
sub_bytes_loop:
    @ Load byte from state
    LDRB r3, [r0]
    
    @ Lookup in S-box
    LDRB r4, [r1, r3]
    
    @ Store substituted byte
    STRB r4, [r0], #1
    
    SUBS r2, r2, #1
    BNE sub_bytes_loop
    
    POP {r4-r7, pc}
    
.size aes_sub_bytes, .-aes_sub_bytes

@ AES S-box lookup table (256 bytes)
.section .rodata
.align 4
aes_sbox:
    .byte 0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5
    .byte 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76
    .byte 0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0
    .byte 0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0
    .byte 0xb7, 0xfd, 0x93, 0x26, 0x36, 0x3f, 0xf7, 0xcc
    .byte 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15
    .byte 0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a
    .byte 0x07, 0x12, 0x80, 0xe2, 0xeb, 0x27, 0xb2, 0x75
    .byte 0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0
    .byte 0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84
    .byte 0x53, 0xd1, 0x00, 0xed, 0x20, 0xfc, 0xb1, 0x5b
    .byte 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf
    .byte 0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85
    .byte 0x45, 0xf9, 0x02, 0x7f, 0x50, 0x3c, 0x9f, 0xa8
    .byte 0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5
    .byte 0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2
    .byte 0xcd, 0x0c, 0x13, 0xec, 0x5f, 0x97, 0x44, 0x17
    .byte 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73
    .byte 0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88
    .byte 0x46, 0xee, 0xb8, 0x14, 0xde, 0x5e, 0x0b, 0xdb
    .byte 0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c
    .byte 0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79
    .byte 0xe7, 0xc8, 0x37, 0x6d, 0x8d, 0xd5, 0x4e, 0xa9
    .byte 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08
    .byte 0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6
    .byte 0xe8, 0xdd, 0x74, 0x1f, 0x4b, 0xbd, 0x8b, 0x8a
    .byte 0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e
    .byte 0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e
    .byte 0xe1, 0xf8, 0x98, 0x11, 0x69, 0xd9, 0x8e, 0x94
    .byte 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf
    .byte 0x8c, 0xa1, 0x89, 0x0d, 0xbf, 0xe6, 0x42, 0x68
    .byte 0x41, 0x99, 0x2d, 0x0f, 0xb0, 0x54, 0xbb, 0x16
```

**AES MixColumns Implementation:**

MixColumns uses Galois Field (GF(2⁸)) multiplication. Optimized using precomputed tables or bit manipulation.

**Example** - AES MixColumns with table lookup:

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ AES MixColumns operation
@ r0 = state pointer (16 bytes arranged as 4x4 column-major)

.global aes_mix_columns
.type aes_mix_columns, %function

aes_mix_columns:
    PUSH {r4-r11, lr}
    
    MOV r1, #4              @ 4 columns to process
    
mix_col_loop:
    @ Load 4 bytes of column
    LDRB r2, [r0, #0]       @ s0
    LDRB r3, [r0, #4]       @ s1
    LDRB r4, [r0, #8]       @ s2
    LDRB r5, [r0, #12]      @ s3
    
    @ MixColumns matrix multiplication in GF(2^8):
    @ [2 3 1 1]   [s0]
    @ [1 2 3 1] * [s1]
    @ [1 1 2 3]   [s2]
    @ [3 1 1 2]   [s3]
    
    @ Compute t = s0 ^ s1 ^ s2 ^ s3
    EOR r6, r2, r3
    EOR r6, r6, r4
    EOR r6, r6, r5          @ r6 = t
    
    @ Compute u = s0 ^ s1, multiply by 2 in GF(2^8)
    EOR r7, r2, r3
    BL gf_mul2
    MOV r8, r7
    
    @ Result[0] = s0 ^ u ^ t
    EOR r9, r2, r8
    EOR r9, r9, r6
    STRB r9, [r0, #0]
    
    @ Compute u = s1 ^ s2, multiply by 2
    EOR r7, r3, r4
    BL gf_mul2
    MOV r8, r7
    
    @ Result[1] = s1 ^ u ^ t
    EOR r9, r3, r8
    EOR r9, r9, r6
    STRB r9, [r0, #4]
    
    @ Compute u = s2 ^ s3, multiply by 2
    EOR r7, r4, r5
    BL gf_mul2
    MOV r8, r7
    
    @ Result[2] = s2 ^ u ^ t
    EOR r9, r4, r8
    EOR r9, r9, r6
    STRB r9, [r0, #8]
    
    @ Compute u = s3 ^ s0, multiply by 2
    EOR r7, r5, r2
    BL gf_mul2
    MOV r8, r7
    
    @ Result[3] = s3 ^ u ^ t
    EOR r9, r5, r8
    EOR r9, r9, r6
    STRB r9, [r0, #12]
    
    @ Next column
    ADD r0, r0, #1
    SUBS r1, r1, #1
    BNE mix_col_loop
    
    POP {r4-r11, pc}

@ Galois Field GF(2^8) multiplication by 2
@ r7 = input/output byte
gf_mul2:
    LSL r10, r7, #1         @ Shift left by 1
    TST r7, #0x80           @ Check if high bit was set
    IT NE
    EORNE r10, r10, #0x1B   @ XOR with 0x1B if overflow
    AND r7, r10, #0xFF
    BX lr
    
.size aes_mix_columns, .-aes_mix_columns
```

### SHA-256 Hash Function

SHA-256 is a cryptographic hash function widely used for data integrity and digital signatures.

**Example** - SHA-256 core compression function (optimized for Cortex-M4):

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ SHA-256 compression function
@ r0 = message block pointer (64 bytes)
@ r1 = hash state pointer (32 bytes, 8 words)

.global sha256_compress
.type sha256_compress, %function

sha256_compress:
    PUSH {r4-r11, lr}
    SUB sp, sp, #64         @ Allocate W[16] on stack
    
    @ Load initial hash values
    LDM r1, {r2-r9}         @ a-h in r2-r9
    
    @ Process 64 rounds
    MOV r10, #0             @ Round counter
    MOV r11, r0             @ Message pointer
    MOV r12, sp             @ W array pointer
    
sha256_round_loop:
    @ Load/compute W[t]
    CMP r10, #16
    BLT sha256_load_w
    
    @ W[t] = W[t-16] + σ0(W[t-15]) + W[t-7] + σ1(W[t-2])
    @ For simplicity, we'll recalculate from message
    @ Full optimization would maintain sliding window
    
sha256_load_w:
    @ Load message word (big-endian)
    LDR r0, [r11], #4
    REV r0, r0              @ Convert to big-endian
    STR r0, [r12], #4
    
    @ Compute T1 = h + Σ1(e) + Ch(e,f,g) + K[t] + W[t]
    
    @ Σ1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    ROR r14, r6, #6
    EOR r14, r14, r6, ROR #11
    EOR r14, r14, r6, ROR #25
    
    @ Ch(e,f,g) = (e & f) ^ (~e & g)
    AND r0, r6, r7
    BIC r1, r8, r6
    EOR r0, r0, r1
    
    @ T1 = h + Σ1(e) + Ch(e,f,g)
    ADD r0, r9, r14
    ADD r0, r0, r0          @ + Ch result
    
    @ Add K[t] (load from table)
    LDR r1, =sha256_k
    LDR r1, [r1, r10, LSL #2]
    ADD r0, r0, r1
    
    @ Add W[t]
    LDR r1, [sp, r10, LSL #2]
    ADD r0, r0, r1          @ r0 = T1
    
    @ Compute T2 = Σ0(a) + Maj(a,b,c)
    
    @ Σ0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    ROR r14, r2, #2
    EOR r14, r14, r2, ROR #13
    EOR r14, r14, r2, ROR #22
    
    @ Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    AND r1, r2, r3
    AND r14, r2, r4
    EOR r1, r1, r14
    AND r14, r3, r4
    EOR r1, r1, r14         @ r1 = Maj result
    
    @ T2 = Σ0(a) + Maj(a,b,c)
    ROR r14, r2, #2
    EOR r14, r14, r2, ROR #13
    EOR r14, r14, r2, ROR #22
    ADD r1, r1, r14         @ r1 = T2
    
    @ Update working variables
    @ h = g, g = f, f = e, e = d + T1
    MOV r9, r8
    MOV r8, r7
    MOV r7, r6
    ADD r6, r5, r0
    
    @ d = c, c = b, b = a, a = T1 + T2
    MOV r5, r4
    MOV r4, r3
    MOV r3, r2
    ADD r2, r0, r1
    
    @ Next round
    ADD r10, r10, #1
    CMP r10, #16            @ Only process first 16 rounds for example
    BLT sha256_round_loop
    
    @ Add compressed chunk to current hash value
    LDM r1!, {r0}
    ADD r2, r2, r0
    LDM r1!, {r0}
    ADD r3, r3, r0
    LDM r1!, {r0}
    ADD r4, r4, r0
    LDM r1!, {r0}
    ADD r5, r5, r0
    LDM r1!, {r0}
    ADD r6, r6, r0
    LDM r1!, {r0}
    ADD r7, r7, r0
    LDM r1!, {r0}
    ADD r8, r8, r0
    LDM r1!, {r0}
    ADD r9, r9, r0
    
    @ Store updated hash
    SUB r1, r1, #32
    STM r1, {r2-r9}
    
    ADD sp, sp, #64
    POP {r4-r11, pc}

.size sha256_compress, .-sha256_compress

@ SHA-256 round constants (first 16 shown)
.section .rodata
.align 4
sha256_k:
    .word 0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5
    .word 0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5
    .word 0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3
    .word 0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174
    @ ... (remaining 48 constants omitted for brevity)
```

### ChaCha20 Stream Cipher

ChaCha20 is a modern stream cipher designed for high performance in software. It uses only addition, XOR, and rotation operations.

**Example** - ChaCha20 quarter round (ARMv7-A with NEON):

```assembly
.arch armv7-a
.fpu neon
.syntax unified
.text

@ ChaCha20 quarter round on NEON vector registers
@ Operates on 4 parallel quarter rounds simultaneously
@ q0-q3 contain state matrix columns

.global chacha20_quarter_round_neon
.type chacha20_quarter_round_neon, %function

chacha20_quarter_round_neon:
    @ a += b; d ^= a; d <<<= 16
    VADD.I32 q0, q0, q1
    VEOR q3, q3, q0
    VSHL.I32 q4, q3, #16
    VSHR.U32 q3, q3, #16
    VORR q3, q4, q3
    
    @ c += d; b ^= c; b <<<= 12
    VADD.I32 q2, q2, q3
    VEOR q1, q1, q2
    VSHL.I32 q4, q1, #12
    VSHR.U32 q1, q1, #20
    VORR q1, q4, q1
    
    @ a += b; d ^= a; d <<<= 8
    VADD.I32 q0, q0, q1
    VEOR q3, q3, q0
    VSHL.I32 q4, q3, #8
    VSHR.U32 q3, q3, #24
    VORR q3, q4, q3
    
    @ c += d; b ^= c; b <<<= 7
    VADD.I32 q2, q2, q3
    VEOR q1, q1, q2
    VSHL.I32 q4, q1, #7
    VSHR.U32 q1, q1, #25
    VORR q1, q4, q1
    
    BX lr

.size chacha20_quarter_round_neon, .-chacha20_quarter_round_neon
```

**ChaCha20 Block Function (Cortex-M4 optimized):**

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ ChaCha20 block function
@ r0 = output pointer (64 bytes)
@ r1 = input state pointer (64 bytes, 16 words)
@ Performs 20 rounds (10 double rounds)

.global chacha20_block
.type chacha20_block, %function

chacha20_block:
    PUSH {r4-r11, lr}
    SUB sp, sp, #64         @ Working state on stack
    
    @ Copy input state to working state
    MOV r2, sp
    LDM r1!, {r3-r10}
    STM r2!, {r3-r10}
    LDM r1!, {r3-r10}
    STM r2!, {r3-r10}
    
    @ Perform 10 double rounds (20 rounds total)
    MOV r11, #10
    
chacha20_double_round:
    @ Load state into registers
    MOV r2, sp
    LDM r2, {r0-r7}         @ Load first 8 words
    
    @ Quarter round (0, 4, 8, 12) - column round
    @ a += b; d ^= a; d <<<= 16
    ADD r0, r0, r4
    EOR r12, r8, r0
    ROR r8, r12, #16
    
    @ c += d; b ^= c; b <<<= 12
    ADD r8, r8, r12
    EOR r4, r4, r8
    ROR r4, r4, #20         @ 32-12 = 20 for right rotation
    
    @ a += b; d ^= a; d <<<= 8
    ADD r0, r0, r4
    EOR r12, r8, r0
    ROR r8, r12, #24        @ 32-8 = 24
    
    @ c += d; b ^= c; b <<<= 7
    ADD r8, r8, r12
    EOR r4, r4, r8
    ROR r4, r4, #25         @ 32-7 = 25
    
    @ Store back (simplified - full implementation would do all 4 quarters)
    MOV r2, sp
    STM r2, {r0-r7}
    
    SUBS r11, r11, #1
    BNE chacha20_double_round
    
    @ Add original state to working state
    LDR r1, [sp, #64 + 40]  @ Reload original state pointer
    MOV r2, sp
    MOV r3, #16             @ 16 words
    
chacha20_add_loop:
    LDR r4, [r1], #4
    LDR r5, [r2]
    ADD r5, r5, r4
    STR r5, [r2], #4
    SUBS r3, r3, #1
    BNE chacha20_add_loop
    
    @ Copy result to output
    LDR r0, [sp, #64 + 36]  @ Reload output pointer
    MOV r1, sp
    LDM r1!, {r2-r9}
    STM r0!, {r2-r9}
    LDM r1!, {r2-r9}
    STM r0!, {r2-r9}
    
    ADD sp, sp, #64
    POP {r4-r11, pc}

.size chacha20_block, .-chacha20_block
```

### Constant-Time Operations for Side-Channel Resistance

Cryptographic implementations must avoid timing variations that could leak secret information. Assembly provides precise control over execution time.

**Example** - Constant-time comparison:

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ Constant-time memory comparison
@ r0 = pointer 1
@ r1 = pointer 2
@ r2 = length in bytes
@ Returns: 0 if equal, non-zero if different

.global ct_memcmp
.type ct_memcmp, %function

ct_memcmp:
    PUSH {r4-r5, lr}
    
    MOV r3, #0              @ Accumulator for differences
    
ct_cmp_loop:
    CBZ r2, ct_cmp_done     @ If length is 0, done
    
    LDRB r4, [r0], #1       @ Load byte from buffer 1
    LDRB r5, [r1], #1       @ Load byte from buffer 2
    
    EOR r4, r4, r5          @ XOR to find differences
    ORR r3, r3, r4          @ Accumulate differences
    
    SUBS r2, r2, #1         @ Decrement length
    BNE ct_cmp_loop         @ Continue (constant time - always branches)
    
ct_cmp_done:
    MOV r0, r3              @ Return accumulated differences
    POP {r4-r5, pc}

.size ct_memcmp, .-ct_memcmp
```

**Key characteristics:**

- No conditional branches based on data values
- Always loads both bytes regardless of differences found
- Accumulates all differences before returning
- Execution time depends only on length, not data content

**Example** - Constant-time conditional select:

```assembly
@ Select between two values in constant time
@ r0 = value if condition true
@ r1 = value if condition false
@ r2 = condition (0 or 1)
@ Returns: selected value in r0

.global ct_select
.type ct_select, %function

ct_select:
    @ Create mask: 0xFFFFFFFF if r2==1, 0x00000000 if r2==0
    NEG r2, r2              @ r2 = -r2 (0 -> 0, 1 -> 0xFFFFFFFF)
    
    @ result = (value_true & mask) | (value_false & ~mask)
    AND r3, r0, r2
    BIC r2, r1, r2
    ORR r0, r3, r2
    
    BX lr

.size ct_select, .-ct_select
```

### RSA Modular Exponentiation

RSA encryption/decryption requires modular exponentiation with large integers (typically 2048-4096 bits).

**Example** - Montgomery multiplication for modular arithmetic (simplified 32-bit version):

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ Montgomery reduction for modular multiplication
@ Computes (a * b * R^-1) mod m where R = 2^32
@ r0 = a (32-bit)
@ r1 = b (32-bit)
@ r2 = m (modulus, 32-bit)
@ r3 = m' (precomputed: -m^-1 mod 2^32)

.global montgomery_mul_32
.type montgomery_mul_32, %function

montgomery_mul_32:
    PUSH {r4-r7, lr}
    
    @ Compute t = a * b (64-bit result)
    UMULL r4, r5, r0, r1    @ r5:r4 = a * b
    
    @ Compute u = (t * m') mod 2^32
    MUL r6, r4, r3          @ r6 = low(t) * m'
    
    @ Compute t = t + u * m
    UMULL r0, r1, r6, r2    @ r1:r0 = u * m
    ADDS r4, r4, r0         @ Add to low word
    ADC r5, r5, r1          @ Add carry to high word
    
    @ Result = t / 2^32 (high word)
    MOV r0, r5
    
    @ Conditional subtraction if result >= m
    CMP r0, r2
    IT HS
    SUBHS r0, r0, r2
    
    POP {r4-r7, pc}

.size montgomery_mul_32, .-montgomery_mul_32
```

[Unverified] For production RSA implementations, multi-precision arithmetic libraries handle integers hundreds of bits long, using optimized assembly for addition, multiplication, and modular reduction across multiple registers and memory locations.

### Elliptic Curve Cryptography (ECC)

ECC provides equivalent security to RSA with much smaller key sizes. Point multiplication on elliptic curves is the core operation.

**Example** - ECC point doubling (conceptual structure for Curve25519):

```assembly
.syntax unified
.cpu cortex-m4
.thumb

@ Elliptic curve point doubling
@ Simplified structure - real implementation requires multi-precision arithmetic

@ Point structure: (X, Y, Z) in projective coordinates
@ Input: r0 = point pointer (3 field elements)
@ Output: point doubled in place

.global ecc_point_double
.type ecc_point_double, %function

ecc_point_double:
    PUSH {r4-r11, lr}
    SUB sp, sp, #48         @ Temporary field elements
    
    @ Load point coordinates (simplified as single registers)
    LDM r0, {r1-r3}         @ X, Y, Z
    
    @ Compute A = X^2
    @ (Call field multiplication routine)
    MOV r0, r1
    MOV r1, r1
    BL field_mul
    STR r0, [sp, #0]        @ Store A
    
    @ Compute B = Y^2
    MOV r0, r2
    MOV r1, r2
    BL field_mul
    STR r0, [sp, #4]        @ Store B
    
    @ Compute C = Z^2
    MOV r0, r3
    MOV r1, r3
    BL field_mul
    STR r0, [sp, #8]        @ Store C
    
    @ Continue with point doubling formulas...
    @ X3 = (B - A - C)^2
    @ Y3 = (A + B) * (A - C)
    @ Z3 = (2*B) * C
    
    @ (Full implementation would continue with field operations)
    
    ADD sp, sp, #48
    POP {r4-r11, pc}

@ Field multiplication placeholder
field_mul:
    @ Multiply r0 * r1 modulo field prime
    @ Real implementation uses multi-precision arithmetic
    UMULL r2, r3, r0, r1
    @ ... modular reduction ...
    MOV r0, r2
    BX lr

.size ecc_point_double, .-ecc_point_double
```

[Inference] Production ECC implementations require careful optimization of field arithmetic (addition, multiplication, squaring, inversion) and often use techniques like sliding window exponentiation for point multiplication.

### Hardware Crypto Acceleration

Many ARM SoCs include cryptographic accelerators that offload operations from the CPU.

**Example** - STM32 hardware AES peripheral configuration:

```assembly
.syntax unified
.cpu cortex-m4
.thumb

.equ AES_BASE,   0x50060000
.equ AES_CR,     0x00
.equ AES_SR,     0x04
.equ AES_DINR,   0x08
.equ AES_DOUTR,  0x0C
.equ AES_KEYR0,  0x10
.equ AES_KEYR1,  0x14
.equ AES_KEYR2,  0x18
.equ AES_KEYR3,  0x1C
.equ AES_IVR0,   0x20

@ Configure and use hardware AES
@ r0 = plaintext pointer
@ r1 = key pointer (128-bit)
@ r2 = ciphertext pointer

.global hw_aes_encrypt
.type hw_aes_encrypt, %function

hw_aes_encrypt:
    PUSH {r4-r7, lr}
    
    LDR r3, =AES_BASE
    
    @ Disable AES
    MOV r4, #0
    STR r4, [r3, #AES_CR]
    
    @ Load key (4 words)
    LDM r1!, {r4-r7}
    STR r4, [r3, #AES_KEYR3]
    STR r5, [r3, #AES_KEYR2]
    STR r6, [r3, #AES_KEYR1]
    STR r7, [r3, #AES_KEYR0]
    
    @ Configure: ECB mode, encryption, enable
    MOV r4, #0x03           @ MODE=00 (ECB), DATATYPE=00, EN=1
    STR r4, [r3, #AES_CR]
    
    @ Wait for key initialization (CCF flag)
1:  LDR r4, [r3, #AES_SR]
    TST r4, #0x01           @ Check CCF bit
    BEQ 1b
    
    @ Clear CCF
    STR r4, [r3, #AES_SR]
    
    @ Load plaintext (4 words)
    LDM r0!, {r4-r7}
    STR r7, [r3, #AES_DINR]
    STR r6, [r3, #AES_DINR]
    STR r5, [r3, #AES_DINR]
    STR r4, [r3, #AES_DINR]
    
    @ Wait for completion (CCF flag)
2:  LDR r4, [r3, #AES_SR]
    TST r4, #0x01
    BEQ 2b
    
    @ Read ciphertext (4 words)
    LDR r7, [r3, #AES_DOUTR]
    LDR r6, [r3, #AES_DOUTR]
    LDR r5, [r3, #AES_DOUTR]
    LDR r4, [r3, #AES_DOUTR]
    STM r2!, {r4-r7}
    
    @ Disable AES
    MOV r4, #0
    STR r4, [r3, #AES_CR]
    
    POP {r4-r7, pc}

.size hw_aes_encrypt, .-hw_aes_encrypt
```

**Performance Comparison:**

[Inference] Performance estimates for different implementations on Cortex-M4 @ 100MHz:

**AES-128 encryption (single block):**

- Software (table-based): ~3,000-4,000 cycles
- Software (bit-sliced): ~5,000-7,000 cycles
- Hardware accelerator: ~100-200 cycles

**SHA-256 (64-byte block):**

- Software (optimized): ~8,000-12,000 cycles
- Hardware accelerator: ~500-1,000 cycles

[Inference] Hardware acceleration provides 10-40x performance improvement but requires careful management of DMA transfers and peripheral state.

**Key Points:**

- Graphics primitives benefit from assembly optimization through direct memory access, SIMD operations, and careful register allocation
- Bresenham's algorithm uses integer-only arithmetic suitable for non-FPU systems
- Alpha blending on RGB565 uses bit field instructions for efficient color channel manipulation
- DMA controllers enable parallel graphics operations without CPU intervention
- Cryptographic algorithms in assembly provide both performance and timing attack resistance
- AES hardware extensions (ARMv8) dramatically accelerate symmetric encryption
- Constant-time implementations prevent side-channel attacks by eliminating data-dependent branches
- Montgomery multiplication enables efficient modular arithmetic for RSA
- Hardware crypto accelerators offload intensive operations, providing significant speedup
- Modern ARM crypto extensions make software-only implementations less critical but assembly knowledge remains valuable for optimization and hardware interaction

---

