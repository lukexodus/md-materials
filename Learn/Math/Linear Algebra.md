# Syllabus

## Course Overview

This syllabus provides a structured path for learning linear algebra from basic concepts to advanced applications. The curriculum is designed for self-study or classroom use, with estimated timeframes and learning objectives for each module.

**Prerequisites:** Basic algebra, familiarity with functions, and introductory calculus (helpful but not strictly required for early modules)

**Total Duration:** 16-20 weeks (adjustable based on pace and depth)

---

## Module 1: Foundations and Vector Spaces (Weeks 1-3)

### Learning Objectives

- Understand vectors as mathematical objects
- Master vector operations and geometric interpretations
- Grasp the concept of vector spaces and subspaces

### Topics Covered

**Week 1: Introduction to Vectors**

- Vectors in R² and R³
- Geometric representation
- Vector addition and scalar multiplication
- Linear combinations
- Unit vectors and normalization

**Week 2: Vector Spaces**

- Definition of vector spaces
- Vector space axioms
- Examples: Rⁿ, polynomial spaces, function spaces
- Subspaces and spanning sets
- Linear independence and dependence

**Week 3: Basis and Dimension**

- Basis of a vector space
- Coordinate systems
- Dimension of vector spaces
- Change of basis
- Row space, column space, and null space

### Key Skills to Develop

- Vector arithmetic and visualization
- Determining linear independence
- Finding bases for vector spaces
- Computing dimensions

---

## Module 2: Matrices and Matrix Operations (Weeks 4-6)

### Learning Objectives

- Master matrix operations and their properties
- Understand matrices as linear transformations
- Develop computational fluency with matrix algebra

### Topics Covered

**Week 4: Matrix Basics**

- Matrix definitions and notation
- Matrix addition and scalar multiplication
- Matrix multiplication
- Special matrices (identity, zero, diagonal, symmetric)
- Transpose and its properties

**Week 5: Matrix Algebra**

- Properties of matrix operations
- Block matrices
- Elementary matrices
- Matrix inverses and conditions for invertibility
- Powers of matrices

**Week 6: Matrix Factorizations**

- LU decomposition
- QR decomposition (introduction)
- Applications to solving systems

### Key Skills to Develop

- Efficient matrix computation
- Understanding when matrices are invertible
- Using factorizations for problem-solving

---

## Module 3: Systems of Linear Equations (Weeks 7-8)

### Learning Objectives

- Solve systems using multiple methods
- Understand the relationship between matrices and linear systems
- Analyze solution existence and uniqueness

### Topics Covered

**Week 7: Gaussian Elimination**

- Row operations and row equivalence
- Row echelon form and reduced row echelon form
- Gaussian elimination algorithm
- Back substitution

**Week 8: Solution Analysis**

- Homogeneous vs. non-homogeneous systems
- Consistency and inconsistency
- Parametric solutions
- Relationship to null space and column space

### Key Skills to Develop

- Systematic equation solving
- Interpreting solution sets geometrically
- Connecting algebraic and geometric perspectives

---

## Module 4: Linear Transformations (Weeks 9-11)

### Learning Objectives

- Understand linear transformations as functions
- Connect matrices to linear transformations
- Analyze transformation properties

### Topics Covered

**Week 9: Linear Transformation Fundamentals**

- Definition and examples
- Matrix representation of linear transformations
- Kernel (null space) and range (image)
- Injective, surjective, and bijective transformations

**Week 10: Transformation Composition**

- Composition of linear transformations
- Matrix multiplication as composition
- Inverse transformations
- Change of basis for transformations

**Week 11: Geometric Transformations**

- Rotations, reflections, and scaling
- Projections
- Transformations in computer graphics
- Affine transformations

### Key Skills to Develop

- Visualizing transformations geometrically
- Computing transformation matrices
- Understanding the kernel-image relationship

---

## Module 5: Determinants (Weeks 12-13)

### Learning Objectives

- Compute determinants using various methods
- Understand geometric and algebraic significance
- Apply determinants to solve problems

### Topics Covered

**Week 12: Determinant Computation**

- Definition via cofactor expansion
- Properties of determinants
- Efficient computation methods
- Determinants of special matrices

**Week 13: Applications of Determinants**

- Determinants and invertibility
- Cramer's rule
- Geometric interpretation (area/volume)
- Determinants and linear transformations

### Key Skills to Develop

- Computing determinants efficiently
- Using determinants for problem-solving
- Understanding geometric meaning

---

## Module 6: Eigenvalues and Eigenvectors (Weeks 14-16)

### Learning Objectives

- Find eigenvalues and eigenvectors
- Understand diagonalization
- Apply eigenvalue concepts to real problems

### Topics Covered

**Week 14: Eigenvalue Fundamentals**

- Eigenvalue and eigenvector definitions
- Characteristic polynomial
- Computing eigenvalues and eigenvectors
- Geometric and algebraic multiplicity

**Week 15: Diagonalization**

- Diagonalizable matrices
- Similar matrices
- Applications of diagonalization
- Powers of matrices via diagonalization

**Week 16: Advanced Eigenvalue Topics**

- Complex eigenvalues
- Eigenspaces
- Applications in differential equations
- Markov chains and steady states

### Key Skills to Develop

- Eigenvalue computation
- Matrix diagonalization
- Applying eigenvalues to real problems

---

## Module 7: Inner Product Spaces (Weeks 17-18)

### Learning Objectives

- Understand inner products and norms
- Master orthogonality concepts
- Work with orthonormal bases

### Topics Covered

**Week 17: Inner Products and Norms**

- Inner product definition and properties
- Euclidean and general inner products
- Norms and distance
- Cauchy-Schwarz inequality

**Week 18: Orthogonality**

- Orthogonal and orthonormal vectors
- Gram-Schmidt process
- Orthogonal projections
- QR decomposition (detailed treatment)
- Least squares solutions

### Key Skills to Develop

- Computing inner products and norms
- Creating orthonormal bases
- Solving least squares problems

---

## Module 8: Advanced Topics and Applications (Weeks 19-20)

### Learning Objectives

- Explore specialized topics
- See linear algebra in real applications
- Connect concepts to other mathematical areas

### Topics Covered

**Week 19: Matrix Decompositions**

- Singular Value Decomposition (SVD)
- Principal Component Analysis (PCA)
- Applications in data science
- Spectral theorem for symmetric matrices

**Week 20: Applications**

- Linear algebra in computer graphics
- Network analysis and graph theory
- Optimization problems
- Differential equations
- Machine learning connections

### Key Skills to Develop

- Using advanced decompositions
- Applying linear algebra to real problems
- Making connections across disciplines

---

## Assessment and Practice Structure

### Weekly Components

- **Theory Review:** 2-3 hours reading/watching lectures
- **Problem Sets:** 3-4 hours working exercises
- **Computational Practice:** 1-2 hours using software tools
- **Conceptual Reflection:** 30 minutes connecting ideas

### Milestone Assessments

- **Module Tests:** End of modules 2, 4, 6, and 8
- **Midterm:** After module 4 (comprehensive review)
- **Final Project:** Real-world application of multiple concepts
- **Final Exam:** Comprehensive assessment

### Recommended Resources

**Textbooks:**

- "Linear Algebra and Its Applications" by David C. Lay
- "Introduction to Linear Algebra" by Gilbert Strang
- "Linear Algebra Done Right" by Sheldon Axler

**Computational Tools:**

- MATLAB/Octave
- Python (NumPy, SciPy)
- R
- Wolfram Alpha for verification

**Online Resources:**

- Khan Academy Linear Algebra
- MIT OpenCourseWare 18.06
- 3Blue1Brown's "Essence of Linear Algebra" series

---

## Study Tips and Success Strategies

### Effective Learning Approaches

1. **Balance Theory and Computation:** Understand concepts deeply while maintaining computational skills
2. **Visualize Concepts:** Use graphing tools and geometric intuition
3. **Practice Regularly:** Work problems daily rather than cramming
4. **Connect Ideas:** See how each module builds on previous work
5. **Teach Others:** Explaining concepts reinforces understanding

### Common Pitfalls to Avoid

- Memorizing procedures without understanding
- Neglecting geometric interpretations
- Rushing through foundational concepts
- Avoiding computational practice
- Not reviewing previous material as course progresses

### Mastery Indicators

By course completion, you should be able to:

- Solve complex systems of equations efficiently
- Analyze linear transformations geometrically and algebraically
- Apply eigenvalue methods to real problems
- Use matrix decompositions for data analysis
- Recognize linear algebra concepts in other mathematical contexts
- Implement linear algebra algorithms computationally

---

# Foundations and Vector Spaces

## Introduction to Vectors

### Vectors in R² and R³

Vectors are mathematical objects that represent both magnitude and direction, distinguished from scalars which only have magnitude. In linear algebra, vectors are typically represented as ordered lists of real numbers.

A vector in R² (two-dimensional real space) is an ordered pair (x, y) where x and y are real numbers called components. Similarly, a vector in R³ (three-dimensional real space) is an ordered triple (x, y, z). These spaces are fundamental building blocks for understanding higher-dimensional vector spaces.

**Key points:**

- R² vectors: v = (v₁, v₂) or written as column vectors [v₁; v₂]
- R³ vectors: v = (v₁, v₂, v₃) or written as column vectors [v₁; v₂; v₃]
- Components represent coordinates along respective axes
- Zero vector: (0, 0) in R² or (0, 0, 0) in R³

The notation Rⁿ represents n-dimensional real coordinate space, where each vector has exactly n real-valued components. While visualization becomes difficult beyond three dimensions, the algebraic properties remain consistent.

### Geometric Representation

Vectors can be visualized geometrically as directed line segments (arrows) in coordinate space. The geometric representation provides intuitive understanding of vector operations and relationships.

In R², vectors are represented as arrows from the origin (0, 0) to the point (x, y). The length of the arrow represents the vector's magnitude, while the direction of the arrow represents the vector's direction. Similarly, R³ vectors are arrows from the origin (0, 0, 0) to the point (x, y, z).

**Key points:**

- Position vectors: arrows from origin to specified points
- Free vectors: arrows that can be translated while maintaining magnitude and direction
- Magnitude (length): calculated using the distance formula
- Direction: determined by the angle the vector makes with coordinate axes

The magnitude of a vector v = (v₁, v₂) in R² is ||v|| = √(v₁² + v₂²). For R³ vectors v = (v₁, v₂, v₃), the magnitude is ||v|| = √(v₁² + v₂² + v₃²).

**Example:** Vector v = (3, 4) in R² has magnitude ||v|| = √(3² + 4²) = √25 = 5. Geometrically, this represents an arrow from the origin to point (3, 4) with length 5 units.

### Vector Addition and Scalar Multiplication

Vector addition combines two vectors by adding their corresponding components. This operation has clear geometric interpretation as placing vectors head-to-tail. Scalar multiplication changes a vector's magnitude and potentially its direction by multiplying each component by a scalar.

For vectors u = (u₁, u₂) and v = (v₁, v₂) in R²:

- Addition: u + v = (u₁ + v₁, u₂ + v₂)
- Scalar multiplication: cu = (cu₁, cu₂) where c is a scalar

These operations extend naturally to R³ and higher dimensions.

**Key points:**

- Vector addition is commutative: u + v = v + u
- Vector addition is associative: (u + v) + w = u + (v + w)
- Additive identity: v + 0 = v (zero vector)
- Additive inverse: v + (-v) = 0
- Scalar multiplication is distributive: c(u + v) = cu + cv
- Scalar multiplication is associative: (cd)v = c(dv)

Geometrically, vector addition follows the parallelogram law or head-to-tail method. Scalar multiplication stretches or compresses the vector when |c| ≠ 1, reverses direction when c < 0, and preserves the zero vector when c = 0.

**Example:** If u = (2, 3) and v = (1, -2), then:

- u + v = (2 + 1, 3 + (-2)) = (3, 1)
- 3u = (3·2, 3·3) = (6, 9)
- -2v = (-2·1, -2·(-2)) = (-2, 4)

### Linear Combinations

A linear combination of vectors v₁, v₂, ..., vₙ is an expression of the form c₁v₁ + c₂v₂ + ... + cₙvₙ, where c₁, c₂, ..., cₙ are scalars called coefficients. Linear combinations are fundamental to understanding vector spaces, spanning sets, and linear independence.

The set of all possible linear combinations of a given set of vectors forms what is called the span of those vectors. This concept is crucial for understanding subspaces and basis vectors.

**Key points:**

- Linear combination: c₁v₁ + c₂v₂ + ... + cₙvₙ
- Coefficients can be any real numbers
- Span of vectors: set of all their linear combinations
- Trivial linear combination: all coefficients equal zero
- Non-trivial linear combination: at least one coefficient is non-zero

Linear combinations have important geometric interpretations. In R², the span of a single non-zero vector is a line through the origin. The span of two non-parallel vectors is the entire plane R².

**Example:** Given vectors v₁ = (1, 2) and v₂ = (3, 1), the linear combination 2v₁ - v₂ equals: 2(1, 2) - (3, 1) = (2, 4) - (3, 1) = (-1, 3)

Any vector in R² can be expressed as a linear combination of v₁ and v₂ since they are linearly independent and span R².

### Unit Vectors and Normalization

A unit vector is a vector with magnitude equal to 1. Unit vectors are particularly useful for representing pure direction without regard to magnitude. Any non-zero vector can be converted to a unit vector through the process of normalization.

The standard unit vectors in R² are i = (1, 0) and j = (0, 1). In R³, the standard unit vectors are i = (1, 0, 0), j = (0, 1, 0), and k = (0, 0, 1). These vectors point along the positive x, y, and z axes respectively.

**Key points:**

- Unit vector: ||u|| = 1
- Standard unit vectors: i, j in R²; i, j, k in R³
- Normalization: dividing a vector by its magnitude
- Direction vector: unit vector indicating direction of a given vector
- Any vector can be written as magnitude times its direction vector

To normalize a non-zero vector v, divide it by its magnitude: û = v/||v||. This process preserves the direction while setting the magnitude to 1.

**Example:** To normalize vector v = (3, 4):

1. Calculate magnitude: ||v|| = √(3² + 4²) = 5
2. Divide by magnitude: û = v/||v|| = (3, 4)/5 = (3/5, 4/5)
3. Verify: ||û|| = √((3/5)² + (4/5)²) = √(9/25 + 16/25) = √(25/25) = 1

**Output:** Understanding vectors in R² and R³ provides the foundation for all linear algebra concepts. These spaces serve as concrete examples where geometric intuition supports algebraic manipulation.

**Conclusion:** Vector operations in R² and R³ follow consistent algebraic rules while maintaining clear geometric interpretations. The concepts of linear combinations and unit vectors extend naturally to higher dimensions, making these fundamental spaces essential for understanding more abstract vector spaces. Mastery of vector operations in these concrete settings prepares students for advanced topics including linear transformations, eigenvalues, and abstract vector spaces.

**Next steps:** Essential related topics include dot products and cross products, linear independence and dependence, vector projections, and the geometric interpretation of systems of linear equations.

---

## Vector Spaces

### Definition of Vector Spaces

A vector space (also called a linear space) is a fundamental algebraic structure in mathematics consisting of a set of objects called vectors, along with two operations: vector addition and scalar multiplication. These operations must satisfy specific properties that make the structure behave predictably under linear combinations.

Formally, a vector space V over a field F (commonly the real numbers ℝ or complex numbers ℂ) is a set equipped with:

- An addition operation: V × V → V, denoted as u + v
- A scalar multiplication operation: F × V → V, denoted as αv

### Vector Space Axioms

The eight fundamental axioms that define a vector space ensure algebraic consistency and enable linear algebraic operations:

### Axioms for Vector Addition

**Closure under addition**: For all u, v ∈ V, the sum u + v is also in V.

**Associativity**: For all u, v, w ∈ V, (u + v) + w = u + (v + w).

**Existence of zero vector**: There exists a unique element 0 ∈ V such that v + 0 = v for all v ∈ V.

**Existence of additive inverses**: For every v ∈ V, there exists a unique element -v ∈ V such that v + (-v) = 0.

**Commutativity**: For all u, v ∈ V, u + v = v + u.

### Axioms for Scalar Multiplication

**Closure under scalar multiplication**: For all α ∈ F and v ∈ V, the product αv is in V.

**Distributivity over vector addition**: For all α ∈ F and u, v ∈ V, α(u + v) = αu + αv.

**Distributivity over scalar addition**: For all α, β ∈ F and v ∈ V, (α + β)v = αv + βv.

**Associativity of scalar multiplication**: For all α, β ∈ F and v ∈ V, α(βv) = (αβ)v.

**Scalar multiplication identity**: For all v ∈ V, 1v = v, where 1 is the multiplicative identity in F.

### Examples of Vector Spaces

### Euclidean Spaces (ℝⁿ)

The most familiar vector spaces are Euclidean spaces ℝⁿ, where vectors are n-tuples of real numbers:

- ℝ² represents the plane with vectors (x, y)
- ℝ³ represents three-dimensional space with vectors (x, y, z)
- ℝⁿ represents n-dimensional space with vectors (x₁, x₂, ..., xₙ)

**Key points**: Addition is component-wise: (a₁, a₂, ..., aₙ) + (b₁, b₂, ..., bₙ) = (a₁ + b₁, a₂ + b₂, ..., aₙ + bₙ). Scalar multiplication is: α(a₁, a₂, ..., aₙ) = (αa₁, αa₂, ..., αaₙ).

### Polynomial Spaces

The set of all polynomials with real coefficients forms infinite-dimensional vector spaces:

**P**: The space of all polynomials p(x) = a₀ + a₁x + a₂x² + ... with real coefficients.

**Pₙ**: The space of polynomials of degree at most n, which is (n+1)-dimensional.

**Example**: In P₂, the polynomial 3x² - 2x + 1 can be added to x² + 4x - 5 to get 4x² + 2x - 4.

### Function Spaces

Various function spaces demonstrate the generality of vector space concepts:

**C[a,b]**: The space of continuous real-valued functions on the closed interval [a,b].

**C∞(ℝ)**: The space of infinitely differentiable functions on the real line.

**L²**: The space of square-integrable functions, fundamental in functional analysis and quantum mechanics.

**Key points**: Function addition is pointwise: (f + g)(x) = f(x) + g(x). Scalar multiplication is: (αf)(x) = αf(x).

### Matrix Spaces

**Mₘₓₙ(ℝ)**: The space of all m×n matrices with real entries forms a vector space of dimension mn.

**Symmetric matrices**: The subset of n×n symmetric matrices forms a subspace of dimension n(n+1)/2.

### Subspaces and Spanning Sets

### Definition of Subspaces

A subspace W of a vector space V is a non-empty subset of V that is closed under vector addition and scalar multiplication, automatically inheriting the vector space structure.

### Subspace Test

To verify that W ⊆ V is a subspace, check:

1. W is non-empty (equivalently, 0 ∈ W)
2. Closure under addition: if u, v ∈ W, then u + v ∈ W
3. Closure under scalar multiplication: if v ∈ W and α is a scalar, then αv ∈ W

**Key points**: These conditions can be combined into a single test: W is a subspace if and only if for all u, v ∈ W and scalars α, β, the linear combination αu + βv ∈ W.

### Common Examples of Subspaces

**Trivial subspaces**: Every vector space V contains at least two subspaces: {0} and V itself.

**Hyperplanes**: In ℝⁿ, sets defined by linear equations ax₁ + bx₂ + ... + cxₙ = 0 form (n-1)-dimensional subspaces.

**Solution spaces**: The set of solutions to a homogeneous system Ax = 0 forms the null space of matrix A.

### Spanning Sets

Given vectors v₁, v₂, ..., vₖ in a vector space V, their span is the set of all possible linear combinations:

span{v₁, v₂, ..., vₖ} = {α₁v₁ + α₂v₂ + ... + αₖvₖ : αᵢ ∈ F}

**Key points**: The span of any set of vectors is always a subspace. If span{v₁, v₂, ..., vₖ} = V, then {v₁, v₂, ..., vₖ} is called a spanning set for V.

**Example**: In ℝ³, the vectors (1,0,0), (0,1,0), and (0,0,1) span all of ℝ³, while (1,1,0) and (0,1,1) span a two-dimensional subspace.

### Linear Independence and Dependence

### Definition of Linear Independence

A set of vectors {v₁, v₂, ..., vₖ} is linearly independent if the only solution to the equation α₁v₁ + α₂v₂ + ... + αₖvₖ = 0 is α₁ = α₂ = ... = αₖ = 0.

Conversely, the vectors are linearly dependent if there exist scalars αᵢ, not all zero, such that α₁v₁ + α₂v₂ + ... + αₖvₖ = 0.

### Geometric Interpretation

**In ℝ²**: Two vectors are linearly independent if and only if they are not collinear (don't lie on the same line through the origin).

**In ℝ³**: Three vectors are linearly independent if and only if they are not coplanar (don't lie in the same plane through the origin).

### Key Properties of Linear Independence

**Subset property**: Any subset of a linearly independent set is linearly independent.

**Extension property**: If {v₁, v₂, ..., vₖ} is linearly independent and vₖ₊₁ ∉ span{v₁, v₂, ..., vₖ}, then {v₁, v₂, ..., vₖ, vₖ₊₁} is linearly independent.

**Maximum size**: In an n-dimensional vector space, any linearly independent set contains at most n vectors.

### Testing for Linear Independence

**Matrix method**: For vectors in ℝⁿ, form a matrix with the vectors as columns. The vectors are linearly independent if and only if the matrix has rank equal to the number of vectors.

**Wronskian test**: For functions, the Wronskian determinant can test linear independence under certain conditions. [Unverified: This applies specifically to solutions of differential equations and requires additional conditions for general function spaces.]

**Example**: The vectors (1,2,0), (0,1,1), and (1,0,-1) in ℝ³ are linearly independent because the only solution to α(1,2,0) + β(0,1,1) + γ(1,0,-1) = (0,0,0) is α = β = γ = 0.

### Relationship Between Independence and Spanning

**Basis**: A set of vectors that is both linearly independent and spans the entire vector space is called a basis.

**Dimension**: All bases of a finite-dimensional vector space contain the same number of vectors, which defines the dimension of the space.

**Key points**: Every linearly independent set can be extended to a basis, and every spanning set contains a basis as a subset.

**Conclusion**: Vector spaces provide the foundational framework for linear algebra, with subspaces, spanning sets, and linear independence forming the core concepts that enable the study of linear transformations, eigenvalues, and more advanced topics. Understanding these concepts is essential for applications in physics, engineering, computer science, and pure mathematics.

---

## Basis and Dimension

### Basis of a Vector Space

A basis is a set of vectors that satisfies two fundamental properties: linear independence and spanning capability. These vectors form the "building blocks" from which every vector in the space can be uniquely constructed.

**Key Points:**

- A basis B = {v₁, v₂, ..., vₙ} for vector space V must satisfy:
    1. Linear independence: No vector can be written as a linear combination of others
    2. Spanning property: Every vector in V can be expressed as a linear combination of basis vectors
- Every vector space has at least one basis (excluding the zero vector space)
- All bases of a finite-dimensional vector space contain the same number of vectors

**Example:** In ℝ³, the standard basis is {(1,0,0), (0,1,0), (0,0,1)}. Any vector (a,b,c) can be written as a(1,0,0) + b(0,1,0) + c(0,0,1). Another valid basis would be {(1,1,0), (1,0,1), (0,1,1)}.

### Coordinate Systems

Once a basis is established, every vector in the space has a unique coordinate representation relative to that basis. This creates a one-to-one correspondence between vectors and coordinate tuples.

**Key Points:**

- Given basis B = {v₁, v₂, ..., vₙ} and vector v = c₁v₁ + c₂v₂ + ... + cₙvₙ
- The coordinate vector [v]ᵦ = (c₁, c₂, ..., cₙ) uniquely represents v in basis B
- Different bases yield different coordinate representations of the same vector
- Coordinate vectors enable algebraic manipulation of geometric objects

The coordinate mapping creates an isomorphism between the vector space V and ℝⁿ, preserving vector addition and scalar multiplication operations.

### Dimension of Vector Spaces

Dimension quantifies the "size" or "degrees of freedom" of a vector space, representing the minimum number of vectors needed to span the entire space.

**Key Points:**

- Dimension of V, denoted dim(V), equals the number of vectors in any basis of V
- All bases of a finite-dimensional space have identical cardinality
- The zero vector space {0} has dimension 0 by convention
- Infinite-dimensional spaces require infinite basis sets

**Example:**

- dim(ℝⁿ) = n
- dim(Pₙ) = n+1 for polynomial space of degree ≤ n
- dim(Mₘ×ₙ) = mn for m×n matrix space

### Change of Basis

Transformation between different coordinate systems requires change of basis matrices, which provide systematic conversion of vector representations.

**Key Points:**

- For bases B = {b₁, ..., bₙ} and C = {c₁, ..., cₙ}, the change of basis matrix P_{C←B} has columns [c₁]ᵦ, [c₂]ᵦ, ..., [cₙ]ᵦ
- Coordinate transformation: [v]_C = P_{C←B}[v]_B
- P_{C←B} is invertible, and P_{B←C} = (P_{C←B})⁻¹
- Change of basis preserves linear relationships between vectors

The matrix P_{C←B} transforms coordinates from basis B to basis C by expressing each vector of basis C in terms of basis B coordinates.

**Example:** If B = {(1,0), (0,1)} and C = {(1,1), (1,-1)}, then P_{C←B} = [1 1; 1 -1]. Vector (3,1) in standard coordinates becomes (2,1) in basis C coordinates.

### Row Space, Column Space, and Null Space

These fundamental subspaces of a matrix reveal essential structural properties and dimensional relationships in linear transformations.

#### Row Space

The row space of matrix A, denoted Row(A), consists of all linear combinations of the matrix's row vectors.

**Key Points:**

- Row(A) = span{row₁(A), row₂(A), ..., rowₘ(A)}
- Elementary row operations preserve row space
- Row space dimension equals matrix rank
- Row(A) = Row(RREF(A)) where RREF denotes reduced row echelon form

#### Column Space

The column space of matrix A, denoted Col(A), encompasses all linear combinations of the matrix's column vectors.

**Key Points:**

- Col(A) = span{col₁(A), col₂(A), ..., colₙ(A)}
- Col(A) represents the range of linear transformation T(x) = Ax
- Pivot columns in RREF(A) correspond to basis vectors for Col(A)
- dim(Col(A)) = rank(A)

#### Null Space

The null space of matrix A, denoted Null(A), contains all vectors x satisfying Ax = 0.

**Key Points:**

- Null(A) = {x ∈ ℝⁿ : Ax = 0}
- Also called kernel of the linear transformation T(x) = Ax
- Null(A) is a subspace of ℝⁿ
- dim(Null(A)) = n - rank(A) (rank-nullity theorem)

**Fundamental Dimensional Relationships:**

- rank(A) + nullity(A) = n (number of columns)
- dim(Row(A)) = dim(Col(A)) = rank(A)
- For m×n matrix A: dim(Row(A)) ≤ min(m,n)

**Example:** For matrix A = [1 2 3; 4 5 6; 7 8 9]:

- Row(A) = span{(1,2,3), (4,5,6), (7,8,9)} has dimension 2
- Col(A) has dimension 2 (two pivot columns)
- Null(A) has dimension 1 (3 - 2 = 1)

These subspaces provide complete characterization of linear transformations, revealing both the transformation's effect (column space) and its kernel structure (null space), while maintaining dimensional consistency through the rank-nullity theorem.

**Related Topics:** Understanding orthogonal complements, eigenspaces, and the singular value decomposition builds naturally from these foundational concepts of basis and dimension.

---

# Matrices and Matrix Operations

## Matrix Basics

### Matrix Definitions and Notation

A matrix is a rectangular array of numbers, symbols, or expressions arranged in rows and columns. Matrices are fundamental objects in linear algebra that represent linear transformations, systems of equations, and data structures.

**Standard notation:** An m×n matrix A has m rows and n columns, written as:

A = [aᵢⱼ] where i ∈ {1,2,...,m} and j ∈ {1,2,...,n}

The element aᵢⱼ represents the entry in the i-th row and j-th column. Alternative notations include A = (aᵢⱼ) or explicit representation:

A = ⎡a₁₁ a₁₂ ... a₁ₙ⎤ ⎢a₂₁ a₂₂ ... a₂ₙ⎥ ⎢ ⋮ ⋮ ⋱ ⋮ ⎥ ⎣aₘ₁ aₘ₂ ... aₘₙ⎦

**Key terminology:**

- **Order/Size:** The dimensions m×n
- **Square matrix:** m = n
- **Row vector:** 1×n matrix
- **Column vector:** m×1 matrix
- **Scalar:** 1×1 matrix

**Matrix equality:** Two matrices A and B are equal if they have the same dimensions and corresponding entries are equal: aᵢⱼ = bᵢⱼ for all valid i,j.

### Matrix Addition and Scalar Multiplication

**Matrix addition** is defined only for matrices of the same dimensions. For matrices A and B of size m×n: (A + B)ᵢⱼ = aᵢⱼ + bᵢⱼ

**Properties of matrix addition:**

- Commutative: A + B = B + A
- Associative: (A + B) + C = A + (B + C)
- Zero matrix existence: A + O = A where O is the m×n zero matrix
- Additive inverse: A + (-A) = O where (-A)ᵢⱼ = -aᵢⱼ

**Scalar multiplication** involves multiplying every entry of a matrix by a scalar k: (kA)ᵢⱼ = k·aᵢⱼ

**Properties of scalar multiplication:**

- Associative with scalars: (kl)A = k(lA)
- Distributive over matrix addition: k(A + B) = kA + kB
- Distributive over scalar addition: (k + l)A = kA + lA
- Identity: 1·A = A
- Zero property: 0·A = O

**Example:** If A = ⎡2 -1⎤ and B = ⎡1 3⎤, then: ⎣3 4⎦ ⎣-2 1⎦

A + B = ⎡3 2⎤ and 3A = ⎡6 -3⎤ ⎣1 5⎦ ⎣9 12⎦

### Matrix Multiplication

Matrix multiplication is more complex than addition and requires specific dimensional compatibility. For matrices A (m×p) and B (p×n), the product AB is an m×n matrix where:

(AB)ᵢⱼ = Σₖ₌₁ᵖ aᵢₖbₖⱼ

**Dimensional requirement:** The number of columns in A must equal the number of rows in B.

**Row-column interpretation:** Each entry (AB)ᵢⱼ is the dot product of the i-th row of A with the j-th column of B.

**Properties of matrix multiplication:**

- **Not commutative:** Generally AB ≠ BA
- **Associative:** (AB)C = A(BC) when dimensions allow
- **Distributive:** A(B + C) = AB + AC and (A + B)C = AC + BC
- **Scalar compatibility:** k(AB) = (kA)B = A(kB)

**Block multiplication:** Large matrices can be partitioned into blocks and multiplied using the same rules, provided block dimensions are compatible.

**Computational complexity:** Standard matrix multiplication has O(n³) complexity for n×n matrices. Advanced algorithms like Strassen's achieve better asymptotic performance.

**Example:** A = ⎡1 2⎤, B = ⎡5 6⎤ ⎣3 4⎦ ⎣7 8⎦

AB = ⎡1·5+2·7 1·6+2·8⎤ = ⎡19 22⎤ ⎣3·5+4·7 3·6+4·8⎦ ⎣43 50⎦

### Special Matrices

**Zero Matrix (O or 0):** All entries are zero. Acts as additive identity: A + O = A.

**Identity Matrix (I or Iₙ):** Square matrix with ones on the main diagonal and zeros elsewhere: I = ⎡1 0 ... 0⎤ ⎢0 1 ... 0⎥ ⎢⋮ ⋮ ⋱ ⋮⎥ ⎣0 0 ... 1⎦

**Properties:** AI = IA = A for compatible dimensions. The identity matrix is the multiplicative identity.

**Diagonal Matrix:** Square matrix with non-zero entries only on the main diagonal: D = diag(d₁, d₂, ..., dₙ) = ⎡d₁ 0 ... 0 ⎤ ⎢0 d₂ ... 0 ⎥ ⎢⋮ ⋮ ⋱ ⋮ ⎥ ⎣0 0 ... dₙ⎦

**Properties:** Diagonal matrices commute with each other, and multiplication/powers are computed element-wise on the diagonal.

**Symmetric Matrix:** Square matrix where A = Aᵀ, meaning aᵢⱼ = aⱼᵢ for all i,j.

**Properties of symmetric matrices:**

- Eigenvalues are always real
- Eigenvectors corresponding to distinct eigenvalues are orthogonal
- Always diagonalizable by orthogonal matrices

**Skew-symmetric Matrix:** Square matrix where A = -Aᵀ, meaning aᵢⱼ = -aⱼᵢ. Diagonal entries must be zero.

**Upper/Lower Triangular Matrices:**

- Upper triangular: aᵢⱼ = 0 for i > j
- Lower triangular: aᵢⱼ = 0 for i < j

**Properties:** Products of triangular matrices remain triangular, determinants equal the product of diagonal entries.

**Orthogonal Matrix:** Square matrix where AᵀA = AAᵀ = I, meaning Aᵀ = A⁻¹.

**Properties:** Preserves lengths and angles, represents rotations and reflections, determinant is ±1.

### Transpose and Its Properties

The **transpose** of an m×n matrix A, denoted Aᵀ or A', is the n×m matrix obtained by interchanging rows and columns: (Aᵀ)ᵢⱼ = aⱼᵢ

**Fundamental properties:**

1. **(Aᵀ)ᵀ = A** (double transpose)
2. **(A + B)ᵀ = Aᵀ + Bᵀ** (transpose of sum)
3. **(kA)ᵀ = kAᵀ** (scalar compatibility)
4. **(AB)ᵀ = BᵀAᵀ** (transpose of product - note the reversal)
5. **If A is invertible, (Aᵀ)⁻¹ = (A⁻¹)ᵀ**

**Geometric interpretation:** Transposition reflects the matrix across its main diagonal. For vectors, it converts between row and column representations.

**Applications of transpose:**

- **Inner products:** xᵀy represents the dot product of vectors x and y
- **Quadratic forms:** xᵀAx for symmetric matrix A
- **Normal equations:** AᵀAx = Aᵀb in least squares problems
- **Orthogonality testing:** Vectors are orthogonal if xᵀy = 0

**Conjugate transpose:** For complex matrices, the conjugate transpose A* combines transposition with complex conjugation: (A*)ᵢⱼ = āⱼᵢ where ā denotes complex conjugate.

**Key points:**

- Matrix operations follow specific algebraic rules that extend scalar arithmetic
- Dimensional compatibility is crucial for matrix multiplication
- Special matrices have unique properties that simplify computations
- Transpose operation reverses the order in products and preserves most algebraic structures
- Understanding these basics enables analysis of linear transformations and systems

**Example of transpose properties:** If A = ⎡1 2⎤ and B = ⎡5 6⎤, then: ⎣3 4⎦ ⎣7 8⎦

Aᵀ = ⎡1 3⎤, (AB)ᵀ = ⎡19 43⎤ = BᵀAᵀ = ⎡5 7⎤⎡1 3⎤ ⎣2 4⎦ ⎣22 50⎦ ⎣6 8⎦⎣2 4⎦

---



---



---

# Systems of Linear Equations



---



---

# Linear Transformations



---



---



---

# Determinants



---



---

# Eigenvalues and Eigenvectors



---



---



---

# Inner Product Spaces



---



---

# Advanced Topics and Applications



---



---

